

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.ico">
  <link rel="icon" href="/img/favicon.ico">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#1aa3ff">
  <meta name="description" content="个人博客&amp;笔记">
  <meta name="author" content="江左时雨">
  <meta name="keywords" content="机器学习,Machine Learning,自然语言处理,NLP,深度学习,Deep Learning,计算机,Coding,生活随笔">
  <meta name="description" content="相似性搜索常见于以图搜图，听歌识曲...这类抽象查找问题中。你没有明确的Key，不能使用SQL之类的方法查找数据库。但是可以通过抽象的 embedding 向量来进行检索。 一般样本向量表示可以通过 Skip-gram with negative sampling 方法、DSSM 这类方法、BERT 这类方法等等。得到向量表达之后，一般还需要高效的召回索引方法，因为暴力匹配在较大数据量场景下">
<meta property="og:type" content="article">
<meta property="og:title" content="Embedding &amp; Searching">
<meta property="og:url" content="https://racleray.github.io/posts/9e991c30.html">
<meta property="og:site_name" content="Racle&#96;s Story">
<meta property="og:description" content="相似性搜索常见于以图搜图，听歌识曲...这类抽象查找问题中。你没有明确的Key，不能使用SQL之类的方法查找数据库。但是可以通过抽象的 embedding 向量来进行检索。 一般样本向量表示可以通过 Skip-gram with negative sampling 方法、DSSM 这类方法、BERT 这类方法等等。得到向量表达之后，一般还需要高效的召回索引方法，因为暴力匹配在较大数据量场景下">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/racleray/image_storage/images/Embedding%20&%20Search_pic/ScaNN%20tom%20export.gif">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/racleray/image_storage/images/Embedding%20&%20Search_pic/image-20210905144457729.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/racleray/image_storage/images/Embedding%20&%20Search_pic/image-20210905141342414.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/racleray/image_storage/images/Embedding%20&%20Search_pic/image-20210905142905349.png">
<meta property="article:published_time" content="2021-09-05T06:59:46.000Z">
<meta property="article:modified_time" content="2023-08-07T11:54:31.028Z">
<meta property="article:author" content="江左时雨">
<meta property="article:tag" content="searching">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/racleray/image_storage/images/Embedding%20&%20Search_pic/ScaNN%20tom%20export.gif">
  
     <meta name="baidu-site-verification" content="code-tH44R5Z2fc" /> <meta name="msvalidate.01" content="4E3B92EC6A38584E946DBE40929107D9" /> <meta name="google-site-verification" content="c-8NXvOa-KKHK4OB0TyzjFeRUuIPFXEXM9h5hYePPpw" /> 
  
  <title>Embedding &amp; Searching - Racle`s Story</title>

  <link  rel="stylesheet" href="https://cdn.staticfile.org/twitter-bootstrap/4.5.3/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.staticfile.org/github-markdown-css/4.0.0/github-markdown.min.css" />
  <link  rel="stylesheet" href="/lib/hint/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.staticfile.org/highlight.js/10.4.0/styles/night-owl.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.css" />
  



<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_kmeydafke9r.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC&display=swap.css">
<link rel="stylesheet" href="/css/custom.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"racleray.github.io","root":"/","version":"1.8.11","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname"}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 4.2.1"><link rel="alternate" href="/atom.xml" title="Racle`s Story" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>


<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="link link--kukuri" href="/", data-letters="Racle`s Story">
      Racle`s Story
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                首页
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                归档
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                分类
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                标签
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                关于
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/talking/">
                <i class="iconfont icon-comment"></i>
                说说
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/links/">
                <i class="iconfont icon-link-fill"></i>
                友链
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('/img/46.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="Embedding &amp; Searching">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2021-09-05 14:59" pubdate>
        2021年9月5日 下午
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      5.6k 字
    </span>
  

  
    <span class="post-meta mr-2">
      <i class="iconfont icon-clock-fill"></i>
      
      
      17 分钟
    </span>
  

  
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Embedding &amp; Searching</h1>
            
            <div class="markdown-body">
              <p>相似性搜索常见于以图搜图，听歌识曲...这类抽象查找问题中。你没有明确的Key，不能使用SQL之类的方法查找数据库。但是可以通过抽象的
embedding 向量来进行检索。 一般样本向量表示可以通过 Skip-gram with
negative sampling 方法、DSSM 这类方法、BERT
这类方法等等。得到向量表达之后，一般还需要高效的召回索引方法，因为暴力匹配在较大数据量场景下，速度通常差强人意。</p>
<p>这类通过抽象 embedding 的具有语义检索能力的方法，如下图所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/racleray/image_storage/images/Embedding & Search_pic/ScaNN tom export.gif" srcset="/img/loading.gif" lazyload alt style="zoom:70%;"></p>
<p>一组相关算法的 benchmarks 对比图如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/racleray/image_storage/images/Embedding & Search_pic/image-20210905144457729.png" srcset="/img/loading.gif" lazyload alt style="zoom:70%;"></p>
<h2><span id="annoy">Annoy</span></h2>
<p>Spotify音乐公司开源的<a href="https://github.com/spotify/annoy" target="_blank" rel="noopener">工具库</a>。</p>
<p>这是一种基于树的方法。</p>
<p>算法简单描述：</p>
<blockquote>
<p>定义： N -- 树模型的总个数；</p>
<p>​ K -- 叶节点最大样本数；</p>
<p>​ M -- 目标Top M样本数；</p>
<p>​ D -- 联通两个子空间所要求的最大距离；</p>
<p>过程：</p>
<p>==建树==</p>
<figure class="highlight markdown"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs markdown">for i in (1, ... , <span class="hljs-code">`N`</span>):<br><span class="hljs-bullet">  	1. </span>随机选取两个样本点，计算两者中点处超平面，划分两侧样本为两个子树；<br><span class="hljs-bullet">  	2. </span>继续划分样本点，直到叶子节点中样本数不大于 <span class="hljs-code">`K`</span>；<br><span class="hljs-bullet">  	3. </span>保存二叉树（即保存每个划分点的值）；<br></code></pre></div></td></tr></table></figure>
<p>==搜索==</p>
<figure class="highlight markdown"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs markdown">for j in (1, ... , <span class="hljs-code">`N`</span>):<br><span class="hljs-bullet"> 	1. </span>从根节点开始，根据二分节点，向下遍历；<br><span class="hljs-bullet"> 	2. </span>if 两个子空间相近（随机选的两个点距离小于 <span class="hljs-code">`D`</span>），同时向两个子节点，向下遍历；<br><span class="hljs-bullet"> 	3. </span>保存找到的叶子节点空间（节点集合）；<br><br>聚合<span class="hljs-code">`N`</span>个叶子节点集合，以找到距离目标节点最近的 Top <span class="hljs-code">`M `</span>个样本点；<br>返回结果<br></code></pre></div></td></tr></table></figure>
</blockquote>
<p>Annoy利用二叉树的结构对空间进行随机划分，建索引阶段效率有所提升。二叉树结构在检索时效率也很高。</p>
<p>特点：</p>
<ol type="1">
<li>召回率较优，和暴力搜索法相比较基本一致</li>
<li>查询速度很快</li>
<li>千万量级的item，时间为若干小时，尚可以忍受</li>
<li>千万量级的item，以100棵树为例，索引文件大约几个G，有点大了</li>
<li>多核利用支持的不是很好</li>
</ol>
<h2><span id="scann">ScaNN</span></h2>
<p>ScaNN是google提出的高效向量检索算法，文中说这个方法比目前已有的其他方法有更高的精度，检索速度也要快两倍。工具库开源地址
<a href="https://github.com/google-research/google-research/tree/master/scann" target="_blank" rel="noopener">GitHub</a>。</p>
<p>这种算法主要是优化 Inner product 距离度量下的搜索。这类问题被叫做 <a href="https://papers.nips.cc/paper/5329-asymmetric-lsh-alsh-for-sublinear-time-maximum-inner-product-search-mips.pdf" target="_blank" rel="noopener">maximum
inner-product search</a>
(MIPS)。在大数据量场景下，MIPS的计算复杂度是比较高的，穷举搜索几乎不可能在期望的时间范围内完成。</p>
<p>论文中，阐述了目前使用的向量压缩（比如聚类或者降维）所使用方法，会使得压缩后的向量间平均距离变小。这可能导致了向量差异性的损失，比如与query向量的内积的相对大小关系会出现错误。比如这样：</p>
<p><img src="https://cdn.jsdelivr.net/gh/racleray/image_storage/images/Embedding & Search_pic/image-20210905141342414.png" srcset="/img/loading.gif" lazyload alt style="zoom:80%;"></p>
<p>上图中，x 被分别压缩到 c 点，其余 q 的内积大小关系发生颠倒。本来
<span class="math inline">\(&lt;q,x_2&gt; greater\ than
&lt;q,x_1&gt;\)</span>， 压缩后 <span class="math inline">\(&lt;q,x_1&gt; greater\ than
&lt;q,x_2&gt;\)</span>。</p>
<p>论文中指出，这种方法压缩，只考虑了距离的长度大小，而没有考虑距离的角度方向。</p>
<p>一个简单的例子，计算两个向量的内积，当一个向量在平行于投影方向（两个向量方向都可作为投影方向）变化
k ，内积的变化为 <span class="math inline">\(d_1\)</span>。而如果实在一个向量的垂直方向，变化 k
，内积的变化为 <span class="math inline">\(d_2\)</span>。那么，<span class="math inline">\(d_2 ≥ d_1\)</span>一定成立。画个图就能验证。</p>
<p>所以，现在在压缩时，对于 x 与 c 在平行于 x
向量方向上的变化，给予一个大的惩罚项；而对于 x 与 c 在垂直于 x
向量方向上的变化，给予一个较小的惩罚项。以此来进行压缩。取了名字叫，<strong>Anisotropic
Vector Quantization</strong>。</p>
<p>效果如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/racleray/image_storage/images/Embedding & Search_pic/image-20210905142905349.png" srcset="/img/loading.gif" lazyload alt style="zoom:80%;"></p>
<p>内积相对大小关系，没有变化。按照论文中的说法，这样做最大化了压缩后各个点之间的平均距离，有利于差异化相似度的值。</p>
<p>算法大致流程：</p>
<blockquote>
<ol type="1">
<li>根据样本数量大小，选择是否将数据进行 Partitioning。若
Partitioning，在检索时，会先选出 Top m
个Partitioning，再进行细化检索。</li>
<li>Scoring：使用快速的粗粒度的距离度量，计算query相对所有样本（或Partition的样本）的距离。选出
Top k 个。</li>
<li>Rescoring: 对 Top k 个Scoring结果，进行更精确的距离度量，重拍后输出
Top k 的结果。</li>
</ol>
</blockquote>
<p>使用 <a href="https://github.com/google-research/google-research/tree/master/scann/docs" target="_blank" rel="noopener">doc</a>
比较简短，可以参考。</p>
<h2><span id="hnsw">HNSW</span></h2>
<p>HNSW是一种图算法。其根据搜索场景的特点，设计出了这种算法。</p>
<p>首先，简单想想图的搜索，麻烦的问题可能有哪些。可能有孤立的节点，或者可能相邻节点太多。在近邻搜索场景下，可能有几个距离目标很近的节点，但是没有相互连通，那么就需要遍历更多的路径，从而遍历完全这几个节点。</p>
<p>另外，节点众多时，当两个节点距离相对较远时，遍历数量会指数级增加。</p>
<p>HNSW的解决方法如下：</p>
<blockquote>
<p>定义：</p>
<p>​ N -- 总样本数；</p>
<p>​ K -- 每个节点最多有K个相连的近邻节点；</p>
<p>​ P1 -- 以 P1 的概率将节点设置为二级索引；</p>
<p>​ P2 -- 以 P2 的概率将节点设置为一级索引（P1 &gt;
P2，两者指数递减）；</p>
<p>​ M -- 目标Top M样本数；</p>
<p>过程：</p>
<p>==建图==</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-string">for</span> <span class="hljs-string">i</span> <span class="hljs-string">in</span> <span class="hljs-string">(1,</span> <span class="hljs-string">...</span> <span class="hljs-string">,</span> <span class="hljs-string">N):</span><br><br>	<span class="hljs-string">if</span> <span class="hljs-string">(随机概率值</span> <span class="hljs-string">p)</span> <span class="hljs-string">&gt;</span> <span class="hljs-attr">P1:</span> <br>		<span class="hljs-string">二级索引图插入节点：</span><br>			<span class="hljs-number">1</span><span class="hljs-string">.</span> <span class="hljs-string">将被插入节点连接指向最近邻的</span> <span class="hljs-string">K</span> <span class="hljs-string">个节点（小于等于</span> <span class="hljs-string">K）；</span><br>			<span class="hljs-number">2</span><span class="hljs-string">.</span> <span class="hljs-string">更新被连接的</span> <span class="hljs-string">K</span> <span class="hljs-string">个节点（小于等于</span> <span class="hljs-string">K）的最近邻的</span> <span class="hljs-string">K</span> <span class="hljs-string">个节点；</span><br>			<span class="hljs-number">3</span><span class="hljs-string">.</span> <span class="hljs-string">保证每个节点都有连接，且最大连接不超过</span> <span class="hljs-string">K；</span><br>			<br>	<span class="hljs-string">if</span> <span class="hljs-string">(随机概率值</span> <span class="hljs-string">p)</span> <span class="hljs-string">&gt;</span> <span class="hljs-attr">P2:</span><br>		<span class="hljs-string">一级索引图插入节点：</span><br>			<span class="hljs-number">1</span><span class="hljs-string">.</span> <span class="hljs-string">将被插入节点连接指向最近邻的</span> <span class="hljs-string">K</span> <span class="hljs-string">个节点（小于等于</span> <span class="hljs-string">K）；</span><br>			<span class="hljs-number">2</span><span class="hljs-string">.</span> <span class="hljs-string">更新被连接的</span> <span class="hljs-string">K</span> <span class="hljs-string">个节点（小于等于</span> <span class="hljs-string">K）的最近邻的</span> <span class="hljs-string">K</span> <span class="hljs-string">个节点；</span><br>			<span class="hljs-number">3</span><span class="hljs-string">.</span> <span class="hljs-string">保证每个节点都有连接，且最大连接不超过</span> <span class="hljs-string">K；</span><br>			<br>	<span class="hljs-string">全量样本索引图插入节点：</span><br>		<span class="hljs-number">1</span><span class="hljs-string">.</span> <span class="hljs-string">将被插入节点连接指向最近邻的</span> <span class="hljs-string">K</span> <span class="hljs-string">个节点（小于等于</span> <span class="hljs-string">K）；</span><br>		<span class="hljs-number">2</span><span class="hljs-string">.</span> <span class="hljs-string">更新被连接的</span> <span class="hljs-string">K</span> <span class="hljs-string">个节点（小于等于</span> <span class="hljs-string">K）的最近邻的</span> <span class="hljs-string">K</span> <span class="hljs-string">个节点；</span><br>		<span class="hljs-number">3</span><span class="hljs-string">.</span> <span class="hljs-string">保证每个节点都有连接，且最大连接不超过</span> <span class="hljs-string">K；</span><br>		<br><span class="hljs-string">返回建立的多级索引图</span><br></code></pre></div></td></tr></table></figure>
<p>==搜索==</p>
<figure class="highlight angelscript"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs angelscript"><span class="hljs-number">1.</span> 在二级索引图中找到最近邻节点 A<br><span class="hljs-number">2.</span> 在一级索引图中从 A 开始找到最近邻节点 B<br><span class="hljs-number">3.</span> 在全量样本索引图中从 B 开始找到 Top M 的目标节点<br></code></pre></div></td></tr></table></figure>
</blockquote>
<p>HNSW设计的插入机制，保证了图具有良好的连通性和局部搜索便捷性。类似跳表结构的多级索引机制，提高了搜索效率，降低了整体搜索复杂度。</p>
<p>算法细节看<a href="https://arxiv.org/abs/1603.09320" target="_blank" rel="noopener">文章</a>，这里只是草草写写思想。开源工具库C++版：<a href="https://github.com/nmslib/hnswlib" target="_blank" rel="noopener"><em>hnswlib</em></a></p>
<p>特点：</p>
<ol type="1">
<li>召回率优秀，和暴力搜索基本一致</li>
<li>千万量级的item，构图可在分钟级别完成</li>
<li>多核利用优秀</li>
<li>查询速度很快</li>
</ol>
<h2><span id="lsh">LSH</span></h2>
<p>在 min hash
之上，更进一步优化了在大量文本相似性聚类的算法。首先需要计算每一个文本的多个不同
min hash 表示，构成 min hash 值向量。然后对 min hash
向量分块进行简单的元素对比，检查是否相似。此处LSH，只关注基于 Jaccard
similarity 的文本处理，其他LSH实现不涉及。</p>
<p>这就是一种基于词统计的 hash
分桶算法。其中没有计算欧式距离这种计算量较大的操作，仅仅是元素对比。</p>
<p>算法如下：</p>
<blockquote>
<p>定义: K -- 取文本中连续 K 个词为bag of words的计数对象（这里被称为
K-Shingling）；</p>
<p>​ N -- min hash向量的维度，即进行随机 min hash 的次数；</p>
<p>​ M -- 文本总数；</p>
<p>​ b -- LSH中hash分组的组数；</p>
<p>​ r -- b组 min hash 子向量的维度；</p>
<p>过程：</p>
<p>==min hash==</p>
<figure class="highlight excel"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs excel"><span class="hljs-number">1</span>. 对整个文本集构建以 K 个连续词为一个对象的bag of words统计矩阵，文本中存在的对象标记为<span class="hljs-number">1</span>，不存在标记为<span class="hljs-number">0</span>；<br><span class="hljs-number">2</span>. for i in (<span class="hljs-number">1</span>, ... , <span class="hljs-built_in">N</span>)<span class="hljs-symbol">:</span><br>	   A. 随机指定统计矩阵中一个 <span class="hljs-built_in">index</span> 为起始位置（保存<span class="hljs-built_in">index</span>）；<br>	   B. 从 <span class="hljs-built_in">index</span> 开始，在每一个文档列中，向下查找第一非 <span class="hljs-number">0</span> 位置；<br>	   C. 取第一非 <span class="hljs-number">0</span> 位置与 <span class="hljs-built_in">index</span> 位置的偏移量为 <span class="hljs-built_in">min</span> hash 向量的第 i 行，有 M 个 <span class="hljs-built_in">min</span> hash 向量；<br><span class="hljs-number">3</span>. 得到 (<span class="hljs-built_in">N</span>, M) 的 <span class="hljs-built_in">min</span> hash 矩阵<br></code></pre></div></td></tr></table></figure>
<p>==局部敏感hash(LSH)==</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs mipsasm"><span class="hljs-number">1</span>. 将 min hash 矩阵均分为 <span class="hljs-keyword">b </span>组；<br><span class="hljs-number">2</span>. 分桶聚类：<br>   for <span class="hljs-keyword">j </span>in (<span class="hljs-number">1</span>, ..., <span class="hljs-keyword">b):</span><br><span class="hljs-keyword"> </span>  	   A. 两两对比第 <span class="hljs-keyword">j </span>组 min hash 子向量；<br>   	   <span class="hljs-keyword">B. </span>子向量完全相同时，将该两个向量对应文本，放到一个相似桶中（类似聚类）；<br><span class="hljs-number">3</span>. 保存分桶结果，每个桶中就是相似的文本<br></code></pre></div></td></tr></table></figure>
</blockquote>
<p>关于 min hash，它就是一种 Jaccrad similarity 的另一种表示。</p>
<p><span class="math inline">\(P(hash(S_i)=hash(S_j))\)</span> 就等价于
<span class="math inline">\(Jaccard(S_i,
S_j)\)</span>。证明也挺简单的：</p>
<blockquote>
<ol type="1">
<li><p>只看两个 min hash 向量中不全为 0 的行，记为 <span class="math inline">\(sub\_hash\)</span>。</p></li>
<li><p>此时 <span class="math inline">\(hash(S_i)=hash(S_j)\)</span>
概率就等价于 <span class="math inline">\(sub\_hash\)</span> 的第一行都为
1 的概率。</p></li>
<li><p><span class="math inline">\(sub\_hash\)</span> 的第一行都为 1
的概率，就等于<span class="math inline">\(sub\_hash\)</span>
中｛两列都为 1 的行数 / 任意一列为 1 的行数｝。</p></li>
<li><p><span class="math inline">\(sub\_hash\)</span> 中｛两列都为 1
的行数 / 任意一列为 1 的行数｝，就是 <span class="math inline">\(Jaccard(S_i, S_j)\)</span>。</p></li>
</ol>
</blockquote>
<p>另外，调整参数 <code>b</code> 和 <code>r</code>
可以间接调整分桶的相似性阈值。假设两个 min hash 向量相似概率为 <span class="math inline">\(p\)</span>。分到同一个桶中的概率可表示为（只要有一组
sub hash 相同就分到一个桶）： <span class="math display">\[
1 - (1 - p^r)^b
\]</span> 开源工具库：<a href="https://github.com/mattilyra/LSH" target="_blank" rel="noopener">mattilyra/LSH</a></p>
<p>特点：</p>
<ol type="1">
<li>查找速度快</li>
<li>方便去重处理</li>
<li>处理大量数据速度较快</li>
</ol>
<h2><span id="product-quantizer">Product Quantizer</span></h2>
<p>Product
Quantizer简称为PQ，是一种建立索引的方法。优化了K-Means聚类的计算量。</p>
<blockquote>
<p>定义：</p>
<p>​ N -- 样本总量； ​ D -- embedding维度； ​ K -- 子空间数量； ​ C --
K-means聚类类别数；</p>
<p>过程：</p>
<p>==建索引==</p>
<figure class="highlight angelscript"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs angelscript"><span class="hljs-number">1.</span> 将 N 个 D 维embedding，切分为 K 组 (N, D/K) 的embedding；<br><span class="hljs-number">2.</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> (<span class="hljs-number">1</span>, ..., K):<br>	   A. 对第 i 个 (N, D/K)的embedding，计算 C 个聚类中心点<br><span class="hljs-number">3.</span> <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> (<span class="hljs-number">1</span>, ..., N):<br>	   <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> (<span class="hljs-number">1</span>, ..., K):<br>	   	   A. 将第 j 个样本标记到第 k 组聚类的所属中心点编号；<br>	   	   B. 循环得到第 j 个样本的 K 维中心点编号向量；<br>   得到 (N, K) 的样本编码矩阵<br></code></pre></div></td></tr></table></figure>
<p>==搜索==</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs mipsasm"><span class="hljs-number">1</span>. 将输入 embedding 划分为 K 组 <span class="hljs-keyword">sub </span>embedding；<br><span class="hljs-number">2</span>. for i in (<span class="hljs-number">1</span>, ..., K):<br>	   for <span class="hljs-keyword">j </span>in (<span class="hljs-number">1</span>, ..., C):<br>	   	   A. 计算 <span class="hljs-keyword">sub </span>embedding 与聚类中心 <span class="hljs-keyword">j </span>的距离；<br>	   	   <span class="hljs-keyword">B. </span>循环得到 C 维的距离向量；<br>   循环得到 (K, C) 的距离矩阵<br><span class="hljs-number">3</span>. 计算输入与所有样本的距离： <br>   for i in (<span class="hljs-number">1</span>, ..., N):<br>   	   取 (N, K) 的样本编码矩阵中第 i 行 (<span class="hljs-number">1</span>, K), 记为 Q;<br>   	   定义输入与样本 i 的距离为 <span class="hljs-keyword">di；</span><br><span class="hljs-keyword">	</span>   for k in (<span class="hljs-number">1</span>, ..., K):<br>           A. 取 Q[k] 的值，记为 q；<br>           <span class="hljs-keyword">B. </span>取 (K, C) 的距离矩阵中第 k 行，记为 <span class="hljs-keyword">dist；</span><br><span class="hljs-keyword"> </span>          C. <span class="hljs-keyword">di </span>+= <span class="hljs-keyword">dist[q];</span><br><span class="hljs-keyword"> </span>     	记录输入与样本 i 的距离为 <span class="hljs-keyword">di </span>的值；<br><span class="hljs-number">4</span>. 从 N 个 <span class="hljs-keyword">di </span>距离中找到需要的样本   <br></code></pre></div></td></tr></table></figure>
</blockquote>
<p>这种方法，将暴力搜索，转化为 K * N 次距离表查找过程。</p>
<h2><span id="inverted-file-system">Inverted File System</span></h2>
<p>这个方法（简称 IVF）相当于使用倒排表来优化索引。比如 IVF + PQ，来优化
K * N 次的查询操作。IVF， PQ 在 <a href="https://github.com/search?q=faiss" target="_blank" rel="noopener">Faiss</a> 库中都有实现。</p>
<p>简述一下方法：</p>
<p>在 PQ 建立索引的过程中增加一步：</p>
<blockquote>
<p>将 (N, K) 的样本，分别保存到 <code>C_ivf</code>个聚类之下，即，保存到
<code>C_ivf</code>个代表一个类的数组中。得到 <code>C_ivf</code>
个类别字典，key 为聚类中心点（id），value 为该聚类中所有样本点的 PQ
编码数组。（按照 <code>C_ivf</code>个聚类类别进行倒排样本）</p>
</blockquote>
<p>在 PQ 查询中增加一步：</p>
<blockquote>
<p>在第三步第二层 for 循环中，先求得与输入样本最近的 <code>C_ivf</code>
聚类中心，然后在该聚类的 PQ 编码数组中计算每个样本与输入的距离。</p>
</blockquote>
<p>利用倒排，减少了搜索范围。从全部样本搜索，变成先找聚类，再在类中搜索。</p>
<p>这里建立倒排的 K-means 聚类与 PQ 中使用的不同，使用一个更粗粒度的
K-means 聚类，但是没有对 N 个 embedding 进行划分。</p>
<p>显然，IVF虽然建立索引的过程更复杂一些，但是搜索的过程会更快速。</p>
<h2><span id="structure-basedapproximations">Structure-based
Approximations</span></h2>
<p>这类近似搜索，将搜索过程抽象为一个神经网络表示的函数，输入对象，输出搜索结果。当然以下方法需要监督数据支持，或者可以构建无监督的共现关系。</p>
<ul>
<li><p>Negative sampling softmax:</p>
<p>简化版的NCE近似计算，将多分类，转为多个正负例二分类。复杂的负例分布抽样时，可以采用importance
sampling进行简化。</p></li>
<li><p>Class-based softmax:</p>
<p>使用两个softmax，第一个预测 class，第二个基于输入和 class
预测搜索目标。</p></li>
<li><p>Hierarchical softmax：</p>
<p>构建 Huffman
tree，然后每个节点只进行一个二值分类预测，直到叶子节点。预测直接变成了
tree height 次二分类计算。</p></li>
<li><p>Binary code prediction：</p>
<p>将所有对象的index，转化为二进制表示，使用多个sigmoid，预测每个位置是
0 还是 1，得到目标index的二进制表示。</p></li>
<li><p>Embedding Prediction：</p>
<p>预测 embedding 表示。这个方法是对 Embedding inference
模型的近似，思路基本上和蒸馏差不多。对于下游搜索匹配，没有影响。这个方法用到一种损失，Von-Mises
Fisher distribution
loss，约束了输出embedding靠近一个单位球面（超球面）。</p></li>
</ul>
<h2><span id="end">End</span></h2>
<p>相似性搜索，和计算 margin loss 的分类问题目标比较接近。 <span class="math display">\[
Loss_{margin}(x,y,\hat{y})=max(0, 1+s(\hat{y}|x)-s(y|x))
\]</span> 另外，基于树的搜索，还有KD Tree算法，这里不涉及。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/Notes/">Notes</a>
                    
                      <a class="hover-with-bg" href="/categories/Notes/NLP/">NLP</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/searching/">searching</a>
                    
                  </div>
                
              </div>
              
                <p class="note note-warning">
                  
                    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.zh" target="_blank" rel="nofollow noopener noopener">CC BY-SA 4.0 协议</a> ，转载请注明出处！
                  
                </p>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/posts/fbbb19a4.html">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">离散数学</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/posts/372ecc5c.html">
                        <span class="hidden-mobile">Practical BERT</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
              <!-- Comments -->
              <article class="comments" id="comments" lazyload>
                
                  
                
                
  <script type="text/javascript">
    Fluid.utils.loadComments('#comments', function() {
      var light = 'github-light';
      var dark = 'github-dark';
      var schema = document.documentElement.getAttribute('data-user-color-scheme');
      if (schema === 'dark') {
        schema = dark;
      } else {
        schema = light;
      }
      window.UtterancesThemeLight = light;
      window.UtterancesThemeDark = dark;
      var s = document.createElement('script');
      s.setAttribute('src', 'https://utteranc.es/client.js');
      s.setAttribute('repo', 'RacleRay/comments');
      s.setAttribute('issue-term', 'title');
      
      s.setAttribute('label', 'utterances');
      
      s.setAttribute('theme', schema);
      s.setAttribute('crossorigin', 'anonymous');
      document.getElementById('comments').appendChild(s);
    })
  </script>
  <noscript>Please enable JavaScript to view the comments</noscript>


              </article>
            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;目录</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <a href="https://github.com/RacleRay" class="hint--bottom hint--rounded" aria-label="GitHub" target="_blank"> <i class="iconfont icon-github-fill" aria-hidden="true"></i> </a>
<a href="mailto:969232057@qq.com" class="hint--bottom hint--rounded" aria-label="Email" target="_blank"> <i class="iconfont icon-mail" aria-hidden="true"></i> </a>
<a href="tencent://AddContact/?fromId=50&amp;fromSubId=1&amp;subcmd=all&amp;uin=969232057" class="hint--bottom hint--rounded" aria-label="QQ" target="_blank"> <i class="iconfont icon-qq-fill" aria-hidden="true"></i> </a>
<a class="qr-trigger" target="_self"> <i class="iconfont icon-wechat-fill" aria-hidden="true"></i> <img class="qr-img" src="/img/wexin.jpg" srcset="/img/loading.gif" lazyload alt="qrcode"> </a>
<a href="/atom.xml" class="hint--bottom hint--rounded" aria-label="Email" target="_blank"> <i class="iconfont icon-rss" aria-hidden="true"></i> </a>
<div></div> <a> POWERED BY </a> <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
  </div>
  

  

  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.staticfile.org/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.staticfile.org/jquery/3.5.1/jquery.min.js" ></script>
<script  src="https://cdn.staticfile.org/twitter-bootstrap/4.5.3/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  <script  src="https://cdn.staticfile.org/tocbot/4.12.0/tocbot.min.js" ></script>



  <script  src="https://cdn.staticfile.org/fancybox/3.5.7/jquery.fancybox.min.js" ></script>



  <script  src="https://cdn.staticfile.org/anchor-js/4.3.0/anchor.min.js" ></script>



  <script defer src="https://cdn.staticfile.org/clipboard.js/2.0.6/clipboard.min.js" ></script>



  <script  src="/js/local-search.js" ></script>






  <script  src="https://cdn.staticfile.org/typed.js/2.0.11/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
      typing(title)
      
    })(window, document);
  </script>





  

  
    <!-- MathJax -->
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        loader: {
          load: ['ui/lazy']
        },
        options: {
          renderActions: {
            findScript: [10, doc => {
              document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
                const display = !!node.type.match(/; *mode=display/);
                const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
                const text = document.createTextNode('');
                node.parentNode.replaceChild(text, node);
                math.start = { node: text, delim: '', n: 0 };
                math.end = { node: text, delim: '', n: 0 };
                doc.math.push(math);
              });
            }, '', false],
            insertedScript: [200, () => {
              document.querySelectorAll('mjx-container').forEach(node => {
                let target = node.parentNode;
                if (target.nodeName.toLowerCase() === 'li') {
                  target.parentNode.classList.add('has-jax');
                }
              });
            }, '', false]
          }
        }
      };
    </script>

    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" ></script>

  











<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->


</body>
</html>
