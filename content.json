{"meta":{"title":"Racle`s Story","subtitle":"Recording & Thinking & Story telling","description":"个人博客&笔记","author":"江左时雨","url":"https://racleray.github.io","root":"/"},"pages":[{"title":"About","date":"2021-09-05T08:58:08.000Z","updated":"2023-08-07T11:54:31.051Z","comments":false,"path":"about/index.html","permalink":"https://racleray.github.io/about/index.html","excerpt":"","text":"Recording &amp; Thinking &amp; Story telling 饭要好好吃，水要慢慢喝，音乐要静静地听，读书要认真地读。 为什么放弃了呢？无论如何，希望心能保持平静。"},{"title":"Categories","date":"2020-04-15T12:18:06.000Z","updated":"2023-08-07T11:54:31.051Z","comments":false,"path":"categories/index.html","permalink":"https://racleray.github.io/categories/index.html","excerpt":"","text":""},{"title":"Contact","date":"2021-09-05T09:00:44.000Z","updated":"2023-08-07T11:54:31.051Z","comments":false,"path":"contact/index.html","permalink":"https://racleray.github.io/contact/index.html","excerpt":"","text":""},{"title":"404","date":"2021-09-05T09:04:04.000Z","updated":"2023-08-07T11:54:31.022Z","comments":false,"path":"404/index.html","permalink":"https://racleray.github.io/404/index.html","excerpt":"","text":""},{"title":"talking","date":"2021-11-01T05:08:08.000Z","updated":"2023-08-07T11:54:31.052Z","comments":false,"path":"talking/index.html","permalink":"https://racleray.github.io/talking/index.html","excerpt":"","text":""},{"title":"Tags","date":"2020-04-15T12:18:26.000Z","updated":"2023-08-07T11:54:31.052Z","comments":false,"path":"tags/index.html","permalink":"https://racleray.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"深入C++对象模型笔记","slug":"深入C++对象模型笔记","date":"2022-06-30T12:42:31.000Z","updated":"2023-08-07T12:02:23.621Z","comments":true,"path":"posts/ae73e7a0.html","link":"","permalink":"https://racleray.github.io/posts/ae73e7a0.html","excerpt":"《深入探索C++对象模型》之前的笔记，有些过时的条款做了点改动。","text":"一、关于对象 1.0 加上封装后的布局成本（Layout Costs for Adding Encapsulation） C++在布局以及存取时间上主要的额外负担是由virtual引起的: virtual function 机制用以支持一个有效率的“执行期绑定”（runtime binding）。 virtual base class 用以实现“多次出现在继承体系中的base class，有一个单一而被共享的实例（ios）。 1.1 C++对象模型 在C++中，有两种class data members：static和nonstatic，以及三种class member functions：static、nonstatic和virtual。 在C++对象模型中，Nonstatic data members在每一个class object之内，static data members则被存放在class object之外。 Static和nonstatic function members被放在class object之外。 Virtual functions： 每一个 class 产生出一堆指向virtual functions的指针，放在表格之中。这个表格被称为 virtual table（vtbl）。 每一个class object 被安插一个指针，指向相关的virtual table。通常这个指针被称为 vptr。vptr的设定（setting）和重置（resetting）都由每一个class的constructor、destructor和copy assignment运算符自动完成。每一个 class所关联的 type_info object（用以支持runtime type identification，RTTI）也经由virtual table被指出来，通常放在表格的第一个slot（地址空间）。 1.2 关键词所带来的差异 C struct在C++中的一个合理用途，是当你要传递“一个复杂的class object的全部或部分”到某个C函数去时，struct声明可以将数据封装起来，并保证拥有与C兼容的空间布局。 然而这项保证只在组合（composition）的情况下才存在。如果是“继承”而不是“组合”，编译器会决定是否应该有额外的data members被安插到base struct subobject之中。 1.3 对象的差异 C++程序设计模型直接支持三种programming paradigms（程序设计范式）： 程序模型（procedural model）。就像 C一样，C++当然也支持它。 抽象数据类型模型（abstract data type model，ADT）。此模型所谓的“抽象”是和一组表达式（public接口）一起提供的，实际仍然未定义。 面向对象模型（object-oriented model）。在此模型中有一些彼此相关的类型，通过一个抽象的 base class（用以提供共同接口）被封装起来。 只有通过pointer或reference间接处理对象，才能支持多态性质。 C++以下列方法支持多态： 经由一组隐式的转化操作。例如把一个 derived class 指针转化为一个指向其 public base type的指针。 经由 virtual function 机制。 经由 dynamic_cast 和 typeid 运算符。 多态的主要用途是经由一个共同的接口来影响类型的封装，这个接口通常被定义在一个抽象的base class中。 需要多少内存才能够表现一个class object？一般而言要有： 其 nonstatic data members的总和大小。 加上任何由于 alignment 的需求而填补（padding）上去的空间。 alignment就是将数值调整到某数的倍数。在32位计算机上，通常alignment为4 bytes（32位），以使bus的“运输量”达到最高效率。 加上为了支持 virtual 而由内部产生的任何额外负担（overhead）。 “指向不同类型之各指针”间的差异，既不在其指针表示法不同，也不在其内容（代表一个地址）不同，而是在其所寻址出来的object类型不同。 也就是说，“指针类型”会教导编译器如何解释某个特定地址中的内存内容及其大小。 转换（cast）其实是一种编译器指令。大部分情况下它并不改变一个指针所含的真正地址，它只影响“被指出之内存的大小和其内容”的解释方式。 总而言之，多态是一种威力强大的设计机制，允许你继承一个抽象的public接口之后，封装相关的类型。需要付出的代价就是额外的间接性——不论是在“内存的获得”或是在“类型的决断”上。C++通过class的pointers和references来支持多态，这种程序设计风格就称为“面向对象”。 C++也支持具体的ADT程序风格，如今被称为object-based（OB）。例如String class，一种非多态的数据类型。String class可以展示封装的非多态形式；它提供一个public 接口和一个private实现，包括数据和算法，但是不支持类型的扩充。如今的 Go，Rust 都是走OB的路线。 一个OB设计可能比一个对等的OO设计速度更快而且空间更紧凑。速度快是因为所有的函数调用操作都在编译时期解析完成，对象建构起来时不需要设置 virtual 机制；空间紧凑则是因为每一个 class object 不需要负担传统上为了支持virtual机制而需要的额外负荷。不过，OB设计比较没有弹性。 二、构造函数语意学 implicit：暗中的、隐式的（在程序源代码中没有出现） explicit：显式的（程序源代码中出现） trivial：没有用的 nontrivial：有用的 memberwise：对每一个member施以…… bitwise：对每一个bit施以…… semantics：语意 2.1 Default Constructor的构造操作 C++新手一般有两个常见的误解： 任何class如果没有定义default constructor，就会被合成出一个来。 编译器合成出来的default constructor会显式设定“class 内每一个 data member的默认值”。 有4种情况，会造成“编译器必须为未声明 constructor 的classes合成一个default constructor”。C++Standard 把那些合成物称为 implicit nontrivial default constructors。 “带有 Default Constructor”的 Member Class Object 如果一个class没有任何constructor，但它内含一个member object，而后者有default constructor，那么这个class的 implicit default constructor就是“nontrivial”，编译器需要为该class 合成出一个default constructor。不过这个合成操作只有在constructor真正需要被调用时才会发生。再一次请你注意，被合成的default constructor只满足编译器的需要，而不是程序的需要。 如果对象的成员没有定义默认构造函数，那么编译器合成的默认构造函数将不会为之提供初始化。例如类A包含两个数据成员对象，分别为：string str和char* Cstr，那么编译器生成的默认构造函数将只提供对string类型成员的初始化，而不会提供对 char* 类型的初始化。 “带有 Default Constructor”的 Base Class 如果一个没有任何constructors的class派生自一个“带有default constructor”的base class，那么这个derived class 的default constructor 会被视为nontrivial，并因此需要被合成出来。它将调用上一层 base classes 的 default constructor（根据它们的声明顺序）。对一个后继派生的class而言，这个合成的constructor和一个“被显式提供的default constructor”没有什么差异。 “带有一个 Virtual Function”的 Class 另有两种情况，也需要合成出default constructor： class声明（或继承）一个 virtual function。 class派生自一个继承串链，其中有一个或更多的 virtual base classes。 “带有一个 Virtual Base Class”的 Class Virtual base class 的实现，在不同的编译器之间有极大的差异。然而，每一种实现法的共同点在于必须使virtual base class在其每一个derived class object中的空间位置，能够于执行期准备妥当。 被合成出来的constructor只能满足编译器（而非程序）的需要。 至于没有存在那4种情况而又没有声明任何constructor的classes，实际上default constructor并不会被合成出来。 在合成的 default constructor 中，只有 base class subobjects 和 member class objects 会被初始化。所有其他的nonstatic data member（如整数、整数指针、整数数组等等）都不会被初始化。 这些初始化操作对程序而言或许有需要，但对编译器则非必要。如果程序需要一个“把某指针设为0”的default constructor，那么提供它的人应该是程序员。 2.2 Copy Constructor的构造操作 Default Memberwise Initialization 当class object 以“相同 class 的另一个 object”作为初值，其内部是以所谓的default memberwise initialization手法完成的，也就是把每一个内建的或派生的data member（例如一个指针或一个数组）的值，从某个object拷贝一份到另一个object身上。 不过它并不会拷贝其中的 member class object，而是以递归的方式施行 memberwise initialization。 C++Standard上说，如果class没有声明一个copy constructor，就会有隐式的声明（implicitly declared）或隐式的定义（implicitly defined）出现。和以前一样，C++Standard 把copy constructor区分为trivial和nontrivial两种。只有nontrivial的实例才会被合成于程序之中。 决定一个copy constructor是否为trivial的标准在于class 是否展现出所谓的“bitwise copy semantics”。 Bitwise Copy Semantics（位逐次拷贝） 在这被合成出来的default copy constructor中，如整数、指针（浅拷贝）、数组等等的non class members也都会被复制。 什么时候一个class不展现出“bitwise copy semantics”呢？有4种情况： 当class内含一个member object而后者的class声明有一个copy constructor时（不论是被 class设计者显式地声明，就像前面的 String那样；或是被编译器合成，像 class Word那样）。 当class继承自一个base class而后者存在一个copy constructor时（不论是被显式声明或是被合成而得）。 当class声明了一个或多个virtual functions时。 当class派生自一个继承串链，其中有一个或多个virtual base classes时。 重新设定Virtual Table的指针 回忆编译期间的两个程序扩张操作（只要有一个class声明了一个或多个virtual functions就会如此）： 增加一个virtual function table（vtbl），内含每一个有作用的virtual function的地址。 一个指向virtual function table的指针（vptr），安插在每一个class object内。 编译器合成出来的 copy constructor 会显式设定 lhs object的 vptr 指向 rhs 的 virtual table，而不是直接从rhs的class object 中将vptr值拷贝过来。 处理 Virtual Base Class Subobject 1234567class Base&#123; int x;&#125;;class Derived : Base &#123; int y;&#125;; 在这个继承体系里，每个 Derived 类型的 object 就包含了一个 Base 类型的 subobject 。 一个subobject可以是成员子对象（member subobject），基类子对象（base class subobject），或者成员数组的元素。 Virtual base class的存在需要特别处理。一个class object 如果以另一个object作为初值，而后者有一个 virtual base class subobject，那么也会使“bitwise copy semantics”失效。 每一个编译器对于虚继承，都必须让“derived class object中的virtual base class subobject位置”在执行期就准备妥当。维护“位置的完整性”是编译器的责任。 “Bitwise copy semantics”可能会破坏这个位置。 小结 Bitwise Copy Semantics部分展示了4种情况，在那些情况下class不再保持“bitwise copy semantics”，而且 default copy constructor 会被视为 nontrivial。 当class内含一个member object而后者的class声明有一个copy constructor时（不论是被 class设计者显式地声明，就像前面的 String那样；或是被编译器合成，像 class Word那样）。 当class继承自一个base class而后者存在一个copy constructor时（不论是被显式声明或是被合成而得）。 当class声明了一个或多个virtual functions时。 当class派生自一个继承串链，其中有一个或多个virtual base classes时。 在这4种情况下，如果缺乏一个已声明的 copy constructor，编译器为了正确处理“以一个class object 作为另一个class object 的初值”，必须合成出一个default copy constructor。 2.3 程序转化语意学 转化 显式的初始化操作（Explicit Initialization） 参数的初始化（Argument Initialization） 返回值的初始化（Return Value Initialization） 优化方法： 在使用者层面做优化（Optimization at the User Level） 在编译器层面做优化（Optimization at the Compiler Level）。Named Return Value（NRV）优化 copy constructor的应用，迫使编译器多多少少对你的程序代码做部分转化。尤其是当一个函数以传值（by value）的方式传回一个class object，而该class有一个copy constructor时。 此外，编译器也将copy constructor的调用操作优化，以一个额外的第一参数（数值被直接存放于其中）取代 NRV。程序员如果了解那些转换，以及copy constructor 优化后的可能状态，就比较能够控制其程序的执行效率。 命名返回值优化 （Named Return Value Optimization） 对于一个如foo()这样的函数，它的每一个返回分支都返回相同的对象，编译器有可能对其做Named return Value优化（下文都简称NRV优化），方法是以引用的方式传入一个参数result取代返回对象。 1234567X foo() &#123; X xx; if(...) return xx; else return xx; &#125; 优化后的foo()以result取代xx： 123456789void foo(X &amp;result) &#123; result.X::X(); if(...) &#123; return; &#125; else &#123; return; &#125;&#125; 对比优化前与优化后的代码可以看出，对于一句类似于X a = foo()这样的代码，NRV优化后的代码相较于原代码节省了一个临时对象的空间（省略了xx），同时减少了两次函数调用（减少xx对象的默认构造函数和析构函数，以及一次拷贝构造函数的调用），增加了一次 X a 的默认构造函数的调用。 2.4 成员们的初始化队伍（Member Initialization List） 当你写下一个constructor时，就有机会设定class members的初值。经由member initialization list，或在constructor函数本体之内。 在下列情况下，为了让你的程序能够被顺利编译，你必须使用member initialization list： 当初始化一个reference member时； 当初始化一个const member时； 当调用一个base class的constructor，而它拥有一组参数，没有默认构造函数时； 当调用一个member class的constructor，而它拥有一组参数，没有默认构造函数时。 前两者因为要求定义时初始化，所以必须明确的在初始化队列中给它们提供初值。后两者因为不提供默认构造函数，所有必须显示的调用它们的带参构造函数来定义即初始化它们。 编译器会一一操作initialization list，以适当顺序在constructor之内安插初始化操作，并且在任何explicit user code之前。list中的项目顺序是由class中的members声明顺序决定的，不是由initialization list中的排列顺序决定的。 简略地说，编译器会对initialization list 一一处理并可能重新排序，以反映出members的声明顺序。它会安插一些代码到constructor体内，并置于任何explicit user code之前。initialzation list 的执行先于用户自定义的函数体。 三、Data语意学（The Semantics of Data） class的data members以及class hierarchy是中心议题。一个class的data members，一般而言，可以表现这个class在程序执行时的某种状态。Nonstatic data members放置的是“个别的class object”感兴趣的数据，static data members则放置的是“整个class”感兴趣的数据。 C++对象模型尽量以空间优化和存取速度优化的考虑来表现nonstatic data members，并且保持和C语言struct数据配置的兼容性。它把数据直接存放在每一个class object之中。对于继承而来的nonstatic data members （不管是virtual还是nonvirtual base class）也是如此。不过并没有强制定义其间的排列顺序。 至于static data members，则被放置在程序的一个global data segment 中，不会影响个别的class object的大小。在程序之中，不管该class被产生出多少个objects（经由直接产生或间接派生），static data members永远只存在一份实例（译注：甚至即使该class没有任何object实例，其static data members也已存在）。 3.1 Data Member的绑定 在一个inline member function体之内的一个data member绑定操作，会在整个class声明完成之后才发生。然而，这对于member function的argument list并不为真。Argument list中的名称还是会在它们第一次遭遇时被适当地决议（resolved）完成。 需要某种防御性程序风格：请总是把“nested type声明”放在class的起始处。 3.2 Data Member的布局 Nonstatic data members在class object中的排列顺序将和其被声明的顺序一样，任何中间介入的static data members都不会被放进对象布局之中。 编译器还可能会合成一些内部使用的data members，以支持整个对象模型。vptr就是这样的东西，目前所有的编译器都把它安插在每一个“内含virtual function之class”的 object 内。一些编译器把vptr放在一个class object的最前端。 在VC中数据成员的布局顺序为： vptr部分（如果基类有，则继承基类的） vbptr （如果需要） 基类成员（按声明顺序） 自身数据成员 虚基类数据成员（按声明顺序） 3.3 Data Member的存取 Static Data Members 每一个static data member只有一个实例，存放在程序的data segment之中。每次程序参阅（取用）static member时，就会被内部转化为对该唯一extern实例的直接参考操作。 static member并不内含在一个class object之中。 若名称冲突，编译器的解决方法是暗中对每一个static data member编码（这种手法有个很美的名称：name-mangling），以获得一个独一无二的程序识别代码。 Nonstatic Data Members Nonstatic data members直接存放在每一个class object 之中。除非经由显式的（explicit）或隐式的（implicit）class object，否则没有办法直接存取它们。 欲对一个nonstatic data member进行存取操作，编译器需要把class object的起始地址加上data member的偏移位置（offset）。 3.4 “继承”与Data Member 在C++继承模型中，一个derived class object所表现出来的东西，是其自己的members加上其base class（es） members的总和。至于derived class members和base class（es）members的排列顺序，则并未在C++Standard中强制指定；理论上编译器可以自由安排之。在大部分编译器上头，base class members总是先出现，但属于virtual base class的除外（一般而言，任何一条通则一旦碰上virtual base class就没辙了，这里亦不例外）。 只要继承不要多态（Inheritance without Polymorphism） 加上多态（Adding Polymorphism） 虚拟继承（Virtual Inheritance） 多重继承的一个语意上的副作用就是，它必须支持某种形式的“shared subobject继承”。 虚继承的子类有自己的一个vptr。 3.5 对象成员的效率 单一继承应该不会影响测试的效率，因为members被连续存储于derived class object中，并且其offset在编译时期就已知了。虚拟继承的效率会降低。 3.6 指向Data Members的指针 如何区分一个“没有指向任何data member”的指针，和一个指向“第一个data member”的指针？ 为了区分p1和p2，每一个真正的member offset值都被加上1。不论编译器或使用者都必须记住，在真正使用该值以指出一个member之前，请先减掉1。 四、Function语意学 C++支持三种类型的member functions：static、nonstatic和virtual。 4.1 Member 的各种调用方式 Nonstatic Member Functions（非静态成员函数） C++的设计准则之一就是：nonstatic member function至少必须和一般的nonmember function有相同的效率。 名称的特殊处理（Name Mangling）一般而言，member的名称前面会被加上class名称，形成独一无二的命名。 Virtual Member Functions（虚拟成员函数） 1( * ptr-&gt;vptr[1])( ptr ) vptr表示由编译器产生的指针，指向virtual table。它被安插在每一个“声明有（或继承自）一个或多个 virtual functions”的class object中。事实上其名称也会被“mangled”，因为在一个复杂的class派生体系中，可能存在多个vptrs。 1是virtual table slot的索引值，关联到 normalize()函数。 第二个ptr表示this指针。 Static Member Functions（静态成员函数） 如果取一个static member function的地址，获得的将是其在内存中的位置，也就是其地址。由于static member function没有this指针，所以其地址的类型并不是一个“指向class member function的指针”，而是一个“nonmember函数指针”。 4.2 Virtual Member Functions（虚拟成员函数） virtual function的一般实现模型：每一个class有一个virtual table，内含该class之中有作用的virtual function的地址，然后每个object有一个vptr，指向virtual table的所在。 在C++中，多态（polymorphism）表示“以一个public base class 的指针（或reference），寻址出一个derived class object”的意思。 C++对“积极多态（active polymorphism）”的唯一支持，就是对于virtual function call的决议（resolution）操作。有了RTTI，就能够在执行期查询一个多态的pointer或多态的reference了。 欲鉴定哪些 classes 展现多态特性，我们需要额外的执行期信息。一如我所说，关键词class和struct并不能够帮助我们。由于没有导入像是polymorphic之类的新关键词，因此识别一个class是否支持多态，唯一适当的方法就是看看它是否有任何virtual function。只要class拥有一个virtual function，它就需要这份额外的执行期信息。 在实现上，首先我可以在每一个多态的class object身上增加两个members： 一个字符串或数字，表示class的类型； 一个指针，指向某表格，表格中持有程序的virtual functions的执行期地址。 然而，执行期备妥那些函数地址，只是解答的一半而已。另一半解答是找到那些地址。两个步骤可以完成这项任务： 为了找到表格，每一个class object被安插了一个由编译器内部产生的指针，指向该表格。 为了找到函数地址，每一个virtual function被指派一个表格索引值。 这些工作都由编译器完成。执行期要做的，只是在特定的virtual table slot（记录着virtual function的地址）中激活virtual function。 一个class只会有一个virtual table。每一个table内含其对应之class object中所有active virtual functions函数实例的地址。 现在，如果我有这样的式子：ptr-&gt;z() 我如何有足够的知识在编译时期设定virtual function的调用呢？ 一般而言，在每次调用 z()时，我并不知道ptr所指对象的真正类型。然而我知道，经由 ptr可以存取到该对象的virtual table。 虽然我不知道哪一个z()函数实例会被调用，但我知道每一个z()函数地址都被放在slot 4中。这些信息使得编译器可以将该调用转化为：(*ptr-&gt;vptr[4])(ptr) 多重继承下的Virtual Functions 在多重继承中支持virtual functions，其复杂度围绕在第二个及后继的base classes身上，以及“必须在执行期调整this指针”这一点。 虚拟继承下的Virtual Functions 右下角的vtbl有问题，指的应该是Point2d的函数地址。 4.3 函数的效能 nonmember、static member或nonstatic member函数都被转化为完全相同的形式。所以我们毫不惊讶地看到三者的效率完全相同。 4.4 指向Member Function的指针 取一个nonstatic data member的地址，得到的结果是该member在class布局中的bytes位置（再加1）。你可以想象它是一个不完整的值，它需要被绑定于某个class object的地址上，才能够被存取。取一个nonstatic member function的地址，如果该函数是nonvirtual，得到的结果是它在内存中真正的地址。然而这个值也是不完全的。它也需要被绑定于某个class object的地址上，才能够通过它调用该函数。所有的nonstatic member functions都需要对象的地址（以参数this指出）。 使用一个“member function指针”，如果并不用于virtual function、多重继承、virtual base class等情况的话，并不会比使用一个“nonmember function指针”的成本更高。 支持“指向 Virtual Member Functions”的指针 对一个nonstatic member function取其地址，将获得该函数在内存中的地址。然而面对一个virtual function，其地址在编译时期是未知的，所能知道的仅是virtual function在其相关之virtual table中的索引值。也就是说，对一个virtual member function取其地址，所能获得的只是一个索引值。 “指向 Member Functions之指针”的效率 一个“指向member function的指针”，是一个结构，内含三个字段：index、faddr和delta。index若不是内含一个相关virtual table的索引值，就是以-1表示函数是nonvirtual。faddr持有nonvirtual member function 的地址。delta持有一个可能的this指针调整值。 4.5 Inline Functions 一般而言，处理一个inline函数，有两个阶段： 分析函数定义，以决定函数的“intrinsic inline ability”（本质的 inline能力）。“intrinsic”（本质的、固有的）一词在这里意指“与编译器相关”。如果函数因其复杂度，或因其建构问题，被判断不可成为inline，它会被转为一个static函数，并在“被编译模块”内产生对应的函数定义。 真正的inline函数扩展操作是在调用的那一点上。这会带来参数的求值操作（evaluation）以及临时性对象的管理。 局部变量（Local Variables） 一般而言，inline函数中的每一个局部变量都必须被放在函数调用的一个封闭区段中，拥有一个独一无二的名称。如果inline函数以单一表达式（expression）扩展多次，则每次扩展都需要自己的一组局部变量。如果inline函数以分离的多个式子（discrete statements）被扩展多次，那么只需一组局部变量，就可以重复使用（译注：因为它们被放在一个封闭区段中，有自己的scope）。 然而一个inline函数如果被调用太多次的话，会产生大量的扩展码，使程序大小暴涨。 五、构造、析构、拷贝语意学 一般而言，class的data member应该被初始化，并且只在constructor中或是在class的其他member functions中指定初值。其他任何操作都将破坏封装性质，使class的维护和修改更加困难。 5.1 “无继承”情况下的对象构造 可以定义和调用（invoke）一个pure virtual function；不过它只能被静态地调用（invoked statically），不能经由虚拟机制调用。 唯一的例外就是pure virtual destructor：class设计者一定得定义它。为什么？因为每一个derived class destructor会被编译器加以扩张，以静态调用的方式调用其“每一个virtual base class”以及“上一层base class”的destructor。因此，只要缺乏任何一个base class destructors的定义，就会导致链接失败。 这样的设计是以C++语言的一个保证为前提：继承体系中每一个class object的destructors都会被调用。所以编译器不能够压抑这一调用操作。一个比较好的替代方案就是，不要把virtual destructor声明为pure。 5.2 继承体系下的对象构造 constructor的执行算法通常如下： 在derived class constructor中，“所有virtual base classes”及“上一层base class”的 constructors会被调用。 上述完成之后，对象的vptr(s)被初始化，指向相关的virtual table(s)。 如果有member initialization list的话，将在constructor体内扩展开来。这必须在vptr被设定之后才做，以免有一个virtual member function被调用。 最后，执行程序员所提供的代码。 5.3 对象复制语意学（Object Copy Semantics） 尽可能不要允许一个virtual base class的拷贝操作。我甚至提供一个比较奇怪的建议：不要在任何virtual base class中声明数据。 5.4 析构语意学（Semantics of Destruction） 如果class没有定义destructor，那么只有在class内含的member object （或class自己的base class）拥有destructor的情况下，编译器才会自动合成出一个来。否则，destructor被视为不需要，也就不需被合成（当然更不需要被调用）。 小结 即使是一个抽象基类，如果它有非静态数据成员，也应该给它提供一个带参数的构造函数，来初始化它的数据成员。类的data member应当被初始化，且只在其构造函数或其member function中初始化。 不要将析构函数设计为纯虚的，这不是一个好的设计。将析构函数设计为纯虚函数意味着，即使纯虚函数在语法上允许我们只声明而不定义纯虚函数，但还是必须实现该纯虚析构函数，否则它所有的继承类都将遇到链接错误。 真的必要的时候才使用虚函数，不要滥用虚函数。虚函数意味着不小的成本，编译很可能给你的类带来膨胀效应。 不能决定一个虚函数是否需要 const ，那么就不要它。 决不在构造函数或析构函数中使用虚函数机制。在构造函数中，每次调用虚函数会被决议为当前构造函数所对应类的虚函数实体，虚函数机制并不起作用。 六、执行期语意学（Runtime Semantics） C++的一件困难事情：不太容易从程序源码看出表达式的复杂度。 一般而言我们会把object尽可能放置在使用它的那个程序区段附近，这么做可以节省非必要的对象产生操作和摧毁操作。 6.1 对象的构造和析构 全局对象（Global Objects） 由于这样的限制，下面这些munch策略就浮现出来了： 为每一个需要静态初始化的文件产生一个_sti()函数，内含必要的constructor调用操作或inline expansions。 在每一个需要静态的内存释放操作（static deallocation）的文件中，产生一个__std()函数（译注：我想std就是static deallocation的缩写），内含必要的destructor调用操作，或是其 inline expansions。 提供一组runtime library“munch”函数：一个_main()函数（用以调用可执行文件中的所有__sti()函数），以及一个exit()函数（以类似方式调用所有的__std()函数）。 Lippman建议：根本就不要使用那些需要静态初始化的全局对象。真的非要一个全局对象，而且这个对象还需要静态初始化？用一个函数封装一个静态局部对象，也是一样的效果。 6.2 new和delete运算符 运算符new expression运算的使用，看起来似乎是个单一运算。int *p=new int (5)实际上包含着两个步骤： 调用一个合适的operator new实体分配足够的未类型化的内存。 调用合适的构造函数初始化这块内存，当然int没有构造函数，但是会进行赋值操作：*p=5。 delete寻找数组维度，对于delete运算符的效率带来极大的冲击，所以才导致这样的妥协：只有在中括号出现时，编译器才寻找数组的维度，否则它便假设只有单独一个objects要被删除。如果程序员没有提供必须的中括号，那么就只有第一个元素会被析构。其他的元素仍然存在——虽然其相关的内存已经被要求归还了。 new expression和operator new new expression和operator new完全不是一回事，但关系不浅——operator new 为new expression分配内存。且不能重定义new或delete expression的行为。 operator new其实也是可以直接利用的，譬如当我们只想分配内存，而不愿意进行初始化的时候，我们就可以直接用operator new 来进行。用法如下： 1T* newelements = static_cast&lt;T*&gt;(operator new ( sizeof(T) ); 标准库重载有两个版本的operator new，分别为单个对象和数组对象服务，单个对象版本的提供给分配单个对象new expression调用，数组版的提供给分配数组的 new expression 调用： 12void *operator new(size_t); // allocate an objectvoid *operator new[](size_t); // allocate an array 我们可以分别重载这两个版本，来定义我们自己的分配单个对象或对象数组的内存方式。当我们自己在重载operator new时，不一定要完全按照上面两个版本的原型重载，唯一的两个要求是：返回一个void*类型和第一个参数的类型必须为size_t。 在类中重载的operator new和operator delete是隐式静态的，因为前者运行于对象构造之前，后者运行于对象析构之后，所以他们不能也不应该拥有一个this指针来存取数据。 placement operator new placement operator new用来在指定地址上构造对象，要注意的是，它并不分配内存，仅仅是对指定地址调用构造函数。 point *pt = new(p) point3d; 它是operator new的一个重载版本。它的实现方式异常简单，传回一个指针即可： 123void* operator new(site_t,void *p)&#123; return p;&#125; 看一份代码： 12345678910111213141516struct Base &#123; int j; virtual void f(); &#125;;struct Derived : Base &#123; void f(); &#125;;void fooBar() &#123; Base b; b.f(); // Base::f() invoked b.~Base(); //析构掉了，但是内存并未释放掉 new (&amp;b) Derived; b.f(); // which f() invoked? &#125; 上述两个类的大小相同，因此将Derived对象放在 Base对象中是安全的，但是在最后一句代码中 b.f()调用的是哪一个类的f()。答案是Base::f() 的。 虽然此时b中存储的实际上是一个Derived对象，但是，通过一个对象来调用虚函数，将被静态决议出来，虚函数机制不会被启用。 6.3 临时性对象（Temporary Objects） 何时生成临时对象 程序片段： 12T a, b;T c = a + b; 编译器更愿意直接调用拷贝构造函数的方式将a+b的值放到c中，这样就不需要临时对象，和它的构造函数和拷贝构造函数的调用了。如果operator +的定义符合NRV优化的条件，那么NRV优化的开启，将使得拷贝构造函数的调用和named object的析构函数都免了。 所以比先声明 c 对象，再进行c = a + b要高效。 临时对象的生命周期 临时性对象在完整表达式尚未评估完全之前，不得被摧毁。临时性对象的摧毁应当作为造成产生这个临时对象的完整表达式的最后一个步骤。 对于下面的程序： 12string s1(\"hello \"), s2(\"world \"), s3(\"by Adoo\");std::cout &lt;&lt; s1 + s2 + s3 &lt;&lt; std::endl; 显然保存s1+s2结果的临时对象，如果在与s3进行加法之前析构，将会带来大麻烦。 七、站在对象模型的尖端 C++语言三个著名的扩充性质，它们都会影响C++对象。它们分别是template、exception handling（EH）和runtime type identification（RTTI）。 7.1 Template 有关template的三个主要讨论方向： template的声明。基本来说就是当你声明一个template class、template class member function等等时，会发生什么事情。 如何“实例化（instantiates）”class object、inline nonmember以及 member template functions。这些是“每一个编译单位都会拥有一份实例”的东西。 如何“实例化（instantiates）”nonmember、member template functions以及static template class members。这些都是“每一个可执行文件中只需要一份实例”的东西。这也就是一般而言 template所带来的问题。 Template的“实例化”行为（Template Instantiation） 一个模板只有被使用到，才会被实例化，否则不会被实例化。对于一个实例化后的模板来说，未被调用的成员函数将不会被实例化，只有成员函数被使用时，C++标准才要求实例化他们。其原因，有两点： 空间和时间效率的考虑，如果模板类中有100个成员函数，对某个特定类型只有2个函数会被使用，针对另一个特定类型只会使用3个，那么如果将剩余的195个函数实例化将浪费大量的时间和空间。 使模板有最大的适用性。并不是实例化出来的每个类型都支持所有模板的全部成员函数所需要的运算符。如果只实例化那些真正被使用的成员函数的话，那么原本在编译期有错误的类型也能够得到支持。 可以明确的要求在一个文件中将整个类模板实例化： 1template class Point3d&lt;float&gt;; 也可以显示指定实例化一个模板类的成员函数： 1template float Point3d&lt;float&gt;::X() const; 或是针对一个模板函数： 12template Point3d&lt;float&gt; operator+( const Point3d&lt;float&gt;&amp;, const Point3d&lt;float&gt;&amp; ); Template的错误报告（Error Reporting within a Template） 所以在一个parsing策略之下，所有语汇（lexing）错误和解析（parsing）错误都会在处理template声明的过程中被标示出来。所有与类型有关的检验，如果牵涉到template参数，都必须延迟到真正的实例化操作（instantiation）发生，才得为之。 目前的编译器，面对一个template声明，在它被一组实际参数实例化之前，只能施行以有限的错误检查。template中那些与语法无关的错误，程序员可能认为十分明显，编译器却让它通过了，只有在特定实例被定义之后，才会发出抱怨。这是目前实现技术上的一个大问题。 模板的错误报告，使用模板并遇到错误的大概都深有体会，那就是一个灾难。 Template中的名称决议（Name Resolution within a Template） Template之中，对于一个nonmember name 的决议结果，是根据这个name的使用是否与“用以实例化该template的参数类型”有关而决定的。如果其使用互不相关，那么就以“scope of the template declaration”来决定name。如果其使用互有关联，那么就以“scope of the tem plate instantiation”来决定name。 这意味着一个编译器必须保持两个scope contexts： “scope of the template declaration”，用以专注于一般的template class。 “scope of the template instantiation”，用以专注于特定的实例。 编译器的决议（resolution）算法必须决定哪一个才是适当的scope，然后在其中搜寻适当的name。 第一种情况： 12345678910111213141516171819// scope of the template declarationextern double foo ( double ); template &lt; class type &gt; class ScopeRules &#123; public: void invariant() &#123; _member = foo( _val ); &#125; type type_dependent() &#123; return foo( _member ); &#125; // ... private: int _val; type _member; &#125;; 第二种情况: 1234567//scope of the template instantiation extern int foo( int ); // ... ScopeRules&lt; int &gt; sr0; sr0.invariant();sr0.type_dependent(); 在“scope of the template instantiation ”中两个foo()都声明在此 scope中。sr0.invariant() 中调用的是： 1extern double foo ( double ); 看上去，应该调用： 1extern int foo( int ); 毕竟，_val 的类型是 int 类型，它们才完全匹配。 而 sr0.type_dependent() 中调用的却在我们意料之中，调用的是: 1extern int foo( int ); 诸上所述，看上去或合理或不合理的选择，原因在于: template 之中， 对于一个非成员名字的决议结果是根据这个 name 的使用是否与“用以实例化该模板的参数类型”有关来决定name。如果其使用互不相干，那么就以“scope of the template declaration”来决定name。如果其使用的互相关联，那么就以“scope of the template instantiation”来决定name。 Member Function的实例化行为（Member Function Instantiation） 对于 template 的支持，最困难的莫过于template function的实例化（instantiation）。目前的编译器提供了两个策略：一个是编译时期策略，程序代码必须在program text file中备妥可用；另一个是链接时期策略，有一些meta-compilation工具可以导引编译器的实例化行为（instantiation）。 7.2 异常处理（Exception Handling） 欲支持exception handling，编译器的主要工作就是找出catch子句，以处理被抛出来的 exception。这多少需要追踪程序堆栈中的每一个函数的目前作用区域（包括追踪函数中local class objects当时的情况）。同时，编译器必须提供某种查询exception objects 的方法，以知道其实际类型（这直接导致某种形式的执行期类型识别，也就是 RTTI）。最后，还需要某种机制用以管理被抛出的object，包括它的产生、存储、可能的析构（如果有相关的destructor）、清理（clean up）以及一般存取。也可能有一个以上的objects同时起作用。一般而言，exception handling机制需要与编译器所产生的数据结构以及执行期的一个exception library紧密合作。 在程序大小和执行速度之间，编译器必须有所抉择： 为了维护执行速度，编译器可以在编译时期建立起用于支持的数据结构。这会使程序的大小发生膨胀，但编译器可以几乎忽略这些结构，直到有个exception被抛出来。 为了维护程序大小，编译器可以在执行期建立起用于支持的数据结构。这会影响程序的执行速度，但意味着编译器只有在必要的时候才建立那些数据结构（并且可以抛弃之）。 Exception Handling 快速检阅C++的exception handling由三个主要的语汇组件构成： 一个throw子句。它在程序某处发出一个exception。被抛出去的exception可以是内建类型，也可以是使用者自定类型。 一个或多个catch子句。每一个catch子句都是一个exception handler。它用来表示说，这个子句准备处理某种类型的exception，并且在封闭的大括号区段中提供实际的处理程序 一个try区段。它被围绕以一系列的叙述句（statements），这些叙述句可能会引发catch子句起作用。 当一个exception被抛出去时，控制权会从函数调用中被释放出来，并寻找一个吻合的catch子句。如果都没有吻合者，那么默认的处理例程terminate()会被调用。当控制权被放弃后，堆栈中的每一个函数调用也就被推离（popped up）。这个程序称为unwinding the stack。在每一个函数被推离堆栈之前，函数的local class objects的destructor会被调用。 支持EH，会使那些拥有member class subobjects或base class subobjects（并且它们也都有constructors）的classes的constructor更复杂。一个class如果被部分构造，其destructor必须只施行于那些已被构造的subobjects和（或）member objects身上。 对Exception Handling的支持 当一个exception发生时，编译系统必须完成以下事情： 检验发生throw操作的函数。 决定throw操作是否发生在try区段中。 若是，编译系统必须把exception type拿来和每一个catch子句进行比较。 如果比较后吻合，流程控制应该交到catch子句手中。 如果throw的发生并不在 try区段中，或没有一个catch子句吻合，那么系统必须（a）摧毁所有active local objects，（b）从堆栈中将目前的函数“unwind”掉，（c）进行到程序堆栈的下一个函数中去，然后重复上述步骤 2～5。 决定throw是否发生在一个try区段中 还记得吗，一个函数可以被想象为好几个区域： try区段以外的区域，而且没有active local objects。 try区段以外的区域，但有一个（或以上）的active local objects需要析构。 try区段以内的区域。 编译器必须标示出以上各区域，并使它们对执行期的exception handling系统有所作用。一个很棒的策略就是构造出program counter-range表格。 回忆一下，program counter内含下一个即将执行的程序指令。好，为了在一个内含try区段的函数中标示出某个区域，可以把program counter的起始值和结束值（或是起始值和范围）存储在一个表格中。 当throw操作发生时，目前的program counter值被拿来与对应的“范围表格”进行比对，以决定目前作用中的区域是否在一个try区段中。如果是，就需要找出相关的catch子句。如果这个exception无法被处理（或者它被再次抛出），目前的这个函数会从程序堆栈中被推出（popped），而program counter会被设定为调用端地址，然后这样的循环再重新开始。 将exception的类型和每一个catch子句的类型做比较 对于每一个被抛出来的exception，编译器必须产生一个类型描述器，对exception的类型进行编码。如果那是一个derived type，编码内容必须包括其所有base class的类型信息。只编进public base class的类型是不够的，因为这个exception可能被一个member function捕捉，而在一个member function的范围（scope）之中，derived class和nonpublic base class之间可以转换。 类型描述器（type descriptor）是必要的，因为真正的exception是在执行期被处理的，其object必须有自己的类型信息。RTTI正是因为支持EH而获得的副产品。 编译器还必须为每一个catch子句产生一个类型描述器。执行期的exception handler会将“被抛出之object的类型描述器”和“每一个catch子句的类型描述器”进行比较，直到找到吻合的一个，或是直到堆栈已经被 “unwind” 而 terminate()已被调用。 每一个函数会产生出一个exception表格，它描述与函数相关的各区域、任何必要的善后处理代码（cleanup code，被local class object destructors调用）以及catch子句的位置（如果某个区域是在try区段之中的话）。 当一个实际对象在程序执行时被抛出，会发生什么事？ 当一个exception被抛出时，exception object会被产生出来并通常放置在相同形式的exception数据堆栈中。从throw端传给catch子句的，是exception object的地址、类型描述器（或是一个函数指针，该函数会传回与该exception type有关的类型描述器对象）以及可能会有的 exception object 描述器（如果有人定义它的话）。 异常与内存 异常抛出有可能带来一些问题，比方在一块内存的lock和unlock内存之间，或是在new和delete之间的代码抛出了异常，那么将导致本该进行的unlock或delete操作不能进行。 在函数被出栈之前，先截住异常，在unlock和delete之后再将异常原样抛出。new expression的调用不用包括在try块之内是因为，不论在new operator调用时还是构造函数调用时抛出异常，都会在抛出异常之前释放已分配好的资源，所以不用再调用delete 。 另一个办法是，将这些资源管理的问题，封装在一个类对象中，由析构函数释放资源，这样就不需要对代码进行上面那样的处理——利用函数释放控制权之前会析构所有局部对象的原理。 同样的道理，适用于数组身上，如果在调用构造函数过程中抛出异常，那么之前所有被构造好的元素的析构函数被调用，对于抛出异常的该元素，则遵循关于单个对象构造的原则，然后释放已经分配好的内存。 12345678910111213141516void mumble( void *arena ) &#123; Point *p; p = new Point; try &#123; smLock( arena ); // ... &#125; catch ( ... ) &#123; smUnLock( arena ); delete p; throw; &#125; smUnLock( arena ); delete p; &#125; 7.3 执行期类型识别（Runtime Type Identification，RTTI） Type-Safe Downcast（保证安全的向下转换操作） 一个type-safe downcast必须在执行期对指针有所查询，看看它是否指向它所展现（表达）之object的真正类型。因此，欲支持type-safe downcast，在object空间和执行时间上都需要一些额外负担： 需要额外的空间以存储类型信息（type information），通常是一个指针，指向某个类型信息节点。 需要额外的时间以决定执行期的类型（runtime type），因为，正如其名所示，这需要在执行期才能决定。 C++的RTTI机制提供了一个安全的downcast设备，但只对那些展现“多态（也就是使用继承和动态绑定）”的类型有效。 在C++中，一个具备多态性质的class（所谓的polymorphic class），正是内含着继承而来（或直接声明）的virtual functions。 从编译器的角度来看，这个策略还有其他优点，就是大量降低额外负担。所有polymorphic classes的objects都维护了一个指针（vptr），指向virtual function table。只要我们把与该class相关的RTTI object 地址放进virtual table 中（通常放在第一个slot），那么额外负担就降低为：每一个class object只多花费一个指针。这一指针只需被设定一次，它是被编译器静态设定的，而非在执行期由 class constructor设定。 Type-Safe Dynamic Cast（保证安全的动态转换） dynamic_cast运算符可以在执行期决定真正的类型。如果downcast是安全的（也就是说，如果base type pointer指向一个derived class object），这个运算符会传回被适当转换过的指针。如果downcast不是安全的，这个运算符会传回0。 什么是dynamic_cast的真正成本呢？pfct的一个类型描述器会被编译器产生出来。由pt所指向的class object类型描述器必须在执行期通过vptr取得。type_info是C++Standard所定义的类型描述器的class名称，该class中放置着待索求的类型信息。virtual table的第一个slot内含type_info object 的地址；此type_info object与pt所指的class type有关。这两个类型描述器被交给一个runtime library函数，比较之后告诉我们是否吻合。 References不同于Pointers 程序执行中对一个class指针类型施以dynamic_cast 运算符： 如果传回真正的地址，则表示这一object的动态类型被确认了，一些与类型有关的操作现在可以施行于其上。 如果传回0，则表示没有指向任何object，意味着应该以另一种逻辑施行于这个动态类型未确定的object身上。 因此当dynamic_cast运算符施行于一个reference 时，会发生下列事情： 如果reference真正cast到适当的derived class，downcast会被执行而程序可以继续进行。 如果reference并不真正是某一种derived class，那么，由于不能够传回0，因此抛出一个 bad_cast exception。 原因在于指针可以被赋值为0，以表示 no object，但是引用不行。 Typeid运算符 使用 typeid 运算符，就有可能以一个reference达到相同的执行期替代路线（runtime“alternative pathway”）。typeid运算符传回一个const reference，类型为type_info。如果两个type_info objects相等，这个equality运算符就传回true。 typeid 可以返回const type_info&amp;，用以获取类型信息。 虽然RTTI只支持多态类，但typeid和type_info同样可用于内建类型及所有非多态类。与多态类的差别在于，非多态类的type_info对象是静态取得，而多态类的是在执行期获得。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"}],"author":"HeRui"},{"title":"optimizing software in cpp","slug":"optimizing-software-in-cpp","date":"2022-06-21T13:14:38.000Z","updated":"2024-01-09T03:31:35.323Z","comments":true,"path":"posts/2ebe459a.html","link":"","permalink":"https://racleray.github.io/posts/2ebe459a.html","excerpt":"optimizing software in cpp 读书笔记","text":"Optimizing in C++ 1 不同的变量存储位置 stack 函数中的临时变量或对象一般存储在内存空间中的stack区。每当调用函数时，参数和临时变量进栈，当函数返回时，参数和临时变量出栈。栈是内存空间中最高效的存储方式。当临时变量中没有较大对象时，访问栈上的临时变量也基本能用上L1 data cache。 global or static 在函数体之外声明的变量称之为global变量，可被任何函数访问。被static修饰的变量称为static变量。 global和static变量在程序运行期间会被放置于内存空间中的静态数据区。 静态数据区域分为三个部分： 一部分存储 const 类型的 global/static 变量 一部分存储已被初始化的 global/static 变量， 最后一部分存储未被初始化的 global/static 变量。 使用静态数据区的好处是，global/static 变量在程序启动前就有专门的存储位置，坏处是在程序的生命周期内，这些存储位置将被一直占据，可能会降低 data cache 的效率。 所以建议尽量不要使用global变量。 register register变量存储在CPU寄存器中，函数中的临时变量特别适合放到register中。优点很明显，访问 register 变量比访问RAM快得多。但是CPU寄存器大小是非常有限的，在64位x86架构中，有14个整数寄存器，16个浮点寄存器。 volatile volatile 用于声明一个变量可被其他线程改变，阻止编译器依赖变量先前缓存的值来进行优化 。 123456789volatile int seconds; // incremented every second by another threadvoid DelayFiveSeconds()&#123; seconds = 0; while (seconds &lt; 5) &#123; // do nothing while seconds count to 5 &#125;&#125; 上面的代码如果不声明为 volatile, 编译器将任务while条件一直成立，即使别的线程中改变了seconds的值。 thread-local 大多数编译器可以使用关键字 __thread 或 __declspec(thread) 来实现静态变量和全局变量的线程本地存储。这样的变量在每个线程都有一个实例 。 线程本地存储是低效的，因为它是通过存储在线程访问块中的指针进行访问的。因此建议尽量避免线程本地存储，代之以stack存储。 dynamic memory allocation c++中通过new/malloc动态分配存储，通过delete/free释放动态分配的的存储，动态分配的存储放在内存空间的heap区中。 优点：使用相对stack存储更加灵活 缺点：动态分配和释放很耗时，容易出现野指针、悬垂指针、内存泄露、内存碎片等问题。 variables declared inside a class 类中声明的变量按照在类中的顺序存储，存储位置由类的对象在哪里定义的决定。static修饰的类成员变量将存储在静态数据区，只有一个实例。 将变量存储在类中的好处是保证了空间局部性，对CPU data cache更友好。 2 整型变量和运算符 整数大小 对于不同的平台，不同整数类型(char/short/int/long)的大小可能不同。 无论大小如何，整数运算基本都很快，除非使用了大于CPU寄存器大小的类型，比如在32位系统中使用64位整数。 建议在与大小无关且没有溢出风险的情况下使用默认整数大小，例如简单变量、循环计数器等。 在大型数组中，为了更好地使用数据缓存，最好使用足够大的最小整数大小。 无符号整形数 vs. 有符号整形数 在大多数情况下，使用有符号整数和无符号整数在速度上没有区别。 除了 除以常数：当你将一个整数除以一个常数时，无符号要快于有符号 对于大多数指令集，有符号整数比无符号整数转换成浮点数要快 有符号变量和无符号变量的溢出行为不同。 整数运算符 整数运算非常快。加减和位操作只需一个时钟周期，乘法需要3-4个时钟周期，除法需要40-80个。 自增和自减运算符 当仅用于递增整数变量时，使用递增前或递增后都没有区别。 例如，for (i=0; i&lt;n; i++)和for (i=0; i&lt;n; ++i)是一样的。 但是当使用表达式的结果时，效率可能会有所不同。 例如，x = array[i++] 比 x = array[++i] 速度更快。因为在后一种情况下，数组元素的地址的计算必须等待 i 的新值，这将使 x 的可用性延迟大约两个时钟周期。BUT，两者表达的意思是完全不同的。 3 浮点计算和运算符 x86架构中有两种浮点计算方法。 原始方法：使用8个浮点寄存器组成寄存器栈(长双精度80位)。 优点：精度高，不同精度之间无需额外转换，有浮点计算指令可用； 缺点：难以生成寄存器变量，浮点计算较慢，整数和浮点数之间转换效率很低 向量方法：使用8个或16个向量寄存器(XMM或YMM)。 优点：可并行计算，寄存器变量容易实现； 缺点：不支持长双精度，数学函数必须使用函数库(但通常比硬件指令更快) XMM向量寄存器在x86_64架构中可用。如果处理器和操作系统支持AVX指令集，则可使用YMM向量寄存器。当XMM可用时，编译器一般用向量方法计算浮点数。 根据微处理器的不同，浮点加法需要 3‐6 个时钟周期。乘法需要 4‐8 个时钟周期。除法需要 14‐45 个时钟周期。 4 枚举 枚举只是隐藏的整数。 5 布尔值 布尔操作数的顺序 短路逻辑：当 &amp;&amp; 的左操作数为false时，便不会计算右操作数。同理, || 的做操作数为true时，也不会计算右操作数。因此建议将通常为true的操作数放在&amp;&amp;表达式最后，或||表达式的开头。 布尔变量被过度检查 由于所有以布尔变量作为输入的运算符都要检查输入是否有除0或1之外的值，因此布尔变量会被过度检查。如果知道操作数除了 0 就是 1，布尔运算可以变得有效率的多。 编译器之所以不这样假设，是因为变量可能是没有被初始化的或者是来自其它未知来源的。 布尔向量操作 一个整数可被当做布尔向量操作。例如 bitmap 6 指针和引用 指针 vs. 引用 指针和引用的效率是一样的，因为它们实际上做的事情是相同的，区别在于编程风格。 指针的优点：功能上更灵活，可以做指针计算(例如访问缓冲区)，当然也更容易出错 引用的优点：语法更简单，也更安全。 效率 运行时，需要一个额外的寄存器来保存指针或引用的值，而寄存器是一种稀缺资源，如果没有足够的寄存器，那么指针每次使用时都必须从内存中加载，这会使程序变慢。 另一个缺点是指针的值需要几个时钟周期才能访问所指向的变量。也就是说，要读取指针或引用的值，最坏情况下需要访问两次内存。 函数指针 通过函数指针调用函数通常要比直接调用函数多花几个时钟周期 。 智能指针 使用智能指针的目的是为了确保对象被正确删除，以及在对象不再使用时释放内存 。 通过智能指针访问对象没有额外的成本。 但是，每当创建、删除、复制或从一个函数转移到另一个函数时，都会产生额外的成本。shared_ptr 的这些成本要高于 unique_ptr。 7 数组 数组是通过在内存中连续存储元素来实现的，没有存储关于数组大小的信息。因此c/c++中数组相比其他语言更快，但也更不安全。 当不按顺序索引时，为了使地址的计算更高效，那么除了第一个维度外，所有维度的大小最好是 2 的幂。当以非顺序访问元素，则对象的大小（以字节为单位）最好为 2 的幂。 上述建议是为了更好利用CPU的data cache。 8 类型转换 signed/unsigned转换 寄存器值不变，只是编译器换了解释方法。因此没有额外的性能成本。 整形类型大小的转换 类型大小转换通常不需要额外的时间 浮点进度转换 当使用浮点寄存器时，浮点、双精度和长双精度之间的转换不需要额外的时间 当用XMM寄存器时，需要 2 到 15个时钟周期（取决于处理器） 因此建议使用向量化寄存器存储浮点数时，不要混用浮点类型。 整形转浮点型 将有符号整数转换为浮点数或双精度浮点数需要 4‐16个时钟周期，这取决于处理器和使用的寄存器类型。无符号整数的转换需要更长的时间。 如果没有溢出的风险，首先将无符号整数转换为有符号整数会更快。 浮点型转化为整形 如果不启用 SSE2 或者更新的指令集，浮点数到整数的转换将花费很长的时间。通常，转换需要 50‐100 个时钟周期。 解决方案是： 使用 64 位模式或启用 SSE2 指令集； 使用四舍五入代替截断，并用汇编语言制作一个舍入函数。 指针类型转换 指针可以转换为另一种类型的指针。同样，可以将指针转换为整数，也可以将整数转换为指针。值还是那些值，只是换了种解释方法，因此没有任何开销。 重新解释对象 c++中的reinterpret_cast，没有任何额外开销。 const_cast const_cast 运算符用于解除 const 对指针的限制 。 没有任何额外开销。 static_cast static_cast 运算符的作用与 C 风格的类型转换相同。例如，它用于将 float 转换为 int reinterpret_cast 没有任何额外开销。 dynamic_cast dynamic_cast 运算符用于将指向一个类的指针转换为指向另一个类的指针。它在运行时检查转换是否有效 。dynamic_cast比static_cast更耗时，也更安全。 转换类对象 只有当程序员定义了构造函数、重载赋值运算符或重载类型转换运算符（指定如何进行转换）时，才有可能进行涉及类对象（而不是指向对象的指针）的转换。 构造函数或重载运算符与成员函数的效率是一样的。 9 分支和switch语句 在微处理器做出正确分支预测的情况下，执行分支指令通常需要 0‐2 个时钟周期。根据处理器的不同，从分支错误预测中恢复所需的时间大约为 12‐25 个时钟周期。这被称为分支预测错误的惩罚。 for 循环或 while 循环也是一种分支。 在每次迭代之后，它决定是重复还是退出循环。嵌套循环只能在某些处理器上得到很好的预测。在许多处理器上，包含多个分支的循环并不能很好地被预测。 switch 语句也是一种分支，它可以有两个以上的分支。 如果 case 标签是遵循每个标签等于前一个标签加 1 的序列，在这个时候 switch语句的效率是最高的，因为它可以被实现为一个目标跳转表。 如果 switch 带有许多标签值，并且彼此相差较大，这将是低效的，因为编译器必须将其转换成一个分支树。 分支和 switch 语句的数量，在程序的关键部分，最好控制在较少的水平 。 因为分支和函数调用的目标保存在称为分支目标缓冲区的特殊缓存中。如果一个程序中有许多分支或函数调用，那么在分支目标缓冲区中就可能产生竞争。这种竞争的结果是，即使分支具有良好的可预测性，它们也可能被错误地预测。 10 循环 循环的效率取决于微处理器对循环控制分支的预测能力。 一个较小并且有固定的重复计数，且没有分支的循环，可以完美地被预测。 循环展开 展开前 123456789int i;for (i &#x3D; 0; i &lt; 20; i++)&#123; if (i % 2 &#x3D;&#x3D; 0); FuncA(i); else FuncB(i); FuncC(i);&#125; 展开后 12345678int i;for (i &#x3D; 0; i &lt; 20; i+&#x3D;2)&#123; FuncA(i); FuncC(i); FuncB(i+1); FuncC(i+1);&#125; 这样做的好处： 循环次数变成了10次而不是20次，CPU可以更完美的进行预测 if分支被消除，有利于编译器自动进行向量化等优化 循环展开的坏处： 展开循环后在代码缓存中占用更多空间 非常小的循环展开不如不展开 如果重复计数为奇数，并将其展开为2， 则必须在循环之外执行额外的迭代。 只有在能够取得特定好处的情况下，才应该使用循环展开。 比如，如果一个循环包含浮点运算，且循环计数器是整数，那么通常可以假设整个计算时间是由浮点代码决定的，而不是由循环控制分支决定的。在这种情况下，展开循环是没有任何好处的 。 循环控制条件 如果循环控制分支依赖于循环内部的计算，则效率较低。 确定最坏情况下的最大重复计数并始终使用此迭代次数的效率会更高。 循环计数器最好是整数。 复制或清除数组 对于诸如复制数组或将数组中的元素全部设置为零这样的琐碎任务，使用循环可能不是最佳选择。 使用 memset 和 memcpy 函数通常会更快。 11 函数 函数调用会让程序慢下来，因为 代码地址跳转，可能需要4个时钟周期 如果代码分散在内存中会降低代码缓存效率 如果函数参数不够放在寄存器中，需要入到栈中，效率不高 需要额外时间设置stack frame, 保存和恢复寄存器 每个函数调用语句需要在分支目标缓冲区（BTB）中占用空间，BTB中发生资源竞争可能会导致分支预测失败。 如何避免函数调用降低效率呢？ 避免不必要函数 不要过度封装。 使用内联函数 如果函数很小，或者只在程序中的一个位置调用它，那么内联函数是有好处的。小函数通常由编译器自动内联。 避免在最内层循环嵌套函数调用 如果在程序关键的最内层循环包含对帧函数的调用，那么代码有可能通过内联帧函数或使帧函数调用的所有函数内联（把帧函数变为叶函数）来提升效率。 帧函数（Frame Function）： 帧函数是指在程序执行期间创建的函数调用帧（Function Frame）或栈帧（Stack Frame）。每当函数被调用时，会在内存中分配一个帧来存储函数的局部变量、参数和其他相关信息。 帧函数用于描述函数调用期间的堆栈结构，包含了函数的执行上下文、局部变量和临时数据等。它提供了函数执行的环境和上下文切换所需的信息。 帧函数还包括函数调用返回时所需的清理操作，如恢复调用者的上下文和处理返回值等。 叶函数（Leaf Function）： 叶函数是指在函数调用期间不会调用其他函数的函数，即它没有其他的子函数调用。叶函数执行完毕后直接返回，而不会进一步调用其他函数。 叶函数通常比较简单，不涉及复杂的递归或函数调用链，并且在性能优化方面具有一定的优势。 使用宏代替函数 不要滥用宏，宏的问题是：名称不能重载或限制作用区域。宏将干扰具有相同名称的任何函数或变量，而与作用域或命名空间无关。 使函数局部化 应该使同一个模块中使用的函数（即当前 .cpp 文件）是局部的。 这使得编译器更容易将函数内联，并对函数调用进行优化。 如何使函数局部化呢？ 对于非类成员函数，直接使用static 对于类成员函数，将函数或类放置于匿名命名空间中 使用全程序优化 一些编译器具有对整个程序进行优化的选项，也可以选择将多个 .cpp 文件组合成一个对象文件。这使得编译器能够在组成程序的所有 .cpp 模块之间优化寄存器分配和参数传递。 使用64位模式 现在服务器端开发都是64位模式了吧？ 函数参数 在大多数情况下，函数参数是按值传递的。这意味着参数的值被复制到一个局部变量中。对于int、float、double、bool、enum 以及指针和引用等简单类型，这非常快。 数组总是使用指针传递，除非它们被打包在类或者结构体中。 如果参数是复合类型，在以下情况下传值更高效，否则使用指针和引用更高效： 对象很小，可以装入一个寄存器 对象没有拷贝构造函数和析构函数 对象没有虚成员 对象没有使用RTTI 将复合对象传递给函数的首选方法是使用 const 引用。其次是使函数成为对象的类成员。 64位 unix 系统允许寄存器中传输最多14个参数 (8个float或double，加上6个整数、指针或引用参数) 函数返回类型 函数的返回类型最好是简单类型、指针、引用或 void。返回复合类型的对象更为复杂，而且常常效率低下。 简单情况下，复合类型对象直接从寄存器返回。否则通过一个隐藏指针将它们复制到调用方指定的位置。 当直接返回复杂类型对象的值时，编译器可能会进行RVO(return value optimization)优化，从而避免复制构造和析构成本，但开发者不应依赖这一点。 函数尾调用 尾调用是优化函数调用的一种方法。如果函数的最后一条语句是对另一个函数的调用，那么编译器可以用跳转到第二个函数来替换该调用。 编译器优化将自动完成此任务。第二个函数不会返回到第一个函数，而是直接返回第一个函数被调用的位 置。这样效率更高，因为它消除了返回操作。 递归函数 函数递归调用对于处理递归数据结构非常有用。递归函数的代价是所有参数和局部变量在每次递归时都会有一个新实例，这会占用栈空间。 较宽的树形结构比较深的树形结构，有更高的递归效率。 无分支递归总是可以用循环代替，这样的效率更高 12 结构体和类 面向对象的好处： 变量存储在一起，数据缓存更有效率 无需将类成员变量作为参数传递给类成员函数，避免参数传递的开销 面向对象的坏处： 非静态成员函数有this指针，有额外开销 虚成员函数的效率较低 如果面向对象的编程风格有利于程序的逻辑结构和清晰性，那么你可以使用这种风格 类的数据成员 类或结构体的数据成员是按创建类或结构实例时声明它们的顺序连续存储。将数据组织到类或结构体中不存在性能损失。 大多数编译器将数据成员对齐到可以被特定数整除的地址以优化访问，副作用是产生字节空洞。 类的成员函数 每次声明或创建类的新对象时，它都会生成数据成员的新实例。但是每个成员函数只有一个实例。函数代码不会被复制。 静态成员函数不能访问任何非静态数据成员或非静态成员函数。静态成员函数比非静态成员函数快。 虚成员函数 多态性是面向对象程序比非面向对象程序效率低的主要原因之一。 如果可以避免使用虚函数，那么你就可以获得面向对象编程的大多数优势，而无需付出性能成本 。 如果函数调用语句总是调用虚函数的相同版本，那么调用虚成员函数所花费的时间要比调用非虚成员函数多几个时钟周期。如果版本发生了变化，你可能会得到10‐20个时钟周期的错误预测惩罚。 有时可以使用模板（编译时多态）而不是虚函数来获得所需的多态性效果。 运行时类型识别（RTTI） 效率不高。如果编译器有RTTI 选项，那么关闭它并使用其他实现。 继承 派生类的对象与包含父类和子类成员的简单类的对象的实现方法相同。父类和子类的成员访问速度相同。 一般来说，你可以假设使用继承几乎没有任何性能损失。 除了： 父类数据成员大小会添加到子类成员的偏移量中。偏移量太大时，会造成数据缓存效率降低。 父类和子类代码可能在不同模块。造成代码缓存效率降低。 另外，尽量不使用多重继承，代之以组合。 联合体 union 是数据成员共享相同内存空间的结构。union 可以通过允许不同时使用的两个数据成员共享同一块内存来节省内存空间。 位域 位域虽然有助于使数据更加紧凑，但是访问位域成员不如访问结构的成员效率高。如果在大数组可以节省缓存空间或使文件更小，那么额外的时间是合理的 重载函数 重载函数的不同版本被简单地视为不同的函数。使用重载函数没有性能损失。 重载运算符 重载的运算符相当于一个函数。使用重载运算符与使用具有相同功能的函数效率一样。 13 模板 模板与宏的相似之处在于，模板参数在编译之前被替换。 模板是高效的，因为模板参数总是在编译时被解析。模板使源代码更加复杂，而不是编译后的代码。一般来说，使用模板在执行速度方面没有任何成本。 使用模板实现编译时多态 模板类可用于实现编译时多态性，这比使用虚拟成员函数获得的运行时多态性更加高效。 模板代码可读性不佳。 14 线程 线程上下文切换非常耗时，可通过设置更长的时间片来减少上下文切换的次数。另外，为不同任务的不同线程分配不同的优先级是非常有用的。 为了充分利用多核，可以将工作划分成多个线程，每个线程在单独的CPU core上执行。但是多线程有四个成本： 启动和停止线程的成本。如果任务执行时间很短，不要为其单独分配线程。 线程切换成本。 线程间同步和通信成本。 不同线程需要单独的存储空间，线程有各自的堆栈，如果线程共享相同的缓存，可能会导致缓存竞争。 多线程程序必须使用线程安全函数，线程安全函数永远不应该使用静态变量 (除非是只读的静态变量) 。 15 异常和错误处理 C++中通过try catch捕获异常。异常处理旨在检测很少发生的错误，并以一种优雅的方式从错误条件中恢复。 但是，即使程序运行时没有错误，异常处理仍需要额外的时间，花销多少取决于编译器实现。 如果你的应用程序不需要异常处理，那么应该禁用它，以便使代码更小、更高效。 可以通过关闭编译器中的异常处理选项来禁用整个程序的异常处理。或者，也可以通过向函数原型中添加 throw() 声明来禁用单个函数的异常处理。 异常和向量代码 向量指令对于并行执行多个计算是有用的。如果代码可以从向量指令中获益，那么最好禁用异常捕获，转而依赖 NAN 和 INF 的传递。 避免异常处理的成本 当不需要尝试从错误中恢复时，不需要异常处理。 建议使用系统的、经过深思熟虑的方法来处理错误。 区分可恢复错误和不可恢复错误。 确保分配的资源在发生错误时得到清理，向用户发送适当的错误消息。 编写异常安全代码 为了保证异常安全，需要在发生异常时清理下列资源： 使用new和malloc分配的内存 句柄 互斥量 数据库连接 网络连接 待删除临时文件 待保护的用户工作 其他已分配的资源 C++ 处理清理工作的方法是创建一个析构函数。C++ 异常处理系统确保调用本地对象的所有析构函数。 如果类有析构函数来处理分配资源的所有清理工作，则程序是异常安全的。如果析构函数引发另一个异常，则系统可能会出现问题。 如果你使用自己的错误处理系统而不是使用异常处理，那么你无法确保是否调用了所有析构函数并清理了资源。 如果错误处理程序调用 exit()、abort()、_endthread() 等，则不能保证所有析构函数被调用。 NAN和INF的传递 浮点溢出和除以 0 得到无穷大 INF。 如果你把无穷大和某数相加或相乘，结果就是无穷大 INF。 如果用一个正常的数字除以 INF，会得到 0。 INF ‐ INF 和 INF / INF 得到 NAN （not‐a‐number）。 当你用 0 除以 0 以及函数的输入超出范围时，比如 sqrt(‐1) 和 log(‐1)，也会出现特殊的代码NAN。 INF 和 NAN 的传播也不需要额外的成本。 当参数为 INF 或 NAN 时，函数 finite() 将返回 false，如果它是一个普通的浮点数，则返回 true。 16 预处理命令 就程序性能而言，预处理指令（以#开头的所有指令）的性能成本很少，因为它们在程序编译之前就已经解析了。 17 命名空间 使用名称空间，对执行速度没有影响。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Book","slug":"Book","permalink":"https://racleray.github.io/tags/Book/"}],"author":"HeRui"},{"title":"More Effective Cpp","slug":"More-Effective-Cpp","date":"2022-05-09T04:01:49.000Z","updated":"2024-01-09T03:31:40.256Z","comments":true,"path":"posts/e584d079.html","link":"","permalink":"https://racleray.github.io/posts/e584d079.html","excerpt":"More Effective Cpp 读书笔记","text":"一、基础 条款 1：仔细区别 pointers和references 没有 null reference。一个 reference 必须总代表某个对象。 所以如果你有一个变量，其目的是用来指向（代表）另一个对象，但是也有可能它不指向（代表）任何对象，那么你应该使用 pointer，才可以将 pointer 设为 null。 Pointers 和 references 之间的另一个重要差异就是，pointers 可以被重新赋值，指向另一个对象，reference 却总是指向（代表）它最初获得的那个对象。 当你需要考虑“不指向任何对象”的可能性时，或是考虑“在不同时间指向不同对象”的能力时，你就应该采用 pointer。 当你确定“总是会代表某个对象”，而且“一旦代表了该对象就不能够再改变”，那么你应该选用 reference。 条款 2：最好使用 C++转型操作符 C++有 4个新的转型操作符（cast operators）：static_cast，const_cast，dynamic_cast 和 reinterpret_cast。 static_cast 基本上拥有与 C 旧式转型相同的威力与意义，以及相同的限制。 const_cast 用来改变表达式中的常量性（constness）或变易性（volatileness）。 dynamic_cast 用来执行继承体系中“安全的向下转型或跨系转型动作”。也就是说你可以利用 dynamic_cast，将“指向 base class objects的 pointers或references” 转型为“指向 derived class objects 的 pointers 或references”。如果转型失败，会以一个 null指针（当转型对象是指针）或一个 exception（当转型对象是 reference）表现出来。 reinterpret_cast 转换结果几乎总是与编译平台息息相关。所以 reinterpret_casts 不具移植性。reinterpret_cast 的最常用用途是转换“函数指针”类型。 1234567typedef void (*FuncPtr)();FuncPtr funcPtrArray[10];int doSomething();// funcPtrArray[0] = &amp;doSomething; // 错误！类型不匹配funcPtrArray[0] = reinterpret_cast&lt;FuncPtr&gt;(&amp;doSomething); 条款 3：绝对不要以多态（polymorphically）方式处理数组 多态（polymorphism）和指针运算不能混用。数组对象几乎总是会涉及指针的算术运算，所以数组和多态不要混用。原因之一，若发生通过父类指针删除一个子类对象，其结果未定义。 条款 4：非必要不提供 default constructor 添加无意义的 default constructors，也会影响 class 的效率。 如果使用 member functions 测试字段是否真被初始化了，其调用者便必须为测试行为付出时间代价，并为测试代码付出空间代价。万一测试结果为否定，对应的处理程序又需要一些空间代价。 如果可以自定义 class constructors 确保对象的所有字段都会被正确地初始化，上述所有成本便都可以免除。default constructors 无法提供这种保证，那么最好避免让 default constructors 出现。 二、操作符 条款 5：对定制的“类型转换函数”保持警觉 两种函数允许编译器执行类型隐式转换：单自变量 constructors 和 隐式类型转换操作符。 所谓单自变量 constructors 是指能够以单一自变量成功调用的 constructors。如此的 constructor 可能声明拥有单一参数，也可能声明拥有多个参数，并且除了第一参数之外都有默认值。 所谓隐式类型转换操作符，是一个拥有奇怪名称的member function：关键词operator 之后加上一个类型名称。 12345678910class Rational &#123;public: Rational(int numerator, int denominator = 1); operator double() const; ...&#125;;Rational r(1,2);Rational a = 1 * r; // constructors转换1为Rational r(1,1)double d = 0.5 * r; // double()转换r为0.5 隐式转换可能带来不易察觉的问题或者错误。 C++引入关键词 explicit，就是为了解决隐式类型转换带来的问题。其用法十分直接易懂，只要将constructors声明为 explicit，编译器便不能因隐式类型转换的需要而调用它们。不过显式类型转换仍是允许的。 对于隐式类型转换操作符，如非必要，最好不要设计，而是设计一个功能对等的成员函数，以供显示调用。 123456class Rational &#123;public: explicit Rational(int numerator, int denominator = 1); double asDouble() const; ...&#125;; 条款 6：自增(increment)、自减(decrement)操作符前缀形式与后缀形式的区别 重载函数是以其参数类型来区分彼此的，然而不论 increment 或 decrement 操作符的前置式或后置式，逻辑上都没有参数。为了做出区分，只好让后置式有一个 int 自变量，并且在它被调用时，编译器默默地为该 int 指定一个 0 值。 处理用户定制类型时，应该尽可能使用前置式 increment。 后置式 increment 和 decrement 操作符的实现应以其前置式兄弟为基础。方便维护，减少代码重复。 123456789101112131415161718192021class UPInt &#123;public: UPInt&amp; operator++(); // 前++ const UPInt operator++(int); // 后++ UPInt&amp; operator--(); const UPInt operator--(int); UPInt&amp; operator+=(int); ...&#125;;// prefix：increment and fetchUPInt&amp; UPInt::operator++() &#123; *this += 1; return *this;&#125; // postfix form: fetch and increment const UPInt UPInt::operator++(int) &#123; UPInt oldValue = *this; ++(*this); return oldValue; &#125; 条款 7：千万不要重载&amp;&amp;，||和，操作符 C++对于“真假值表达式”采用“骤死式”评估方式。意思是一旦该表达式的真假值确定，即使表达式中还有部分尚未检验，整个评估工作仍告结束。 “函数调用”语义和“骤死式”语义有两个重大的区别。 第一，当函数调用动作被执行，所有参数值都必须评估完成，所以当我们调用 operator&amp;&amp;和 operator||时，两个参数都已评估完成。换句话说没有什么骤死式语义。 第二，C++语言规范并未明确定义函数调用动作中各参数的评估顺序，所以没办法知道 expression1 和 expression2 哪个会先被评估。这与骤死式评估法形成一个明确的对比，后者总是由左向右评估其自变量。 C++同样也有一些规则用来定义逗号操作符面对内建类型的行为。表达式如果内含逗号，那么逗号左侧会先被评估，然后逗号的右侧再被评估；最后，整个逗号表达式的结果以逗号右侧的值为代表。 12// 其中 ++i, --j 表达式的结果是 --j 的值for (int i = 0, j = strlen(s)-1; i &lt; j; ++i, --j) ... 其他不能重载的操作符还有: 123. .* new delete :: sizeof typeid ?:static_cast dynamic_cast const_cast reinterpret_cast 可以重载： 123456789101112operator new operator deleteoperator new[] operator delete[]! + * / % ^ &amp; | ~ = &lt; &gt; += -= *= /= %=^= &amp;= |= &lt;&lt; &gt;&gt; &gt;&gt;= &lt;&lt;= == !=&lt;= &gt;= &amp;&amp; || ++ -- , -&gt;* -&gt;() [] 条款 8：了解各种不同意义的new和 delete new operator 1string *ps = new string(\"Memory Management\"); 以上使用的 new 是 new operator。这个操作符是由语言内建的，就像sizeof 那样，不能被改变意义，总是做相同的事情。它的动作分为两方面。 第一，它分配足够的内存，用来放置某类型的对象。第二，它调用一个 constructor，为刚才分配的内存中的那个对象设定初值。 new operator 总是做这两件事，无论如何你不能够改变其行为。 operator new 你能够改变的是用来容纳对象的那块内存的分配行为。new operator 调用某个函数，执行必要的内存分配动作，你可以重写或重载那个函数，改变其行为。这个函数的名称叫做 operator new。 1void * operator new(size_t size); 返回值类型是 void*，因为这个函数返回一个未经处理（raw）的指针，未初始化的内存。就象 malloc 一样，operator new 的职责只是分配内存。它对构造函数一无所知。 当你的编译器遇见这样的语句： 1string *ps = new string(\"Memory Management\"); 它生成的代码或多或少与下面的伪代码相似： 123void *memory = operator new(sizeof(string)); call string::string(\"Memory Management\") on *memory;string *ps = static_cast&lt;string*&gt;(memory); placement new 如果被调用的 operator new 除了接受“一定得有的 size_t 自变量”之外，还接受了一个 void＊ 参数，指向一块内存，准备用来接受构造好的对象。这样的operator new 就是 placement new。 1void * operator new(size_t, void *location); 总结 如果你希望将对象产生于 heap，请使用 new operator。它不但分配内存而且为该对象调用一个constructor。 如果你只是打算分配内存，请调用 operator new，那就没有任何 constructor 会被调用。 如果你打算在 heap objects 产生时自己决定内存分配方式，请写一个自己的 operator new，并使用 new operator，它将会自动调用你所写的 operator new。 如果你打算在已分配（并拥有指针）的内存中构造对象，请使用placement new。 三、异常 如果一个函数利用“设定状态变量”的方式或是利用“返回错误码”的方式发出一个异常信号，无法保证此函数的调用者会检查那个变量或检验那个错误码。于是程序的执行可能会一直继续下去，远离错误发生地点。 但是如果函数以抛出 exception 的方式发出异常信号，而该 exception 未被捕捉，程序的执行便会立刻中止。 如果你需要一个“绝对不会被忽略的”异常信号发射方法，而且发射后的 stack 处理过程又能够确保局部对象的 destructors 被调用，那么你需要 C++ exceptions。 条款 9：利用 destructors避免泄漏资源 解决办法就是，以一个“类似指针的对象”取代指针。如此一来，当这个类似指针的对象被销毁，我们可以令其 destructor 调用delete。“行为类似指针”的对象我们称为 smart pointers。 C++提供一个名为 auto_ptr 的智能指针。隐藏在 auto_ptr 背后的观念是，以一个对象存放“必须自动释放的资源”，并依赖该对象的destructor 释放。 只要坚持这个规则，把资源封装在对象内，通常便可以在 exceptions 出现时避免泄漏资源。 一个 auto_ptr 的实现示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960template &lt;typename T&gt; class autoPtr &#123;public: explicit autoPtr(T* p = 0); template &lt;typename U&gt; autoPtr(autoPtr&lt;U&gt;&amp; rhs); ~autoPtr(); template &lt;typename U&gt; autoPtr&lt;T&gt;&amp; operator=(autoPtr&lt;U&gt;&amp; rhs); T&amp; operator*() const; T* operator-&gt;() const; T* get() const; T* release(); void reset(T* p = 0);private: T* pointee; // 书上示例代码，这里有个特化模板友元，显然是个错误&#125;;template &lt;typename T&gt; autoPtr&lt;T&gt;::autoPtr(T* p) : pointee(p) &#123;&#125;template &lt;typename T&gt;template &lt;typename U&gt;autoPtr&lt;T&gt;::autoPtr(autoPtr&lt;U&gt;&amp; rhs) : pointee(rhs.release()) &#123;&#125;// operator= 使用 copy and swap 也可template &lt;typename T&gt;template &lt;typename U&gt;autoPtr&lt;T&gt;&amp; autoPtr&lt;T&gt;::operator=(autoPtr&lt;U&gt;&amp; rhs) &#123; if (this != rhs) reset(rhs.release()); return *this;&#125;template &lt;typename T&gt; autoPtr&lt;T&gt;::~autoPtr() &#123; delete pointee; &#125;template&lt;typename T&gt;T&amp; autoPtr&lt;T&gt;::operator*() const &#123; return *pointee; &#125;template&lt;typename T&gt;T* autoPtr&lt;T&gt;::operator-&gt;() const &#123; return pointee; &#125;template&lt;typename T&gt;T* autoPtr&lt;T&gt;::get() const &#123; return pointee; &#125;template&lt;typename T&gt;T* autoPtr&lt;T&gt;::release() &#123; T* oldPointee = pointee; pointee = 0; return oldPointee;&#125;template&lt;typename T&gt;void autoPtr&lt;T&gt;::reset(T* p) &#123; if (pointee != p) &#123; delete pointee; pointee = p; &#125;&#125; 条款 10：在 constructors内阻止资源泄漏（resource leak） C++只能析构被完全构造的对象（fully contructed objects）, 只有一个对象的构造函数完全运行完毕，这个对象才被完全地构造。若因为异常导致构造函数没有执行完毕，那么也不会调用析构函数。 由于 C++不自动清理那些“构造期间抛出exceptions”的对象，所以你必须设计你的constructors，使它们能够自我清理。 通常这只需将所有可能的 exceptions 捕捉起来，执行某种清理工作，然后重新抛出exception，使它继续传播出去即可。 另外，member initializaion list 是在构造函数之前进行的，所以可以利用这一点，可以让某系操作在构造函数之前进行，并处理异常。比如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445class BookEntry &#123; public: ...private: string theName; string theAddress; list&lt;PhoneNumber&gt; thePhones; // Image *theImage; // AudioClip *theAudioClip; Image * initImage(const string&amp; imageFileName); AudioClip * initAudioClip(const string&amp; audioClipFileName);&#125;; BookEntry::BookEntry(const string&amp; name, const string&amp; address, const string&amp; imageFileName, const string&amp; audioClipFileName): theName(name), theAddress(address), theImage(initImage(imageFileName)), theAudioClip(initAudioClip(audioClipFileName))&#123;&#125;// theImage 被首先初始化,所以即使这个初始化失败也 // 不用担心资源泄漏，这个函数不用进行异常处理。Image * BookEntry::initImage(const string&amp; imageFileName) &#123; if (imageFileName != \"\") return new Image(imageFileName); else return 0;&#125;// theAudioClip 被第二个初始化, 所以如果在 theAudioClip // 初始化过程中抛出异常，它必须确保 theImage 的资源被释放。 // 因此这个函数使用 try...catch 。 AudioClip * BookEntry::initAudioClip(const string&amp;, audioClipFileName)&#123; try &#123; if (audioClipFileName != \"\") return new AudioClip(audioClipFileName); else return 0; &#125; catch (...) &#123; delete theImage; throw; &#125;&#125; 如果你以 auto_ptr 对象来取代 pointer class members，免除了“exceptions 出现时发生资源泄漏”的危机，不再需要在 destructors 内亲自动手释放资源。 12345678910111213141516class BookEntry &#123; public: ...private: ... const auto_ptr&lt;Image&gt; theImage; const auto_ptr&lt;AudioClip&gt; theAudioClip; &#125;;BookEntry::BookEntry(const string&amp; name, const string&amp; address, const string&amp; imageFileName, const string&amp; audioClipFileName): theName(name), theAddress(address), theImage(imageFileName != \"\" ? new Image(imageFileName) : 0),theAudioClip(audioClipFileName != \"\" ? new AudioClip(audioClipFileName) : 0)&#123;&#125; 条款 11：禁止异常（exceptions）流出destructors之外 两种情况下 destructor 会被调用。 第一种情况是当对象在正常状态下被销毁，也就是当它离开了它的生存空间（scope）或是被明确地删除； 第二种情况是当对象被 exception 处理机制销毁，也就是exception 传播过程中的 stack-unwinding（栈展开）机制。 如果控制权基于 exception 的因素离开 destructor，而此时正有另一个 exception 处于作用状态，C++会调用 terminate 函数，将你的程序结束掉，甚至不等局部对象被销毁。 全力阻止 exceptions 传出 destructors之外： 第一，它可以避免 terminate 函数在 exception 传播过程的栈展开（stack-unwinding）机制中被调用； 第二，它可以协助确保 destructors 完成其应该完成的所有事情。 条款 12：了解“抛出一个exception”与“传递一个参数”或“调用一个虚函数”之间的差异 第一，exception objects 总是会被复制，如果以 by value 方式捕捉，它们甚至被复制两次。至于传递给函数参数的对象则不一定得复制。 第二，“被抛出成为 exceptions”的对象，相比于“被传递到函数去”的对象，其合法的类型转换更少。 第三，catch 子句以其“出现于源代码的顺序”被编译器检验比对，其中第一个匹配成功者便执行；而当我们以某对象调用一个虚函数，被选中执行的是那个“与对象类型最佳吻合”的函数，不论它是不是源代码所列的第一个。 条款 13：以 by reference方式捕捉 exceptions 如果 catch by reference， 可以避开对象删除问题； 可以避开 exception objects 的切割（slicing）问题； 可以保留捕捉标准 exceptions 的能力； 约束了 exception objects 需被复制的次数。 12345try &#123; ...&#125; catch (exception&amp; ex) &#123; ...&#125; 条款 14：明智运用 exception specification 1234567void f1(); // 可以抛出任意的异常// exception specification 声明其只能抛出 int 类型的异常void f2() throw(int) &#123; ... f1(); // 即使 f1 抛出不是 int 类型的异常，也是合法的 ... &#125; 结论是： 不应该将 templates 和 exception specifications 混合使用。 如果A 函数内调用了 B 函数，而 B 函数无 exception specifications，那么 A 函数本身也不要设定exception specifications。 处理“系统”可能抛出的exceptions。其中最常见的就是 bad_alloc，那是在内存分配失败时由operator new 和 operator new[]抛出的。 条款 15：了解异常处理（exception handling）的成本 为了能够在运行时期处理 exceptions，程序必须做大量记录工作。在每一个执行点，它们必须能够确认“如果发生 exception，哪些对象需要析构”，它们必须在每一个 try 语句块的进入点和离开点做记号，针对每个 try 语句块它们必须记录对应的 catch 子句及能够处理的 exceptions 类型。 try 语句块，无论何时使用它，都得为此付出代价。不同的编译器实现 try 块的方法不同，所以编译器与编译器间的开销也不一样。粗略地估计，如果你使用 try 块，代码将膨胀5％－10％并且运行速度也同比例减慢。exception specification 通常也有与 try 块一样多的系统开销。 如果是因为异常而导致函数返回，函数的执行速度通常会比正常情况下慢 3 个数量级。当然，只有在抛出 exception 时才会承受这样的开销。 四、效率 条款 16：谨记 80-20 法则 80-20 法则说：一个程序 80%的资源用于 20%的代码身上。是的，80%的执行时间花在大约 20%的代码身上，80%的内存被大约 20%的代码使用，80%的磁盘访问动作由 20%的代码执行，80%的维护力气花在 20%的代码上面。 软件的整体性能几乎总是由代码的一小部分决定。 条款 17：考虑使用 lazy evaluation（缓式评估） lazy evaluation（缓式评估）。延缓运算，直到那些运算结果刻不容缓地被迫切需要为止。如果其运算结果一直不被需要，运算也就一直不执行。 在你真正需要之前，不必着急为某物做一个副本。在某些应用领域，你常有可能永远不需要提供那样一个副本。 实现 lazy fetching 时，你必须面对一个问题：null 指针可能会在任何 member functions（包括const member functions）内被赋值，以指向真正的数据。然而当你企图在 const member functions 内修改 data members，编译器不会同意。除非将指针字段声明为 mutable。 lazy evaluation 在许多领域中都可能有用途：可避免非必要的对象复制，可区别 operator[] 的读取和写动作，可避免非必要的数据库读取动作，可避免非必要的数值计算动作。 条款 18：分期摊还预期的计算成本 另一种改善软件性能的方法是：令它超前进度地做“要求以外”的更多工作。该方法可称为超急评估（over-eager evaluation）：在被要求之前就先把事情了。 Over-eager evaluation 背后的观念是，如果你预期程序常常会用到某个计算，你可以降低每次计算的平均成本，办法就是设计一份数据结构以便能够极有效率地处理需求，比如实时更新max、min等值，当需要使用时直接取值，而不用计算。 Caching 是“分期摊还预期计算之成本”的一种做法，Prefetching（预先取出）则是另一种做法。 这些思想很常见很有用，cache 自不用多说。对于 prefetching，比如，prefetch内存数据时，总是按页大小进行成块取数据，局部性原理告诉我们相邻的数据通常会更可能被需要。有时对象太大超过页大小，就会增加换页活动，缓存命中率下降，造成性能损失。 可通过over-eager evaluation，如 caching 和 prefetching 等做法分摊预期运算成本，这和 lazy evaluation 并不矛盾。 当你必须支持某些运算而其结果并不总是需要的时候，lazy evaluation 可以改善程序效率。 当你必须支持某些运算而其结果几乎总是被需要，或其结果常常被多次需要的时候，over-eager evaluation 可以改善程序效率。 条款 19：了解临时对象的来源 C++ 临时对象是不可见的，不会在你的源代码中出现。只要你产生一个 non-heap object 而没有为它命名，便诞生了一个临时对象。 这种匿名对象通常发生于两种情况：一是当隐式类型转换（implicit type conversions）时产生，以求函数调用能够成功；二是当函数返回对象的时候。 隐式类型转换 123456size_t countChar(const string&amp; str, char ch);...char buffer[MAX_STRING_LEN];char c;...int ret = countChar(buffer, c); 看一下 countChar 的调用。第一个被传送的参数是字符数组，但是对应函数的正被绑定 的参数的类型是 const string&amp;。仅当消除类型不匹配后，才能成功进行这个调用。 编译器会建立一个 string 类型的临时对象。通过以 buffer 做为参数调用 string 的构造函数来初始化这个临时对象。countChar 的参数 str 被绑定在这个临时的 string 对象上。当 countChar 返回时，临时对象自动释放。 仅当通过传值（by value）方式传递对象 或 传递常量引用（reference-to-const）参数时，才会发生这些类型转换。当传递一个非常量引用（reference-to-non-const）参数对象，就不会发生。比如： 1234void uppercasify(string&amp; str);...char subtleBookPlug[] = \"Effective C++\"; uppercasify(subtleBookPlug); 这里假如产生一个临时对象，uppercasify会对string&amp;所指的临时对象进行修改，而不是对subtleBookPlug字符数组进行修改，显然和uppercasify函数设计的本意是不符合的，这显然是一个错误，却不易察觉。 所以，C++语言禁止为非常量引用（reference-to-non-const） 产生临时对象。以上情况并不会发生。 函数返回对象 1const Number operator+(const Number&amp; lhs, const Number&amp; rhs); 这个函数的返回值是临时的，因为它没有被命名，它只是函数的返回值。你必须为每次调用operator+ 构造和释放这个对象而付出代价。有时可以通过 返回值优化（return value optimization）可以将这个临时对象消灭掉。 总结 任何时候只要你看到一个 reference-to-const 参数，就极可能会有一个临时对象被产生出来绑定至该参数上。 任何时候只要你看到函数返回一个对象，就会产生临时对象（并于稍后销毁）。 条款 20：协助完成“返回值优化（RVO）” 可以用某种特殊写法来撰写函数，使它在返回对象时，能够让编译器消除临时对象的成本。 方法是：返回 constructor arguments 以取代对象。 123456789101112// 错误方法const Rational&amp; operator*(const Rational&amp; lhs, const Rational&amp; rhs) &#123; Rational result(lhs.numerator() * rhs.numerator(), lhs.denominator() * rhs.denominator()); return result; // 返回时，其指向的对象已经不存在了 &#125;// 正确方法const Rational operator*(const Rational&amp; lhs, const Rational&amp; rhs) &#123; return Rational(lhs.numerator() * rhs.numerator(), lhs.denominator() * rhs.denominator());&#125; 虽然仍旧必须为在函数内临时对象的构造和释放而付出代价。但是此时，编译器可以进行优化了。 1Rational c = a * b; 编译器会消除在 operator* 内的临时变量和 operator* 返回的临时变量。直接在 c 的内存里构造 return 表达式定义的对象。调用 operator* 的临时对象的开销就是零：没有建立临时对象。 利用函数的 return 点消除一个局部临时对象，这种方法很常见，被称之为 return value optimization。 条款 21：利用重载技术（overload）避免隐式类型转换（implicit type conversion） 在重载操作符时，每个重载函数的参数必须至少一个是“用户定制类型”的自变量。如果不是，就会改变C++内部预先定义的操作符意义（参数类型全是内置类型），而那当然会导致天下大乱。 12345678910// 合理的设计const UPInt operator+(const UPInt&amp; lhs, const UPInt&amp; rhs);const UPInt operator+(const UPInt&amp; lhs, int rhs);const UPInt operator+(int lhs, const UPInt&amp; rhs);// 错误const UPInt operator+(int lhs, int rhs); 增加一堆重载函数不一定是好事，除非能保证这样对程序效率有很大的改善。 条款 22：考虑以操作符复合形式（op=）取代其独身形式（op） 一个好方法就是以复合形式（例如，operator+=）为基础实现独身形式（例如，operator+）。 3 个与效率有关的情况值得注意。 第一，一般而言，复合操作符比其对应的独身版本效率高。因为独身版本通常必须返回一个新对象，而我们必须因此负担一个临时对象的构造和析构成本（见条款 19和 20）。复合版本则是直接将结果写入其左端自变量，所以不需要产生一个临时对象来放置返回值。 第二，如果同时提供某个操作符的复合形式和独身形式，那就是在允许你的客户在效率与便利性之间自行取舍。 第三、匿名对象总是比命名对象更容易被消除，所以当你面临命名对象或临时对象的抉择时，最好选择临时对象。匿名对象有可能降低成本（尤其在搭配旧式编译器时）。 条款 23：考虑使用其他程序库 不同的程序库即使提供相似的功能，也往往有不同的性能取舍策略，所以一旦你找出程序的瓶颈，你应该思考是否有可能使用其他程序库，来移除了那些瓶颈。 比如，iostream 相比于 stdio，iostream 有类型安全的特性，可扩展性好；而 stdio 更节省程序空间、速度更快。 条款 24：了解 virtual functions、multiple inheritance、virtual base class、runtime type identification的成本 当一个虚函数被调用，执行的代码必须对应于“调用者（对象）的动态类型”。 大部分编译器使用 virtual tables（vtbls）和 virtual table pointers（vptrs）实现动态类型。 virtual tables（vtbls） vtbl 通常是一个由“函数指针”数组。某些编译器会以链表（linked list）取代数组，但其基本策略相同。程序中的每一个class ，只要声明（或继承）虚函数者，都有自己的一个 vtbl，而其中的条目（entries）就是该 class 的各个虚函数具体实现的指针。 虚函数的第一个成本：你必须为每个拥有虚函数的 class 耗费一个 vtbl 空间，其大小视虚函数的个数（包括继承而来的）而定。 12345678910class C1 &#123; public: C1(); virtual ~C1(); virtual void f1(); virtual int f2(char c) const; virtual void f3(const string&amp; s); void f4() const; ...&#125;; C1 的 virtual table 数组看起来如下图所示: 注意非虚函数 f4 不在表中，而且 C1 的构造函数也不在。 12345678class C2: public C1 &#123; public: C2(); virtual ~C2(); virtual void f1(); virtual void f5(char *str); ... &#125;; 它的 virtual table 中包括指向没有被 C2 重定义的 C1 虚函数的指针： virtual table pointers（vptrs） Virtual tables 只是虚函数实现机构的一半而已。如果只有它，不能成气候。还需要某种方法可以指示出每个对象对应于哪一个 vtbl，vtbl 才真的有用。而这正是virtual table pointer（vptr）的任务。 凡声明有虚函数的 class，其对象都含有一个隐藏的 data member，vptr，用来指向该class 的 vtbl。这个隐藏的 data member 被编译器加入对象内某个只有编译器才知道的位置。 虚函数的第二个成本：你必须在每一个拥有虚函数的对象内付出“一个额外指针”的代价。 上述C1、C2对象关系可以表示为： 虚函数的调用 编译器必须产生代码，完成以下动作： 根据对象的 vptr 找出其 vtbl。编译器成本只有一个偏移调整（offset adjustment）就能获得 vptr，和一个指针间接动作，以便获得 vtbl。 找出被调用函数在 vtbl 内的对应指针。编译器为每个虚函数指定了一个独一无二的表格索引。本步骤的成本只是一个偏移（offset），在 vtbl 数组中索引。 调用步骤 2 所得指针所指向的函数。 RTTI 运行时期类型辨识（runtime typeidentification，RTTI）的成本。RTTI 让我们得以在运行时期获得 objects 和 class 的相关信息。它们被存放在类型为 type_info 的对象内。你可以利用 typeid 操作符取得某个class 相应的 type_info 对象。 C++规范书上说，只有当某种类型拥有至少一个虚函数，才保证我们能够检验该类型对象的动态类型。RTTI 的设计理念是：根据 class 的 vtbl 来实现。 RTTI 耗费的空间是在每个类的 vtbl 中的占用的额外单元再加上存储 type_info 对象的空间。就像在多数程序里 virtual table 所占的内存空间并不值得注意一样，你也不太可能因为 type_info 对象大小而遇到问题。 RTTI的代价：type_info 占用的空间。 假如 type_info 才是完整的 vtbl 内存布局： 多继承 多继承经常导致对虚基类的需求。 没有虚基类，如果一个派生类有一个以上从基类的继承路径，基类的数据成员被复制到每一个继承类对象里，继承类与基类间的每条路径都有一个拷贝。 把基类定义为虚基类则可以消除这种复制。 虚基类的实现经常使用指向虚基类的指针做为避免复制的手段，一个或者更多的指针被存储在对象里。 另一种代价：虚基类的实现经常使用指向虚基类的指针。 比如： 1234class A: &#123;...&#125;;class B: virtual public A &#123; ... &#125;;class C: virtual public A &#123; ... &#125;;class D: public B, public C &#123;...&#125;; 如果 A 中没有虚函数，D对象内存布局为： 如果 A 中有虚函数，D对象内存布局为： 五、技术 条款 25：将 constructor 和 non-member functions 虚化 此处所谓 virtual 不是虚函数的 virtual，而是类似、形似的意思。 模仿 constructor 的行为，但能够视其获得的输入，产生不同类型的对象，所以称之为 virtual constructor。Virtual constructor 在许多情况下有用，其中之一就是从磁盘（或网络或磁带等）读取对象信息。 例如，假设你编写一个程序，用来进行新闻报道的工作，每一条新闻报道都由文字或图片组成。 12345678910111213141516171819202122232425262728class NLComponent &#123; // 抽象基类，包含至少一个纯虚函数public: ...&#125;; class TextBlock: public NLComponent &#123; public: ... // 不包含纯虚函数&#125;; class Graphic: public NLComponent &#123; public: ... // 不包含纯虚函数&#125;; class NewsLetter &#123; public: NewsLetter(istream&amp; str); ...private: list&lt;NLComponent*&gt; components; // virtual constructor // 为建立下一个 NLComponent 对象从 str 读取数据 // 建立 component 并返回一个指针 static NLComponent* readComponent(istream&amp; str);&#125;;NewsLetter::NewsLetter(istream&amp; str) &#123; while (str) &#123; components.push_back(readComponent(str));&#125; &#125; readComponent 所做的工作。它根据所读取的数据建立了一个新对象，或是 TextBlock 或是 Graphic。 virtual copy constructor 是一种特别的 virtual constructor 。Virtual copy constructor 会返回一个指针，指向其调用者（某对象）的一个新副本。基于这种行为，virtual copy constructors 通常以 copySelf 或cloneSelf 命名，或者像下面一样命名为 clone。 12345678910111213141516171819202122232425262728293031323334class NLComponent &#123; // 抽象基类，包含至少一个纯虚函数public: virtual NLComponent * clone() const = 0; ...&#125;; class TextBlock: public NLComponent &#123; public: virtual TextBlock * clone() const &#123; return new TextBlock(*this); &#125; ...&#125;; class Graphic: public NLComponent &#123; public: virtual Graphic * clone() const &#123; return new Graphic(*this); &#125; ... &#125;;class NewsLetter &#123; public: NewsLetter(istream&amp; rhs); ...private: list&lt;NLComponent*&gt; components;&#125;;NewsLetter::NewsLetter(const NewsLetter&amp; rhs) &#123; // 遍历整个 rhs 链表，使用每个元素的虚拟拷贝构造函数 for (list&lt;NLComponent*&gt;::const_iterator it = rhs.components.begin(); it != rhs.components.end(); ++it) &#123; components.push_back((*it)-&gt;clone()); &#125;&#125; non-member functions 也可以进行虚化。编写一个虚函数来完成工作，然后再写一个非虚函数，它什么也不做只是调用这个虚函数。 1234567891011121314151617181920class NLComponent &#123; // 抽象基类，包含至少一个纯虚函数public: virtual ostream&amp; print(ostream&amp; s) const = 0; ...&#125;; class TextBlock: public NLComponent &#123; public: virtual ostream&amp; print(ostream&amp; s) const; ...&#125;; class Graphic: public NLComponent &#123; public: virtual ostream&amp; print(ostream&amp; s) const; ... &#125;;// 非虚函数，只调用虚 print，让print完成对应工作inline ostream&amp; operator&lt;&lt;(ostream&amp; s, const NLComponent&amp; c) &#123; return c.print(s); &#125; 条款 26：限制某个 class 所能产生的对象数量 每产生一个对象，会有一个 constructor被调用。 使用 static 控制对象数量的产生，是一种方法。首先要知道： class 拥有一个static成员对象时，即使从未使用到，也会被构造，且其初始化时机，并不明确； function 中有一个static对象，此对象在函数第一次被调用时才产生，且其初始化时机是明确的。 “阻止某个 class 产出对象” 的最简单方法就是将其 constructor 声明为 private。 一个限制对象数量的 class 设计，一个 Printer 对象实现： 12345678910111213141516171819202122232425262728293031323334353637383940class Printer &#123;public: class TooManyObjects&#123;&#125;; // 伪构造函数 static Printer * makePrinter(); static Printer * makePrinter(const Printer&amp; rhs); ...private: static size_t numObjects; static const size_t maxObjects = 10; Printer(); Printer(const Printer&amp; rhs); ~Printer() &#123; --numObjects; &#125;&#125;;// class static 必须进行定义size_t Printer::numObjects = 0; const size_t Printer::maxObjects;// 提取一个 init() 函数完成公用的初始化工作也是可以的Printer::Printer() &#123; if (numObjects &gt;= maxObjects) &#123; throw TooManyObjects(); &#125; ++numObjects; ...&#125; Printer::Printer(const Printer&amp; rhs) &#123; if (numObjects &gt;= maxObjects) &#123; throw TooManyObjects(); &#125; ++numObjects; ...&#125; Printer * Printer::makePrinter() &#123; return new Printer; &#125; Printer * Printer::makePrinter(const Printer&amp; rhs) &#123; return new Printer(rhs); &#125; 条款 27：要求（或禁止）对象产生于 heap 之中 有时你想这样管理某些对象，要让某种类型的对象能够自我销毁，也就是能够“delete this”。很明显这种管理方式需要此类型对象被分配在堆中。而其它一些时候你想获得一种 保障：“不在堆中分配对象，从而保证某种类型的类不会发生内存泄漏。” 判断对象是否在堆上，可以使用地址比较法，栈段地址从高到低生长，堆段地址从低到高生长。以下方法可以实现： 1234bool onHeap(const void *address) &#123; char onTheStack; return address &lt; &amp;onTheStack; &#125; 但是，static 对象的地址在堆段地址下方，以上方法并不能确定是否是静态对象。 另一种方式，是设计 abstract mixin base class 来实现判断堆对象的功能。 所谓 abstract base class 是一个不能够被实例化的 base class。也就是说它至少有一个纯虚函数。所谓 mixin（“mix in”）class 则提供一组定义完好的能力，能够与其 derived class 所可能提供的其他任何能力兼容。如此的 class 几乎总是abstract。于是可以设计 abstract mixin base class，用来为 derived class 提供“判断某指针是否以 oeprator new 分配出来”的能力。 1234567891011121314151617181920212223242526272829303132333435363738class HeapTracked &#123; // 混合类，跟踪从 operator new 返回的 ptrpublic: class MissingAddress&#123;&#125;; virtual ~HeapTracked() = 0; static void *operator new(size_t size); static void operator delete(void *ptr); bool isOnHeap() const;private: typedef const void* RawAddress; static list&lt;RawAddress&gt; addresses;&#125;;list&lt;RawAddress&gt; HeapTracked::addresses;// HeapTracked 的析构函数是纯虚函数，使得该类变为抽象类HeapTracked::~HeapTracked() &#123;&#125;void * HeapTracked::operator new(size_t size) &#123; void *memPtr = ::operator new(size); addresses.push_front(memPtr); return memPtr;&#125;void HeapTracked::operator delete(void *ptr) &#123; list&lt;RawAddress&gt;::iterator it = find(addresses.begin(), addresses.end(), ptr); if (it != addresses.end()) &#123; addresses.erase(it); ::operator delete(ptr); &#125; else &#123; throw MissingAddress(); &#125;&#125;bool HeapTracked::isOnHeap() const &#123; // 得到一个指针，指向*this占据的内存空间的起始处 const void *rawAddress = dynamic_cast&lt;const void*&gt;(this); list&lt;RawAddress&gt;::iterator it = find(addresses.begin(), addresses.end(), rawAddress); return it != addresses.end();&#125; 如果是在堆上分配内存，就会调用 operator new，可以通过 isOnHeap 判断是否在堆上。只要继承自 HeapTracked 类的子类，就都有了 isOnHeap 的功能。 如果要禁止对象在堆上分配内存，将 operator new 设计为 private 是一种简单的方式。 条款 28：Smart Pointers（智能指针） 当你以 smart pointers 取代 C++的内建指针（亦即 dumb pointers），你将获得以下各种指针行为的控制权： 构造和析构（Construction and Destruction）。你可以决定smart pointer 被产生以及被销毁时发生什么事。通常我们会给 smart pointers 一个默认值 nullptr，以避免“指针未获初始化”的头痛问题。某些 smart pointers 可以删除它们所指的对象，比如当指向该对象的最后一个 smart pointer 被销毁时。这是消除资源泄漏问题的一大进步。 复制和赋值（Copying and Assignment）。当一个 smart pointer 被复制或涉及赋值动作时，你可以控制发生什么事。某些 smart pointer 会希望在此时刻自动为其所指之物进行复制或赋值动作，也就是执行深复制（deep copy）。另一些 smart pointer 则可能只希望指针本身被复制或赋值就好。还有一些则根本不允许复制和赋值。不论你希望什么样的行为，smart pointer 都可以让你如愿。 解引（Dereferencing）。当 client 解引（取用）smart pointer 所指之物时，你有权决定发生什么事情。例如你可以利用 smart pointer 协助实现出条款 17 所说的 lazy fetching 策略。 Smart pointer的构造行为通常明确易解：确定一个目标物（通常是利用smart pointer 的 constructor 参数），然后让 smart pointer 内部的 dumb pointer 指向它。如果尚未决定目标物，就将内部指针设为 nullptr，或是发出一个错误消息（可能是抛出 exception）。 Smart pointer 不要提供对 dumb pointer 的隐式转换操作符，除非不得已。 在涉及继承相关的类型转换时，smart pointer 是做不到 dumb pointer 所能做的一切的。此时，别使用 smart pointer ，而是 dumb pointer 。 条款 29：Reference counting（引用计数） 通过 reference counting 可以建构出垃圾回收机制（garbage collection）的一个简单形式。Reference counting 的另一个发展动机则只是为了实现一种常识。如果许多对象有相同的值，将那个值存储多次是件愚蠢的事。最好是让所有等值对象共享一份实值就好。 copy-on-write “和其他对象共享一份实值，直到我们必须对自己所拥有的那一份实值进行写动作，才进行复制”，这就是：copy-on-write（写时才复制）。 特别是在操作系统领域，各进程（processes）之间往往允许共享某些内存分页（memory pages），直到它们打算修改属于自己的那一分页，才进行复制。这是提升效率的一般化做法（也就是 lazy evaluation，条款 17）。 实现 首先产生一个 base class RCObject，作为“reference-counted 对象”之用。RCObject 组成为： “引用计数器” 增减计数值的函数 一个函数，用来将不再被使用（也就是其引用次数为 0）的对象值销毁掉。 一个成员，用来追踪资源是否“可共享”，并提供查询其值、将该成员设为 false 等相关函数。在默认情况下为可共享状态。一旦某个对象被贴上“不可共享”标签，就没有办法再恢复其“可共享”的身份了。 其他 简单地说，以下是使用 reference counting 改善效率的最适当时机： 相对多数的对象共享相对少量的实值。这种共享行为通常是通过assignment operators 和 copy constructors。 对象实值的产生或销毁成本很高，或是资源占用内存很多。若实值（资源）可被多个对象共享，reference counting 能带来较高收益。 RCObject 的设计目的是用来作为有引用计数能力之“实值对象”的基类。 那些“实值对象”即实际的资源，设计 RCPtr smart pointer 进行管理（RAII保证）。 RCObject、RCPtr 不应该被外界看到，应为私有成员，以限制其用途。 条款 30：Proxy class（代理类） 凡“用来代表（象征）其他对象”的对象，常被称为 proxy objects（替身对象），而用以表现 proxy objects 者，我们称为 proxy class。 当 class 的身份从“与真实对象”移转到“与替身对象（proxies）”，往往会造成 class 语义的改变，因为 proxy objects 所展现的行为常常和真正对象的行为有些隐微差异。 在很多情况下，proxy 对象可以完美替代实际对象。当它们可以工作时，意味着两者间的差异并不影响什么。 多维数组 优化二维数组的使用形式： 12345678910111213141516171819template&lt;class T&gt;class Array2D &#123; public: class Array1D &#123; public: T&amp; operator[](int index); const T&amp; operator[](int index) const; ... &#125;; Array1D operator[](int index); const Array1D operator[](int index) const; ...&#125;;// Array1D 使得 data[][] 访问合法，否则只实现 Array2D 的话// 势必只能 data(dim1, dim2) 这样调用Array2D&lt;float&gt; data(10, 20);...cout &lt;&lt; data[3][6]; 左值/右值的区分 operator[]可以在两种不同的情况下调用：读一个字符或写一个字符。读是个 右值操作；写是个左值操作。（这个名词来自于编译器，左值出现在赋值运算的左边，右值 出现在赋值运算的右边。） 通常，将一个对象做左值使用意味着它可能被修改，做右值用意 味着它不能够被修改。 虽然或许不可能知道 operator[] 是在左值或右值情境下被调用，我们还是可以区分读和写。只要将处理动作推迟，直至知道 operator[] 的返回结果将如何被使用为止。（lazy evaluation） Proxy class 可是实现此 lazy evaluation。可以修改 operator[]，令它返回字符串中字符的 proxy，而不返回字符本身。然后等待，看看这个 proxy 如何被运用。如果它被读，就将 operator[] 的调用动作视为一个读取动作。如果它被写，就将 operator[] 的调用视为一个写动作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class String &#123; public: class CharProxy &#123; public: CharProxy(String&amp; str, int index); CharProxy&amp; operator=(const CharProxy&amp; rhs); CharProxy&amp; operator=(char c); operator char() const; private: String&amp; theString; int charIndex; &#125;; const CharProxy operator[](int index) const &#123; return CharProxy(const_cast&lt;String&amp;&gt;(*this), index); &#125; // for const Strings CharProxy operator[](int index) &#123; return CharProxy(*this, index); &#125; // for non-const Strings ... friend class CharProxy; private: // Reference counting ptr RCPtr&lt;StringValue&gt; value;&#125;;String::CharProxy::CharProxy(String&amp; str, int index) : theString(str), charIndex(index) &#123;&#125;String::CharProxy::operator char() const &#123; return theString.value-&gt;data[charIndex];&#125;String::CharProxy::operator=(const CharProxy&amp; rhs) &#123; // copy on write if (theString.value-&gt;isShared()) &#123; theString.value = new StringValue(theString.value-&gt;data); &#125; theString.value-&gt;data[charIndex] = rhs.theString.value-&gt;data[rhs.charIndex]; return *this;&#125;String::CharProxy&amp; String::CharProxy::operator=(char c) &#123; if (theString.value-&gt;isShared()) &#123; theString.value = new StringValue(theString.value-&gt;data); &#125; theString.value-&gt;data[charIndex] = c; return *this;&#125; 123456789101112131415String s1, s2;// 表达式s1[5]返回的是一 CharProxy 对象。没有为这样的对象&lt;&lt;操作// 在 CahrProxy 类内部申明了一个隐式转换到 char 的操作。// 这个 CharProxy-to-char 的转换是代理对象作右值使用时的典型行为。cout &lt;&lt; s1[5];// 表达式s2[5]返回的是一个 CharProxy 对象，作为赋值操作的目标。 // 调用的是 CharProxy 类中的赋值操作。// 在 CharProxy 的赋值操作中，被赋值的 CharProxy 对象是作左值使用的// proxy 类扮演的字符是作左值使用的s2[5] = 'x';// 左边是一个左值，右边一个作右值s1[3] = s2[8]; 条款 31：让函数根据一个以上的对象类型来决定如何虚化 假设你必须以 C++完成任务，也就是你必须自行想办法完成上述需求（常被称为 double-dispatching）。此名称来自面向对象程序设计社区，在那个领域里，人们把一个“虚函数调用动作”称为一个“message dispatch”（消息分派）。 因此某个函数调用如果根据两个参数而虚化（两个参数发生动态类型绑定），自然而然地就被称为“double dispatch”。更广泛的情况（函数根据多个参数而虚化）则被称为 multiple dispatch。 虚函数+ RTTI（运行时期类型辨识），根据不同的 typeid() 结果，进行条件判断实现不同处理逻辑。 只使用虚函数，在两个类型中，分别按照 single dispatch 的方式处理，然后组合使用。比RTTI方法更安全。 自行仿真虚函数表格（Virtual Function Tables），略 六、杂项讨论 条款 32：在未来时态下发展程序 所谓在未来时态下设计程序，就是接受“事情总会改变”的事实，并准备应对方法。 也许程序库会加入新的函数，导致新的重载（overloading）发生，于是导致潜在的歧义。 也许继承体系会加入新的 class，致使今天的 derived class 成为明天的 base class。 也许新的应用软件会出现，函数会在新的环境下被调用，而我们必须考虑那种情况下仍能正确执行任务。 程序的维护者通常都不是当初的开发者，所以设计和实现时应该注意到如何帮助其他人理解、修改、强化你的程序。 未来式思维只不过是加上一些额外的考虑： 提供完整的 class，即使某些部分目前用不到。当新的需求进来，你不太需要回头去修改那些 class。 设计你的接口，让这些 class 轻易地被正确运用，难以被错误运用。例如，面对那些“copying 和 assignment 并不合理”的 class，请禁止那些动作的发生。 尽量使你的代码一般化（泛化），除非有不良的巨大后果。举个例子，如果你正在写一个算法，用于树状结构（tree）的来回遍历，请考虑将它一般化，以便能够处理任何种类的 directed acyclic（非环状的）graph。 使用设计模式封装变化。 条款 33：将非尾端类（non～leaf class）设计为抽象类（abstract class） 将函数声明为纯虚函数，并非暗示它没有实现，而是意味着： 目前这个 class 是抽象的。 任何继承此 class 的具体类，都必须将该纯虚函数重新声明为一个正常的虚函数（也就是说，不可以再令它“=0”）。 的确，大部分纯虚函数并没有实现码，但是 pure virtual destructors 是个例外。它们必须被实现出来，因为只要有一个 derived classdestructor 被调用，它们便会被调用。此外，它们通常执行一些有用的工作，如释放资源或记录运转消息等等。纯虚函数的实现或许并不常见，但对 pure virtual destructors 而言，实现不仅是平常的事，甚至是必要的事。 一般性的法则是：继承体系中的 non-leaf（非尾端）类应该是抽象类。 条款 34：如何在同一个程序中结合 C++和 C 有 4 件事情你需要考虑：name mangling（名称重整）、statics（静态对象）初始化、动态内存分配、数据结构的兼容性。 Name Mangling（名称重整） Name mangling 是一种程序。通过它，你的 C++编译器为程序内的每一个函数编出独一无二的名称。在 C 语言中，此程序并无必要，因为你无法将函数名称重载（overload）；但是几乎所有的 C++程序都有一些函数拥有相同的名称。 Statics 的初始化 许多代码会在 main之前和之后执行起来。更明确地说，static class 对象、全局对象、namespace 内的对象以及文件范围（file scope）内的对象，其 constructors 总是在 main 之前就获得执行。这个过程称为static initialization。同样道理，通过 static initialization 产生出来的对象，其destructors 必须在 static destruction 过程中被调用。static destruction 发生在 main 结束之后。 动态内存分配 动态内存分配的一般规则很简单：程序的 C++部分使用 new 和delete，程序的 C 部分则使用 malloc（及其变种）和free。只要内存是以 new 分配而得，就以 delete 删除。只要内存是以 malloc 分配而得，就以 free 释放。 数据结构的兼容性 从数据结构的观点来看，我们可以说：在 C 和 C++之间对数据结构做双向交流，应该是安全的——前提是那些结构的定义式在 C 和C++ 中都可编译。为 C++ struct 加上非虚函数，可能不影响其兼容性；其他任何改变则几乎都会影响。 准则 如果你打算在同一个程序中混用 C++和 C，请记住以下几个简单守则： 确定你的 C++和 C 编译器产出兼容的目标文件（object files）。 将双方都使用的函数声明为 extern ＂C＂。 如果可能，尽量在 C++ 中撰写 main。 总是以 delete 删除 new返回的内存；总是以 free 释放 malloc 返回的内存。 将两个语言间的“数据结构传递”限制于 C 所能了解的形式。 条款 35：让自己习惯于标准 C++语言 C++最重要的几项改变如下所示（时间在C++11之前）。 增加了一些新的语言特性：RTTI、namespaces、bool、关键词mutable 和explicit、enums 作为重载函数之自变量所引发的类型晋升转换，以及“在class 定义区内直接为整数型（integral）conststatic class members 设定初值”的能力。 扩充了 Templates 的弹性：允许 member templates 存在等。 强化了异常处理机制（Exception handling）：编译期间更严密地检验 exception specifications等。 修改了内存分配例程：加入 operator new[] 和 operator delete[]，内存未能分配成功时由 operator new/new[] 抛出一个exception，在内存分配失败时返回 0。 增加了新的转型形式：static_cast，dynamic_cast，const_cast 和reinterpret_cast。 语言规则更为优雅精练：重新定义虚函数时，其返回类型不再一定得与原定义完全吻合。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Book","slug":"Book","permalink":"https://racleray.github.io/tags/Book/"}],"author":"HeRui"},{"title":"再看Lookahead","slug":"再看Lookahead","date":"2022-04-18T15:22:07.000Z","updated":"2023-08-07T12:02:23.617Z","comments":true,"path":"posts/89ba64e7.html","link":"","permalink":"https://racleray.github.io/posts/89ba64e7.html","excerpt":"这是摘要","text":"Lookahead 优化器的本质目的： \\[ \\theta_{t+1}=\\theta_{t}+dt*\\nabla L(\\theta) \\] 简单变化： \\[ \\frac{\\theta_{t+1}-\\theta_{t}}{dt}=\\nabla L(\\theta) \\\\ \\frac{\\partial \\theta}{\\partial t}=\\nabla L(\\theta) \\] t 表示time step，这个推导不严谨，但是这么写也有点道理。且先这么假设。 优化求解的过程，就是求解 \\(\\frac{\\partial \\theta}{\\partial t} \\rightarrow 0\\) 的最优参数（局部最优）的过程，也就是对 \\(\\nabla L(\\theta) \\rightarrow 0\\) 的优化。 Lookahead要的是啥？不仅要最优化的参数，我还要参数在损失函数空间中处于一个相对更稳定的区域。 怎么稳定？让优化器自己试试周围的情况，多更新几次。怎么实现？设计一个Function（过程）: \\[ F(\\theta)=\\theta+\\alpha \\nabla L(\\theta) \\] 同时，令： \\[ \\frac{\\partial \\theta}{\\partial t} = F^k(\\theta) - \\theta \\rightarrow 0 \\] 要求 F 的 k 次复合函数变化之后，能满足以上条件。 此时发生了什么？看看，\\(F(\\theta)=\\theta\\) 这种变换成立的时候， F 的 k 次复合函数变化的结果还是 \\(\\theta\\)，完美满足条件。也几就是说，经过 k 次 \\(\\nabla L(\\theta)\\) 的变化，参数会处于一个比较稳定的区域。 虽然有点简化了过程，但是Lookahead确实就是这么个思路。 。。。 Lookahead这个算法搭配RAdam倒是见过几次了。感觉上，应该是不错的。 毕竟，RAdam相较于其它自适应学习率的优化器，对于步长的计算进行了优化。Lookahead 同时选择理论上好像更好的步长，比较地NICE。 RAdam 主要针对 Adam 在训练初始阶段，二阶动量在不同batch输入时可能面临方差过大的情况，导致训练不稳定。Bias-correction 并不能解决这个问题，也不是用来解决这个问题的。 所以 RAdam 设计了参数 \\(\\rho_t, \\rho_{\\infty}\\) 。 \\(\\rho_{\\infty}\\) 很大，而 \\(\\rho_t\\) 逐渐增大趋于 \\(\\rho_{\\infty}\\)。 根据 \\(\\rho_t\\) 的大小，调整参数更新策略。先是 momentum 方式，只考虑一阶动量；当 \\(\\rho_t\\) 达到一定大小，再进行 RAdam 方式更新。 RAdam 方式相较于 Adam，多了一个随time step逐渐增大的参数 \\(r_t\\) 和学习率一起作用于权重参数的更新过程。 甚至有点warm-up的味道了。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"optimizer","slug":"optimizer","permalink":"https://racleray.github.io/tags/optimizer/"},{"name":"lookahead","slug":"lookahead","permalink":"https://racleray.github.io/tags/lookahead/"}],"author":"HeRui"},{"title":"Effective Modern Cpp","slug":"Effective-Modern-Cpp","date":"2022-04-12T13:00:52.000Z","updated":"2024-01-09T03:12:58.691Z","comments":true,"path":"posts/b3d88a54.html","link":"","permalink":"https://racleray.github.io/posts/b3d88a54.html","excerpt":"Effective Modern Cpp 读书笔记","text":"为什么const成员函数应当线程安全？怎样使用std::unique_ptr实现Pimpl惯用法？为何要避免lambda表达式用默认捕获模式？std::atomic与volatile的区别是什么？ 基础简介 C++中的许多东西都可被声明和定义。声明（declarations）引入名字和类型，并不给出比如存放在哪或者怎样实现等的细节： 1234567extern int x; //对象声明class Widget; //类声明bool func(const Widget&amp; w); //函数声明enum class Color; //限域enum声明 定义（definitions）提供存储位置或者实现细节： 1234567891011int x; //对象定义class Widget &#123; //类定义 …&#125;;bool func(const Widget&amp; w)&#123; return w.size() &lt; 10; &#125; //函数定义enum class Color&#123; Yellow, Red, Blue &#125;; //限域enum定义 定义也有资格称为声明。 定义一个函数的签名（signature）为它声明的一部分，这个声明指定了形参类型和返回类型。函数名和形参名不是签名的一部分。在上面的例子中，func的签名是bool(const Widget&amp;)。函数声明中除了形参类型和返回类型之外的元素（比如noexcept或者constexpr，如果存在的话）都被排除在外。 另外 std::auto_ptr在C++11中被废弃，因为std::unique_ptr可以做同样的工作，而且只会做的更好。 如果一个操作的结果有未定义的行为（undefined behavior）。这意味着运行时表现是不可预测的。比如，在std::vector范围外使用方括号（“[]”），解引用未初始化的迭代器，或者数据竞争（即有两个或以上线程，至少一个是writer，同时访问相同的内存位置）。 还有，智能指针通常重载指针解引用运算符（operator-&gt;和operator*），但 std::weak_ptr是个例外。 1 类型推导 C++11修改了一些类型推导规则并增加了两套规则，一套用于auto，一套用于decltype。C++14扩展了auto和decltype可能使用的范围。 条款1：理解模板类型推导 C++最重要最吸引人的特性auto是建立在模板类型推导的基础上的。 考虑像这样一个函数模板： 12template&lt;typename T&gt;void f(ParamType param); 更具体一点是： 12template&lt;typename T&gt;void f(const T&amp; param); //ParamType是const T&amp; 编译器对 f 函数的参数进行两个类型推导：一个是针对T的，另一个是针对ParamType的。这两个类型通常是不同的，因为ParamType包含一些修饰，比如const和引用修饰符。 然后这样进行调用： 12int x = 0;f(x); //用一个int类型的变量调用f T被推导为int，ParamType却被推导为const int&amp;。 T的类型推导不仅取决于expr的类型，也取决于ParamType的类型。有三种情况： ParamType是一个指针或引用，但不是通用引用（参见 Item24，它不同于左值引用和右值引用） ParamType是一个通用引用 ParamType既不是指针也不是引用 ParamType是一个指针或引用，但不是通用引用 f(expr) 的类型推导会这样进行： 如果expr的类型是一个引用，忽略引用部分 然后expr的类型与ParamType进行模式匹配来决定T 如果这是我们的模板， 12template&lt;typename T&gt;void f(T&amp; param); //param是一个引用 我们声明这些变量， 123int x=27; //x是intconst int cx=x; //cx是const intconst int&amp; rx=x; //rx是指向作为const int的x的引用 在不同的调用中，对param和T推导的类型会是这样： 123f(x); //T是int，param的类型是int&amp;f(cx); //T是const int，param的类型是const int&amp;f(rx); //T是const int，param的类型是const int&amp; 即使rx的类型是一个引用，T也会被推导为一个非引用 ，这是因为rx的引用性（reference-ness）在类型推导中会被忽略。 当param是reference-to-const，const不再被推导为T的一部分： 12345678910template&lt;typename T&gt;void f(const T&amp; param); //param现在是reference-to-constint x = 27; //如之前一样const int cx = x; //如之前一样const int&amp; rx = x; //如之前一样f(x); //T是int，param的类型是const int&amp;f(cx); //T是int，param的类型是const int&amp;f(rx); //T是int，param的类型是const int&amp; 同之前一样，rx的reference-ness在类型推导中被忽略了。 如果param是一个指针（或者指向const的指针）而不是引用，情况本质上也一样： 12345678template&lt;typename T&gt;void f(T* param); //param现在是指针int x = 27; //同之前一样const int *px = &amp;x; //px是指向作为const int的x的指针f(&amp;x); //T是int，param的类型是int*f(px); //T是const int，param的类型是const int* ParamType是一个通用引用 形参被声明为像右值引用一样（也就是，在函数模板中假设有一个类型形参T，那么通用引用声明形式就是T&amp;&amp;)。 如果expr是左值，T和ParamType都会被推导为左值引用。 这是模板类型推导中唯一一种T被推导为引用的情况。 虽然ParamType被声明为右值引用类型，但是最后推导的结果是左值引用。 如果expr是右值，就使用正常的（也就是上一节中的）推导规则 123456789101112131415161718template&lt;typename T&gt;void f(T&amp;&amp; param); //param现在是一个通用引用类型 int x=27; const int cx=x; const int &amp; rx=cx; f(x); //x是左值，所以T是int&amp;， //param类型也是int&amp;f(cx); //cx是左值，所以T是const int&amp;， //param类型也是const int&amp;f(rx); //rx是左值，所以T是const int&amp;， //param类型也是const int&amp;f(27); //27是右值，所以T是int， //param类型就是int&amp;&amp; 通用引用的类型推导规则是不同于普通的左值或者右值引用的。尤其是，当通用引用被使用时，类型推导会区分左值实参和右值实参，但是对非通用引用时不会区分。 ParamType既不是指针也不是引用 当ParamType既不是指针也不是引用时，我们通过传值（pass-by-value）的方式处理： 12template&lt;typename T&gt;void f(T param); //以传值的方式处理param 这意味着无论传递什么param都会成为它的一份拷贝——一个完整的新对象。事实上param成为一个新对象这一行为会影响T如何从expr中推导出结果。 如果expr的类型是一个引用，忽略这个引用部分 如果忽略expr的引用性（reference-ness）之后，expr是一个const，那就再忽略const。如果它是volatile，也忽略volatile（volatile对象不常见，它通常用于驱动程序的开发中） 因此： 1234567int x=27; //如之前一样const int cx=x; //如之前一样const int &amp; rx=cx; //如之前一样f(x); //T和param的类型都是intf(cx); //T和param的类型都是intf(rx); //T和param的类型都是int 即使cx和rx表示const值，param也不是const。这是有意义的。param是一个完全独立于cx和rx的对象——是cx或rx的一个拷贝。 认识到只有在传值给形参时才会忽略const（和volatile）这一点很重要。对于reference-to-const和pointer-to-const形参来说，expr的常量性constness在推导时会被保留。 例如： 1234567template&lt;typename T&gt;void f(T param); //仍然以传值的方式处理paramconst char* const ptr = //ptr是一个常量指针，指向常量对象 \"Fun with pointers\";f(ptr); //传递const char * const类型的实参 当ptr作为实参传给f，组成这个指针的每一比特都被拷贝进param。ptr自身的值会被传给形参。 在类型推导中，这个指针指向的数据的常量性constness将会被保留，但是当拷贝ptr来创造一个新指针param时，ptr自身的常量性constness将会被忽略。 数组实参 123const char name[] = \"J. P. Briggs\"; //name的类型是const char[13]const char * ptrToName = name; //数组退化为指针 在这里const char*指针ptrToName会由name初始化，而name的类型为const char[13]，这两种类型（const char*和const char[13]）是不一样的，但是由于数组退化为指针的规则，编译器允许这样的代码。 但要是一个数组传值给一个模板会怎样？会发生什么？ 1234template&lt;typename T&gt;void f(T param); //传值形参的模板f(name); //T和param会推导成什么类型? 数组与指针形参这样的等价是C语言的产物，C++又是建立在C语言的基础上，它让人产生了一种数组和指针是等价的的错觉。 因为数组形参会视作指针形参，所以传值给模板的一个数组类型会被推导为一个指针类型。这意味着在模板函数f的调用中，它的类型形参T会被推导为const char*： 1f(name); //name是一个数组，但是T被推导为const char* 但是在C++中，虽然函数不能声明形参为真正的数组，但是可以接受指向数组的引用！所以我们修改f为传引用： 12template&lt;typename T&gt;void f(T&amp; param); //传引用形参的模板 我们这样进行调用， 1f(name); //传数组给f T被推导为了真正的数组！这个类型包括了数组的大小，在这个例子中T被推导为const char[13]，f的形参（对这个数组的引用）的类型则为const char (&amp;)[13]。 可声明指向数组的引用的能力，使得我们可以创建一个模板函数来推导出数组的大小： 12345//在编译期间返回一个数组大小的常量值，这里的数组形参没有名字template&lt;typename T, std::size_t N&gt; constexpr std::size_t arraySize(T (&amp;)[N]) noexcept &#123; return N; &#125; 将一个函数声明为constexpr使得结果在编译期间可用。这使得我们可以用一个花括号声明一个数组，然后第二个数组可以使用第一个数组的大小作为它的大小，就像这样： 123int keyVals[] = &#123; 1, 3, 7, 9, 11, 22, 35 &#125;; //keyVals有七个元素int mappedVals[arraySize(keyVals)]; //mappedVals也有七个 当然作为一个现代C++程序员，你自然应该想到使用std::array而不是内置的数组： 1std::array&lt;int, arraySize(keyVals)&gt; mappedVals; //mappedVals的大小为7 至于arraySize被声明为noexcept，会使得编译器生成更好的代码。 函数实参 在C++中不只是数组会退化为指针，函数类型也会退化为一个函数指针。对于数组类型推导的全部讨论都可以应用到函数类型推导和退化为函数指针上来。结果是： 12345678910111213void someFunc(int, double); //someFunc是一个函数， //类型是void(int, double)template&lt;typename T&gt;void f1(T param); //传值给f1template&lt;typename T&gt;void f2(T &amp; param); //传引用给f2f1(someFunc); //param被推导为指向函数的指针， //类型是void(*)(int, double)f2(someFunc); //param被推导为指向函数的引用， //类型是void(&amp;)(int, double) 这个实际上没有什么不同，只是函数退化为指针。 结论 在模板类型推导时，有引用的实参会被视为无引用，引用会被忽略 对于通用引用的推导，左值实参会被特殊对待 对于传值类型推导，const和/或volatile实参会被认为是non-const的和non-volatile的 在模板类型推导时，数组名或者函数名实参会退化为指针，除非它们被用于初始化引用模板参数类型。 2 理解 auto 类型推导 模板类型推导使用下面这个函数模板 1234template&lt;typename T&gt;void f(ParmaType param);f(expr); //使用一些表达式调用f 在f的调用中，编译器使用expr推导T和ParamType的类型。 当一个变量使用auto进行声明时，auto扮演了模板中T的角色，变量的类型说明符扮演了ParamType的角色。 考虑这个例子： 1auto x = 27; 这里x的类型说明符是auto自己，另一方面，在这个声明中： 1const auto cx = x; 类型说明符是const auto。另一个： 1const auto &amp; rx=cx; 类型说明符是const auto&amp;。 在这里例子中要推导x，rx和cx的类型，编译器的行为看起来就像是认为这里每个声明都有一个模板，然后使用合适的初始化表达式进行调用： 1234567891011121314151617template&lt;typename T&gt; //概念化的模板用来推导x的类型void func_for_x(T param);func_for_x(27); //概念化调用： //param的推导类型是x的类型template&lt;typename T&gt; //概念化的模板用来推导cx的类型void func_for_cx(const T param);func_for_cx(x); //概念化调用： //param的推导类型是cx的类型template&lt;typename T&gt; //概念化的模板用来推导rx的类型void func_for_rx(const T &amp; param);func_for_rx(x); //概念化调用： //param的推导类型是rx的类型 在使用auto作为类型说明符的变量声明中，类型说明符代替了ParamType： 123auto x = 27; //类型说明符既不是指针也不是引用const auto cx = x; //类型说明符既不是指针也不是引用const auto &amp; rx=cx; //类型说明符是一个指针或引用但不是通用引用 123456auto&amp;&amp; uref1 = x; //x是int左值， //所以uref1类型为int&amp;auto&amp;&amp; uref2 = cx; //cx是const int左值， //所以uref2类型为const int&amp;auto&amp;&amp; uref3 = 27; //27是int右值， //所以uref3类型为int&amp;&amp; 1234567891011const char name[] = //name的类型是const char[13] \"R. N. Briggs\";auto arr1 = name; //arr1的类型是const char*auto&amp; arr2 = name; //arr2的类型是const char (&amp;)[13]void someFunc(int, double); //someFunc是一个函数， //类型为void(int, double)auto func1 = someFunc; //func1的类型是void (*)(int, double)auto&amp; func2 = someFunc; //func2的类型是void (&amp;)(int, double) auto类型推导和模板类型推导几乎一样的工作。 如果你想声明一个带有初始值27的int，C++98提供两种语法选择： 12int x1 = 27;int x2(27); C++11由于也添加了用于支持统一初始化（uniform initialization）的语法： 12int x3 = &#123; 27 &#125;;int x4&#123; 27 &#125;; 总之，这四种不同的语法只会产生一个相同的结果：变量类型为int值为27。 使用 auto 进行的类型推导，其结果却不一样： 12345auto x1 = 27; //类型是int，值是27auto x2(27); //同上auto x3 = &#123; 27 &#125;; //类型是std::initializer_list&lt;int&gt;， //值是&#123; 27 &#125;auto x4&#123; 27 &#125;; //同上 这就造成了auto类型推导不同于模板类型推导的特殊情况。当用auto声明的变量使用花括号进行初始化，auto类型推导推出的类型则为std::initializer_list。如果这样的一个类型不能被成功推导（比如花括号里面包含的是不同类型的变量），编译器会拒绝这样的代码： 1auto x5 = &#123; 1, 2, 3.0 &#125;; //错误！无法推导std::initializer_list&lt;T&gt;中的T 对于花括号的处理是auto类型推导和模板类型推导唯一不同的地方。 然而如果在模板中指定T是std::initializer_list&lt;T&gt;而留下未知T，模板类型推导就能正常工作： 12345template&lt;typename T&gt;void f(std::initializer_list&lt;T&gt; initList);f(&#123; 11, 23, 9 &#125;); //T被推导为int，initList的类型为 //std::initializer_list&lt;int&gt; 在C++11编程中一个典型的错误就是偶然使用了std::initializer_list&lt;T&gt;类型的变量。 但是对于C++14故事还在继续，C++14允许auto用于函数返回值并会被推导。 而且C++14的lambda函数也允许在形参声明中使用auto。但是在这些情况下auto实际上使用模板类型推导的那一套规则在工作，而不是auto类型推导，所以说下面这样的代码不会通过编译： 1234auto createInitList()&#123; return &#123; 1, 2, 3 &#125;; //错误！不能推导&#123; 1, 2, 3 &#125;的类型&#125; 同样在C++14的 lambda 函数中这样使用auto也不能通过编译： 123456std::vector&lt;int&gt; v;auto resetV = [&amp;v](const auto&amp; newValue)&#123; v = newValue; &#125;; //C++14resetV(&#123; 1, 2, 3 &#125;); //错误！不能推导&#123; 1, 2, 3 &#125;的类型 结论 auto类型推导通常和模板类型推导相同，但是auto类型推导假定花括号初始化代表std::initializer_list，而模板类型推导不这样做 在C++14中auto允许出现在函数返回值或者lambda函数形参中，但是它的工作机制是模板类型推导那一套方案，而不是auto类型推导 3 decltype decltype，给它一个名字或者表达式decltype就会告诉你这个名字或者表达式的类型。通常，它会精确的告诉你你想要的结果。 decltype只是简单的返回名字或者表达式的类型： 123456789101112const int i = 0; //decltype(i)是const intbool f(const Widget&amp; w); //decltype(w)是const Widget&amp; //decltype(f)是bool(const Widget&amp;)struct Point&#123; int x,y; //decltype(Point::x)是int&#125;; //decltype(Point::y)是intWidget w; //decltype(w)是Widgetif (f(w))… //decltype(f(w))是bool 1234567891011template&lt;typename T&gt; //std::vector的简化版本class vector&#123;public: … T&amp; operator[](std::size_t index); …&#125;;vector&lt;int&gt; v; //decltype(v)是vector&lt;int&gt;…if (v[0] == 0)… //decltype(v[0])是int&amp; 在C++11中，decltype最主要的用途就是用于声明函数模板，而这个函数返回类型依赖于形参类型。 对一个T类型的容器使用operator[] 通常会返回一个T&amp;对象，比如std::deque就是这样。 但是std::vector有一个例外，对于std::vector&lt;bool&gt;，operator[]不会返回bool&amp;，它会返回一个全新的对象（MSVC的STL实现中返回的是std::_Vb_reference&lt;std::_Wrap_alloc&lt;std::allocator&lt;unsigned int&gt;&gt;&gt;对象）。 decltype 获取返回值类型的示例 使用decltype计算返回类型的一个例子是： 1234567template&lt;typename Container, typename Index&gt; //可以工作，auto authAndAccess(Container&amp; c, Index i) //但是需要改良 -&gt;decltype(c[i])&#123; authenticateUser(); return c[i];&#125; 函数名称前面的auto不会做任何的类型推导工作。相反的，他只是暗示使用了C++11的尾置返回类型语法，即在函数形参列表后面使用一个”-&gt;“符号指出函数的返回类型，尾置返回类型的好处是我们可以在函数返回类型中使用函数形参相关的信息。在authAndAccess函数中，我们使用c和i指定返回类型。 在C++14标准下我们可以忽略尾置返回类型，只留下一个auto。使用这种声明形式，auto标示这里会发生类型推导。更准确的说，编译器将会从函数实现中推导出函数的返回类型。 123456template&lt;typename Container, typename Index&gt; //C++14版本，auto authAndAccess(Container&amp; c, Index i) //不那么正确&#123; authenticateUser(); return c[i]; //从c[i]中推导返回类型&#125; 从返回对象进行修改 上述代码出现的一个问题是： operator[]对于大多数T类型的容器会返回一个T&amp;，但是 条款1 解释了在模板类型推导期间，表达式的引用性（reference-ness）会被忽略。基于这样的规则，考虑它会对下面用户的代码有哪些影响： 12345std::deque&lt;int&gt; d;…authAndAccess(d, 5) = 10; //认证用户，返回d[5]， //然后把10赋值给它 //无法通过编译器！ 在这里d[5]本该返回一个int&amp;，但是模板类型推导会剥去引用的部分，因此产生了int返回类型。函数返回的那个int是一个右值，上面的代码尝试把10赋值给右值int，C++11禁止这样做，所以代码无法编译。 要想让authAndAccess像我们期待的那样工作，我们需要使用decltype类型推导来推导它的返回值，即指定authAndAccess应该返回一个和c[i]表达式类型一样的类型。 因此我们可以这样写authAndAccess： 1234567template&lt;typename Container, typename Index&gt; //C++14版本，decltype(auto) //可以工作，authAndAccess(Container&amp; c, Index i) //但是还需要&#123; //改良 authenticateUser(); return c[i];&#125; 现在authAndAccess将会真正的返回c[i]的类型。现在事情解决了，一般情况下c[i]返回T&amp;，authAndAccess也会返回T&amp;，特殊情况下c[i]返回一个对象，authAndAccess也会返回一个对象。 decltype(auto)的使用不仅仅局限于函数返回类型，当你想对初始化表达式使用decltype推导的规则，你也可以使用： 12345678Widget w;const Widget&amp; cw = w;auto myWidget1 = cw; //auto类型推导 //myWidget1的类型为Widgetdecltype(auto) myWidget2 = cw; //decltype类型推导 //myWidget2的类型是const Widget&amp; 形参传递问题 authAndAccess声明： 12template&lt;typename Container, typename Index&gt;decltype(auto) authAndAccess(Container&amp; c, Index i); 容器通过传引用的方式传递非常量左值引用（lvalue-reference-to-non-const），因为返回一个引用允许用户可以修改容器。 但是这意味着在不能给这个函数传递右值容器，右值不能被绑定到左值引用上，除非这个左值引用是一个const（lvalue-references-to-const）。 一个右值容器，是一个临时对象，通常会在authAndAccess调用结束被销毁，这意味着authAndAccess返回的引用将会成为一个悬置的（dangle）引用。 为了使authAndAccess的引用可以绑定左值和右值，可以使用通用引用。所以我们这样声明： 12template&lt;typename Containter, typename Index&gt; //现在c是通用引用decltype(auto) authAndAccess(Container&amp;&amp; c, Index i); 这行代码中还有一个问题： 在这个模板中，我们不知道我们操纵的容器的类型是什么，也就是说不知道它使用的索引对象（index objects）的类型。 对一个未知类型的对象使用传值通常会造成不必要的拷贝，对程序的性能有极大的影响，还会造成对象切片行为。 但是只针对 STL 容器（比如std::string，std::vector和std::deque的operator[]），这样处理是合理的。 为了保持参数本身的左右值属性，还需要进行 std::forward： 1234567template&lt;typename Container, typename Index&gt; //最终的C++14版本decltype(auto)authAndAccess(Container&amp;&amp; c, Index i)&#123; authenticateUser(); return std::forward&lt;Container&gt;(c)[i];&#125; C++11版本： 12345678template&lt;typename Container, typename Index&gt; //最终的C++11版本autoauthAndAccess(Container&amp;&amp; c, Index i)-&gt;decltype(std::forward&lt;Container&gt;(c)[i])&#123; authenticateUser(); return std::forward&lt;Container&gt;(c)[i];&#125; 将decltype应用于变量名会产生该变量名的声明类型。虽然变量名都是左值表达式，但这不会影响decltype的行为。但是对于一些表达式，其类型推导结果，可能出现&amp;引用类型： 12345678910decltype(auto) f1() &#123; int x = 0; … return x; //decltype(x）是int，所以f1返回int&#125;decltype(auto) f2() &#123; int x = 0; return (x); //decltype((x))是int&amp;，所以f2返回int&amp;&#125; 对于名字来说，x是一个左值，C++11定义了表达式(x)也是一个左值。decltype((x))是int&amp;。用小括号覆盖一个名字可以改变decltype对于名字产生的结果。 因此，当使用decltype(auto)的时候一定要加倍的小心，在表达式中看起来无足轻重的细节将会影响到decltype(auto)的推导结果。 结论 decltype产生变量或者表达式的类型 对于T类型的不是单纯的变量名的左值表达式，decltype总是产出T的引用即T&amp; C++14支持decltype(auto)，推导出类型，但是它使用decltype的规则进行推导，而不是 auto 4 查看类型推导结果 三种方案： IDE编辑器获得类型推导的结果 在编译期间获得结果 在运行时获得结果 IDE IDE之所以能提供这些信息是因为一个C++编译器（或者至少是前端中的一个部分）运行于IDE中。如果这个编译器对你的代码不能做出有意义的分析或者推导，它就不会显示推导的结果。 编译 可以首先声明一个类模板但不定义。就像这样： 12template&lt;typename T&gt; //只对TD进行声明class TD; //TD == \"Type Displayer\" 尝试实例化这个类模板就会引出一个错误消息，因为这里没有用来实例化的类模板定义。为了查看x和y的类型，只需要使用它们的类型去实例化TD： 12TD&lt;decltype(x)&gt; xType; //引出包含x和yTD&lt;decltype(y)&gt; yType; //的类型的错误消息 出现 undefined template TD&lt;xxx&gt;。 运行时 使用printf的方法使类型信息只有在运行时才会显示出来（尽管不建议使用printf）。 12std::cout &lt;&lt; typeid(x).name() &lt;&lt; '\\n'; //显示x和y的类型std::cout &lt;&lt; typeid(y).name() &lt;&lt; '\\n'; 这种方法对一个对象如x或y调用typeid产生一个std::type_info的对象，然后std::type_info里面的成员函数name()来产生一个C风格的字符串（即一个const char*）表示变量的名字。 调用std::type_info::name不保证返回任何有意义的东西，但是库的实现者尝试尽量使它们返回的结果有用。 举个例子，GNU和Clang环境下x的类型会显示为\"i\"，y会显示为\"PKi\"。\"i\"表示\"int\"，\"\"PK\"表示\"pointer to konst const\"（指向常量的指针）。 如果传递的是一个引用，那么引用部分（reference-ness）将被忽略，如果忽略后还具有const或者volatile，那么常量性constness或者易变性volatileness也会被忽略。 std::type_info::name的结果并不总是可信的，因为std::type_info::name规范批准像传值形参一样来对待这些类型。Boost TypeIndex库（Boost.TypeIndex）是更好的选择。 例如： 123456789101112131415161718#include &lt;boost/type_index.hpp&gt;template&lt;typename T&gt;void f(const T&amp; param)&#123; using std::cout; using boost::typeindex::type_id_with_cvr; //显示T cout &lt;&lt; \"T = \" &lt;&lt; type_id_with_cvr&lt;T&gt;().pretty_name() &lt;&lt; '\\n'; //显示param类型 cout &lt;&lt; \"param = \" &lt;&lt; type_id_with_cvr&lt;decltype(param)&gt;().pretty_name() &lt;&lt; '\\n';&#125; boost::typeindex::type_id_with_cvr获取一个类型实参（我们想获得相应信息的那个类型），它不消除实参的const，volatile和引用修饰符（因此模板名中有“with_cvr”）。结果是一个boost::typeindex::type_index对象，它的pretty_name成员函数输出一个std::string，包含我们能看懂的类型表示。 基于这个f的实现版本，再次考虑那个使用typeid时获取param类型信息出错的调用： 123456std::vetor&lt;Widget&gt; createVec(); //工厂函数const auto vw = createVec(); //使用工厂函数返回值初始化vwif (!vw.empty())&#123; f(&amp;vw[0]); //调用f …&#125; 在GNU和Clang的编译器环境下，使用Boost.TypeIndex版本的f最后会产生下面的（准确的）输出： 12T = Widget const *param = Widget const * const&amp; 在Microsoft的编译器环境下，结果也是极其相似： 12T = class Widget const *param = class Widget const * const &amp; 结论 类型推断可以使用IDE，使用编译器报错，使用Boost.TypeIndex库 这些工具可能既不准确也无帮助，所以理解C++类型推导规则才是最重要的 5 优先考虑auto而非显式类型声明 从程序员的角度来说，如果按照符合规定的流程走，那auto类型推导的一些结果是错误的。当这些情况发生时，引导auto产生正确的结果是很重要的。 auto变量从初始化表达式中推导出类型，所以我们必须初始化。 12345678910111213141516171819int x1; //潜在的未初始化的变量 auto x2; //错误！必须要初始化auto x3 = 0; //没问题，x已经定义了template&lt;typename It&gt; void dwim(It b,It e)&#123; while (b != e) &#123; auto currValue = *b; … &#125;&#125;auto derefUPLess = [](const std::unique_ptr&lt;Widget&gt; &amp;p1, //用于std::unique_ptr const std::unique_ptr&lt;Widget&gt; &amp;p2) //指向的Widget类型的 &#123; return *p1 &lt; *p2; &#125;; //比较函数 如果使用C++14，将会变得更酷，因为lambda表达式中的形参也可以使用auto： 1234567891011auto derefLess = //C++14版本 [](const auto&amp; p1, //被任何像指针一样的东西 const auto&amp; p2) //指向的值的比较函数 &#123; return *p1 &lt; *p2; &#125;;// 也即std::function&lt;bool(const std::unique_ptr&lt;Widget&gt; &amp;, const std::unique_ptr&lt;Widget&gt; &amp;)&gt;derefUPLess = [](const std::unique_ptr&lt;Widget&gt; &amp;p1, const std::unique_ptr&lt;Widget&gt; &amp;p2) &#123; return *p1 &lt; *p2; &#125;; 实例化std::function并声明一个对象这个对象将会有固定的大小。这个大小可能不足以存储一个闭包，这个时候std::function的构造函数将会在堆上面分配内存来存储，这就造成了使用std::function比auto声明变量会消耗更多的内存。 通过std::function调用一个闭包几乎无疑比auto声明的对象调用要慢。换句话说，std::function方法比auto方法要更耗空间且更慢，还可能有out-of-memory异常。并且正如上面的例子，比起写std::function实例化的类型来，使用auto要方便得多。 考虑以下问题： 123std::vector&lt;int&gt; v;…unsigned sz = v.size(); v.size()的标准返回类型是std::vector&lt;int&gt;::size_type，但是只有少数开发者意识到这点。std::vector&lt;int&gt;::size_type实际上被指定为无符号整型。上述的代码，会造成一些有趣的结果。 举个例子，在Windows 32-bit上std::vector&lt;int&gt;::size_type和unsigned是一样的大小，但是在Windows 64-bit上std::vector&lt;int&gt;::size_type是64位，unsigned是32位。这意味着这段代码在Windows 32-bit上正常工作，但是当把应用程序移植到Windows 64-bit上时就可能会出现一些问题。 所以使用auto可以确保你不需要浪费时间： 1auto sz =v.size(); //sz的类型是std::vector&lt;int&gt;::size_type 考虑下面的代码： 1234567std::unordered_map&lt;std::string, int&gt; m;…for(const std::pair&lt;std::string, int&gt;&amp; p : m)&#123; … //用p做一些事&#125; 看起来好像很合情合理的表达，但是这里有一个问题： std::unordered_map的key是const的，所以hash table中的std::pair的类型不是std::pair&lt;std::string, int&gt;，而是std::pair&lt;const std::string, int&gt;。 编译器会努力的找到一种方法把std::pair&lt;const std::string, int&gt;（即hash table中的东西）转换为std::pair&lt;std::string, int&gt;（p的声明类型）。它会成功的，因为它会通过拷贝m中的对象创建一个临时对象，是m中元素的类型。然后把p的引用绑定到这个临时对象上。在每个循环迭代结束时，临时对象将会销毁。 所以不只是让p指向m中各个元素的引用而已。 使用auto可以避免这些很难被意识到的类型不匹配的错误： 123for(const auto&amp; p : m) &#123; … //如之前一样&#125; 这样无疑更具效率，且更容易书写。而且，这个代码有一个非常吸引人的特性，如果你获取p的地址，你确实会得到一个指向m中元素的指针。在没有auto的版本中p会指向一个临时变量，这个临时变量在每次迭代完成时会被销毁。 讲究！ 有时候，显式的指定类型可能会导致你不想看到的类型转换。如果你使用auto声明目标变量你就不必担心这个问题。 然而auto也不是完美的。每个auto变量都从初始化表达式中推导类型，有一些表达式的类型和我们期望的大相径庭，比如在 理解auto类型推导 小结的内容。 另外，一个适当的变量名称就能体现大量的抽象类型信息，所以不用考虑 auto 带来的信息不可见性。 结论 auto变量必须初始化，通常它可以避免一些移植性和效率性的问题，也使得重构更方便，还能让你少打几个字。 注意 auto 可能出现一些类型推导不一致的问题。 6 auto 遇上代理类型，使用显式类型初始化 假如我有一个函数，参数为Widget，返回一个std::vector&lt;bool&gt;，这里的bool表示Widget是否提供一个独有的特性。 1std::vector&lt;bool&gt; features(const Widget&amp; w); 更进一步假设第5个bit表示Widget是否具有高优先级，我们可以写这样的代码： 12345Widget w;…bool highPriority = features(w)[5]; //w高优先级吗？…processWidget(w, highPriority); //根据它的优先级处理w 这个代码没有任何问题。它会正常工作，但是如果我们使用auto代替highPriority的显式指定类型做一些看起来很无害的改变： 1auto highPriority = features(w)[5]; //w高优先级吗？ 情况变了。所有代码仍然可编译，但是行为不再可预测： 1processWidget(w,highPriority); //未定义行为！ 因为 features(w)[5] 调用 operator[]不会返回容器中元素的引用，取而代之它返回一个std::vector&lt;bool&gt;::reference的对象。 调用features将返回一个std::vector&lt;bool&gt;临时对象，这个对象没有名字，为了方便我们的讨论，我这里叫他temp。operator[]在temp上调用，它返回的std::vector&lt;bool&gt;::reference包含一个指向存着这些 bits 的指针（temp管理这些bits）。highPriority是这个std::vector&lt;bool&gt;::reference的拷贝，所以highPriority也包含一个指针，指向temp中的管理 bits 。在这个语句结束的时候temp将会被销毁，因为它是一个临时变量。因此highPriority包含一个悬挂（dangling）指针，如果用于processWidget调用中将会造成未定义行为： 12processWidget(w, highPriority); //未定义行为！ //highPriority包含一个悬置指针！ 代理类问题 std::vector&lt;bool&gt;::reference是一个代理类（proxy class）的例子：所谓代理类就是以模仿和增强一些类型的行为为目的而存在的类。 C++标准模板库中的智能指针也是用代理类实现了对原始指针的资源管理行为。 一些代理类被设计于用以对客户可见。比如std::shared_ptr和std::unique_ptr。其他的代理类则或多或少不可见，比如std::vector&lt;bool&gt;::reference就是不可见代理类的一个例子，还有std::bitset::reference 等。 一些C++库也是用了表达式模板（expression templates）的黑科技。这些库通常被用于提高数值运算的效率。给出一个矩阵类Matrix和矩阵对象m1，m2，m3，m4，举个例子，这个表达式 1Matrix sum = m1 + m2 + m3 + m4; 可以使计算更加高效，只需要使让operator+返回一个代理类代理结果 Sum&lt;Matrix, Matrix&gt; 而不是返回结果本身。 作为一个通则，不可见的代理类通常不适用于auto。这样类型的对象的生命期通常不会活过一条语句，所以创建那样的对象是危险的。 显式类型初始化器（the explicitly typed initialized idiom) 1auto highPriority = static_cast&lt;bool&gt;(features(w)[5]); 这里，features(w)[5]还是返回一个std::vector&lt;bool&gt;::reference对象，但是这个转型使得表达式类型为bool，然后auto才被用于推导highPriority。 12auto sum = static_cast&lt;Matrix&gt;(m1 + m2 + m3 + m4);auto ep = static_cast&lt;float&gt;(calcEpsilon()); // 转换精度 结论 不可见的代理类可能会使auto从表达式中推导出“错误的”类型 显式类型初始化器强制auto推导出你想要的结果 7 区分()和{}创建对象 C++11使用统一初始化（uniform initialization） 1std::vector&lt;int&gt; v&#123; 1, 3, 5 &#125;; //v初始内容为1,3,5 C++11允许\"=\"初始化不加花括号也拥有这种能力，括号初始化也能被用于为非静态数据成员指定默认初始值： 12345678class Widget&#123; …private: int x&#123; 0 &#125;; //没问题，x初始值为0 int y = 0; //也可以 int z(0); //错误！&#125; 另一方面，不可拷贝的对象（例如std::atomic）可以使用花括号初始化或者圆括号初始化，但是不能使用\"=\"初始化： 123std::atomic&lt;int&gt; ai1&#123; 0 &#125;; //没问题std::atomic&lt;int&gt; ai2(0); //没问题std::atomic&lt;int&gt; ai3 = 0; //错误！ 内置类型间的隐式变窄转换 (narrowing conversion) 括号表达式还有一个少见的特性，即它不允许内置类型间隐式的变窄转换（narrowing conversion）。如果一个使用了括号初始化的表达式的值，不能保证由被初始化的对象的类型来表示，代码就不会通过编译： 123double x, y, z;int sum1&#123; x + y + z &#125;; //错误！double的和可能不能表示为int 使用圆括号和\"=\"的初始化不检查是否转换为变窄转换，因为由于历史遗留问题它们必须要兼容老旧代码： 123int sum2(x + y +z); //可以（表达式的值被截为int）int sum3 = x + y + z; //同上 被误认为是声明 C++规定任何可以被解析为一个声明的东西必须被解析为声明。 1Widget w1(10); //使用实参10调用Widget的一个构造函数 但是如果你尝试使用相似的语法调用Widget无参构造函数，它就会变成函数声明： 1Widget w2(); //最令人头疼的解析！声明一个函数w2，返回Widget 由于函数声明中形参列表不能带花括号，所以使用花括号初始化表明你想调用默认构造函数构造对象就没有问题： 1Widget w3&#123;&#125;; //调用没有参数的构造函数构造对象 initializer_list关联问题 在构造函数调用中，只要不包含std::initializer_list形参，那么花括号初始化和圆括号初始化都会产生一样的结果： 12345678910class Widget &#123; public: Widget(int i, bool b); //构造函数未声明 Widget(int i, double d); //std::initializer_list这个形参 …&#125;;Widget w1(10, true); //调用第一个构造函数Widget w2&#123;10, true&#125;; //也调用第一个构造函数Widget w3(10, 5.0); //调用第二个构造函数Widget w4&#123;10, 5.0&#125;; //也调用第二个构造函数 然而，如果有一个或者多个构造函数的声明包含一个std::initializer_list形参，那么使用括号初始化语法的调用更倾向于选择带std::initializer_list的那个构造函数。 1234567class Widget &#123; public: Widget(int i, bool b); //同上 Widget(int i, double d); //同上 Widget(std::initializer_list&lt;long double&gt; il); //新添加的 …&#125;; w2和w4将会使用新添加的构造函数，即使另一个非std::initializer_list构造函数和实参更匹配： 12345678910111213Widget w1(10, true); //使用圆括号初始化，同之前一样 //调用第一个构造函数Widget w2&#123;10, true&#125;; //使用花括号初始化，但是现在 //调用带std::initializer_list的构造函数 //(10 和 true 转化为long double)Widget w3(10, 5.0); //使用圆括号初始化，同之前一样 //调用第二个构造函数 Widget w4&#123;10, 5.0&#125;; //使用花括号初始化，但是现在 //调用带std::initializer_list的构造函数 //(10 和 5.0 转化为long double) 编译器一遇到括号初始化就选择带std::initializer_list的构造函数的决心是如此强烈，以至于就算带std::initializer_list的构造函数不能被调用，它也会硬选。 123456789class Widget &#123; public: Widget(int i, bool b); //同之前一样 Widget(int i, double d); //同之前一样 Widget(std::initializer_list&lt;bool&gt; il); //现在元素类型为bool … //没有隐式转换函数&#125;;Widget w&#123;10, 5.0&#125;; //错误！要求变窄转换 这里，编译器会直接忽略前面两个构造函数（其中第二个构造函数是所有实参类型的最佳匹配），然后尝试调用std::initializer_list&lt;bool&gt;构造函数。调用这个函数将会把int(10)和double(5.0)转换为bool，由于会产生变窄转换（bool不能准确表示其中任何一个值），括号初始化拒绝变窄转换，所以这个调用无效，代码无法通过编译。 只有当没办法把括号初始化中实参的类型转化为std::initializer_list时，编译器才会回到正常的函数决议流程中。 12345678910111213class Widget &#123; public: Widget(int i, bool b); //同之前一样 Widget(int i, double d); //同之前一样 //现在std::initializer_list元素类型为std::string Widget(std::initializer_list&lt;std::string&gt; il); … //没有隐式转换函数&#125;;Widget w1(10, true); // 使用圆括号初始化，调用第一个构造函数Widget w2&#123;10, true&#125;; // 使用花括号初始化，现在调用第一个构造函数Widget w3(10, 5.0); // 使用圆括号初始化，调用第二个构造函数Widget w4&#123;10, 5.0&#125;; // 使用花括号初始化，现在调用第二个构造函数 空的花括号意味着没有实参，不是一个空的std::initializer_list： 1234567891011class Widget &#123; public: Widget(); //默认构造函数 Widget(std::initializer_list&lt;int&gt; il); //std::initializer_list构造函数 … //没有隐式转换函数&#125;;Widget w1; //调用默认构造函数Widget w2&#123;&#125;; //也调用默认构造函数Widget w3(); //最令人头疼的解析！声明一个函数 12Widget w4(&#123;&#125;); //使用空花括号列表调用std::initializer_list构造函数Widget w5&#123;&#123;&#125;&#125;; //同上 作为一个类库使用者，你必须认真的在花括号和圆括号之间选择一个来创建对象。大多数开发者都使用其中一种作为默认情况，只有当他们不能使用这种的时候才会考虑另一种。 结论 花括号初始化是最广泛使用的初始化语法，它防止变窄转换，并且对于C++最令人头疼的解析（括号解析为函数）有天生的免疫性 在构造函数重载决议中，编译器会尽最大努力将括号初始化与std::initializer_list参数匹配，即便其他构造函数看起来是更好的选择 对于数值类型的std::vector来说使用花括号初始化和圆括号初始化会造成巨大的不同 在模板类选择使用圆括号初始化或使用花括号初始化创建对象是需要仔细考虑 8 优先 nullptr 在C++98中，对指针类型和整型进行重载意味着可能导致奇怪的事情。如果给下面的重载函数传递0或NULL，它们绝不会调用指针版本的重载函数： 12345678void f(int); //三个f的重载函数void f(bool);void f(void*);f(0); //调用f(int)而不是f(void*)f(NULL); //可能不会被编译，一般来说调用f(int)， //绝对不会调用f(void*) 而f(NULL)的不确定行为是由NULL的实现不同造成的。如果NULL被定义为0L（指的是0为long类型），这个调用就具有二义性，因为从long到int的转换或从long到bool的转换或0L到void*的转换都同样好。 nullptr的优点是它不是整型。它也不是一个指针类型，但是你可以把它认为是所有类型的指针。nullptr的真正类型是std::nullptr_t，在一个完美的循环定义以后，std::nullptr_t又被定义为nullptr。std::nullptr_t可以隐式转换为指向任何内置类型的指针。 使用nullptr调用f将会调用void*版本的重载函数，因为nullptr不能被视作任何整型： 1f(nullptr); //调用重载函数f的f(void*)版本 看下面的例子： 1234auto result = findRecord( /* arguments */ );if (result == 0) &#123; …&#125; 如果你不知道findRecord返回了什么，那么你就不太清楚到底result是一个指针类型还是一个整型。但是换一种假设如果你看到这样的代码： 12345auto result = findRecord( /* arguments */ );if (result == nullptr) &#123; …&#125; 这就没有任何歧义：result的结果一定是指针类型。 再考虑下面例子： 假如你有一些函数只能被合适的已锁互斥量调用。每个函数都有一个不同类型的指针： 123int f1(std::shared_ptr&lt;Widget&gt; spw); //只能被合适的double f2(std::unique_ptr&lt;Widget&gt; upw); //已锁互斥量bool f3(Widget* pw); //调用 如果这样传递空指针： 12345678910111213141516171819std::mutex f1m, f2m, f3m; //用于f1，f2，f3函数的互斥量using MuxGuard = std::lock_guard&lt;std::mutex&gt;;…&#123; MuxGuard g(f1m); //为f1m上锁 auto result = f1(0); //向f1传递0作为空指针&#125; //解锁 …&#123; MuxGuard g(f2m); //为f2m上锁 auto result = f2(NULL); //向f2传递NULL作为空指针&#125; //解锁 …&#123; MuxGuard g(f3m); //为f3m上锁 auto result = f3(nullptr); //向f3传递nullptr作为空指针&#125; //解锁 令人遗憾前两个调用没有使用nullptr，但是代码可以正常运行。 模板化这个调用流程： 12345678910template&lt;typename FuncType, typename MuxType, typename PtrType&gt;auto lockAndCall(FuncType func, MuxType&amp; mutex, PtrType ptr) -&gt; decltype(func(ptr))&#123; MuxGuard g(mutex); return func(ptr); &#125; 12345678910template&lt;typename FuncType, typename MuxType, typename PtrType&gt;decltype(auto) lockAndCall(FuncType func, //C++14 MuxType&amp; mutex, PtrType ptr)&#123; MuxGuard g(mutex); return func(ptr); &#125; 可以写这样的代码调用lockAndCall模板： 12345auto result1 = lockAndCall(f1, f1m, 0); //错误！...auto result2 = lockAndCall(f2, f2m, NULL); //错误！...auto result3 = lockAndCall(f3, f3m, nullptr); //没问题 代码虽然可以这样写，但是就像注释中说的，前两个情况不能通过编译。 当0被传递给lockAndCall模板，模板类型推导会尝试去推导实参类型，0的类型总是int。 这意味着lockAndCall中func会被int类型的实参调用，这与f1期待的std::shared_ptr&lt;Widget&gt;形参不符。把int类型看做std::shared_ptr&lt;Widget&gt;类型给f1自然是一个类型错误。在模板lockAndCall中使用0之所以失败是因为在模板中，传给的是int但实际上函数期待的是一个std::shared_ptr&lt;Widget&gt;。 当nullptr传给lockAndCall时，ptr被推导为std::nullptr_t。当ptr被传递给f3的时候，隐式转换使std::nullptr_t转换为Widget*，因为std::nullptr_t可以隐式转换为任何指针类型。 结论 优先考虑 nullptr 避免重载指针和整型 9 优先 alias 而不是 typedef typedef即可： 123typedef std::unique_ptr&lt;std::unordered_map&lt;std::string, std::string&gt;&gt; UPtrMapSS; 但typedef是C++98的东西。 C++11也提供了一个别名声明（alias declaration）： 12using UPtrMapSS = std::unique_ptr&lt;std::unordered_map&lt;std::string, std::string&gt;&gt;; 由于这里给出的typedef和别名声明做的都是完全一样的事情。 使用别名模板，会容易很多： 12345template&lt;typename T&gt; //MyAllocList&lt;T&gt;是using MyAllocList = std::list&lt;T, MyAlloc&lt;T&gt;&gt;; //std::list&lt;T, MyAlloc&lt;T&gt;&gt; //的同义词MyAllocList&lt;Widget&gt; lw; //用户代码 使用typedef，你就只能从头开始： 123456template&lt;typename T&gt; //MyAllocList&lt;T&gt;是struct MyAllocList &#123; //std::list&lt;T, MyAlloc&lt;T&gt;&gt; typedef std::list&lt;T, MyAlloc&lt;T&gt;&gt; type; //的同义词 &#125;;MyAllocList&lt;Widget&gt;::type lw; //用户代码 如果你想使用在一个模板内使用typedef声明一个链表对象，而这个对象又使用了模板形参，你就不得不在typedef前面加上typename： 123456template&lt;typename T&gt;class Widget &#123; //Widget&lt;T&gt;含有一个private: //MyAllocLIst&lt;T&gt;对象 typename MyAllocList&lt;T&gt;::type list; //作为数据成员 …&#125;; 这里MyAllocList&lt;T&gt;::type使用了一个类型，这个类型依赖于模板参数T。 如果使用别名声明定义一个MyAllocList，就不需要使用typename（同时省略麻烦的“::type”后缀）： 123456789template&lt;typename T&gt; using MyAllocList = std::list&lt;T, MyAlloc&lt;T&gt;&gt;; //同之前一样template&lt;typename T&gt;class Widget &#123;private: MyAllocList&lt;T&gt; list; //没有“typename” … //没有“::type”&#125;; C++11在type traits（类型特性）中给了你一系列工具去实现类型转换，如果要使用这些模板请包含头文件&lt;type_traits&gt;。里面有许许多多type traits，也不全是类型转换的工具，也包含一些可预测接口的工具。给一个你想施加转换的类型T，结果类型就是std::transformation&lt;T&gt;::type，比如： 123std::remove_const&lt;T&gt;::type //从const T中产出Tstd::remove_reference&lt;T&gt;::type //从T&amp;和T&amp;&amp;中产出Tstd::add_lvalue_reference&lt;T&gt;::type //从T中产出T&amp; 注释仅仅简单的总结了类型转换做了什么，所以不要太随便的使用。在你的项目使用它们之前，你最好看看它们的详细说明书。 这些别名声明有一个通用形式：对于C++11的类型转换std::transformation&lt;T&gt;::type在C++14中变成了std::transformation_t。举个例子或许更容易理解： 12345678std::remove_const&lt;T&gt;::type //C++11: const T → T std::remove_const_t&lt;T&gt; //C++14 等价形式std::remove_reference&lt;T&gt;::type //C++11: T&amp;/T&amp;&amp; → T std::remove_reference_t&lt;T&gt; //C++14 等价形式std::add_lvalue_reference&lt;T&gt;::type //C++11: T → T&amp; std::add_lvalue_reference_t&lt;T&gt; //C++14 等价形式 C++11的的形式在C++14中也有效。其简单实现形式是： 123456789template &lt;class T&gt; using remove_const_t = typename remove_const&lt;T&gt;::type;template &lt;class T&gt; using remove_reference_t = typename remove_reference&lt;T&gt;::type;template &lt;class T&gt; using add_lvalue_reference_t = typename add_lvalue_reference&lt;T&gt;::type; 结论 typedef不支持模板化，但是别名声明支持。 别名模板避免了使用“::type”后缀，而且在模板中使用typedef还需要在前面加上typename C++14提供了C++11所有type traits转换的别名声明版本 10 限域`enum 12345678enum class Color &#123; black, white, red &#125;; //black, white, red //限制在Color域内auto white = false; //没问题，域内没有其他“white”Color c = white; //错误，域中没有枚举名叫whiteColor c = Color::white; //没问题auto c = Color::white; //也没问题（也符合Item5的建议） 因为限域enum是通过“enum class”声明，所以它们有时候也被称为枚举类(enum classes)。 使用限域enum来减少命名空间污染，这是一个足够合理使用它而不是它的同胞未限域enum的理由。 其实限域enum还有第二个吸引人的优点：在它的作用域中，枚举名是强类型。未限域enum中的枚举名会隐式转换为整型（现在，也可以转换为浮点类型）。因此下面这种歪曲语义的做法也是完全有效的： 12345678910111213enum Color &#123; black, white, red &#125;; //未限域enumstd::vector&lt;std::size_t&gt; //func返回x的质因子 primeFactors(std::size_t x);Color c = red;…if (c &lt; 14.5) &#123; // Color与double比较 (!) auto factors = // 计算一个Color的质因子(!) primeFactors(c); …&#125; 在enum后面写一个class就可以将非限域enum转换为限域enum，接下来就是完全不同的故事展开了。现在不存在任何隐式转换可以将限域enum中的枚举名转化为任何其他类型： 12345678910111213enum class Color &#123; black, white, red &#125;; //Color现在是限域enumColor c = Color::red; //和之前一样，只是... //多了一个域修饰符if (c &lt; 14.5) &#123; //错误！不能比较 //Color和double auto factors = //错误！不能向参数为std::size_t primeFactors(c); //的函数传递Color参数 …&#125;// 除非 static_cast&lt;double&gt;(c) 在C++中所有的enum都有一个由编译器决定的整型的底层类型。对于非限域enum比如Color， 1enum Color &#123; black, white, red &#125;; 编译器可能选择char作为底层类型，因为这里只需要表示三个值。然而，有些enum中的枚举值范围可能会大些，比如： 123456enum Status &#123; good = 0, failed = 1, incomplete = 100, corrupt = 200, indeterminate = 0xFFFFFFFF &#125;; 这里值的范围从0到0xFFFFFFFF。除了在不寻常的机器上（比如一个char至少有32bits的那种），编译器都会选择一个比char大的整型类型来表示Status。 为了高效使用内存，编译器通常在确保能包含所有枚举值的前提下为enum选择一个最小的底层类型。 为此，C++98只支持enum定义（所有枚举名全部列出来）；enum声明是不被允许的。编译器才能在使用之前为每一个enum选择一个底层类型。 不能前置声明enum也是有缺点的。最大的缺点莫过于它可能增加编译依赖。系统中某个枚举类型的头文件包含在多个文件中。如果引入一个新状态值，那么可能整个系统都得重新编译。 C++11中的前置声明enums可以解决这个问题。 12enum class Status; //前置声明void continueProcessing(Status s); //使用前置声明enum 即使Status的定义发生改变，包含这些声明的头文件也不需要重新编译。 默认情况下，限域枚举的底层类型是int： 1enum class Status; //底层类型是int 如果默认的int不适用，你可以重写它： 123enum class Status: std::uint32_t; //Status的底层类型 //是std::uint32_t //（需要包含 &lt;cstdint&gt;） 不管怎样，编译器都知道限域enum中的枚举名占用多少字节。 要为非限域enum指定底层类型，你可以同上，结果就可以前向声明： 123enum Color: std::uint8_t; //非限域enum前向声明 //底层类型为 //std::uint8_t 底层类型说明也可以放到enum定义处： 1234567enum class Status: std::uint32_t &#123; good = 0, failed = 1, incomplete = 100, corrupt = 200, audited = 500, indeterminate = 0xFFFFFFFF &#125;; 假设我们有一个tuple保存了用户的名字，email地址，声望值： 1234using UserInfo = //类型别名，参见Item9 std::tuple&lt;std::string, //名字 std::string, //email地址 std::size_t&gt; ; //声望 虽然注释说明了tuple各个字段对应的意思，但当你在另一文件遇到下面的代码那之前的注释就不是那么有用了： 123UserInfo uInfo; //tuple对象…auto val = std::get&lt;1&gt;(uInfo); //获取第一个字段 在 get 时，显示写明1随代表的字段。 用非限域enum将名字和字段编号关联起来以避免上述需求： 12345enum UserInfoFields &#123; uiName, uiEmail, uiReputation &#125;;UserInfo uInfo; //同之前一样…auto val = std::get&lt;uiEmail&gt;(uInfo); //啊，获取用户email字段的值 之所以它能正常工作是因为UserInfoFields中的枚举名隐式转换成std::size_t。 对应的限域enum版本就很啰嗦了： 1234567enum class UserInfoFields &#123; uiName, uiEmail, uiReputation &#125;;UserInfo uInfo; //同之前一样…auto val = std::get&lt;static_cast&lt;std::size_t&gt;(UserInfoFields::uiEmail)&gt; (uInfo); 为避免这种冗长的表示，我们可以写一个函数传入枚举名并返回对应的std::size_t值，但这有一点技巧性。 将枚举名变换为std::size_t值的函数必须在编译期产生这个结果。 它该是一个constexpr函数模板，因为它应该能用于任何enum。 底层类型可以通过std::underlying_type这个type trait获得。 12345678template&lt;typename E&gt;constexpr typename std::underlying_type&lt;E&gt;::type toUType(E enumerator) noexcept&#123; return static_cast&lt;typename std::underlying_type&lt;E&gt;::type&gt;(enumerator);&#125; 在C++14中，toUType还可以进一步用std::underlying_type_t代替typename std::underlying_type&lt;E&gt;::type打磨： 123456template&lt;typename E&gt; //C++14constexpr std::underlying_type_t&lt;E&gt; toUType(E enumerator) noexcept&#123; return static_cast&lt;std::underlying_type_t&lt;E&gt;&gt;(enumerator);&#125; 还可以再用C++14 auto打磨一下代码： 123456template&lt;typename E&gt; //C++14constexpr auto toUType(E enumerator) noexcept&#123; return static_cast&lt;std::underlying_type_t&lt;E&gt;&gt;(enumerator);&#125; 不管它怎么写，toUType现在允许这样访问tuple的字段了： 1auto val = std::get&lt;toUType(UserInfoFields::uiEmail)&gt;(uInfo); 结论 限域enum的枚举名仅在enum内可见。要转换为其它类型只能使用cast。 非限域/限域enum都支持底层类型说明语法，限域enum底层类型默认是int。非限域enum没有默认底层类型。 限域enum总是可以前置声明。非限域enum仅当指定它们的底层类型时才能前置。 11 使用 delete 而不是私有化其声明 123456789template &lt;class charT, class traits = char_traits&lt;charT&gt; &gt;class basic_ios : public ios_base &#123;public: … basic_ios(const basic_ios&amp; ) = delete; basic_ios&amp; operator=(const basic_ios&amp;) = delete; …&#125;; deleted函数不能以任何方式被调用，即使你在成员函数或者友元函数里面调用deleted函数也不能通过编译。这是较之C++98行为的一个改进，C++98中不正确的使用这些函数在链接时才被诊断出来。 通常，deleted函数被声明为public而不是private。这也是有原因的。当客户端代码试图调用成员函数，C++会在检查deleted状态前检查它的访问性。当客户端代码调用一个私有的deleted函数，一些编译器只会给出该函数是private的错误。 deleted函数还有一个重要的优势是任何函数都可以标记为deleted，而只有成员函数可被标记为private。 1234bool isLucky(int number); //原始版本bool isLucky(char) = delete; //拒绝charbool isLucky(bool) = delete; //拒绝boolbool isLucky(double) = delete; //拒绝float和double 另一个deleted函数用武之地（private成员函数做不到的地方）是禁止一些模板的实例化。假如你要求一个模板仅支持原生指针: 1234567891011121314template&lt;typename T&gt;void processPointer(T* ptr);template&lt;&gt;void processPointer&lt;void&gt;(void*) = delete;template&lt;&gt;void processPointer&lt;char&gt;(char*) = delete;template&lt;&gt;void processPointer&lt;const void&gt;(const void*) = delete;template&lt;&gt;void processPointer&lt;const char&gt;(const char*) = delete; 如果你想做得更彻底一些，你还要删除const volatile void*和const volatile char*重载版本，另外还需要一并删除其他标准字符类型的重载版本：std::wchar_t，std::char16_t和std::char32_t。 类模板在命名空间作用域中，删除特定实例化（private 是做不到的）： 123456789101112class Widget &#123;public: … template&lt;typename T&gt; void processPointer(T* ptr) &#123; … &#125; …&#125;;template&lt;&gt; //还是public，void Widget::processPointer&lt;void&gt;(void*) = delete; //但是已经被删除了 结论 使用delete函数更好 任何函数都能 delete，包括非成员函数和模板实例 12 使用override声明重写函数 派生类的虚函数重写基类同名函数，很可能一不小心就错了。 1234567891011121314151617181920class Base &#123;public: virtual void doWork(); //基类虚函数 …&#125;;class Derived: public Base &#123;public: virtual void doWork(); //重写Base::doWork … //（这里“virtual”是可以省略的）&#125;; std::unique_ptr&lt;Base&gt; upb = //创建基类指针指向派生类对象 std::make_unique&lt;Derived&gt;(); //关于std::make_unique… //请参见Item21 upb-&gt;doWork(); //通过基类指针调用doWork， //实际上是派生类的doWork //函数被调用 要想重写一个函数，必须满足下列要求： 基类函数必须是virtual 基类和派生类函数名必须完全一样（除非是析构函数) 基类和派生类函数形参类型必须完全一样 基类和派生类函数常量性constness必须完全一样 基类和派生类函数的返回值和异常说明（exception specifications）必须兼容 除了这些C++98就存在的约束外，C++11又添加了一个： 函数的引用限定符（reference qualifiers）必须完全一样。它可以限定成员函数只能用于左值或者右值： 1234567891011121314class Widget &#123;public: … void doWork() &amp;; //只有*this为左值的时候才能被调用 void doWork() &amp;&amp;; //只有*this为右值的时候才能被调用&#125;; …Widget makeWidget(); //工厂函数（返回右值）Widget w; //普通对象（左值）…w.doWork(); //调用被左值引用限定修饰的Widget::doWork版本 //（即Widget::doWork &amp;）makeWidget().doWork(); //调用被右值引用限定修饰的Widget::doWork版本 //（即Widget::doWork &amp;&amp;） 对于下面的例子： 1234567class Base &#123;public: virtual void mf1() const; virtual void mf2(int x); virtual void mf3() &amp;; void mf4() const;&#125;; C++11提供一个方法让你可以显式地指定一个派生类函数是基类版本的重写：将它声明为override。还是上面那个例子，我们可以这样做： 1234567class Derived: public Base &#123;public: virtual void mf1() override; virtual void mf2(unsigned int x) override; virtual void mf3() &amp;&amp; override; virtual void mf4() const override;&#125;; 代码不能编译，当然了，因为这样写的时候，编译器会显示所有与重写有关的问题。这也是你想要的，以及为什么要在所有重写函数后面加上override。 没有override，你只能寄希望于完善的单元测试。 C++11引入了两个上下文关键字（contextual keywords），override和final（向虚函数添加final可以防止派生类重写。final也能用于类，这时这个类不能用作基类）。 函数引用限定符 reference qualifiers。如果我们想写一个函数只接受左值实参，我们声明一个non-const左值引用形参： 1void doSomething(Widget&amp; w); //只接受左值Widget对象 如果我们想写一个函数只接受右值实参，我们声明一个右值引用形参： 1void doSomething(Widget&amp;&amp; w); //只接受右值Widget对象 成员函数的引用限定可以很容易的区分一个成员函数被哪个对象（即*this）调用。它和在成员函数声明尾部添加一个const很相似，暗示了调用这个成员函数的对象（即*this）是const的。 考虑下面一个例子： 123456789class Widget &#123;public: using DataType = std::vector&lt;double&gt;; … DataType&amp; data() &#123; return values; &#125; …private: DataType values;&#125;; 客户端代码： 123Widget w;…auto vals1 = w.data(); //拷贝w.values到vals1 Widget::data函数的返回值是一个左值引用（准确的说是std::vector&lt;double&gt;&amp;）, 因为左值引用是左值，所以vals1是从左值初始化的。因此vals1由w.values拷贝构造而得。 现在假设我们有一个创建Widgets的工厂函数， 1Widget makeWidget(); 我们想用makeWidget返回的Widget里的std::vector初始化一个变量： 1auto vals2 = makeWidget().data(); //拷贝Widget里面的值到vals2 Widget是makeWidget返回的临时对象（即右值），所以将其中的std::vector进行拷贝纯属浪费。最好是移动，但是因为data返回左值引用，C++的规则要求编译器不得不生成一个拷贝。 指明当data被右值Widget对象调用的时候结果也应该是一个右值。现在就可以使用引用限定： 1234567891011121314class Widget &#123;public: using DataType = std::vector&lt;double&gt;; … DataType&amp; data() &amp; //对于左值Widgets, &#123; return values; &#125; //返回左值 DataType data() &amp;&amp; //对于右值Widgets, &#123; return std::move(values); &#125; //返回右值 …private: DataType values;&#125;; data重载的返回类型是不同的，左值引用重载版本返回一个左值引用（即一个左值），右值引用重载返回一个临时对象（即一个右值）。这意味着现在客户端的行为和我们的期望相符了： 1234auto vals1 = w.data(); //调用左值重载版本的Widget::data， //拷贝构造vals1auto vals2 = makeWidget().data(); //调用右值重载版本的Widget::data, //移动构造vals2 结论 为重写函数加上override 成员函数引用限定，区别对待左值对象和右值对象（即*this) 13 优先考虑 const_iterator 而不是 iterator STL const_iterator等价于指向常量的指针（pointer-to-const）。它们都指向不能被修改的值。标准实践是能加上const就加上。 只是需要注意，C++11 和 C++98 对 const_iterator 的支持不一样。 没办法简简单单的从non-const容器中获取const_iterator。 123456789101112typedef std::vector&lt;int&gt;::iterator IterT; //typedeftypedef std::vector&lt;int&gt;::const_iterator ConstIterT;std::vector&lt;int&gt; values;…ConstIterT ci = std::find(static_cast&lt;ConstIterT&gt;(values.begin()), //cast static_cast&lt;ConstIterT&gt;(values.end()), //cast 1983);values.insert(static_cast&lt;IterT&gt;(ci), 1998); //可能无法通过编译， //原因见下 因为向 insert 传入const_iterator不能通过编译，所以我们将const_iterator 转换为iterator的。 上面的代码仍然可能无法编译，因为没有一个可移植的从const_iterator到iterator的方法，即使使用static_cast也不行。 所有的这些都在C++11中改变了，现在const_iterator既容易获取又容易使用。容器的成员函数cbegin和cend产出const_iterator，甚至对于non-const容器也可用，那些之前使用iterator指示位置（如insert和erase）的STL成员函数也可以使用const_iterator了。 12345std::vector&lt;int&gt; values; //和之前一样…auto it = //使用cbegin std::find(values.cbegin(), values.cend(), 1983); //和cendvalues.insert(it, 1998); C++11 的一个缺陷是，对于 非成员函数，没有类似的 cbegin，cend 函数支持。C++14补上了这一空白。 非成员函数也叫 自由函数free function，即一个函数，只要不是成员函数就可被称作free function。 举个例子，我们可以泛化下面的findAndInsert： 12345678910111213template&lt;typename C, typename V&gt;void findAndInsert(C&amp; container, //在容器中查找第一次 const V&amp; targetVal, //出现targetVal的位置， const V&amp; insertVal) //然后在那插入insertVal&#123; using std::cbegin; using std::cend; auto it = std::find(cbegin(container), //非成员函数cbegin cend(container), //非成员函数cend targetVal); container.insert(it, insertVal);&#125; 它可以在C++14工作良好，但是很遗憾，C++11不在良好之列。 如果你使用C++11，并且想写一个最大程度通用的代码，而你使用的STL没有提供缺失的非成员函数cbegin，你可以简单的写下你自己的实现。比如，下面就是非成员函数cbegin的实现： 1234template &lt;class C&gt;auto cbegin(const C&amp; container)-&gt;decltype(std::begin(container)) &#123; return std::begin(container); //解释见下&#125; 结论 优先考虑const_iterator而非iterator 在最大程度通用的代码中，优先考虑非成员函数版本的begin，end，rbegin等，而非同名成员函数 14 如果函数不抛出异常请使用noexcept 调用者可以查看函数是否声明为noexcept，这个可以影响到调用代码的异常安全性（exception safety）和效率。就其本身而言，函数是否为noexcept和成员函数是否const一样重要。当你知道这个函数不会抛异常而没加上noexcept，那这个接口说明就有点差劲了。 noexcept 允许编译器生成更好的目标代码。 两种表达方式如下： 12int f(int x) throw(); //C++98风格，没有来自f的异常int f(int x) noexcept; //C++11风格，没有来自f的异常 如果在运行时，f出现一个异常，那么就和f的异常说明冲突了。在C++98的异常说明中，调用栈（the call stack）会展开至f的调用者，在一些与这地方不相关的动作后，程序被终止。C++11异常说明的运行时行为有些不同：调用栈只是可能在程序终止前展开。 展开调用栈和可能展开调用栈两者对于代码生成（code generation）有非常大的影响。在一个noexcept函数中，当异常可能传播到函数外时，优化器不需要保证运行时栈（the runtime stack）处于可展开状态；也不需要保证当异常离开noexcept函数时，noexcept函数中的对象按照构造的反序析构。而标注 “throw()” 异常声明的函数缺少这样的优化灵活性，没加异常声明的函数也一样。可以总结一下： 123RetType function(params) noexcept; //极尽所能优化RetType function(params) throw(); //较少优化RetType function(params); //较少优化 这是一个充分的理由使得你当知道它不抛异常时加上noexcept。 另外对于一些容器数据结构的构造，如果可以就移动，如果必要则复制。对于这个函数只有在知晓移动不抛异常的情况下用C++11的移动操作替换C++98的复制操作才是安全的。 但是如何知道一个函数中的移动操作是否产生异常？答案很明显：它检查这个操作是否被声明为noexcept。 像是 std::vector::push_back 之类的函数调用std::move_if_noexcept，这是个std::move的变体，根据其中类型的移动构造函数是否为noexcept的. std::move_if_noexcept查阅std::is_nothrow_move_constructible这个type trait. swap函数是noexcept的另一个绝佳用地。swap是STL算法实现的一个关键组件，它也常用于拷贝运算符重载中。它的广泛使用意味着对其施加不抛异常的优化是非常有价值的。有趣的是，标准库的swap是否noexcept有时依赖于用户定义的swap是否noexcept。比如，数组和std::pair的swap声明如下： 1234567891011template &lt;class T, size_t N&gt;void swap(T (&amp;a)[N], T (&amp;b)[N]) noexcept(noexcept(swap(*a, *b))); //见下文template &lt;class T1, class T2&gt;struct pair &#123; … void swap(pair&amp; p) noexcept(noexcept(swap(first, p.first)) &amp;&amp; noexcept(swap(second, p.second))); …&#125;; 这些函数视情况noexcept：它们是否noexcept依赖于noexcept声明中的表达式是否noexcept。 一些函数很自然的不应该抛异常，尤其是移动操作和swap。使其noexcept有重大意义，只要可能就应该将它们实现为noexcept。 对于一些函数，使其成为noexcept是很重要的，它们应当默认如是。在C++98，允许内存释放（memory deallocation）函数（即operator delete和operator delete[]）和析构函数抛出异常是糟糕的代码设计，C++11将这种作风升级为语言规则。 默认情况下，内存释放函数和析构函数——不管是用户定义的还是编译器生成的——都是隐式noexcept。因此它们不需要声明noexcept。 析构函数非隐式noexcept的情况仅当类的数据成员（包括继承的成员还有继承成员内的数据成员）明确声明它的析构函数可能抛出异常（如声明“noexcept(false)”）。 如果一个对象的析构函数可能被标准库使用（比如在容器内或者被传给一个算法），析构函数又可能抛异常，那么程序的行为是未定义的。 有时候, 一些库函数, C++98的函数, 即使是决不抛出异常的, 也没有标识为 noexcept. 因为从C标准库移动到了std命名空间，也可能缺少异常规范，std::strlen就是一个例子，它没有声明noexcept. 另外C++98异常规范和C++11不同. 结论 noexcept是函数接口的一部分，这意味着调用者可能会依赖它 noexcept函数较之于non-noexcept函数更容易优化 noexcept对于移动语义，swap，内存释放函数和析构函数非常有用 大多数函数是异常中立的（可能抛也可能不抛异常）而不是noexcept 15 尽量使用 constexpr 从概念上来说，constexpr表明一个值不仅仅是常量，还是编译期可知的。这个表述并不全面，因为当constexpr被用于函数的时候，事情就有一些细微差别了。 但是，并不能保证constexpr函数的结果是const，也不能保证它们的返回值是在编译期可知的。 和const一样，constexpr是编译期可知的。技术上来讲，它们的值在翻译期（translation）决议，所谓翻译（translation）不仅仅包含是编译（compilation）也包含链接（linking）。 编译期可知的值“享有特权”，它们可能被存放到只读存储空间中。对于那些嵌入式系统的开发者，这个特性是相当重要的。更广泛的应用是 “其值编译期可知” 的常量整数会出现在需要 “整型常量表达式（integral constant expression）的上下文中：包括数组大小，整数模板参数（包括std::array对象的长度），枚举名的值，对齐修饰符（alignas(val)），等等。 12345678int sz; //non-constexpr变量…constexpr auto arraySize1 = sz; //错误！sz的值在 //编译期不可知std::array&lt;int, sz&gt; data1; //错误！一样的问题constexpr auto arraySize2 = 10; //没问题，10是 //编译期可知常量std::array&lt;int, arraySize2&gt; data2; //没问题, arraySize2是constexpr 注意const不提供constexpr所能保证之事，因为const对象不需要在编译期初始化它的值。 1234int sz; …const auto arraySize = sz; //没问题，arraySize是sz的const复制std::array&lt;int, arraySize&gt; data; //错误，arraySize值在编译期不可知 简而言之，所有constexpr对象都是const，但不是所有const对象都是constexpr。如果你想编译器保证一个变量有一个值，这个值可以放到那些需要编译期常量（compile-time constants）的上下文的地方，你需要的工具是constexpr而不是const。 注意，I/O语句一般不被允许出现在constexpr函数里。 constexpr 限制 因为constexpr函数必须能在编译期值调用的时候返回编译期结果，就必须对它的实现施加一些限制。这些限制在C++11和C++14标准间有所出入。 C++11中，constexpr函数的代码不超过一行语句：一个return。听起来很受限，但实际上有两个技巧可以扩展constexpr函数的表达能力。第一，使用三元运算符“?:”来代替if-else语句，第二，使用递归代替循环。因此pow可以像这样实现： 123constexpr int pow(int base, int exp) noexcept &#123; return (exp == 0 ? 1 : base * pow(base, exp - 1));&#125; 在C++11中，有两个限制使得Point的成员函数setX和setY不能声明为constexpr。第一，它们修改它们操作的对象的状态， 并且在C++11中，constexpr成员函数是隐式的const。第二，它们有void返回类型，void类型不是C++11中的字面值类型。这两个限制在C++14中放开了，所以C++14中Point的setter（赋值器）也能声明为constexpr： 1234567class Point &#123;public: … constexpr void setX(double newX) noexcept &#123; x = newX; &#125; //C++14 constexpr void setY(double newY) noexcept &#123; y = newY; &#125; //C++14 …&#125;; 现在也能写这样的函数： 12345678//返回p相对于原点的镜像constexpr Point reflection(const Point&amp; p) noexcept&#123; Point result; //创建non-const Point result.setX(-p.xValue()); //设定它的x和y值 result.setY(-p.yValue()); return result; //返回它的副本&#125; C++14 constexpr 在C++14中，constexpr函数的限制变得非常宽松了，所以下面的函数实现成为了可能： 1234567constexpr int pow(int base, int exp) noexcept //C++14&#123; auto result = 1; for (int i = 0; i &lt; exp; ++i) result *= base; return result;&#125; constexpr函数限制为只能获取和返回字面值类型，这基本上意味着那些有了值的类型能在编译期决定。在C++11中，除了void外的所有内置类型，以及一些用户定义类型都可以是字面值类型，因为构造函数和其他成员函数可能是constexpr： 123456789101112131415class Point &#123;public: constexpr Point(double xVal = 0, double yVal = 0) noexcept : x(xVal), y(yVal) &#123;&#125; constexpr double xValue() const noexcept &#123; return x; &#125; constexpr double yValue() const noexcept &#123; return y; &#125; void setX(double newX) noexcept &#123; x = newX; &#125; void setY(double newY) noexcept &#123; y = newY; &#125;private: double x, y;&#125;; Point的构造函数可被声明为constexpr，因为如果传入的参数在编译期可知，Point的数据成员也能在编译器可知。因此这样初始化的Point就能为constexpr： 123constexpr Point p1(9.4, 27.7); //没问题，constexpr构造函数 //会在编译期“运行”constexpr Point p2(28.8, 5.3); //也没问题 类似的，xValue和yValue的getter（取值器）函数也能是constexpr，因为如果对一个编译期已知的Point对象（如一个constexpr Point对象）调用getter，数据成员x和y的值也能在编译期知道。这使得我们可以写一个constexpr函数，里面调用Point的getter并初始化constexpr的对象： 12345678constexprPoint midpoint(const Point&amp; p1, const Point&amp; p2) noexcept&#123; return &#123; (p1.xValue() + p2.xValue()) / 2, //调用constexpr (p1.yValue() + p2.yValue()) / 2 &#125;; //成员函数&#125;constexpr auto mid = midpoint(p1, p2); //使用constexpr函数的结果 //初始化constexpr对象 mid对象通过调用构造函数，getter和非成员函数来进行初始化过程就能在只读内存中被创建出来。 constexpr对象和constexpr函数可以使用的范围比non-constexpr对象和函数大得多。使用constexpr关键字可以最大化你的对象和函数可以使用的场景。 还有个重要的需要注意的是constexpr是对象和函数接口的一部分。加上constexpr相当于宣称“我能被用在C++要求常量表达式的地方”。 结论 constexpr对象是const，它被在编译期可知的值初始化 当传递编译期可知的值时，constexpr函数可以产出编译期可知的结果 constexpr对象和函数可以使用的范围比non-constexpr对象和函数要大 constexpr是对象和函数接口的一部分 16 让 const 成员函数线程安全 考虑下面的例子，计算多项式的根，多项式的根在多项式确定时，根一般是确定的，声明为 const 。 123456789101112131415161718class Polynomial &#123;public: using RootsType = std::vector&lt;double&gt;; RootsType roots() const &#123; if (!rootsAreValid) &#123; //如果缓存不可用 … //计算根 //用rootVals存储它们 rootsAreValid = true; &#125; return rootVals; &#125; private: mutable bool rootsAreValid&#123; false &#125;; //初始化器（initializer）的 mutable RootsType rootVals&#123;&#125;; &#125;; roots是const成员函数，那就表示着它是一个读操作。在没有同步的情况下，让多个线程执行读操作是安全的。 但是，在roots中，这些线程中的一个或两个可能尝试修改成员变量rootsAreValid和rootVals。这就意味着在没有同步的情况下，这些代码会有不同的线程读写相同的内存，这就是数据竞争（data race）的定义。这段代码的行为是未定义的。 问题就是roots被声明为const，但不是线程安全的。 解决这个问题最普遍简单的方法就是——使用mutex（互斥量）： 123456789101112131415161718192021class Polynomial &#123;public: using RootsType = std::vector&lt;double&gt;; RootsType roots() const &#123; std::lock_guard&lt;std::mutex&gt; g(m); //锁定互斥量 if (!rootsAreValid) &#123; //如果缓存无效 … //计算/存储根值 rootsAreValid = true; &#125; return rootsVals; &#125; //解锁互斥量 private: mutable std::mutex m; mutable bool rootsAreValid &#123; false &#125;; mutable RootsType rootsVals &#123;&#125;;&#125;; std::mutex 既不可移动，也不可复制。因而包含他们的类也同时是不可移动和不可复制的 在某些情况下，互斥量的副作用显会得过大。例如，如果你所做的只是计算成员函数被调用了多少次，使用std::atomic 修饰的计数器通常会是一个开销更小的方法（当然是否更小，取决于你使用的硬件和标准库对互斥量的实现）。 1234567891011121314class Point &#123; //2D点public: … double distanceFromOrigin() const noexcept //noexcept的使用 &#123; ++callCount; //atomic的递增 return std::sqrt((x * x) + (y * y)); &#125;private: mutable std::atomic&lt;unsigned&gt; callCount&#123; 0 &#125;; double x, y;&#125;; 与 std::mutex 类似的，实际上 std::atomic 既不可移动，也不可复制。因而包含他们的类也同时是不可移动和不可复制的 但是只使用 std::atomic 存在以下问题： 123456789101112131415161718class Widget &#123;public: … int magicValue() const &#123; if (cacheValid) return cachedValue; else &#123; auto val1 = expensiveComputation1(); auto val2 = expensiveComputation2(); cachedValue = val1 + val2; //第一步 cacheValid = true; //第二步 return cachedValid; &#125; &#125; private: mutable std::atomic&lt;bool&gt; cacheValid&#123; false &#125;; mutable std::atomic&lt;int&gt; cachedValue;&#125;; 仍然可能出现重复计算。考虑： 一个线程调用Widget::magicValue，将cacheValid视为false，执行这两个昂贵的计算，并将它们的和分配给cachedValue。 此时，第二个线程调用Widget::magicValue，也将cacheValid视为false，因此执行刚才完成的第一个线程相同的计算。（这里的“第二个线程”实际上可能是其他几个线程。） 这种行为与使用缓存的目的背道而驰。将cachedValue和CacheValid的赋值顺序交换可以解决这个问题，但结果会更糟。 假设cacheValid是false，那么： 一个线程调用Widget::magicValue，刚执行完将cacheValid设置true的语句。 在这时，第二个线程调用Widget::magicValue，检查cacheValid。看到它是true，就返回cacheValue，即使第一个线程还没有给它赋值。因此返回的值是不正确的。 对于需要同步的是单个的变量或者内存位置，使用std::atomic就足够了。不过，一旦你需要对两个以上的变量或内存位置作为一个单元来操作的话，就应该使用互斥量。 结论 确保const成员函数线程安全（先得明白什么是线程不安全），除非你确定它们永远不会在并发上下文（concurrent context）中使用。 使用std::atomic变量可能比互斥量提供更好的性能，但是它只适合操作单个变量或内存位置。 17 理解特殊成员函数的生成 C++98有四个：默认构造函数，析构函数，拷贝构造函数，拷贝赋值运算符。默认构造函数仅在类完全没有构造函数的时候才生成。 C++11特殊成员函数俱乐部迎来了两位新会员：移动构造函数和移动赋值运算符。它们的签名是： 1234567class Widget &#123;public: … Widget(Widget&amp;&amp; rhs); //移动构造函数 Widget&amp; operator=(Widget&amp;&amp; rhs); //移动赋值运算符 …&#125;; 移动操作仅在需要的时候生成，如果生成了，就会对类的non-static数据成员执行逐成员的移动。 逐成员移动的核心是对对象使用std::move，然后函数决议时会选择执行移动还是拷贝操作。记住如果支持移动就会逐成员移动类成员和基类成员，如果不支持移动就执行拷贝操作就好了。 拷贝构造与移动构造生成方式 如果你声明一个拷贝构造函数，但是没有声明拷贝赋值运算符，如果写的代码用到了拷贝赋值，编译器会帮助你生成拷贝赋值运算符。同样的，如果你声明拷贝赋值运算符但是没有拷贝构造函数，代码用到拷贝构造函数时编译器就会生成它。 两个移动操作不是相互独立的。如果你声明了其中一个，编译器就不再生成另一个。如果你给类声明了，比如，一个移动构造函数，就表明对于移动操作应怎样实现，与编译器应生成的默认逐成员移动有些区别。如果逐成员移动构造有些问题，那么逐成员移动赋值同样也可能有问题。所以声明移动构造函数阻止移动赋值运算符的生成，声明移动赋值运算符同样阻止编译器生成移动构造函数。 如果一个类显式声明了拷贝操作，编译器就不会生成移动操作。这种限制的解释是如果声明拷贝操作（构造或者赋值）就暗示着平常拷贝对象的方法（逐成员拷贝）不适用于该类，编译器会明白如果逐成员拷贝对拷贝操作来说不合适，逐成员移动也可能对移动操作来说不合适。 Rule of Three 如果你声明了拷贝构造函数，拷贝赋值运算符，或者析构函数三者之一，你应该也声明其余两个。 如果一个类显式声明了拷贝操作，编译器就不会生成移动操作。所以，C++11不会为那些有用户定义的析构函数的类生成移动操作。 仅当下面条件成立时才会生成移动操作（当需要时）： 类中没有拷贝操作 类中没有移动操作 类中没有用户定义的析构 假设编译器生成的函数行为是正确的（即逐成员拷贝类non-static数据是你期望的行为），你的工作很简单，C++11的= default就可以表达你想做的： 1234567891011class Widget &#123; public: … ~Widget(); //用户声明的析构函数 … //默认拷贝构造函数 Widget(const Widget&amp;) = default; //的行为还可以 Widget&amp; //默认拷贝赋值运算符 operator=(const Widget&amp;) = default; //的行为还可以 … &#125;; 就算编译器乐于为你的类生成拷贝和移动操作，生成的函数也如你所愿，你也应该手动声明它们然后加上= default。这看起来比较多余，但是它让你的意图更明确，也能帮助你避免一些微妙的bug。 声明析构有潜在的副作用：它阻止了移动操作的生成。然而，拷贝操作的生成是不受影响的。所以手动声明为 =default 是有意义的。 C++11 处理规则 默认构造函数：和C++98规则相同。仅当类不存在用户声明的构造函数时才自动生成。 析构函数：基本上和C++98相同；稍微不同的是现在析构默认noexcept（参见Item14）。和C++98一样，仅当基类析构为虚函数时该类析构才为虚函数。 拷贝构造函数：和C++98运行时行为一样：逐成员拷贝non-static数据。仅当类没有用户定义的拷贝构造时才生成。如果类声明了移动操作它就是delete的。但是，当用户声明了拷贝赋值或者析构，该函数自动生成已被废弃。 拷贝赋值运算符：和C++98运行时行为一样：逐成员拷贝赋值non-static数据。仅当类没有用户定义的拷贝赋值时才生成。如果类声明了移动操作它就是delete的。但是，当用户声明了拷贝构造或者析构，该函数自动生成已被废弃。 移动构造函数和移动赋值运算符：都对非static数据执行逐成员移动。仅当类没有用户定义的拷贝操作，移动操作或析构时才自动生成。 结论 特殊成员函数是编译器可能自动生成的函数：默认构造函数，析构函数，拷贝操作，移动操作。 移动操作仅当类没有显式声明移动操作，拷贝操作，析构函数时才自动生成。 拷贝构造函数仅当类没有显式声明拷贝构造函数时才自动生成，并且如果用户声明了移动操作，拷贝构造就是delete。 拷贝赋值运算符仅当类没有显式声明拷贝赋值运算符时才自动生成，并且如果用户声明了移动操作，拷贝赋值运算符就是delete。当用户声明了析构函数，拷贝操作的自动生成已被废弃。 成员函数模板不抑制特殊成员函数的生成。 18 对独占资源使用 std::unique_ptr 默认情况下，std::unique_ptr大小等同于原始指针，而且对于大多数操作（包括取消引用），他们执行的指令完全相同。这意味着你甚至可以在内存和时间都比较紧张的情况下使用它。如果原始指针够小够快，那么std::unique_ptr一样可以。 std::unique_ptr体现了专有所有权（exclusive ownership）语义。 一个non-null std::unique_ptr始终拥有其指向的内容。移动一个std::unique_ptr将所有权从源指针转移到目的指针。（源指针被设为null） 拷贝一个std::unique_ptr是不允许的，因为如果你能拷贝一个std::unique_ptr，你会得到指向相同内容的两个std::unique_ptr，每个都认为自己拥有（并且应当最后销毁）资源，销毁时就会出现重复销毁。 因此，std::unique_ptr是一种只可移动类型（move-only type）。当析构时，一个non-null std::unique_ptr销毁它指向的资源。默认情况下，资源析构通过对std::unique_ptr里原始指针调用delete来实现。 Investment继承关系的工厂函数可以这样声明： 123template&lt;typename... Ts&gt; //返回指向对象的std::unique_ptr，std::unique_ptr&lt;Investment&gt; //对象使用给定实参创建makeInvestment(Ts&amp;&amp;... params); 调用者应该在单独的作用域中使用返回的std::unique_ptr智能指针： 123456&#123; … auto pInvestment = //pInvestment是 makeInvestment( arguments ); //std::unique_ptr&lt;Investment&gt;类型 …&#125; //销毁 *pInvestment std::unique_ptr将保证指向内容的析构函数被调用，销毁对应资源。 这个规则也有些例外。大多数情况发生于不正常的程序终止。 如果一个异常传播到线程的基本函数，比如程序初始线程的main函数外，或者违反noexcept说明，局部变量可能不会被销毁；如果std::abort或者退出函数（如std::_Exit，std::exit，或std::quick_exit）被调用，局部变量一定没被销毁。 自定义删除器 1234567891011121314151617181920212223242526auto delInvmt = [](Investment* pInvestment) //自定义删除器 &#123; //（lambda表达式） makeLogEntry(pInvestment); delete pInvestment; &#125;;template&lt;typename... Ts&gt;std::unique_ptr&lt;Investment, decltype(delInvmt)&gt; //更改后的返回类型makeInvestment(Ts&amp;&amp;... params)&#123; std::unique_ptr&lt;Investment, decltype(delInvmt)&gt; //应返回的指针 pInv(nullptr, delInvmt); if (/*一个Stock对象应被创建*/) &#123; pInv.reset(new Stock(std::forward&lt;Ts&gt;(params)...)); &#125; else if ( /*一个Bond对象应被创建*/ ) &#123; pInv.reset(new Bond(std::forward&lt;Ts&gt;(params)...)); &#125; else if ( /*一个RealEstate对象应被创建*/ ) &#123; pInv.reset(new RealEstate(std::forward&lt;Ts&gt;(params)...)); &#125; return pInv;&#125; 上述代码中： delInvmt是从makeInvestment返回的对象的自定义的删除器。 删除器类型必须作为第二个类型实参传给std::unique_ptr。 尝试将原始指针（比如new创建）赋值给std::unique_ptr通不过编译，因为是一种从原始指针到智能指针的隐式转换。这种隐式转换会出问题，所以C++11的智能指针禁止这个行为。这就是通过reset来让pInv接管通过new创建的对象的所有权的原因。 使用new时，我们使用std::forward把传给makeInvestment的实参完美转发出去。 自定义删除器的一个形参，类型是Investment*，不管在makeInvestment内部创建的对象的真实类型（如Stock，Bond，或RealEstate）是什么，它最终在lambda表达式中，作为Investment*对象被删除。这意味着我们通过基类指针删除派生类实例，为此，基类Investment必须有虚析构函数。 C++14中，存在返回类型推导，写法更为简单： 12345678910111213141516171819202122232425template&lt;typename... Ts&gt;auto makeInvestment(Ts&amp;&amp;... params) //C++14&#123; auto delInvmt = [](Investment* pInvestment) //现在在 &#123; //makeInvestment里 makeLogEntry(pInvestment); delete pInvestment; &#125;; std::unique_ptr&lt;Investment, decltype(delInvmt)&gt; //同之前一样 pInv(nullptr, delInvmt); if ( … ) //同之前一样 &#123; pInv.reset(new Stock(std::forward&lt;Ts&gt;(params)...)); &#125; else if ( … ) //同之前一样 &#123; pInv.reset(new Bond(std::forward&lt;Ts&gt;(params)...)); &#125; else if ( … ) //同之前一样 &#123; pInv.reset(new RealEstate(std::forward&lt;Ts&gt;(params)...)); &#125; return pInv; //同之前一样&#125; 当使用默认删除器时（如delete），你可以合理假设std::unique_ptr对象和原始指针大小相同。 但是当自定义删除器时，情况可能不再如此。函数指针形式的删除器，通常会使std::unique_ptr的从一个字（word）大小增加到两个。这可能导致 std::unique_ptr对象变得过大。 123456789void delInvmt2(Investment* pInvestment) //函数形式的&#123; //自定义删除器 makeLogEntry(pInvestment); delete pInvestment;&#125;template&lt;typename... Ts&gt; //返回类型大小是std::unique_ptr&lt;Investment, void (*)(Investment*)&gt; //Investment*的指针makeInvestment(Ts&amp;&amp;... params); //加至少一个函数指针的大小 对于函数对象形式的删除器来说，变化的大小取决于函数对象中存储的状态多少，无状态函数（stateless function）对象（比如不捕获变量的lambda表达式）对大小没有影响，这意味当自定义删除器可以实现为函数或者lambda时，尽量使用lambda。 123456789auto delInvmt1 = [](Investment* pInvestment) //无状态lambda的 &#123; //自定义删除器 makeLogEntry(pInvestment); delete pInvestment; &#125;;template&lt;typename... Ts&gt; //返回类型大小是std::unique_ptr&lt;Investment, decltype(delInvmt1)&gt; //Investment*的大小makeInvestment(Ts&amp;&amp;... args); 向 std::shared_ptr 的自动转化 std::unique_ptr是C++11中表示专有所有权的方法，但是其最吸引人的功能之一是它可以轻松高效的转换为std::shared_ptr： 12std::shared_ptr&lt;Investment&gt; sp = //将std::unique_ptr makeInvestment(arguments); //转为std::shared_ptr 这就是std::unique_ptr非常适合用作工厂函数返回类型的原因的关键部分。 工厂函数无法知道调用者是否要对它们返回的对象使用专有所有权语义，或者共享所有权（即std::shared_ptr）是否更合适。 结论 std::unique_ptr是轻量级、快速的、只可移动（move-only）的管理专有所有权语义资源的智能指针 默认情况，资源销毁通过delete实现，但是支持自定义删除器。有状态的删除器（捕获变量的lambda表达式）和函数指针（带参数）会增加std::unique_ptr对象的大小。所以是一般使用无状态的 lambda 表达式 将std::unique_ptr转化为std::shared_ptr非常简单 19 对于共享资源使用std::shared_ptr std::shared_ptr通过引用计数（reference count）来确保它是否是最后一个指向某种资源的指针，引用计数关联资源并跟踪有多少std::shared_ptr指向该资源。如果std::shared_ptr在计数值递减后发现引用计数值为零，没有其他std::shared_ptr指向该资源，它就会销毁资源。 引用计数暗示着性能问题： std::shared_ptr大小是原始指针的两倍，因为它内部包含一个指向资源的原始指针，还包含一个指向资源的引用计数值的原始指针。 引用计数的内存几乎使用动态分配。 std::make_shared创建std::shared_ptr可以避免引用计数的动态分配，但是还存在一些std::make_shared不能使用的场景，这时候引用计数就会动态分配。 递增递减引用计数必须是原子性的，因为多个reader、writer可能在不同的线程。比如，指向某种资源的std::shared_ptr可能在一个线程执行析构（于是递减指向的对象的引用计数），在另一个不同的线程，std::shared_ptr指向相同的对象，但是执行的却是拷贝操作（因此递增了同一个引用计数）。原子操作通常比非原子操作要慢，所以即使引用计数通常只有一个word大小，你也应该假定读写它们是存在开销的。 移动std::shared_ptr会比拷贝它要快：拷贝要求递增引用计数值，移动不需要。移动赋值运算符同理，所以移动构造比拷贝构造快，移动赋值运算符也比拷贝赋值运算符快。 与 std::unique_ptr 的区别 std::shared_ptr使用delete作为资源的默认销毁机制，但是它也支持自定义的删除器。这种支持有别于std::unique_ptr。对于std::unique_ptr来说，删除器类型是智能指针类型的一部分。对于std::shared_ptr则不是： 1234567891011auto loggingDel = [](Widget *pw) //自定义删除器 &#123; makeLogEntry(pw); delete pw; &#125;;std::unique_ptr&lt; //删除器类型是 Widget, decltype(loggingDel) //指针类型的一部分 &gt; upw(new Widget, loggingDel);std::shared_ptr&lt;Widget&gt; //删除器类型不是 spw(new Widget, loggingDel); //指针类型的一部分 std::shared_ptr的设计更为灵活。考虑有两个std::shared_ptr&lt;Widget&gt;，每个自带不同的删除器（比如通过lambda表达式自定义删除器）： 1234auto customDeleter1 = [](Widget *pw) &#123; … &#125;; //自定义删除器，auto customDeleter2 = [](Widget *pw) &#123; … &#125;; //每种类型不同std::shared_ptr&lt;Widget&gt; pw1(new Widget, customDeleter1);std::shared_ptr&lt;Widget&gt; pw2(new Widget, customDeleter2); 因为 pw1 和 pw2 有相同的类型，所以它们都可以放到存放那个类型的对象的容器中： 1std::vector&lt;std::shared_ptr&lt;Widget&gt;&gt; vpw&#123; pw1, pw2 &#125;; 它们也能相互赋值，也可以传入一个形参为std::shared_ptr&lt;Widget&gt;的函数。但是自定义删除器类型不同的std::unique_ptr就不行，因为std::unique_ptr把删除器视作类型的一部分。 另一个不同于std::unique_ptr的地方是，指定自定义删除器不会改变std::shared_ptr对象的大小。不管删除器是什么，一个std::shared_ptr对象都是两个指针大小。这是个好消息，但是它应该让你隐隐约约不安。自定义删除器可以是函数对象，函数对象可以包含任意多的数据。它意味着函数对象是任意大的。 引用计数是另一个更大的数据结构的一部分，那个数据结构通常叫做控制块（control block）。每个std::shared_ptr管理的对象都有个相应的控制块。控制块除了包含引用计数值外还有一个自定义删除器的拷贝，当然前提是存在自定义删除器。如果用户还指定了自定义分配器，控制块也会包含一个分配器的拷贝。控制块可能还包含一些额外的数据，一个次级引用计数weak count。 对于一个创建指向对象的std::shared_ptr的函数来说不可能知道是否有其他std::shared_ptr早已指向那个对象，所以控制块的创建会遵循下面几条规则： std::make_shared总是创建一个控制块。它创建一个要指向的新对象，所以可以肯定std::make_shared调用时对象不存在其他控制块。 当从独占指针（即std::unique_ptr或者std::auto_ptr）上构造出std::shared_ptr时会创建控制块。独占指针没有使用控制块，所以指针指向的对象没有关联控制块。（作为构造的一部分，std::shared_ptr侵占独占指针所指向的对象的独占权，所以独占指针被设置为null） 当从原始指针上构造出std::shared_ptr时会创建控制块。如果你想从一个早已存在控制块的对象上创建std::shared_ptr，你将假定传递一个std::shared_ptr或者std::weak_ptr。作为构造函数实参，而不是原始指针。用std::shared_ptr或者std::weak_ptr作为构造函数实参创建std::shared_ptr不会创建新控制块，因为它可以依赖传递来的智能指针指向控制块。 从原始指针上构造超过一个std::shared_ptr会造成未定义行为，因为指向的对象有多个控制块关联。多个控制块意味着多个引用计数值，多个引用计数值意味着对象将会被销毁多次（每个引用计数一次）。那意味着像下面的代码是有问题的： 12345auto pw = new Widget; //pw是原始指针…std::shared_ptr&lt;Widget&gt; spw1(pw, loggingDel); //为*pw创建控制块…std::shared_ptr&lt;Widget&gt; spw2(pw, loggingDel); //为*pw创建第二个控制块 使用智能指针而不是原始指针。 使用 std::shared_ptr 的建议是：第一，避免传给std::shared_ptr构造函数原始指针。通常替代方案是使用std::make_shared，不过用std::make_shared就没办法使用自定义删除器。第二，如果你必须传给std::shared_ptr构造函数原始指针，直接传new出来的结果，不要传指针变量。 12std::shared_ptr&lt;Widget&gt; spw1(new Widget, //直接使用new的结果 loggingDel); 创建spw2也会很自然的用spw1作为初始化参数（即用std::shared_ptr拷贝构造函数），那就没什么问题了： 1std::shared_ptr&lt;Widget&gt; spw2(spw1); //spw2使用spw1一样的控制块 this 指针：避免创建多余的控制块 std::enable_shared_from_this。如果你想创建一个用std::shared_ptr管理的类，这个类能够用this指针安全地创建一个std::shared_ptr，std::enable_shared_from_this就可作为基类的模板类。Widget将会继承自std::enable_shared_from_this： 123456class Widget: public std::enable_shared_from_this&lt;Widget&gt; &#123;public: … void process(); …&#125;; 这个标准名字就是奇异递归模板模式（The Curiously Recurring Template Pattern（CRTP））。 std::enable_shared_from_this 定义了一个成员函数，成员函数会创建指向当前对象的std::shared_ptr 却不创建多余控制块。这个成员函数就是shared_from_this，无论在哪当你想在成员函数中使用std::shared_ptr指向this所指对象时都请使用它。这里有个Widget::process的安全实现： 1234567void Widget::process()&#123; //和之前一样，处理Widget … //把指向当前对象的std::shared_ptr加入processedWidgets processedWidgets.emplace_back(shared_from_this());&#125; 从内部来说，shared_from_this查找当前对象控制块，然后创建一个新的std::shared_ptr关联这个控制块。设计的依据是当前对象已经存在一个关联的控制块。 要想符合设计依据的情况，必须已经存在一个指向当前对象的std::shared_ptr（比如调用shared_from_this的成员函数外面已经存在一个std::shared_ptr）。如果没有std::shared_ptr指向当前对象（即当前对象没有关联控制块），行为是未定义的，shared_from_this通常抛出一个异常。 要想防止客户端在存在一个指向对象的std::shared_ptr前先调用含有shared_from_this的成员函数，继承自std::enable_shared_from_this的类通常将它们的构造函数声明为private，并且让客户端通过返回std::shared_ptr的工厂函数创建对象。以Widget为例，代码可以是这样： 1234567891011class Widget: public std::enable_shared_from_this&lt;Widget&gt; &#123;public: //完美转发参数给private构造函数的工厂函数 template&lt;typename... Ts&gt; static std::shared_ptr&lt;Widget&gt; create(Ts&amp;&amp;... params); … void process(); //和前面一样 …private: … //构造函数&#125;; 确保需要先调用 create，才能调用 process。 shared_ptr 开销 控制块通常只占几个word大小，自定义删除器和分配器可能会让它变大一点。通常控制块的实现比你想的更复杂一些。它使用继承，甚至里面还有一个虚函数（用来确保指向的对象被正确销毁）。这意味着使用std::shared_ptr ，会带来使用虚函数带来的成本。 使用默认删除器和默认分配器，使用std::make_shared创建std::shared_ptr，产生的控制块只需三个word大小。它的分配基本上是无开销的。 对std::shared_ptr解引用的开销不会比原始指针高。执行需要原子引用计数修改的操作需要承担一两个原子操作开销，这些操作通常都会一一映射到机器指令上，所以即使对比非原子指令来说，原子指令开销较大，但是它们仍然只是单个指令上的。对于每个被std::shared_ptr指向的对象来说，控制块中的虚函数机制产生的开销通常只需要承受一次，即对象销毁的时候。 如果独占资源可行或者可能可行，用std::unique_ptr是一个更好的选择。它的性能表现更接近于原始指针，并且从std::unique_ptr升级到std::shared_ptr也很容易，因为std::shared_ptr可以从std::unique_ptr上创建。 从 std::shared_ptr转换到 std::unique_ptr 是不行的 。当你的资源由std::shared_ptr管理，现在又想修改资源生命周期管理方式是没有办法的。即使引用计数为一，你也不能重新修改资源所有权，改用std::unique_ptr管理它。资源和指向它的std::shared_ptr的签订的所有权协议是“除非死亡否则永不分开”。不能分离，不能废除，没有特许。 std::shared_ptr不能处理的另一个东西是数组。和std::unique_ptr不同的是，std::shared_ptr的API设计之初就是针对单个对象的，没有办法std::shared_ptr&lt;T[]&gt;。 std::shared_ptr没有提供operator[]，所以数组索引操作需要借助怪异的指针算术。另一方面，std::shared_ptr支持转换为指向基类的指针，这对于单个对象来说有效，但是当用于数组类型时这是容易出问题。（出于这个原因，std::unique_ptr&lt;T[]&gt; API禁止这种转换。） 更重要的是，C++11已经提供了很多内置数组的候选方案（比如std::array，std::vector，std::string）。所以，声明一个指向数组的智能指针几乎总是糟糕的设计。 结论 std::shared_ptr为有共享所有权的任意资源提供一种自动垃圾回收的便捷方式。 较之于std::unique_ptr，std::shared_ptr对象通常大两倍，控制块会产生开销，需要原子性的引用计数修改操作。 std::shared_ptr 默认资源销毁是通过delete，但是也支持自定义删除器。但是删除器的类型不是 std::shared_ptr 的类型的一部分。 避免从原始指针变量上创建std::shared_ptr。 20 当std::shared_ptr可能悬空时使用std::weak_ptr 一个真正的智能指针应该跟踪所指对象，在悬空时知晓，悬空（dangle）就是指针指向的对象不再存在。这就是对std::weak_ptr最精确的描述。 std::weak_ptr不能解引用，也不能测试是否为空值。因为std::weak_ptr不是一个独立的智能指针。它是std::shared_ptr的增强。 std::weak_ptr通常从std::shared_ptr上创建。当从std::shared_ptr上创建std::weak_ptr时两者指向相同的对象，但是std::weak_ptr不会影响所指对象的引用计数。 12345678auto spw = //spw创建之后，指向的Widget的 std::make_shared&lt;Widget&gt;(); //引用计数（ref count，RC）为1。 …std::weak_ptr&lt;Widget&gt; wpw(spw); //wpw指向与spw所指相同的Widget。RC仍为1…spw = nullptr; //RC变为0，Widget被销毁。 //wpw现在悬空 悬空的std::weak_ptr被称作已经expired（过期）。你可以用它直接做测试： 1if (wpw.expired()) … //如果wpw没有指向对象… 但是通常你期望的是检查std::weak_ptr是否已经过期，如果没有过期则访问其指向的对象。不过，将检查是否过期和解引用分开会引入竞态条件：在调用expired和解引用操作之间，另一个线程可能对指向这对象的std::shared_ptr重新赋值或者析构，并由此造成对象已析构。这种情况下，你的解引用将会产生未定义行为。 你需要的是一个原子操作检查std::weak_ptr是否已经过期，如果没有过期就访问所指对象。这可以通过从std::weak_ptr创建std::shared_ptr来实现，具体有两种形式可以从std::weak_ptr上创建std::shared_ptr，具体用哪种取决于std::weak_ptr过期时你希望std::shared_ptr表现出什么行为。 一种形式是std::weak_ptr::lock，它返回一个std::shared_ptr，如果std::weak_ptr过期这个std::shared_ptr为空： 123std::shared_ptr&lt;Widget&gt; spw1 = wpw.lock(); //如果wpw过期，spw1就为空 auto spw2 = wpw.lock(); //同上，但是使用auto 另一种形式是以std::weak_ptr为实参构造std::shared_ptr。这种情况中，如果std::weak_ptr过期，会抛出一个异常： 1std::shared_ptr&lt;Widget&gt; spw3(wpw); //如果wpw过期，抛出std::bad_weak_ptr异常 一个例子 考虑一个工厂函数，它基于一个唯一ID从只读对象上产出智能指针。根据条款18的描述，工厂函数会返回一个该对象类型的std::unique_ptr： 1std::unique_ptr&lt;const Widget&gt; loadWidget(WidgetID id); 如果调用loadWidget是一个昂贵的操作（比如它操作文件或者数据库I/O）并且重复使用ID很常见，一个合理的优化是再写一个函数除了完成loadWidget做的事情之外再缓存它的结果。另一个合理的优化可以是当Widget不再使用的时候销毁它的缓存。 对于可缓存的工厂函数，返回std::unique_ptr不是好的选择。调用者应该接收缓存对象的智能指针，调用者也应该确定这些对象的生命周期，但是缓存本身也需要一个指针指向它所缓存的对象。缓存对象的指针需要知道它是否已经悬空，因为当工厂客户端使用完工厂产生的对象后，对象将被销毁，关联的缓存条目会悬空。所以缓存应该使用std::weak_ptr，这可以知道是否已经悬空。这意味着工厂函数返回值类型应该是std::shared_ptr，因为只有当对象的生命周期由std::shared_ptr管理时，std::weak_ptr才能检测到悬空。 一个简版的实现： 123456789101112131415std::shared_ptr&lt;const Widget&gt; fastLoadWidget(WidgetID id)&#123; static std::unordered_map&lt;WidgetID, std::weak_ptr&lt;const Widget&gt;&gt; cache; //std::weak_ptr&lt;const Widget&gt; auto objPtr = cache[id].lock(); //objPtr是去缓存对象的 //std::shared_ptr（或 //当对象不在缓存中时为null） if (!objPtr) &#123; //如果不在缓存中 objPtr = loadWidget(id); //加载它 cache[id] = objPtr; //缓存它 &#125; return objPtr;&#125; fastLoadWidget的实现仍有以下问题：缓存可能会累积过期的std::weak_ptr，这些指针对应了不再使用的Widget（也已经被销毁了）。 另一个例子 考虑第二个用例：观察者设计模式（Observer design pattern）。 此模式的主要组件是subjects（状态可能会更改的对象）和observers（状态发生更改时要通知的对象）。在大多数实现中，每个subject都包含一个数据成员，该成员持有指向其observers的指针。这使subjects很容易发布状态更改通知。 subjects对控制observers的生命周期（即它们什么时候被销毁）没有兴趣，但是subjects对确保另一件事具有极大的兴趣，那事就是一个observer被销毁时，不再尝试访问它。一个合理的设计是每个subject持有一个std::weak_ptrs容器指向observers，因此可以在使用前检查是否已经悬空。 最后一个例子 考虑一个持有三个对象A、B、C的数据结构，A和C共享B的所有权，因此持有std::shared_ptr： item20_fig1 假定从B指向A的指针也很有用。应该使用哪种指针？ item20_fig2 有三种选择： 原始指针。使用这种方法，如果A被销毁，但是C继续指向B，B就会有一个指向A的悬空指针。而且B不知道指针已经悬空，所以B可能会继续访问，就会导致未定义行为。 std::shared_ptr。这种设计，A和B都互相持有对方的std::shared_ptr，导致的std::shared_ptr环状结构（A指向B，B指向A）阻止A和B的销毁。甚至A和B无法从其他数据结构访问了（比如，C不再指向B），每个的引用计数都还是1。如果发生了这种情况，A和B都被泄漏：程序无法访问它们，但是资源并没有被回收。 std::weak_ptr。这避免了上述两个问题。如果A被销毁，B指向它的指针悬空，但是B可以检测到这件事。尤其是，尽管A和B互相指向对方，B的指针不会影响A的引用计数，因此在没有std::shared_ptr指向A时不会导致A无法被销毁。 但是，需要注意使用std::weak_ptr打破std::shared_ptr循环并不常见。在严格分层的数据结构比如树中，子节点只被父节点持有。当父节点被销毁时，子节点就被销毁。从父到子的链接关系可以使用std::unique_ptr很好的表征。从子到父的反向连接可以使用原始指针安全实现，因为子节点的生命周期肯定短于父节点。因此没有子节点解引用一个悬空的父节点指针这样的风险。 从效率角度来看，std::weak_ptr与std::shared_ptr基本相同。两者的大小是相同的，使用相同的控制块。构造、析构、赋值操作涉及引用计数的原子操作。 虽然，std::weak_ptr不参与对象的共享所有权，因此不影响指向对象的引用计数。实际上在控制块中还是有第二个引用计数，std::weak_ptr操作的是第二个引用计数。 结论 用std::weak_ptr替代可能会悬空的std::shared_ptr。 std::weak_ptr的潜在使用场景包括：缓存、观察者列表、打破std::shared_ptr环状结构。 21 优先考虑使用std::make_unique和std::make_shared，而非直接使用new std::make_shared是C++11标准的一部分，但是，std::make_unique是从C++14开始加入标准库。 一个基础版本的std::make_unique是很容易自己写出的，如下： 1234template&lt;typename T, typename... Ts&gt;std::unique_ptr&lt;T&gt; make_unique(Ts&amp;&amp;... params) &#123; return std::unique_ptr&lt;T&gt;(new T(std::forward&lt;Ts&gt;(params)...));&#125; 这种形式的函数不支持数组和自定义析构（见条款18） std::make_unique和std::make_shared是三个make函数 中的两个：接收任意的多参数集合，完美转发到构造函数去动态分配一个对象，然后返回这个指向这个对象的指针。 第三个make函数是std::allocate_shared。它行为和std::make_shared一样，只不过第一个参数是用来动态分配内存的allocator对象。 使用 std::make_unique 的理由一 例如： 1234auto upw1(std::make_unique&lt;Widget&gt;()); //使用make函数std::unique_ptr&lt;Widget&gt; upw2(new Widget); //不使用make函数auto spw1(std::make_shared&lt;Widget&gt;()); //使用make函数std::shared_ptr&lt;Widget&gt; spw2(new Widget); //不使用make函数 我高亮了关键区别：使用new的版本重复了类型，但是make函数的版本没有。 重复写类型和软件工程里面一个关键原则相冲突：应该避免重复代码。源代码中的重复增加了编译的时间，会导致目标代码冗余，并且通常会让代码库使用更加困难。 它经常演变成不一致的代码，而代码库中的不一致常常导致bug。 使用 std::make_unique 的理由二 在调用processWidget时使用了new而不是std::make_shared： 12processWidget(std::shared_ptr&lt;Widget&gt;(new Widget), //潜在的资源泄漏！ computePriority()); 内存泄漏的原因在于： 在运行时，一个函数的实参必须先被计算，这个函数再被调用，所以在调用processWidget之前，必须执行以下操作，processWidget才开始执行： 表达式“new Widget”必须计算，例如，一个Widget对象必须在堆上被创建 负责管理new出来指针的std::shared_ptr&lt;Widget&gt;构造函数必须被执行 computePriority必须运行 而编译器不保证按照顺序生成代码。 虽然“new Widget”必须在std::shared_ptr的构造函数被调用前执行，因为new出来的结果作为构造函数的实参，但computePriority可能在这之前，之后，或者之间执行。也就是说，编译器可能按照这个执行顺序生成代码： 执行“new Widget” 执行computePriority 运行std::shared_ptr构造函数 在运行时computePriority产生了异常，那么第一步动态分配的Widget就会泄漏。因为它永远都不会被第三步的std::shared_ptr所管理了。 使用std::make_shared可以防止这种问题。调用代码看起来像是这样： 12processWidget(std::make_shared&lt;Widget&gt;(), //没有潜在的资源泄漏 computePriority()); 在运行时，std::make_shared和computePriority其中一个会先被调用。 使用 std::make_unique 的理由三 使用std::make_shared允许编译器生成更小，更快的代码，并使用更简洁的数据结构。考虑以下对new的直接使用： 1std::shared_ptr&lt;Widget&gt; spw(new Widget); 显然，这段代码需要进行内存分配，但它实际上执行了两次。 每个std::shared_ptr指向一个控制块，其中包含被指向对象的引用计数，还有其他东西。这个控制块的内存在std::shared_ptr构造函数中分配。因此，直接使用new需要为Widget进行一次内存分配，为控制块再进行一次内存分配。 如果使用std::make_shared代替： 1auto spw = std::make_shared&lt;Widget&gt;(); 一次分配足矣。这是因为std::make_shared分配一块内存，同时容纳了Widget对象和控制块。这种优化减少了程序的静态大小，因为代码只包含一个内存分配调用，并且它提高了可执行代码的速度，因为内存只分配一次。此外，使用std::make_shared避免了对控制块中的某些簿记信息的需要，潜在地减少了程序的总内存占用。 不使用 std::make_shared 的情况 需要自定义删除器时 make函数都不允许指定自定义删除器，但是std::unique_ptr和std::shared_ptr有构造函数这么做。有个Widget的自定义删除器： 1auto widgetDeleter = [](Widget* pw) &#123; … &#125;; 创建一个使用它的智能指针只能直接使用new： 1234std::unique_ptr&lt;Widget, decltype(widgetDeleter)&gt; upw(new Widget, widgetDeleter);std::shared_ptr&lt;Widget&gt; spw(new Widget, widgetDeleter); 对于make函数，没有办法做同样的事情。 不支持花括号调用 std::initializer_list 常规的用花括号创建的对象更倾向于使用std::initializer_list作为形参的重载形式，而用小括号创建对象将调用不用std::initializer_list作为参数的的重载形式。 但是，在这些调用中， 12auto upv = std::make_unique&lt;std::vector&lt;int&gt;&gt;(10, 20);auto spv = std::make_shared&lt;std::vector&lt;int&gt;&gt;(10, 20); 生成的智能指针指向带有10个元素的std::vector，每个元素值为20。 如果你想用花括号初始化指向的对象，你必须直接使用new。 一个变通的方法：使用auto类型推导从花括号初始化创建std::initializer_list对象，然后将auto创建的对象传递给make函数。 1234//创建std::initializer_listauto initList = &#123; 10, 20 &#125;;//使用std::initializer_list为形参的构造函数创建std::vectorauto spv = std::make_shared&lt;std::vector&lt;int&gt;&gt;(initList); 对于std::unique_ptr，只有这两种情景（自定义删除器和花括号初始化）使用make函数有点问题。对于std::shared_ptr和它的make函数，还有2个问题。都属于边缘情况，但是一些开发者常碰到。 类重载了operator new和operator delete 例如，Widget类的operator new和operator delete只会处理sizeof(Widget)大小的内存块的分配和释放。因为std::allocate_shared需要的内存总大小不等于动态分配的对象大小，还需要再加上控制块大小。 与直接使用new相比，std::make_shared在大小和速度上的优势源于std::shared_ptr的控制块与指向的对象放在同一块内存中。当对象的引用计数降为0，对象被销毁（即析构函数被调用）。但是，因为控制块和对象被放在同一块分配的内存块中，直到控制块的内存也被销毁，对象占用的内存才被释放。 控制块除了引用计数，还包含簿记信息。引用计数追踪有多少std::shared_ptrs指向控制块，但控制块还有第二个计数，记录多少个std::weak_ptrs指向控制块。第二个引用计数就是weak count。当一个std::weak_ptr检测它是否过期时，它会检测指向的控制块中的引用计数（而不是weak count）。 如果引用计数是0（即对象没有std::shared_ptr再指向它，已经被销毁了），std::weak_ptr就已经过期。 但是只要std::weak_ptrs引用一个控制块（即weak count大于零），该控制块必须继续存在。只要控制块存在，包含它的内存就必须保持分配。 所以，通过std::shared_ptr的make函数分配的内存，直到最后一个std::shared_ptr和最后一个指向它的std::weak_ptr已被销毁，才会释放。 所以，如果对象类型非常大，而且销毁最后一个std::shared_ptr和销毁最后一个std::weak_ptr之间的时间很长，那么在销毁对象和释放它所占用的内存之间可能会出现延迟。 例如，下面这种情况，明显，直接只用new，对象的释放会立即执行。 123456789101112131415class ReallyBigType &#123; … &#125;;auto pBigObj = //通过std::make_shared std::make_shared&lt;ReallyBigType&gt;(); //创建一个大对象 … //创建std::shared_ptrs和std::weak_ptrs //指向这个对象，使用它们… //最后一个std::shared_ptr在这销毁， //但std::weak_ptrs还在… //在这个阶段，原来分配给大对象的内存还分配着… //最后一个std::weak_ptr在这里销毁； //控制块和对象的内存被释放 直接只用new，一旦最后一个std::shared_ptr被销毁，ReallyBigType对象的内存就会被释放： 12345678910111213141516class ReallyBigType &#123; … &#125;; //和之前一样std::shared_ptr&lt;ReallyBigType&gt; pBigObj(new ReallyBigType); //通过new创建大对象… //像之前一样，创建std::shared_ptrs和std::weak_ptrs //指向这个对象，使用它们 … //最后一个std::shared_ptr在这销毁, //但std::weak_ptrs还在； //对象的内存被释放… //在这阶段，只有控制块的内存仍然保持分配… //最后一个std::weak_ptr在这里销毁； //控制块内存被释放 一个优化的例子 考虑前面的 processWidget 函数，现在我们指定一个自定义删除器: 123void processWidget(std::shared_ptr&lt;Widget&gt; spw, //和之前一样 int priority);void cusDel(Widget *ptr); //自定义删除器 下面这个是非异常安全的调用： 1234processWidget( //和之前一样， std::shared_ptr&lt;Widget&gt;(new Widget, cusDel), //潜在的内存泄漏！ computePriority() ); 还是实参调用的顺序问题。 一个优化方式如下： 12std::shared_ptr&lt;Widget&gt; spw(new Widget, cusDel);processWidget(spw, computePriority()); // 正确，但是没优化，见下 但是有一个性能问题，实参在前一个非异常安全调用中，std::shared_ptr形参是传值，从右值构造只需要移动。 而优化后，传递左值构造需要拷贝。对std::shared_ptr而言，这种区别是有意义的，因为拷贝std::shared_ptr需要对引用计数原子递增，移动则不需要对引用计数有操作。 所以，更高效安全的版本是： 1processWidget(std::move(spw), computePriority()); //高效且异常安全 结论 和直接使用new相比，make函数消除了代码重复，提高了异常安全性。对于std::make_shared和std::allocate_shared，生成的代码更小更快。 不适合使用make函数的情况包括需要指定自定义删除器和希望用花括号初始化。 对于std::shared_ptrs，其他不建议使用make函数的情况包括：(1) 有自定义内存管理的类；(2) 特别关注内存的系统，非常大的对象，以及std::weak_ptrs比对应的std::shared_ptrs活得更久。 22 当使用 Pimpl Idiom，请在实现文件中定义特殊成员函数 Pimpl Idiom 将类数据成员替换成一个指向包含具体实现的类（implementation class）（或结构体）的指针，并将原本放在主类（primary class）的相关数据成员们移动到实现类（implementation class）去，而这些数据成员的访问将通过指针间接访问。 举个例子： 123456789class Widget() &#123; //定义在头文件“widget.h”public: Widget(); …private: std::string name; std::vector&lt;double&gt; data; Gadget g1, g2, g3; //Gadget是用户自定义的类型&#125;; 因为类Widget的数据成员包含有类型std::string，std::vector和Gadget， 定义有这些类型的头文件在类Widget编译的时候，必须被包含进来，这意味着类Widget的使用者必须要#include &lt;string&gt;，&lt;vector&gt;以及gadget.h。 这些头文件将会增加类Widget使用者的编译时间，并且让这些使用者依赖于这些头文件。 如果一个头文件的内容变了，类Widget使用者也必须要重新编译。 标准库文件&lt;string&gt;和&lt;vector&gt;不是很常变，但是gadget.h可能会经常修订。 在C++98中使用Pimpl惯用法，可以把Widget的数据成员替换成一个原始指针，指向一个已经被声明过却还未被定义的结构体: 1234567891011class Widget //仍然在“widget.h”中&#123;public: Widget(); ~Widget(); …private: struct Impl; //声明一个 实现结构体 Impl *pImpl; //以及指向它的指针&#125;; 因为类Widget不再提到类型std::string，std::vector以及Gadget，Widget的使用者不再需要为了这些类型而引入头文件。 这可以加速编译，并且意味着，如果这些头文件中有所变动，Widget的使用者不会受到影响。 一个已经被声明，却还未被实现的类型，被称为未完成类型（incomplete type）。 Widget::Impl就是这种类型。 下一步是对 实现类（implementation class） 的内存管理： 在Widget.cpp里: 1234567891011121314151617#include \"widget.h\" //以下代码均在实现文件“widget.cpp”里#include \"gadget.h\"#include &lt;string&gt;#include &lt;vector&gt;struct Widget::Impl &#123; //含有之前在Widget中的数据成员的 std::string name; //Widget::Impl类型的定义 std::vector&lt;double&gt; data; Gadget g1,g2,g3;&#125;;Widget::Widget() //为此Widget对象分配数据成员: pImpl(new Impl)&#123;&#125;Widget::~Widget() //销毁数据成员&#123; delete pImpl; &#125; 它使用了原始指针，原始的new和原始的delete，一切都让它如此的...原始。 使用智能指针 如果我们想要的只是在类Widget的构造函数动态分配Widget::impl对象，在Widget对象销毁时一并销毁它， std::unique_ptr是最合适的工具。 123456789class Widget &#123; //在“widget.h”中public: Widget(); …private: struct Impl; std::unique_ptr&lt;Impl&gt; pImpl; //使用智能指针而不是原始指针&#125;; 实现文件也可以改成如下： 1234567891011121314#include \"widget.h\" //在“widget.cpp”中#include \"gadget.h\"#include &lt;string&gt;#include &lt;vector&gt;struct Widget::Impl &#123; //跟之前一样 std::string name; std::vector&lt;double&gt; data; Gadget g1,g2,g3;&#125;;Widget::Widget() : pImpl(std::make_unique&lt;Impl&gt;()) &#123;&#125; 问题出现了 以上的代码能编译，但是，最普通的Widget用法却会导致编译出错： 123#include \"widget.h\"Widget w; //错误！ 在对象w被析构时（例如离开了作用域），问题出现了。 在这个时候，它的析构函数被调用。我们在类的定义里使用了std::unique_ptr，所以我们没有声明析构函数。编译器会自动为我们生成一个析构函数。 在这个析构函数里，编译器会插入一些代码来调用类Widget的数据成员pImpl的析构函数。 问题就在于，此时pImpl的析构函数调用默认的删除器。默认删除器是一个函数，它使用delete来销毁内置于std::unique_ptr的原始指针。然而，在使用delete之前，通常会使默认删除器使用C++11的特性static_assert来确保原始指针指向的类型不是一个未完成类型。 需要确保在编译器生成销毁std::unique_ptr&lt;Widget::Impl&gt;的代码之前， Widget::Impl已经是一个完成类型（complete type）。 当编译器“看到”它的定义的时候，该类型就成为完成类型了。 但是 Widget::Impl的定义在widget.cpp里。 成功编译的关键，就是在widget.cpp文件内，让编译器在“看到” Widget的析构函数实现之前（也即编译器插入的，用来销毁std::unique_ptr这个数据成员的代码段之前），先定义Widget::Impl。 修改方法就是，在widget.h里只声明类Widget的析构函数，但不要在这里定义它： 12345678910class Widget &#123; //跟之前一样，在“widget.h”中public: Widget(); ~Widget(); //只有声明语句 …private: //跟之前一样 struct Impl; std::unique_ptr&lt;Impl&gt; pImpl;&#125;; 在widget.cpp文件中，在结构体Widget::Impl被定义之后，再定义析构函数： 123456789101112131415161718#include \"widget.h\" //跟之前一样，在“widget.cpp”中#include \"gadget.h\"#include &lt;string&gt;#include &lt;vector&gt;// 先于析构函数定义struct Widget::Impl &#123; //跟之前一样，定义Widget::Impl std::string name; std::vector&lt;double&gt; data; Gadget g1,g2,g3;&#125;Widget::Widget() //跟之前一样: pImpl(std::make_unique&lt;Impl&gt;())&#123;&#125;Widget::~Widget() //析构函数的定义&#123;&#125; 如果你想强调编译器自动生成的析构函数会做和你一样正确的事情，你可以直接使用“= default”定义析构函数体 1Widget::~Widget() = default; //同上述代码效果一致 移动 编译器自动生成的移动操作对其中的std::unique_ptr进行移动。但是，声明一个类Widget的析构函数会阻止编译器生成移动操作，所以你需要这样做： 12345678910111213class Widget &#123; //仍然在“widget.h”中public: Widget(); ~Widget(); Widget(Widget&amp;&amp; rhs) = default; //思路正确， Widget&amp; operator=(Widget&amp;&amp; rhs) = default; //但代码错误 …private: //跟之前一样 struct Impl; std::unique_ptr&lt;Impl&gt; pImpl;&#125;; 问题在于： 编译器生成的移动赋值操作符，在重新赋值之前，需要先销毁指针pImpl指向的对象。然而在Widget的头文件里，pImpl指针指向的是一个未完成类型。 移动构造函数的情况有所不同。 移动构造函数的问题是编译器自动生成的代码里，包含有抛出异常的事件，在这个事件里会生成销毁pImpl的代码。 这些都需要，Impl是一个完成类型。 解决方法： 把移动操作的定义移动到实现文件里 12345678910111213class Widget &#123; //仍然在“widget.h”中public: Widget(); ~Widget(); Widget(Widget&amp;&amp; rhs); //只有声明 Widget&amp; operator=(Widget&amp;&amp; rhs); …private: //跟之前一样 struct Impl; std::unique_ptr&lt;Impl&gt; pImpl;&#125;; 12345678910111213#include &lt;string&gt; //跟之前一样，仍然在“widget.cpp”中… struct Widget::Impl &#123; … &#125;; //跟之前一样Widget::Widget() //跟之前一样: pImpl(std::make_unique&lt;Impl&gt;())&#123;&#125;Widget::~Widget() = default; //跟之前一样Widget::Widget(Widget&amp;&amp; rhs) = default; //这里定义Widget&amp; Widget::operator=(Widget&amp;&amp; rhs) = default; 拷贝 对于 struct Impl 中数据成员，可以使用默认拷贝函数，完成拷贝动作。 1234567891011class Widget &#123; //仍然在“widget.h”中public: … Widget(const Widget&amp; rhs); //只有声明 Widget&amp; operator=(const Widget&amp; rhs);private: //跟之前一样 struct Impl; std::unique_ptr&lt;Impl&gt; pImpl;&#125;; 12345678910111213141516#include &lt;string&gt; //跟之前一样，仍然在“widget.cpp”中… struct Widget::Impl &#123; … &#125;; //跟之前一样Widget::~Widget() = default; //其他函数，跟之前一样Widget::Widget(const Widget&amp; rhs) //拷贝构造函数: pImpl(std::make_unique&lt;Impl&gt;(*rhs.pImpl))&#123;&#125;Widget&amp; Widget::operator=(const Widget&amp; rhs) //拷贝operator=&#123; *pImpl = *rhs.pImpl; return *this;&#125; 利用了编译器会为我们自动生成结构体Impl的复制操作函数的机制，而不是逐一复制结构体Impl的成员，自动生成的复制操作能自动复制每一个成员。 std::shared_ptr 如果我们使用std::shared_ptr而不是std::unique_ptr来做pImpl指针，本条款的建议不再适用。 不需要在类Widget里声明析构函数，没有了用户定义析构函数，编译器将会成移动操作，并且将会如我们所期望般工作。widget.h里的代码如下， 123456789class Widget &#123; //在“widget.h”中public: Widget(); … //没有析构函数和移动操作的声明private: struct Impl; std::shared_ptr&lt;Impl&gt; pImpl; //用std::shared_ptr&#125;; //而不是std::unique_ptr 这是#include了widget.h的客户代码， 123Widget w1;auto w2(std::move(w1)); //移动构造w2w1 = std::move(w2); //移动赋值w1 这些都能编译，并且工作地如我们所望：w1将会被默认构造，它的值会被移动进w2，随后值将会被移动回w1，然后两者都会被销毁（指向的Widget::Impl对象一并也被销毁）。 std::unique_ptr和std::shared_ptr在pImpl指针上的表现上的区别的深层原因在于，他们支持自定义删除器的方式不同。 对std::unique_ptr而言，删除器的类型是这个智能指针的一部分，这让编译器有可能生成更小的运行时数据结构和更快的运行代码。 这种更高效率的后果之一就是std::unique_ptr指向的类型，在编译器的生成特殊成员函数（如析构函数，移动操作）被调用时，必须已经是一个完成类型。 而对std::shared_ptr而言，删除器的类型不是该智能指针的一部分，这让它会生成更大的运行时数据结构和稍微慢点的代码，但是当编译器生成的特殊成员函数被使用的时候，指向的对象不必是一个完成类型。 结论 Pimpl 惯用法通过减少在类实现和类使用者之间的编译依赖来减少编译时间。 对于std::unique_ptr类型的pImpl指针，需要在头文件的类里声明特殊成员函数，并在实现文件中 struct Impl定义之后来实现他们。即使是编译器自动生成的代码可以工作，也要这么做。 以上的建议只适用于std::unique_ptr，不适用于std::shared_ptr。 23 理解std::move和std::forward Intro 移动语义使编译器有可能用廉价的移动操作来代替昂贵的拷贝操作。正如拷贝构造函数和拷贝赋值操作符给了你控制拷贝语义的权力，移动构造函数和移动赋值操作符也给了你控制移动语义的权力。移动语义也允许创建只可移动（move-only）的类型，例如std::unique_ptr，std::future和std::thread。 完美转发使接收任意数量实参的函数模板成为可能，它可以将实参转发到其他的函数，使目标函数接收到的实参与被传递给转发函数的实参保持一致。 右值引用是连接这两个截然不同的概念的胶合剂。它是使移动语义和完美转发变得可能的基础语言机制。 但是，std::move并不移动任何东西，完美转发也并不完美。移动操作并不永远比复制操作更廉价。构造“type&amp;&amp;”也并非总是代表一个右值引用。 非常重要的一点是要牢记形参永远是左值，即使它的类型是一个右值引用。比如，假设 1void f(Widget&amp;&amp; w); 形参w是一个左值，即使它的类型是一个rvalue-reference-to-Widget。 std::move和std::forward不会做什么 std::move不移动（move）任何东西，std::forward也不转发（forward）任何东西。在运行时，它们不做任何事情。它们不产生任何可执行代码，一字节也没有。 std::move C++11的std::move的示例实现。它并不完全满足标准细则，但是它已经非常接近了。 123456789template&lt;typename T&gt; typename remove_reference&lt;T&gt;::type&amp;&amp;move(T&amp;&amp; param)&#123; using ReturnType = typename remove_reference&lt;T&gt;::type&amp;&amp;; return static_cast&lt;ReturnType&gt;(param);&#125; 该函数返回类型的&amp;&amp;部分表明std::move函数返回的是一个右值引用，但是，如果类型T恰好是一个左值引用，那么T&amp;&amp;将会成为一个左值引用。 所以，使用 std::remove_reference，得到 ReturnType。这保证了std::move返回的真的是右值引用。 std::move在C++14中可以被更简单地实现。 123456template&lt;typename T&gt;decltype(auto) move(T&amp;&amp; param) //在std命名空间&#123; using ReturnType = remove_referece_t&lt;T&gt;&amp;&amp;; return static_cast&lt;ReturnType&gt;(param);&#125; 因此，std::move将它的实参转换为一个右值，这就是它的全部作用。 const 的限制 1234567891011class Annotation &#123;public: explicit Annotation(const std::string text) ：value(std::move(text)) //“移动”text到value里；这段代码执行起来 &#123; … &#125; //并不是看起来那样 …private: std::string value;&#125;; 这段代码可以编译，可以链接，可以运行。 text通过std::move被转换到右值，但是text被声明为const std::string，所以在转换之前，text是一个左值的const std::string，而转换的结果是一个右值的const std::string 那么，string 对 value 赋值时，调用的是哪个构造函数？ 1234567class string &#123; //std::string事实上是public: //std::basic_string&lt;char&gt;的类型别名 … string(const string&amp; rhs); //拷贝构造函数 string(string&amp;&amp; rhs); //移动构造函数 …&#125;; 右值不能被传递给std::string的移动构造函数，因为移动构造函数只接受一个指向non-const的std::string的右值引用。 该右值却可以被传递给std::string的拷贝构造函数，因为lvalue-reference-to-const允许被绑定到一个const右值上。因此，std::string在成员初始化的过程中调用了拷贝构造函数。 可以总结出两点： 第一，不要在你希望能移动对象的时候，声明他们为const。 第二，std::move不仅不移动任何东西，而且它也不保证它执行转换的对象可以被移动。 std::forward 与std::move总是无条件的将它的实参为右值不同，std::forward是有条件的转换。 最常见的情景是一个模板函数，接收一个通用引用形参（T&amp;&amp;），并将它传递给另外的函数： 123456789101112void process(const Widget&amp; lvalArg); //处理左值void process(Widget&amp;&amp; rvalArg); //处理右值template&lt;typename T&gt; //用以转发param到process的模板void logAndProcess(T&amp;&amp; param)&#123; auto now = //获取现在时间 std::chrono::system_clock::now(); makeLogEntry(\"Calling 'process'\", now); process(std::forward&lt;T&gt;(param));&#125; 考虑两次对logAndProcess的调用，一次左值为实参，一次右值为实参： 1234Widget w;logAndProcess(w); //用左值调用logAndProcess(std::move(w)); //用右值调用 std::forward 将保留实参的值类型，传递到 process 函数，调用正确的函数重载。 对比 考虑一个类，我们希望统计有多少次移动构造函数被调用了。我们只需要一个static的计数器，它会在移动构造的时候自增。假设在这个类中，唯一一个非静态的数据成员是std::string，一种经典的移动构造函数（即，使用std::move）可以被实现如下： 123456789101112class Widget &#123;public: Widget(Widget&amp;&amp; rhs) : s(std::move(rhs.s)) &#123; ++moveCtorCalls; &#125; …private: static std::size_t moveCtorCalls; std::string s;&#125;; 如果要用std::forward来达成同样的效果，代码可能会看起来像： 123456789class Widget&#123;public: Widget(Widget&amp;&amp; rhs) //不自然，不合理的实现 : s(std::forward&lt;std::string&gt;(rhs.s)) &#123; ++moveCtorCalls; &#125; …&#125; std::forward不但需要一个函数实参（rhs.s），还需要一个模板类型实参std::string。 std::move的使用代表着无条件向右值的转换，而使用std::forward只对绑定了右值的引用进行到右值转换。这是两种完全不同的动作。前者是典型地为了移动操作，而后者只是传递（亦为转发）一个对象到另外一个函数，保留它原有的左值属性或右值属性。 结论 std::move执行到右值的无条件的转换，但就自身而言，它不移动任何东西。 std::forward只有当它的参数被绑定到一个右值时，才将参数转换为右值。 std::move和std::forward在运行期什么也不做。 24 区分通用引用与右值引用 为了声明一个指向某个类型T的右值引用，你写下了T&amp;&amp;。但是，这不一定是一个右值引用： 123456789void f(Widget&amp;&amp; param); //右值引用Widget&amp;&amp; var1 = Widget(); //右值引用auto&amp;&amp; var2 = var1; //不是右值引用template&lt;typename T&gt;void f(std::vector&lt;T&gt;&amp;&amp; param); //右值引用template&lt;typename T&gt;void f(T&amp;&amp; param); //不是右值引用 “T&amp;&amp;”有两种不同的意思。第一种，当然是右值引用。它们只绑定到右值上，并且它们主要的存在原因就是为了识别可以移动操作的对象。 “T&amp;&amp;”的另一种意思是，它既可以是右值引用，也可以是左值引用。它们的二重性使它们既可以绑定到右值上（就像右值引用），也可以绑定到左值上（就像左值引用）。 此外，它们还可以绑定到const或者non-const的对象上，也可以绑定到volatile或者non-volatile的对象上，甚至可以绑定到既const又volatile的对象上。它们可以绑定到几乎任何东西。它叫做通用引用（universal references）。 一些C++社区的成员已经开始将这种通用引用称之为转发引用（forwarding references） 通用引用，其特性是引用折叠决定的。 初始化 通用引用是引用，所以它们必须被初始化。一个通用引用的初始值决定了它是代表了右值引用还是左值引用。如果初始值是一个右值，那么通用引用就会是对应的右值引用，如果初始值是一个左值，那么通用引用就会是一个左值引用。对那些是函数形参的通用引用来说，初始值在调用函数的时候被提供： 123456789template&lt;typename T&gt;void f(T&amp;&amp; param); //param是一个通用引用Widget w;f(w); //传递给函数f一个左值；param的类型 //将会是Widget&amp;，也即左值引用f(std::move(w)); //传递给f一个右值；param的类型会是 //Widget&amp;&amp;，即右值引用 对一个通用引用而言，类型推导是必要的，但是其必须是 T&amp;&amp; 形式，如果是 std::vector&lt;T&gt;&amp;&amp; 的形式，那就变成了 右值引用。 而如果传入左值，那么是不能传入右值参数的。 一个例子 考虑如下push_back成员函数，来自std::vector： 1234567template&lt;class T, class Allocator = allocator&lt;T&gt;&gt; //来自C++标准class vector&#123;public: void push_back(T&amp;&amp; x); …&#125; push_back函数的形参当然有一个通用引用的正确形式，然而，在这里并没有发生类型推导。类型推导发生在 vector 实例化时。 作为对比，std::vector内的概念上相似的成员函数emplace_back，却确实包含类型推导: 1234567template&lt;class T, class Allocator = allocator&lt;T&gt;&gt; //依旧来自C++标准class vector &#123;public: template &lt;class... Args&gt; void emplace_back(Args&amp;&amp;... args); …&#125;; 类型参数（type parameter）Args是独立于vector的类型参数T的，所以Args会在每次emplace_back被调用的时候被推导。 所以，此时是一个通用引用。 auto&amp;&amp; 类型声明为auto&amp;&amp;的变量是通用引用，因为会发生类型推导，并且它们具有正确形式(T&amp;&amp;)。auto类型的通用引用不如函数模板形参中的通用引用常见，但是它们在C++11中常常突然出现。而它们在C++14中出现得更多，因为C++14的lambda表达式可以声明auto&amp;&amp;类型的形参。 举个例子，如果你想写一个C++14标准的lambda表达式，来记录任意函数调用的时间开销，你可以这样写： 123456789auto timeFuncInvocation = [](auto&amp;&amp; func, auto&amp;&amp;... params) //C++14 &#123; start timer; std::forward&lt;decltype(func)&gt;(func)( //对params调用func std::forward&lt;delctype(params)&gt;(params)... ); stop timer and record elapsed time; &#125;; 结论 如果一个函数模板形参的类型为type&amp;&amp;，并且type需要被推导得知，或者如果一个对象被声明为auto&amp;&amp;，这个形参或者对象就是一个通用引用。 如果类型声明的形式不是标准的type&amp;&amp;，或者如果类型推导没有发生，那么type&amp;&amp;代表一个右值引用。 通用引用，如果它被右值初始化，就会对应地成为右值引用；如果它被左值初始化，就会成为左值引用。 25 对右值引用使用std::move，对通用引用使用std::forward 在参数传递时，std::forward是有条件的传递，会根据参数的类型，传递实际的参数形式，右值还是右值，左值还是左值。 std::move是无条件的将其变为右值。 有的时候，并不一定需要对象的移动操作。区分移动和拷贝是有必要的。遇到下面这种情况： 12345678910class Widget &#123;public: void setName(const std::string&amp; newName) //用const左值设置 &#123; name = newName; &#125; void setName(std::string&amp;&amp; newName) //用右值设置 &#123; name = std::move(newName); &#125; …&#125;; 如果不用 通用引用，那么实现会变得冗长，尤其是参数数量较多的时候。 使用通用引用 + 完美转发 std::forward，那么实现会优雅得多。 并且对于变参函数模板： 12345template&lt;class T, class... Args&gt; //来自C++11标准shared_ptr&lt;T&gt; make_shared(Args&amp;&amp;... args);template&lt;class T, class... Args&gt; //来自C++14标准unique_ptr&lt;T&gt; make_unique(Args&amp;&amp;... args); 对于这种函数，对于左值和右值分别重载就不能考虑了：通用引用是仅有的实现方案。对这种函数，我向你保证，肯定使用std::forward传递通用引用形参给其他函数。 返回值的情况 如果你在按值返回的函数中，返回值绑定到右值引用或者通用引用上，需要对返回的引用使用std::move或者std::forward。 123456Matrix //按值返回operator+(Matrix&amp;&amp; lhs, const Matrix&amp; rhs)&#123; lhs += rhs; return std::move(lhs); //移动lhs到返回值中&#125; 通过在return语句中将lhs转换为右值（通过std::move），lhs可以移动到返回值的内存位置。如果省略了std::move调用， 123456Matrix //同之前一样operator+(Matrix&amp;&amp; lhs, const Matrix&amp; rhs)&#123; lhs += rhs; return lhs; //拷贝lhs到返回值中&#125; lhs是个左值的事实，会强制编译器拷贝它到返回值的内存空间。 假定Matrix支持移动操作，并且比拷贝操作效率更高，在return语句中使用std::move的代码效率更高。 如果Matrix不支持移动操作，将其转换为右值不会变差，因为右值可以直接被Matrix的拷贝构造函数拷贝。 使用通用引用和std::forward的情况类似。考虑函数模板reduceAndCopy收到一个未reduce（unreduced）对象Fraction，将其规约，并返回一个reduce (规约，好难听的名字) 后的副本。 如果原始对象是右值，可以将其移动到返回值中（避免拷贝开销），但是如果原始对象是左值，必须创建副本，因此如下代码： 1234567template&lt;typename T&gt;Fraction //按值返回reduceAndCopy(T&amp;&amp; frac) //通用引用的形参&#123; frac.reduce(); return std::forward&lt;T&gt;(frac); //移动右值，或拷贝左值到返回值中&#125; 如果std::forward被忽略，frac就被无条件复制到reduceAndCopy的返回值内存空间。 注意，对于函数内部的局部变量，这是不成立的。 123456Widget makeWidget() //makeWidget的“拷贝”版本&#123; Widget w; //局部对象 … //配置w return w; //“拷贝”w到返回值中&#125; 他们想要“优化”代码，把“拷贝”变为移动： 123456Widget makeWidget() //makeWidget的移动版本&#123; Widget w; … return std::move(w); //移动w到返回值中（不要这样做！）&#125; 因为有 RVO 的存在，makeWidget的“拷贝”版本实际上不拷贝任何东西。在返回的地址上，进行对象的构造。 但是 move 版本，不支持 RVO。 返回的已经不是局部对象w，而是w的引用——std::move(w)的结果。 结论 对于传入函数的形参，在函数内最后一次使用时，在右值引用上使用std::move，在通用引用上使用std::forward。 对按值返回的函数要返回的右值引用使用std::move，和通用引用使用std::forward。 如果局部对象可以被返回值优化消除，就绝不使用std::move或者std::forward。 26 避免重载通用引用 弊端一 比如，下面的例子： 1234567891011121314template&lt;typename T&gt;void logAndAdd(T&amp;&amp; name)&#123; auto now = std::chrono::system_clock::now(); log(now, \"logAndAdd\"); names.emplace(std::forward&lt;T&gt;(name));&#125;void logAndAdd(int idx) //新的重载&#123; auto now = std::chrono::system_clock::now(); log(now, \"logAndAdd\"); names.emplace(nameFromIdx(idx));&#125; 如果有以下调用： 123short nameIdx;logAndAdd(nameIdx); //错误！ 由于没有 short 类型的重载，但是有通用引用存在，所以name形参绑定到要传入的short上，然后name被std::forward给names（一个std::multiset&lt;std::string&gt;）的emplace成员函数，然后又被转发给std::string构造函数。std::string没有接受short的构造函数，所以logAndAdd调用里的multiset::emplace调用里的std::string构造函数调用失败。 所有这一切的原因就是对于short类型通用引用重载优先于int类型的重载。这导致了代码执行出错。 使用通用引用的函数在C++中是最贪婪的函数。它们几乎可以精确匹配任何类型的实参。 通用引用的实现会匹配比开发者预期要多得多的实参类型。 弊端二 有以下Person类： 12345678910111213class Person &#123;public: template&lt;typename T&gt; explicit Person(T&amp;&amp; n) //完美转发的构造函数，初始化数据成员 : name(std::forward&lt;T&gt;(n)) &#123;&#125; explicit Person(int idx) //int的构造函数 : name(nameFromIdx(idx)) &#123;&#125; …private: std::string name;&#125;; 函数模板能实例化产生与拷贝和移动构造函数一样的签名。如果拷贝和移动构造被生成，Person类看起来就像这样： 123456789101112class Person &#123;public: template&lt;typename T&gt; //完美转发的构造函数 explicit Person(T&amp;&amp; n) : name(std::forward&lt;T&gt;(n)) &#123;&#125; explicit Person(int idx); //int的构造函数 Person(const Person&amp; rhs); //拷贝构造函数 Person(Person&amp;&amp; rhs); //移动构造函数 …&#125;; 如果通过 non-const左值类型的Person 来拷贝构造一个新的对象，完美转发的构造函数会优先匹配。 如果通过 const左值类型的Person 来拷贝构造一个新的对象，拷贝构造函数会优先匹配，因为这是精确匹配。 如果在继承关系中，会有以下行为： 12345678910class SpecialPerson: public Person &#123;public: SpecialPerson(const SpecialPerson&amp; rhs) //拷贝构造函数，调用基类的 : Person(rhs) //基类的完美转发构造函数！ &#123; … &#125; SpecialPerson(SpecialPerson&amp;&amp; rhs) //移动构造函数，调用基类的 : Person(std::move(rhs)) //基类的完美转发构造函数！ &#123; … &#125;&#125;; 派生类的拷贝和移动构造函数没有调用基类的拷贝和移动构造函数，而是调用了基类的完美转发构造函数。 派生类将SpecialPerson类型的实参传递给其基类，然后通过模板实例化和重载解析规则作用于基类Person。最终，代码无法编译，因为std::string没有接受一个SpecialPerson的构造函数（只有完美转发构造函数初始化了 name ）。 结论 对通用引用形参的函数进行重载时，通用引用函数可匹配的类型，几乎总会比你期望的多得多。 完美转发构造函数是糟糕的实现，因为对于non-const左值，它们会优先于拷贝构造函数匹配，而且会劫持派生类对于基类的拷贝和移动构造函数的调用。 27 通用引用重载的替代方法 一个直接的思路，放弃重载，另外声明一个函数签名。 另一种思路，放弃重载，但是使用 lvalue-refrence-to-const 的方式，参数类型变为 const T&amp;。 放弃重载的另一种思路是，直接传值 + std::mov，的方式。 另外两种方案，保留了重载，但是都有局限，效率更高但是并不是万能的。这两种方案是：tag dispatch 和 enable_if 约束模板。 tag dispath 实现形式： 12345678910template&lt;typename T&gt;void logAndAdd(T&amp;&amp; name)&#123; logAndAddImpl( std::forward&lt;T&gt;(name), std::is_integral&lt;typename std::remove_reference&lt;T&gt;::type&gt;() // C++14 // std::is_integral&lt;std::remove_reference&lt;T&gt;&gt;() );&#125; 原来函数模板不变，但是将实际的函数调用，进行了分发（dispatch）。 之所以 remove_reference，是因为 T 可能被推导为左值 T&amp;，这不是 type trait std::is_integral 识别为真的类型。 分发实现为： 12345678910111213template&lt;typename T&gt; //非整型实参：添加到全局数据结构中void logAndAddImpl(T&amp;&amp; name, std::false_type) &#123; auto now = std::chrono::system_clock::now(); log(now, \"logAndAdd\"); names.emplace(std::forward&lt;T&gt;(name));&#125;std::string nameFromIdx(int idx); // 整型实参：查找名字并用它调用logAndAddvoid logAndAddImpl(int idx, std::true_type) &#123; logAndAdd(nameFromIdx(idx)); &#125; 这里的 std::false_type 和 std::true_type ，是 T 分别在不满足 std::is_integral和满足 std::is_integral 的情况下的父类。 enable_if 约束模板 tag dispath 并不能解决父类的通用引用重载函数的问题（见上一条款 26）。 enable_if 约束模板，基于以下机制： 默认情况下，所有模板是启用的（enabled），但是使用std::enable_if可以使得仅在std::enable_if指定的条件满足时模板才启用。不满足条件，模板就是被禁止（disabled）的。 首先解决传入 Person 类对象，导致通用引用重载中，string 用 Person 对象初始化报错的问题： 123456789101112131415// C++11class Person &#123;public: template&lt; typename T, typename = typename std::enable_if&lt; // !std::is_base_of&lt;Person, !std::is_base_of&lt;Person, typename std::decay&lt;T&gt;::type &gt;::value &gt;::type &gt; explicit Person(T&amp;&amp; n); …&#125;; 使用 type trait is_same ，可以在传入 Person 对象时，禁用模板。但是，传入子类对象，同样时不允许的，所以使用了 std::is_base_of。 std::decay&lt;T&gt; 去掉了对于T的引用，const，volatile修饰。 再结合对传入 int 类型参数的限制，可以的得到下面的代码： 123456789101112131415161718192021222324252627282930class Person &#123;public: template&lt; typename T, typename = std::enable_if_t&lt; !std::is_base_of&lt;Person, std::decay_t&lt;T&gt;&gt;::value &amp;&amp; !std::is_integral&lt;std::remove_reference_t&lt;T&gt;&gt;::value &gt; &gt; explicit Person(T&amp;&amp; n) : name(std::forward&lt;T&gt;(n)) //std::strings的实参的构造函数 &#123; //断言可以用T对象创建std::string static_assert( std::is_constructible&lt;std::string, T&gt;::value, \"Parameter n can't be used to construct a std::string\" ); ... &#125; explicit Person(int idx) //整型实参的构造函数 : name(nameFromIdx(idx)) &#123; … &#125; … //拷贝、移动构造函数等private: std::string name;&#125;; 其中： 12template&lt; bool B, class T = void &gt;using enable_if_t = typename enable_if&lt;B,T&gt;::type; // (since C++14) 这里的 static_assert 断言虽然可以识别，T 类型是否可以构建 string，但是这发生在 name(std::forward&lt;T&gt;(n)) 之后。所以，报错先于断言。 结论 通用引用重载的替代方法：使用不同的函数名，通过lvalue-reference-to-const传递形参，按值传递形参，使用tag dispatch，使用 enable_if 约束模板。 通用引用参数通常具有高效率的优势，但是其使用需要仔细分析。 28 引用折叠 目的就是禁止你生成引用的引用。 存在两种类型的引用（左值和右值），所以有四种可能的引用组合（左值的左值，左值的右值，右值的右值，右值的左值）。 这些组合的的结果：如果两个中任一引用为左值引用，则结果为左值引用。否则（即，如果引用都是右值引用），结果为右值引用。 组合 结果 左值的右值 左值 左值的左值 左值 右值的左值 左值 右值的右值 右值 std::forword实现 std::forward应用在通用引用参数上，所以经常能看到这样使用： 123456template&lt;typename T&gt;void f(T&amp;&amp; fParam)&#123; … //做些工作 someFunc(std::forward&lt;T&gt;(fParam)); //转发fParam到someFunc&#125; 因为fParam是通用引用，类型参数T的类型根据f被传入实参（即用来实例化fParam的表达式）是左值还是右值来决定。 std::forward的作用是当且仅当传给f的实参为右值时（此时T为非引用类型），才将fParam转化为一个右值。 std::forward可以这样实现： 12345template&lt;typename T&gt; //在std命名空间T&amp;&amp; forward(typename remove_reference&lt;T&gt;::type&amp; param)&#123; return static_cast&lt;T&amp;&amp;&gt;(param);&#125; 在C++14中，std::remove_reference_t的存在使得实现变得更简洁： 12345template&lt;typename T&gt; //C++14；仍然在std命名空间T&amp;&amp; forward(remove_reference_t&lt;T&gt;&amp; param)&#123; return static_cast&lt;T&amp;&amp;&gt;(param);&#125; 假设传入到f的实参是Widget的左值类型。T被推导为Widget&amp;，然后调用std::forward将实例化为std::forward&lt;Widget&amp;&gt;。 Widget&amp;带入到上面的std::forward的实现中： 123Widget&amp; &amp;&amp; forward(typename remove_reference&lt;Widget&amp;&gt;::type&amp; param)&#123; return static_cast&lt;Widget&amp; &amp;&amp;&gt;(param); &#125; std::remove_reference&lt;Widget&amp;&gt;::type这个type trait产生Widget ，所以std::forward成为： 12Widget&amp; &amp;&amp; forward(Widget&amp; param)&#123; return static_cast&lt;Widget&amp; &amp;&amp;&gt;(param); &#125; 根据引用折叠规则，返回值和强制转换可以化简，最终版本的std::forward调用就是： 12Widget&amp; forward(Widget&amp; param)&#123; return static_cast&lt;Widget&amp;&gt;(param); &#125; 当左值实参被传入到函数模板f时，std::forward被实例化为接受和返回左值引用。 如果传入右值，那么结果会是这样： 12Widget&amp;&amp; forward(Widget&amp; param)&#123; return static_cast&lt;Widget&amp;&amp;&gt;(param); &#125; auto 在auto的写法中，规则是类似的。声明 1auto&amp;&amp; w1 = w; 用一个左值初始化w1，因此为auto推导出类型Widget&amp;。把Widget&amp;代回w1声明中的auto里，产生了引用的引用， 1Widget&amp; &amp;&amp; w1 = w; 应用引用折叠规则，就是 1Widget&amp; w1 = w 结果就是w1是一个左值引用。 下面这个声明， 1auto&amp;&amp; w2 = widgetFactory(); 使用右值初始化w2，为auto推导出非引用类型Widget。把Widget代入auto得到： 1Widget&amp;&amp; w2 = widgetFactory() 没有引用的引用，这就是最终结果，w2是个右值引用。 通用引用 通用引用不是一种新的引用，它实际上是满足以下两个条件下的右值引用： 类型推导区分左值和右值。T类型的左值被推导为T&amp;类型，T类型的右值被推导为T。 发生引用折叠。 结论 引用折叠发生在四种情况下：模板实例化，auto类型推导，typedef与别名声明的创建和使用，decltype。 当编译器在引用折叠环境中生成了引用的引用时，结果就是单个引用。带有左值引用的引用折叠，结果就是左值引用。否则就是右值引用。 通用引用就是引用折叠的结果。 29 移动操作的缺点 升级C++11之前的代码 C++11倾向于为缺少移动操作的类生成它们，但是只有在没有声明复制操作，移动操作，析构函数的类中才会生成移动操作。 另外数据成员或者某类型的基类禁止移动操作，编译器不生成移动操作的支持。 所以，对于没有明确支持移动操作的类型，并且不符合编译器默认生成的条件的类，没有理由期望C++11会比C++98进行任何性能上的提升。 移动大对象 1234567std::vector&lt;Widget&gt; vm1;//把数据存进vw1…//把vw1移动到vw2。以常数时间运行。只有vw1和vw2中的指针被改变auto vm2 = std::move(vm1); std::array没有这种指针实现，数据就保存在std::array对象中： 1234567std::array&lt;Widget, 10000&gt; aw1;//把数据存进aw1…//把aw1移动到aw2。以线性时间运行。aw1中所有元素被移动到aw2auto aw2 = std::move(aw1); 移动还是遍历了所有元素。 移动小字符串 std::string提供了常数时间的移动操作和线性时间的复制操作。 许多字符串的实现采用了小字符串优化（small string optimization，SSO）。“小”字符串（比如长度小于15个字符的）存储在了std::string的缓冲区中，并没有存储在堆内存，移动这种存储的字符串并不比复制操作更快（并不会执行指针的复制，而是将字符串完全从一个位置拷贝到另一个位置，再清空原来的内存）。 结论 C++11的移动语义并无优势： 没有移动操作：要移动的对象没有提供移动操作，所以移动的写法也会变成复制操作。 移动不会更快：要移动的对象提供的移动操作并不比复制速度更快。 移动不可用：进行移动的上下文要求移动操作不会抛出异常，但是该操作没有被声明为noexcept。 源对象是左值：除了极少数的情况外，只有右值可以作为移动操作的来源。 30 完美转发失败的情况 完美转发（perfect forwarding）意味着我们不仅转发对象，我们还转发显著的特征：它们的类型，是左值还是右值，是const还是volatile。 有以下函数： 12345template&lt;typename... Ts&gt;void fwd(Ts&amp;&amp;... params) //接受任何实参&#123; f(std::forward&lt;Ts&gt;(params)...); //转发给f&#125; 讨论下面函数调用失败的情况： 12f( expression ); //调用f执行某个操作fwd( expression ); //但调用fwd执行另一个操作，则fwd不能完美转发expression给f 花括号初始化器 假定f这样声明： 1void f(const std::vector&lt;int&gt;&amp; v); 在这个例子中，用花括号初始化调用f通过编译， 1f(&#123; 1, 2, 3 &#125;); //可以，“&#123;1, 2, 3&#125;”隐式转换为std::vector&lt;int&gt; 但是传递相同的列表初始化给fwd不能编译 1fwd(&#123; 1, 2, 3 &#125;); //错误！不能编译 当通过调用函数模板fwd间接调用f时，编译器不再把调用地传入给fwd的实参和f的声明中形参类型进行比较。 而是推导传入给fwd的实参类型，然后比较推导后的实参类型和f的形参声明类型。 编译器不允许在对fwd的调用中推导表达式{ 1, 2, 3 }的类型，因为fwd的形参没有声明为std::initializer_list。对于fwd形参的推导类型被阻止，编译器只能拒绝该调用。 但是，使用花括号初始化的auto的变量的类型推导是成功的。这种变量被视为std::initializer_list对象，在转发函数应推导出类型为std::initializer_list的情况，这提供了一种简单的解决方法——使用auto声明一个局部变量，然后将局部变量传进转发函数： 12auto il = &#123; 1, 2, 3 &#125;; //il的类型被推导为std::initializer_list&lt;int&gt;fwd(il); //可以，完美转发il给f 0或者NULL 当你试图传递0或者NULL作为空指针给模板时，类型推导会出错，会把传来的实参推导为一个整型类型（典型情况为int）而不是指针类型。结果就是不管是0还是NULL都不能作为空指针被完美转发。解决方法非常简单，传一个nullptr而不是0或者NULL。 只有声明的 static const 数据成员 下面的代码： 123456789class Widget &#123;public: static const std::size_t MinVals = 28; //MinVal的声明 …&#125;;… //没有MinVals定义std::vector&lt;int&gt; widgetData;widgetData.reserve(Widget::MinVals); //使用MinVals 使用MinVals调用f是可以的，因为编译器直接将值28代替MinVals： 1f(Widget::MinVals); //可以，视为“f(28)” 不过如果我们尝试通过fwd调用f，事情不会进展那么顺利： 1fwd(Widget::MinVals); //错误！ 代码可以编译，但是不应该链接。 尽管代码中没有使用MinVals的地址，但是fwd的形参是通用引用。 而引用，在编译器生成的代码中，通常被视作指针。 在程序的二进制底层代码中（以及硬件中）指针和引用是一样的。在这个水平上，引用只是可以自动解引用的指针。在这种情况下，通过引用传递MinVals实际上与通过指针传递MinVals是一样的，因此，必须有内存使得指针可以指向。 链接时，链接不到内存，就会报错。 只要给整型static const提供一个定义，就可以解决问题了，比如这样： 1const std::size_t Widget::MinVals; //在Widget的.cpp文件 注意定义中不要重复初始化。如果在两个地方都提供了初始化，编译器就会报错，提醒你只能初始化一次。 重载函数的名称和模板名称 假设有了一个重载函数，processVal： 1234void f(int (*pf)(int));int processVal(int value);int processVal(int value, int priority); 传递给 f 是没问题的，因为编译器是可以基于现有信息判断调用哪一个重载函数的。 但是，fwd(processVal); 不行。 单用processVal是没有类型信息的，所以就不能类型推导，完美转发失败。 需要这样使用： 123456using ProcessFuncType = int (*)(int);ProcessFuncType processValPtr = processVal; //指定所需的processVal签名fwd(processValPtr); //可以fwd(static_cast&lt;ProcessFuncType&gt;(workOnVal)); //也可以 对于模板，有相似的问题。一个函数模板不代表单独一个函数，它表示一个函数族： 12345template&lt;typename T&gt;T workOnVal(T param) //处理值的模板&#123; … &#125;fwd(workOnVal); //错误！哪个workOnVal实例？ 要让像fwd的完美转发函数接受一个重载函数名或者模板名，方法是指定要转发的那个重载或者实例。 位域 IPv4的头部有如下模型：（假定位域是按从最低有效位（least significant bit，lsb）到最高有效位（most significant bit，msb） 12345678struct IPv4Header &#123; std::uint32_t version:4, IHL:4, DSCP:6, ECN:2, totalLength:16; …&#125;; 如果声明我们的函数f（转发函数fwd的目标）为接收一个std::size_t的形参，则使用IPv4Header对象的totalLength字段进行调用没有问题： 12345void f(std::size_t sz); //要调用的函数IPv4Header h;…f(h.totalLength); //可以 如果通过fwd转发h.totalLength给f呢，那就是一个不同的情况了： 1fwd(h.totalLength); //错误！ 问题在于fwd的形参是引用，而h.totalLength是non-const位域，这是C++不允许的行为。 位域可能包含了一个字的任意部分（比如32位int的3-5位），但是这些东西无法直接寻址。在硬件层面引用和指针是一样的，所以没有办法创建一个指向任意bit的指针。 传递位域给完美转发的方法就是，创建副本然后利用副本调用完美转发。在IPv4Header的例子中，可以如下写法： 1234//拷贝位域值auto length = static_cast&lt;std::uint16_t&gt;(h.totalLength);fwd(length); //转发这个副本 结论 导致完美转发失败的实参种类有：花括号初始化，作为空指针的0或者NULL，仅有声明的static const数据成员，模板和重载函数的名字，位域。 30 Lambda 表达式 闭包（enclosure）是lambda创建的运行时对象。依赖捕获模式，闭包持有被捕获数据的副本或者引用。 闭包类（closure class）是从中实例化闭包的类。每个lambda都会使编译器生成唯一的闭包类。lambda中的语句成为其闭包类的成员函数中的可执行指令。 lambda通常被用来创建闭包，该闭包仅用作函数的实参。闭包通常可以拷贝，所以可能有多个闭包对应于一个lambda。 但是对于闭包，需要明白的一点是：区分什么存在于编译期（lambdas 和闭包类），什么存在于运行时（闭包）以及它们之间的相互关系。 避免使用默认的捕获模式 按默认引用捕获会导致闭包中包含了对某个局部变量或者形参的引用。如果该lambda创建的闭包生命周期超过了局部变量，那么闭包中的引用将会变成悬空引用。 另外，成员函数中，使用捕获需要明白 this 指针的存在，直接捕获成员变量是会出错的。 一个解决方案是： 123456789void Widget::addFilter() const&#123; auto divisorCopy = divisor; //拷贝数据成员 filters.emplace_back( [divisorCopy](int value) //捕获副本 &#123; return value % divisorCopy == 0; &#125; //使用副本 );&#125; 在C++14中，一个更好的捕获成员变量的方式时使用通用的lambda捕获： 1234567void Widget::addFilter() const&#123; filters.emplace_back( //C++14： [divisor = divisor](int value) //拷贝divisor到闭包 &#123; return value % divisor == 0; &#125; //使用这个副本 );&#125; 如果是 static 成员，那么默认捕获行为将什么也不会捕获。 结论 默认的按引用捕获可能会导致悬空引用。 默认的按值捕获对于悬空指针很敏感（尤其是this指针），并且它会误导人产生lambda是独立的想法。 31 使用 init capture 来移动对象到闭包 在某些场景下，按值捕获和按引用捕获都不是你所想要的。如果你有一个只能被移动的对象（例如std::unique_ptr或std::future）要进入到闭包里，使用C++11是无法实现的。到了C++14就另一回事了，它能支持将对象移动到闭包中。 init capture C++14中，这是使用初始化捕获将std::unique_ptr移动到闭包中的方法： 123456789101112131415161718class Widget &#123; //一些有用的类型public: … bool isValidated() const; bool isProcessed() const; bool isArchived() const;private: …&#125;;auto pw = std::make_unique&lt;Widget&gt;(); //创建Widget；使用std::make_unique //的有关信息参见条款21… //设置*pwauto func = [pw = std::move(pw)] //使用std::move(pw)初始化闭包数据成员 &#123; return pw-&gt;isValidated() &amp;&amp; pw-&gt;isArchived(); &#125;; “pw = std::move(pw)”的意思是“在闭包中创建一个数据成员pw，并使用将std::move应用于局部变量pw的结果来初始化该数据成员”。 在C++11中，无法捕获表达式的结果。 因此，初始化捕获的另一个名称是通用lambda捕获（generalized lambda capture）。 lambda表达式只是生成一个类和创建该类型对象的一种简单方式而已。没什么是你用lambda可以做而不能自己手动实现的。 C++14的示例代码可以用C++11重新编写，如下所示： 123456789101112131415class IsValAndArch &#123; //“is validated and archived”public: using DataType = std::unique_ptr&lt;Widget&gt;; explicit IsValAndArch(DataType&amp;&amp; ptr) //条款25解释了std::move的使用 : pw(std::move(ptr)) &#123;&#125; bool operator()() const &#123; return pw-&gt;isValidated() &amp;&amp; pw-&gt;isArchived(); &#125; private: DataType pw;&#125;;auto func = IsValAndArch(std::make_unique&lt;Widget&gt;()); 使用 bind 的解决方法： C++11的等效代码如下，其中我强调了相同的关键事项： 12345678910std::vector&lt;double&gt; data; … auto func = std::bind( //C++11模拟初始化捕获 [](const std::vector&lt;double&gt;&amp; data) //译者注：本行高亮 &#123; /*使用data*/ &#125;, std::move(data) //译者注：本行高亮 ); 默认情况下，从lambda生成的闭包类中的operator()成员函数为const的。在lambda主体内把闭包中的所有数据成员渲染为const。 但是，bind对象内部的移动构造的data副本不是const的，因此，为了防止在lambda内修改该data副本，lambda的形参应声明为reference-to-const。 如果将lambda声明为mutable，则闭包类中的operator()将不会声明为const，并且在lambda的形参声明中省略const也是合适的： 123456auto func = std::bind( //C++11对mutable lambda [](std::vector&lt;double&gt;&amp; data) mutable //初始化捕获的模拟 &#123; /*使用data*/ &#125;, std::move(data) ); 结论 使用C++14的初始化捕获将对象移动到闭包中。 在C++11中，可以通过手写类或std::bind的方式来模拟初始化捕获。 33 对auto&amp;&amp;形参使用decltype 12auto f = [](auto&amp;&amp; x) &#123; return func(normalize(std::forward&lt;???&gt;(x))); &#125;; 这里的???该是什么？ 在泛型lambda中，没有可用的类型参数T。在lambda生成的闭包里，模版化的operator()函数中的确有一个T，但在lambda里却无法直接使用它。 如果一个左值实参被传给通用引用的形参，那么形参类型会变成左值引用。传递的是右值，形参就会变成右值引用。这意味着在这个lambda中，可以通过检查形参x的类型来确定传递进来的实参是一个左值还是右值，decltype就可以实现这样的效果。 传递给lambda的是一个左值，decltype(x)就能产生一个左值引用；如果传递的是一个右值，decltype(x)就会产生右值引用。 在调用std::forward时，惯例决定了类型实参是左值引用时来表明要传进左值，类型实参是非引用就表明要传进右值。 在前面的lambda中，如果x绑定的是一个左值，decltype(x)就能产生一个左值引用。 然而如果x绑定的是一个右值，decltype(x)就会产生右值引用，而不是常规的非引用。 但是decltype(x)就会产生右值引用传入 std::forward 后，引用折叠后的结果和传入非引用的结果是相同的。 所以decltype(x) 完美解决了问题。 C++14中的lambda也可以是可变形参的，最后的实现如下： 123456auto f = [](auto&amp;&amp;... params) &#123; return func(normalize(std::forward&lt;decltype(params)&gt;(params)...)); &#125;; 34 优先考虑lambda而非std::bind 优先lambda而不是std::bind的最重要原因是lambda更易读。 但是，在C++11中，可以在两种情况下使用std::bind是合理的： 移动捕获。C++11的lambda不提供移动捕获，但是可以通过结合lambda和std::bind来模拟。 多态函数对象。因为bind对象上的函数调用运算符使用完美转发，所以它可以接受任何类型的实参 例如： 123456class PolyWidget &#123;public: template&lt;typename T&gt; void operator()(const T&amp; param); …&#125;; std::bind可以如下绑定一个PolyWidget对象： 12PolyWidget pw;auto boundPW = std::bind(pw, _1); boundPW可以接受任意类型的对象了： 123boundPW(1930); //传int给PolyWidget::operator()boundPW(nullptr); //传nullptr给PolyWidget::operator()boundPW(\"Rosebud\"); //传字面值给PolyWidget::operator() 这一点无法使用C++11的lambda做到。 但是，在C++14中，可以通过带有auto形参的lambda轻松实现： 12auto boundPW = [pw](const auto&amp; param) //C++14 &#123; pw(param); &#125;; 结论 与使用std::bind相比，lambda更易读，更具表达力并且可能更高效。 只有在C++11中，std::bind 对实现移动捕获，或者绑定函数模板，会很有用。 并发API C++11的伟大成功之一是将并发整合到语言和库中。 35 优先考虑基于任务的编程而非基于线程的编程 如果开发者想要异步执行doAsyncWork函数，通常有两种方式。其一是通过创建std::thread执行doAsyncWork，这是应用了基于线程（thread-based）的方式： 12int doAsyncWork();std::thread t(doAsyncWork); 其二是将doAsyncWork传递给std::async，一种基于任务（task-based）的策略： 1auto fut = std::async(doAsyncWork); //“fut”表示“future” 这种方式中，传递给std::async的函数对象被称为一个任务（task）。 基于任务的方法通常比基于线程的方法更优： 代码量更少 如果 task 的返回值是必需的，那么 thread-based 的方式将无能为力。而基于任务的方法就简单了，因为std::async返回的future提供了get函数（从而可以获取返回值）。 如果doAsycnWork发生了异常，get函数就显得更为重要，因为get函数可以提供抛出异常的访问，而基于线程的方法，如果doAsyncWork抛出了异常，程序会直接终止（通过调用std::terminate）。 区别 基于线程与基于任务最根本的区别在于，基于任务的抽象层次更高。基于任务的方式使得开发者从线程管理的细节中解放出来，对此在C++并发软件中总结了“thread”的三种含义： 硬件线程（hardware threads）是真实执行计算的线程。现代计算机体系结构为每个CPU核心提供一个或者多个硬件线程。 软件线程（software threads）（也被称为系统线程（OS threads、system threads））是操作系统管理的在硬件线程上执行的线程。通常可以存在比硬件线程更多数量的软件线程。当软件线程被阻塞的时候（比如 I/O、同步锁或者条件变量），操作系统可以调度其他未阻塞的软件线程执行提供吞吐量。 std::thread 是C++执行过程的对象，并作为软件线程的句柄（handle）。 软件线程是有限的资源。如果开发者试图创建大于系统支持的线程数量，会抛出std::system_error异常。 即使没有超出软件线程的限额，仍然可能会遇到资源超额（oversubscription）的麻烦。当前准备运行的（即未阻塞的）软件线程大于硬件线程的数量时，线程调度器会将软件线程时间切片，分配到硬件上。 当一个软件线程的时间片执行结束，会让给另一个软件线程，此时发生上下文切换。软件线程的上下文切换会增加系统的软件线程管理开销。当软件线程安排到与上次时间片运行时不同的硬件线程上，这个开销会更高。 使用std::async 将线程管理的职责转交给C++标准库的开发者。举个例子，这种调用方式会减少抛出资源超额异常的可能性，因为这个调用可能不会开启一个新的线程。合理的调度器在系统资源超额或者线程耗尽时就会利用这个自由度。 通过向std::async传递std::launch::async启动策略来保证想运行函数在不同的线程上执行，处理不同线程响应优先级的问题。 线程调度器使用系统级线程池（thread pool）来避免资源超额的问题，并且通过工作窃取算法（work-stealing algorithm）来提升了跨硬件核心的负载均衡。实现更为繁琐。 直接使用std::thread编程，处理线程耗尽、资源超额、负责均衡问题的责任就压在了你身上，更不用说你对这些问题的解决方法与同机器上其他程序采用的解决方案配合得好不好了。 基于任务的设计为开发者避免了手动线程管理的痛苦，并且自然提供了一种获取异步执行程序的结果（即返回值或者异常）的方式。当然，仍然存在一些场景直接使用std::thread会更有优势： 你需要访问非常基础的线程API。C++并发API通常是通过操作系统提供的系统级API（pthreads或者Windows threads）来实现的，系统级API通常会提供更加灵活的操作方式（举个例子，C++没有线程优先级和亲和性的概念）。为了提供对底层系统级线程API的访问，std::thread对象提供了native_handle的成员函数，而std::future（即std::async返回的东西）没有这种能力。 你需要且能够优化应用的线程使用。 你需要实现C++并发API之外的线程技术，比如，实现未支持的平台的线程池。 结论 std::thread API不能直接访问异步执行的结果，如果执行函数有异常抛出，代码会终止执行。 基于线程的编程方式需要手动的线程耗尽、资源超额、负责均衡、平台适配性管理。 通过带有默认启动策略的std::async进行基于任务的编程方式会解决大部分问题。 36 如有必要指定std::launch::async std::launch::async启动策略意味着f必须异步执行，即在不同的线程。 std::launch::deferred启动策略意味着f仅当在std::async返回的future上调用get或者wait时才执行。这表示f推迟到存在这样的调用时才执行。当get或wait被调用，f会同步执行，即调用方被阻塞，直到f运行结束。如果get和wait都没有被调用，f将不会被执行。 std::async的默认启动策略，如果你不显式指定一个策略，不是上面中任意一个。相反，是求或在一起的。下面的两种调用含义相同： 1234auto fut1 = std::async(f); //使用默认启动策略运行fauto fut2 = std::async(std::launch::async | //使用async或者deferred运行f std::launch::deferred, f); 因此默认策略允许f异步或者同步执行。 这导致了三种结果： 无法预测f是否会与t并发运行，因为f可能被安排延迟运行。 无法预测f是否会在与某线程相异的另一线程上执行，这个某线程在fut上调用get或wait。如果对fut调用函数的线程是t，无法预测f是否在异于t的另一线程上执行。 无法预测f是否执行。 所以，以下循环看似应该最终会终止，但可能实际上永远运行： 1234567891011121314using namespace std::literals; //为了使用C++14中的时间段后缀void f() //f休眠1秒，然后返回&#123; std::this_thread::sleep_for(1s);&#125;auto fut = std::async(f); //异步运行f（理论上）while (fut.wait_for(100ms) != //循环，直到f完成运行时停止... std::future_status::ready) //但是有可能永远不会发生！&#123; …&#125; 如果f与调用std::async的线程并发运行（即，如果为f选择的启动策略是std::launch::async），这里没有问题（假定f最终会执行完毕），但是如果f是延迟执行，fut.wait_for将总是返回std::future_status::deferred。这永远不等于std::future_status::ready，循环会永远执行下去。 改进的方式如下： 1234567891011121314auto fut = std::async(f); //同上if (fut.wait_for(0s) == //如果task是deferred（被延迟）状态 std::future_status::deferred)&#123; … //在fut上调用wait或get来异步调用f&#125; else &#123; //task没有deferred（被延迟） while (fut.wait_for(100ms) != //不可能无限循环（假设f完成） std::future_status::ready) &#123; … //task没deferred（被延迟），也没ready（已准备） //做并行工作直到已准备 &#125; … //fut是ready（已准备）状态&#125; 一个总是使用 std::launch::async 的函数实现如下： C++11版本如下： 123456789template&lt;typename F, typename... Ts&gt;inlinestd::future&lt;typename std::result_of&lt;F(Ts...)&gt;::type&gt;reallyAsync(F&amp;&amp; f, Ts&amp;&amp;... params) //返回异步调用f(params...)得来的future&#123; return std::async(std::launch::async, std::forward&lt;F&gt;(f), std::forward&lt;Ts&gt;(params)...);&#125; 在C++14中，reallyAsync返回类型的推导能力可以简化函数的声明： 123456789template&lt;typename F, typename... Ts&gt;inlineauto // C++14reallyAsync(F&amp;&amp; f, Ts&amp;&amp;... params)&#123; return std::async(std::launch::async, std::forward&lt;F&gt;(f), std::forward&lt;Ts&gt;(params)...);&#125; 结论 std::async的默认启动策略是异步和同步执行兼有的。 std::async的默认启动策略，隐含了任务可能不会被执行的意思，会影响调用基于超时的wait的程序逻辑。 如果异步执行任务非常关键，则指定std::launch::async。 37 使std::thread 是 unjoinable 的 每个std::thread对象处于两个状态之一：可结合的（joinable）或者不可结合的（unjoinable）。 可结合状态的std::thread对应于正在运行或者可能要运行的异步执行线程。比如，对应于一个阻塞的（blocked）或者等待调度的线程的std::thread是可结合的；对应于运行结束的线程的std::thread也可以认为是可结合的。 不可结合的 std::thread 包括： 默认构造的std::threads。这种std::thread没有函数执行，因此没有对应到底层执行线程上。 已经被移动走的std::thread对象。移动的结果就是一个std::thread原来对应的执行线程现在对应于另一个std::thread。 已经被join的std::thread 。在join之后，std::thread不再对应于已经运行完了的执行线程。 已经被detach的std::thread 。detach断开了std::thread对象与执行线程之间的连接。 如果发生 std::thread 析构，而 std::thread 是 joinable，那么会造成程序终止。析构时发生的 隐式join 可能还会访问已经被回收的值。隐式detach ，可能出现访问或者修改没有所有权的内存的行为。 解决方法，使用 RAII 对象类管理，保证每当在执行跳至块之外时，调用局部对象的析构函数。 123456789101112131415161718192021222324252627class ThreadRAII &#123;public: enum class DtorAction &#123; join, detach &#125;; ThreadRAII(std::thread&amp;&amp; t, DtorAction a) // 在析构函数中对t实行a动作 : action(a), t(std::move(t)) &#123;&#125; // `std::thread`不可以复制 ~ThreadRAII() &#123; if (t.joinable()) &#123; //可结合性测试见下 if (action == DtorAction::join) &#123; t.join(); &#125; else &#123; t.detach(); &#125; &#125; &#125; ThreadRAII(ThreadRAII&amp;&amp;) = default; //支持移动 ThreadRAII&amp; operator=(ThreadRAII&amp;&amp;) = default; std::thread&amp; get() &#123; return t; &#125; private: // as before DtorAction action; std::thread t; // 最后声明，实例化时，前面的成员都是可用状态&#125;; 结论 析构时join会导致难以调试的表现异常问题。 析构时detach会导致难以调试的未定义行为。 声明类数据成员时，最后声明std::thread对象。 38 future析构行为 结论 future的正常析构行为就是销毁future本身的数据成员。 使用std::async启动的 future，引用了共享状态（std::shared_future）的最后一个future的析构函数会阻塞住，直到任务完成。 39 简单事件通信 一个任务通知另一个异步执行的任务发生了特定的事件很有用，因为第二个任务要等到这个事件发生之后才能继续执行。事件也许是一个数据结构已经初始化，也许是计算阶段已经完成，或者检测到重要的传感器值。 使用条件变量 如果我们将检测条件的任务称为检测任务（detecting task），对条件作出反应的任务称为反应任务（reacting task），策略很简单：反应任务等待一个条件变量，检测任务在事件发生时改变条件变量。代码如下： 12std::condition_variable cv; //事件的条件变量std::mutex m; //配合cv使用的mutex 检测任务中的代码不能再简单了： 12… //检测事件cv.notify_one(); //通知反应任务 如果有多个反应任务需要被通知，使用notify_all代替notify_one。 线程API的存在一个事实（不只是C++），等待一个条件变量的代码即使在条件变量没有被通知时，也可能被唤醒，这种唤醒被称为虚假唤醒（spurious wakeups）。 正确的代码通过确认要等待的条件确实已经发生来处理这种情况，并将这个操作作为唤醒后的第一个操作。C++条件变量的API使得这种问题很容易解决，因为允许把一个测试要等待的条件的lambda（或者其他函数对象）传给wait。因此，可以将反应任务wait调用这样写： 12cv.wait(lk, []&#123; return whether the evet has occurred; &#125;); 使用 condition variable 的示例： 123456789std::condition_variable cv; std::mutex m;bool flag(false); //不是std::atomic… //检测某个事件&#123; std::lock_guard&lt;std::mutex&gt; g(m); //通过g的构造函数锁住m flag = true; //通知反应任务（第1部分）&#125; //通过g的析构函数解锁mcv.notify_one(); //通知反应任务（第2部分） 反应任务代码如下: 1234567… //准备作出反应&#123; std::unique_lock&lt;std::mutex&gt; lk(m); cv.wait(lk, [] &#123; return flag; &#125;); //使用lambda来避免虚假唤醒 … //对事件作出反应（m被锁定）&#125;… //继续反应动作（m现在解锁） 原子变量轮询 使用原子变量的示例： 当检测线程识别到发生的事件，将flag置位： 123std::atomic&lt;bool&gt; flag(false); //共享的flag… //检测某个事件flag = true; //告诉反应线程 就其本身而言，反应线程轮询该flag。当发现flag被置位，它就知道等待的事件已经发生了： 123… //准备作出反应while (!flag); //等待事件… //对事件作出反应 这里多出了轮询的开销。 promise + future 检测任务使用std::promise&lt;void&gt;，反应任务使用std::future&lt;void&gt;或者std::shared_future&lt;void&gt;。当感兴趣的事件发生时，检测任务设置std::promise&lt;void&gt;，反应任务在future上wait。 尽管反应任务不从检测任务那里接收任何数据，通信信道也可以让反应任务知道，检测任务什么时候已经通过对std::promise&lt;void&gt;调用set_value“写入”了void数据。 1std::promise&lt;void&gt; p; //通信信道的promise 检测任务代码很简洁： 12… //检测某个事件p.set_value(); //通知反应任务 反应任务代码也同样简单： 123… //准备作出反应p.get_future().wait(); //等待对应于p的那个future… //对事件作出反应 像使用flag的方法一样，此设计不需要互斥锁，无论在反应线程调用wait之前检测线程是否设置了std::promise都可以工作，并且不受虚假唤醒的影响（只有条件变量才容易受到此影响）。 与基于条件变量的方法一样，反应任务在调用wait之后是真被阻塞住的，不会一直占用系统资源。 但是以上代码中，std::promise和future之间有个共享状态，并且共享状态是动态分配的。因此你应该假定此设计会产生基于堆的分配和释放开销。 std::promise只能设置一次。std::promise和future之间的通信是一次性的：不能重复使用。这是与基于条件变量或者基于flag的设计的明显差异，条件变量和flag都可以通信多次。 假设你仅仅想要对某线程挂起一次（在创建后，运行线程函数前），使用void的future就是一个可行方案。 通过share获得的shared_future要被在反应线程中运行的lambda按值捕获： 1234567891011121314151617std::promise&lt;void&gt; p; //跟之前一样void detect() //现在针对多个反映线程&#123; // 写在前面，防止异常发生，p.set_value() 不执行 auto sf = p.get_future().share(); //sf的类型是std::shared_future&lt;void&gt; std::vector&lt;std::thread&gt; vt; //反应线程容器 for (int i = 0; i &lt; threadsToRun; ++i) &#123; vt.emplace_back([sf]&#123; sf.wait(); //在sf的局部副本上wait； react(); &#125;); &#125; … //如果这个“…”抛出异常，detect挂起！ p.set_value(); //所有线程解除挂起 … for (auto&amp; t : vt) &#123; //使所有线程不可结合； t.join(); //“auto&amp;”见条款2 &#125;&#125; 结论 三种简单事件通信：使用条件变量、使用原子变量、使用 promise + future promise + future 的方式，在单次事件通信时，更有优势 40 并发使用std::atomic，特殊内存使用volatile 如下使用std::atmoic的代码： 12345std::atomic&lt;int&gt; ai(0); //初始化ai为0ai = 10; //原子性地设置ai为10std::cout &lt;&lt; ai; //原子性地读取ai的值++ai; //原子性地递增ai到11--ai; //原子性地递减ai到10 在这些语句执行过程中，其他线程读取ai，只能读取到0，10，11三个值其中一个。假设只有这个线程会修改ai，没有其他可能的值。 使用volatile在多线程中实际上不保证任何事情： 12345volatile int vi(0); //初始化vi为0vi = 10; //设置vi为10 std::cout &lt;&lt; vi; //读vi的值++vi; //递增vi到11--vi; //递减vi到10 代码的执行过程中，如果其他线程读取vi，可能读到任何值，比如-12，68，4090727——任何值！这份代码有未定义行为，因为这里的语句修改vi，所以如果同时其他线程读取vi，同时存在多个readers和writers读取没有std::atomic或者互斥锁保护的内存，这就是数据竞争的定义。 指令排序 代码执行本身，即使编译器没有重排顺序，底层硬件也可能重排（或者可能使它看起来运行在其他核心上），因为有时这样代码执行更快。 然而，std::atomic会限制这种重排序，保持了指令执行的有序性。 123std::atomic&lt;bool&gt; valVailable(false); auto imptValue = computeImportantValue(); //计算值valAvailable = true; //告诉另一个任务，值可用了 编译器不仅要保证imptValue和valAvailable的赋值顺序，还要保证生成的硬件代码不会改变这个顺序。结果就是，将valAvailable声明为std::atomic确保了必要的顺序——其他线程看到的是imptValue值的改变不会晚于valAvailable。 将valAvailable声明为volatile不能保证上述顺序： 123volatile bool valVailable(false); auto imptValue = computeImportantValue();valAvailable = true; //其他线程可能看到这个赋值操作早于imptValue的赋值操作 这份代码编译器可能将imptValue和valAvailable赋值顺序对调。 结论 std::atomic用于在不使用互斥锁情况下，来使变量被多个线程访问的情况。是用来编写并发程序的一个工具。 volatile用在读取和写入不应被优化掉的内存上。是用来处理特殊内存的一个工具。 其他优化 41 对于移动成本低且总是被拷贝的可拷贝形参，考虑按值传递 三个版本的addName： 12345678910111213141516171819202122232425class Widget &#123; //方法1：对左值和右值重载public: void addName(const std::string&amp; newName) &#123; names.push_back(newName); &#125; // rvalues void addName(std::string&amp;&amp; newName) &#123; names.push_back(std::move(newName)); &#125; …private: std::vector&lt;std::string&gt; names;&#125;;class Widget &#123; //方法2：使用通用引用public: template&lt;typename T&gt; void addName(T&amp;&amp; newName) &#123; names.push_back(std::forward&lt;T&gt;(newName)); &#125; …&#125;;class Widget &#123; //方法3：传值public: void addName(std::string newName) &#123; names.push_back(std::move(newName)); &#125; …&#125;; 我将前两个版本称为“按引用方法”，因为都是通过引用传递形参。 仍然考虑这两种调用方式： 123456Widget w;…std::string name(\"Bart\");w.addName(name); //传左值…w.addName(name + \"Jenne\"); //传右值 现在分别考虑三种实现中，给Widget添加一个名字的两种调用方式，拷贝和移动操作的开销。 重载：无论传递左值还是传递右值，调用都会绑定到一个叫newName的引用上。从拷贝和移动操作方面看，这个过程零开销。左值重载中，newName拷贝到Widget::names中，右值重载中，移动进去。 ​ 开销总结：左值一次拷贝，右值一次移动。 使用通用引用：同重载一样，调用也绑定到addName这个引用上，没有开销。由于使用了std::forward，左值std::string实参会拷贝到Widget::names，右值std::string实参移动进去。 ​ 对std::string实参来说，开销同重载方式一样：左值一次拷贝，右值一次移动。 按值传递：无论传递左值还是右值，都必须构造newName形参。如果传递的是左值，需要拷贝的开销，如果传递的是右值，需要移动的开销。在函数的实现中，newName总是采用移动的方式到Widget::names。 ​ 开销总结：左值实参，一次拷贝一次移动，右值实参两次移动。 对于特殊的场景，可拷贝且移动开销小的类型，传递给总是会拷贝他们的一个函数，切片也不需要考虑，。这时，按值传递就提供了一种简单的实现方式，效率接近传递引用的函数，但是避免了传引用方案的缺点。 当移动的开销较低，额外的一次移动才能被开发者接受，但是当移动的开销很大，执行不必要的移动就类似执行一个不必要的拷贝，也就是避免不必要的拷贝。 对于可拷贝形参使用按值传递。不符合此条件的的形参必须有只可移动的类型（move-only types）。 只会在目标代码中生成一个函数。避免了通用引用的种种问题。 只对总是被拷贝的形参考虑按值传递。因为传引用可以避免这个不必要的开销。 结论 对于可拷贝，移动开销低，而且无条件被拷贝的形参，按值传递效率基本与按引用传递效率一致，而且易于实现，还生成更少的目标代码。 某些情况下，通过构造拷贝形参可能比通过赋值拷贝形参开销大的多。 按值传递会引起切片问题，所说不适合基类形参类型。 42 emplacement 而不是 insertion 编译器处理的下面的调用： 1vs.push_back(std::string(\"xyzzy\")); //创建临时std::string，把它传给push_back 为了在std::string容器中创建新元素，调用了std::string的构造函数，但是这份代码并不仅调用了一次构造函数，而是调用了两次，而且还调用了std::string析构函数。下面是在push_back运行时发生了什么： 一个std::string的临时对象从字面量“xyzzy”被创建。这个对象没有名字，我们可以称为temp。temp的构造是第一次std::string构造。因为是临时变量，所以temp是右值。 temp被传递给push_back的右值重载函数，绑定到右值引用形参x。在std::vector的内存中一个x的副本被创建。这次构造——也是第二次构造——在std::vector内部真正创建一个对象。 在push_back返回之后，temp立刻被销毁，调用了一次std::string的析构函数。 使用传递给它的任何实参直接在std::vector内部构造一个std::string。没有临时变量会生成： 1vs.emplace_back(\"xyzzy\"); //直接用“xyzzy”在vs内构造std::string emplace_back使用完美转发，因此只要你没有遇到使用完美转发的限制，就可以传递任何实参以及组合到emplace_back。 emplace_back： 值是通过构造函数添加到容器，而不是直接赋值。 传递的实参类型与容器的初始化类型不同。 容器不拒绝重复项作为新值。 12vs.emplace_back(\"xyzzy\"); //在容器末尾构造新值；不是传递的容器中元 //素的类型；没有使用拒绝重复项的容器 资源管理 假定你有一个盛放std::shared_ptr&lt;Widget&gt;s的容器， 1std::list&lt;std::shared_ptr&lt;Widget&gt;&gt; ptrs; 使用push_back的代码如下： 1ptrs.push_back(std::shared_ptr&lt;Widget&gt;(new Widget, killWidget)); 也可以像这样： 1ptrs.push_back(&#123;new Widget, killWidget&#125;); 不管哪种写法，在调用push_back前会生成一个临时std::shared_ptr对象。push_back的形参是std::shared_ptr的引用，因此必须有一个std::shared_ptr。 用emplace_back应该可以避免std::shared_ptr临时对象的创建，但是在这个场景下，临时对象值得被创建。考虑如下可能的时间序列： 在上述的调用中，一个std::shared_ptr&lt;Widget&gt;的临时对象被创建来持有“new Widget”返回的原始指针。称这个对象为temp。 push_back通过引用接受temp。在存储temp的副本的list节点的内存分配过程中，内存溢出异常被抛出。 随着异常从push_back的传播，temp被销毁。作为唯一管理这个Widget的std::shared_ptr，它自动销毁Widget，在这里就是调用killWidget。 这样的话，即使发生了异常，没有资源泄漏。 考虑使用emplace_back代替push_back： 1ptrs.emplace_back(new Widget, killWidget); 通过new Widget创建的原始指针完美转发给emplace_back中，list节点被分配的位置。如果分配失败，还是抛出内存溢出异常。 当异常从emplace_back传播，原始指针是仅有的访问堆上Widget的途径，但是因为异常而丢失了，那个Widget的资源（以及任何它所拥有的资源）发生了泄漏。 在这个场景中，生命周期不良好，这个失误不能赖std::shared_ptr。使用带自定义删除器的std::unique_ptr也会有同样的问题。 解决方法是： 123std::shared_ptr&lt;Widget&gt; spw(new Widget, //创建Widget，让spw管理它 killWidget);ptrs.push_back(std::move(spw)); //添加spw右值 emplace_back的版本如下： 12std::shared_ptr&lt;Widget&gt; spw(new Widget, killWidget);ptrs.emplace_back(std::move(spw)); 无论哪种方式，都会产生spw的创建和销毁成本。 与explicit的构造函数的交互 相似的初始化语句导致了多么不一样的结果： 12std::regex r1 = nullptr; //错误！不能编译std::regex r2(nullptr); //可以编译 在标准的官方术语中，用于初始化r1的语法（使用等号）是所谓的拷贝初始化。相反，用于初始化r2的语法是（使用小括号，有时也用花括号）被称为直接初始化。 emplace_back 使用直接初始化，这意味着可能使用explicit的构造函数。 push_back 使用拷贝初始化，所以不能用explicit的构造函数。因此： 123regexes.emplace_back(nullptr); //可编译。直接初始化允许使用接受指针的 //std::regex的explicit构造函数regexes.push_back(nullptr); //错误！拷贝初始化不允许用那个构造函数 获得的经验是，当你使用emplace_back时，请特别小心确保传递了正确的实参，因为即使是explicit的构造函数也会被编译器考虑，编译器会试图以有效方式解释你的代码。 结论 原则上，emplace_back有时会比push_back高效，并且不会更差。 实际上，当以下条件满足时，emplace_back更快：（1）值被构造到容器中，而不是直接赋值；（2）传入的类型与容器的元素类型不一致；（3）容器不拒绝已经存在的重复值。 emplace_back可能执行push_back拒绝的类型转换。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Book","slug":"Book","permalink":"https://racleray.github.io/tags/Book/"}],"author":"HeRui"},{"title":"Prompt, Adaptor, ...","slug":"Prompt-Adaptor","date":"2022-04-05T14:57:55.000Z","updated":"2024-01-09T03:12:40.582Z","comments":true,"path":"posts/7695beab.html","link":"","permalink":"https://racleray.github.io/posts/7695beab.html","excerpt":"Prompt and Adaptor and ... Record.","text":"预训练模型没有那么完美，其中两个问题：一，面对数据稀缺的情况，微调效果很可能一般；二，模型参数量大，存储空间占用较大，计算量大。目前相应的对策，效果较好的有：Prompt取代fine-tuning，设计Apdater减少模型训练参数。 Prompt 设计fine-tuning输入模板，让模型根据上下文填充MASK。 Input: [CLS] The spring break is coming soon. [MASK]. the spring break was over? | Label: no Input: [CLS] I am going to have dinner. [MASK]. I am going to eat something? | Label: yes MASK 部分可以是多个词，长度可调。 如何加入任务label信息？通过 verbalizer，将 MASK 部分预测的词与label的单词形成一个map。预测的时候，根据预测的词和map，找到对应的label。 通过预训练model，自己学习处任务相关的 prompt。充分利用预训练信息，这种方式看起来是不错的。 问题来了，怎么确定一个好的 verbalizer mapping ？ PET(Pattern-Exploiting Training) PET 方案（少量样本半监督情景）： 在监督数据上，进行多个Prompt范式tuning，得到多组 prompt 单词，分别对应到相应的label。 在无监督数据上，多个模型进行预测，每个模型分别在各自的 prompt 单词候选集中，进行输出的softmax。多个模型结果取平均，得到soft label。（蒸馏也用这招，一些半监督或者模型精调都有用到这种方法）。 联合监督数据和无监督数据，进行训练得到最终的模型。 iPET (Iterative PET) PET 作者对 PET 的改进版。只是将 PET的三步，进行多轮，同时增大label mapping的范围。实验效果，在小样本场景下是超过 fine-tuning 的。但是，这对比实验，有点不公平。因为是在 fine-tuning 并没精心设计过的条件下的比较。 另外，PET多个模型训练的时间成本和资源消耗明显更高。那么，这些成本转换成对 fine-tuning 方式下，构建人工标注数据的成本呢？又该怎么说？是不是还简单直接一些？ LM-BFF(better few-shot fine-tuning of language models) LM-BFF 则是另一种思路，增加更多的提示信息，输入到model。 将上图改为： 后面加上了一组示例输入，作为一种显示提示。和GPT3的方式有点像。但是LM-BFF对模型进行训练的数据，有梯度更新。GPT3没有梯度更新，只是生成模型的inference提示。 这里的 mapping 设计方式，论文使用了人工设计和模型自己学习推断两种方法。效果这能说相差无几。 Multitask Prompt Multitask Prompt 使用多任务的方式，每个任务设计一个 prompt template，进行学习。然后在模型没有见过的任务上，再进行prompt inference，期望模型实现 zero-shot inference。论文实验结果显示，使用T5或者T0模型进行多任务Prompt，更少的参数量就可以达到甚至超过GPT3的效果。 T0 Prompt repo Other multitask based： SPoT P-tuning v2 参数化 Prompt 设计 verbalizer 显然不够 AI。有研究者就直接时用可训练的 token 来替代 prompt，直接训练 token 对应的特殊的 embedding。 对于 GPT 这种自回归模型，设计 prefix token，加在输入 sentence 之前；对于 T5、BART 这类Encoder-Decoder 模型，在Encoder和Decoder两边加上 prefix-E 和 prefix-D。训练时将原预训练模型参数freeze。 还有像 P-tuning 这样的方法，使用 LSTM 对 Prompt 输入进行额外编码。同时开放 原预训练模型参数进行训练。效果不错，超越了 fine-tuning 效果。但是这个方法训练起来的成本显然比较高。 Tricks prompt 单词使用 label 单词初始化，效果相比随机初始化要好。 Adaptor Parameter-Efficient Fine-tuning，不训练原预训练模型的参数，只训练设计的 Adaptor 结构的参数。 fine-tuning 的想法是训练原模型参数，产生相应任务的有效 hidden representation。但是 Adaptor 是固定原模型参数，设计结构在原 hidden representation 基础上得到任务相关的 hidden representation。 工具库 Adaptor Adaptor 只训练 Adapter 层参数。更少的训练参数，更好的训练效果。 LoRA(Low-Rank Adaptation) LoRA 只在 Feed-forward 层，增加 Low-Rank Adaptation。 Prefix Tuning Prefix Tuning 是 Prompt 中的一种方法，它也是高效训练的一种设计。只更新 Prefix 的参数。 另外，使用了这种形式的 Prompt Tuning 对于多任务训练，有更高的效率。 直接将不同的 prefix 一起多任务训练。这比 PET 这种“老古董模型”效率上强多了。ref 混合型 TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING 结合了几种常见方法： 得到更好的效果。另外总结了几种常见的形式。 Early Exit 另一种思路，不是 Adaptor 类型的，但是放在这里一起对比了。 这种方法认为，模型在越高的层的输出存在 over thinking 的可能。所以，考虑提前结束训练，在较低的层就输出。 有几种方法： Multi Exit 在每一层都设置一个输出层，联合每一层输出进行损失计算，并给更高的层更大的权重。最后选择其中一层最为 inference 输出。可以指定某一层。动态输出的方法有下面几种。 Shallow-deep Shallow-deep其中使用方法是，对于每一层分类结果，选择第一个大于某个阈值的层，进行最终输出。 DeeBERT DeeBERT 相比Shallow-deep，只是指标不一样，使用entropy进行比较。 PABEE PABEE 思路比较简单，模型从下往上层，连续输出同一个 label 的次数超过限制，就进行输出。 对于分类任务，次数限制设计为: 回归任务： 其他 Apdaptor 设计方式，在实验中更不易过拟合，并且模型迁移训练效果更好。对于小数据集也有不错的效果。 长序列优化 主要针对 self-attention 在长序列任务上的大计算量进行设计优化。 思路有几种： Longformer, BigBird: 更改attention window，设计局部 attention，空洞 attention，随机 attention，以及只对部分词进行全局 attention，这些方法一起使用。 Reformer：对key 和 query先进性内存聚类，再按簇进行attention。 Linformer：对key进行线性变换，降低key的个数。 Efficient attention，Linear Transformer，Performer：将query和key的矩阵计算，经过一个变量分解，因为长序列这两个的计算量会O(n^2)级变大，而且这是可以通过类似核函数的方法进行分解的。将key和value的计算先进性，有效减少计算量。关键就是这个变换的设计形式。 Synthesizer：不计算得到 attention weights 了，直接设为设计的固定参数矩阵。 MLP-Mixer，Fnet：attention free，直接不attention了，使用其他方式。不同维度全连接层融合，或者转化到“频域”进行融合。 有空整理。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://racleray.github.io/tags/nlp/"},{"name":"prompt","slug":"prompt","permalink":"https://racleray.github.io/tags/prompt/"},{"name":"adaptor","slug":"adaptor","permalink":"https://racleray.github.io/tags/adaptor/"}],"author":"HeRui"},{"title":"Effective Cpp","slug":"Effective-Cpp","date":"2022-03-15T13:43:52.000Z","updated":"2024-01-09T03:12:58.684Z","comments":true,"path":"posts/95b1972c.html","link":"","permalink":"https://racleray.github.io/posts/95b1972c.html","excerpt":"Effective Cpp 读书笔记","text":"一、习惯C++ 条款01：视C++为一个语言联邦 C++已经是个多重范型编程语言（multiparadigm programminglanguage），一个同时支持过程形式（procedural）、面向对象形式（object-oriented）、函数形式（functional）、泛型形式（generic）、元编程形式（metaprogramming）的语言。 C++的重要组成：C、Object-Oriented C++、Template C++、STL。 C++是包含四种次语言的一体多面语言，关键看你怎么用。 比如，只在C语言部分，pass-by-value通常比pass-by-reference高效，但在面向对象部分，正好相反，pass-by-reference-to-const是相对更好的选择。 而在STL中，迭代器和函数对象是在C pointer之上，所以pass-by-value更高效。 条款02：尽量以const，enum，inline替换＃define “宁可让编译器替换预处理器”。 对于单纯常量，最好以const对象或enums替换 #define。 对于形似函数的宏（macros），最好改用inline函数替换 #define，避免出错。 #ifdef / #ifndef 继续扮演控制编译的重要角色。 123const double Ratio = 1.5;#define RATIO 1.5 在编译器错误处理时，#define不会告诉你 RATIO 的出现信息，而是被替换的1.5。 enum 可以作为一种in class常量初值设定的方式。这样就取不到成员变量的地址。 12345class Player&#123;private: enum &#123;NumTurns = 5;&#125;; ...&#125;; 条款03：尽可能使用const 如果关键字 const出现在星号左边，表示被指物是常量；如果出现在星号右边，表示指针自身是常量；如果出现在星号两边，表示被指物和指针两者都是常量。 将const实施于成员函数的目的，是为了确认该成员函数可作用于const对象身上。 这一类成员函数之所以重要，基于两个理由。第一，它使 class 接口比较容易被理解。这是因为，得知哪个函数可以改动对象内容而哪个函数不行，很是重要。第二，它使“操作const对象”成为可能。 两个成员函数如果只是常量性（constness）不同，也可以被重载。比如 const T&amp; getXXX() const;和T&amp; getXXX(); 在const成员函数需要被修改的变量，使用mutable修饰。mutable释放掉non-static成员变量的bitwise constness约束。 1234567891011121314151617class CBook&#123; ...public: std::size_t len() const;private: mutable std::size_t length; mutable bool isValid;&#125;;std::size_t len() const&#123; if (!isValid)&#123; length = std::strlen(text); isValid = true; &#125; return length;&#125;; 利用const_cast将常量性移除，可以运用const成员函数实现出其non-const孪生兄弟。当 const和non-const成员函数有着实质等价的实现时，令non-const版本调用const版本可避免代码重复。 123456789101112class CBook&#123;public: const char&amp; operator[] (std::size_t pos) const&#123; ... return text[pos]; &#125; char&amp; operator[] (std::size_t pos) &#123; return const_cast&lt;char&amp;&gt;( static_cast&lt;const CBook*&gt;(*this)[pos]); &#125;&#125;; 另外，将某些东西声明为 const 可帮助编译器侦测出错误用法。const可被施加于任何作用域内的对象、函数参数、函数返回类型、成员函数本体。 条款04：确定对象被使用前已先被初始化 永远在使用对象之前先将它初始化。确保每一个构造函数都将对象的每一个成员初始化。应该尽量使用initialization list。 C++有着十分固定的“成员初始化次序”。base classes更早于其derived classes被初始化（见条款12），而class的成员变量总是以其声明次序被初始化。 为内置型对象进行手工初始化，因为C++不保证初始化它们。 构造函数最好使用成员初值列（member initialization list），而不要在构造函数本体内使用赋值操作（assignment）。initialization list列出的成员变量，其排列次序应该和它们在class中的声明次序相同。 为免除“跨编译单元之初始化次序”问题，请以local static对象替换non-localstatic对象，确保在使用对象前，初始化对象。Singleton模式的一个常见实现手法。 12345678910111213141516171819class FileSys&#123;...&#125;;FileSys&amp; tfs()&#123; static FileSys fs; return fs;&#125;class Dir&#123;...&#125;;Dir::Dir(params)&#123; ... std::size_t disks = tfs().numDisks(); ...&#125;Dir&amp; tmpDir()&#123; static Dir td; return td;&#125; 二、构造/析构/赋值运算 条款05：了解C++默默编写并调用哪些函数 编译器就会为它声明（编译器版本的）一个copy构造函数、一个copy assignment操作符和一个析构函数。此外如果你没有声明任何构造函数，编译器也会为你声明一个default构造函数。所有这些函数都是public且inline （见条款30）。 copy构造函数被用来“以同型对象初始化自我对象”，copy assignment操作符被用来“从另一个同型对象中拷贝其值到自我对象”。copy构造函数是一个尤其重要的函数，因为它定义一个对象如何passed by value 。 编译器可自动为class创建default构造函数、copy构造函数、copyassignment 操作符，以及析构函数。 12345678class Empty&#123;public: Empty() &#123;...&#125;; Empty(const Empty&amp; rhs) &#123;...&#125;; ~Empty() &#123;...&#125;; Empty&amp; operator=(const Empty&amp; rhs) &#123;...&#125;;&#125; 条款06：不用默认构造函数时，需要明确即拒绝 明确声明一个成员函数，可以替代编译器默认版本。 或者拒绝编译器默认版本，可将相应的成员函数声明为private并且不予实现。 或者使用delete关键字，明确不使用。 条款07：为多态基类声明virtual析构函数 当derived class经由一个base class指针被删除时，base class若是non-virtual析构函数，则不会执行derived class的析构函数，导致内存泄露。 消除这个问题的做法很简单：给base class一个virtual析构函数。此后删除derived class 对象就会如你想要的那般。 对象需要在运行期决定哪一个virtual函数该被调用。由一个所谓vptr（virtual table pointer）指针指出。vptr指向一个由函数指针构成的数组，称为vtbl（virtual table）；每一个带有virtual函数的class都有一个相应的vtbl。当对象调用某一virtual函数，实际被调用的函数取决于该对象的vptr所指的那个vtbl——编译器在其中寻找适当的函数指针。 析构函数的运作方式是，最深层派生（most derived）的那个class其析构函数最先被调用，然后是其每一个derived class的析构函数被调用。 如果class带有任何virtual函数，它就应该拥有一个virtual析构函数。 Class 的设计目的如果不是作为 base classes 使用，或不是为了具备多态性（polymorphical），就不该声明virtual析构函数。 条款08：别让异常逃离析构函数 析构函数绝对不要吐出异常。如果一个被析构函数调用的函数可能抛出异常，析构函数应该捕捉任何异常，然后不传递或结束程序。否则可能出现不可预知的风险。 如果客户需要对某个操作函数运行期间抛出的异常做出反应，那么 class 应该提供一个普通函数（而非在析构函数中）执行该操作。 123456789101112131415161718192021class DBConn&#123;public: ... void close()&#123; db.close(); // 可能出错 closed=true; &#125; ~DBConn&#123; if (!closed)&#123; try&#123; db.closed(); &#125;catch (...)&#123; ... &#125; &#125; &#125;private: DBConnection db; bool closed = false;&#125; 条款09：绝不在构造和析构过程中调用virtual函数 derived class对象内的base class会在derived class自身被构造之前先构造。所以调用virtual 函数，derived class并为被完全初始化，导致出现参数未初始化错误。 在derived class对象的base class构造期间，对象的类型是 base class 而不是 derived class。不只 virtual 函数会被编译器解析至（resolve to）base class，若使用运行期类型信息（runtime typeinformation，例如dynamic_cast（见条款27）和typeid），也会把对象视为base class类型。 唯一能够避免此问题的做法就是：确定你的构造函数和析构函数都没有（在对象被创建和被销毁期间）调用 virtual 函数。 在构造函数或者析构函数中调用virtual 函数，不会调用到 derived class 层级的函数（只是 base class 那层）。 **条款10：令operator=返回一个 reference to *this** 为了实现“连锁赋值”，赋值操作符必须返回一个reference指向操作符的左侧实参。令赋值（assignment）操作符返回一个reference to *this。 12345678910111213141516171819202122232425class CC&#123;public: CC&amp; operator=(const CC&amp; rhs)&#123; ... return *this; &#125; CC&amp; operator+=(const CC&amp; rhs)&#123; ... return *this; &#125; CC&amp; operator=(int rhs)&#123; ... return *this; &#125; CC&amp; operator++() &#123; ... return *this; &#125; // 后置++，带参，且返回值 const CC operator++(int) &#123; CC tmp = *this; this-&gt;operator++(); return tmp; &#125;&#125; 条款11：在operator=中处理“自我赋值” 欲阻止这种错误，传统做法是藉由operator=最前面的一个“证同测试（identity test）”达到“自我赋值”的检验目的。 在operator=函数内确保代码不但“异常安全”而且“自我赋值安全”的一个替代方案是，使用所谓的copy and swap技术。不仅解决了代码复用，还保证了赋值操作的安全性。 123456789101112131415161718192021222324252627template &lt;typename T&gt;class Matrix &#123; ... friend void swap(Matrix &amp;a, Matrix &amp;b) noexcept &#123; using std::swap; // 这一步允许编译器基于ADL寻找合适的swap函数 swap(a.x, b.x); swap(a.y, b.y); swap(a.data, b.data); &#125; ...&#125;;Matrix&lt;T&gt;&amp; Matrix&lt;T&gt;::operator=(const Matrix &amp;rhs)&#123; // 检测自赋值 if (&amp;rhs == this) &#123; return *this; &#125; Matrix tmp = rhs; // copy swap(tmp, *this); // swap return *this;&#125;// 甚至于 move and swapMatrix&lt;T&gt;&amp; Matrix&lt;T&gt;::operator=(Matrix2 &amp;&amp;rhs) noexcept &#123; Matrix2 tmp&#123;std::forward&lt;Matrix2&gt;(rhs)&#125;; swap(*this, tmp); return *this;&#125; 条款12：复制对象时勿忘其每一个成分 如果你为class添加一个成员变量，你必须同时修改copy函数。你也需要修改class的所有构造函数（见条款4和条款45）以及任何非标准形式的operator=。 derived class必须复制其base class成分。那些成分往往是private（见条款22），所以你无法直接访问它们，应该让derived class的copy函数调用相应的base class函数。 当你编写一个copy函数，请确保复制所有 local 成员变量，调用所有 base classes 内的适当的copy函数。 注意，copy构造函数和copy assignment操作符，可以提取公共操作，但是不能互相嵌套使用。 1234567891011121314151617181920212223242526272829class Base&#123;private: string name;&#125;class Derived: public Base&#123;public: ... Derived(const Derived&amp; rhs); Derived&amp; operator=(const Derived&amp; rhs); ...private: int priority;&#125;Derived::Derived(const Derived&amp; rhs) :Base(rhs), priority(rhs.priority)&#123; ... &#125;// copyDerived&amp;Derived::operator=(const Derived&amp; rhs)&#123; ... Base::operator=(rhs); priority = rhs.priority; return *this;&#125; 三、资源管理 C++程序中最常使用的资源就是动态分配内存，但内存只是你必须管理的众多资源之一。其他常见的资源还包括文件描述符（file descriptors）、互斥锁（mutex locks）、图形界面中的字型和笔刷、数据库连接、以及网络sockets。 条款13：以对象管理资源 把资源放进对象内， C++的“析构函数自动调用机制”确保资源被释放。 获得资源后立刻放进管理对象（managing object）内。“以对象管理资源”常被称为“资源取得时机便是初始化时机”（Resource Acquisition Is Initialization；RAII），几乎总是在获得资源后于同一语句内用它初始化某个管理对象。 管理对象（managing object）运用析构函数确保资源被释放。不论控制流如何离开区块，一旦对象被销毁，其析构函数会被自动调用。 为防止资源泄漏，请使用RAII对象，它们在构造函数中获得资源并在析构函数中释放资源 两个常被使用的RAII classes分别是 std::shared_ptr 和 std::auto_ptr。前者通常是较佳选择，因为其copy行为比较直观。若选择auto_ptr，复制动作会使 被复制的ptr 指向null。 由于 std::shared_ptr 和 std::auto_ptr 内部析构使用的是 delete 而不是 delete[]，所以以下代码是个错误： 123std::auto_ptr&lt;std::string&gt; aps(new std::string[10]);std::shared_ptr&lt;std::string&gt; aps2(new std::string[10]); 别对动态分配而得到的array使用 std::shared_ptr 和 std::auto_ptr。 条款14：复制RAII对象需要注意 复制 RAII 对象必须一并复制它所管理的资源，所以资源的 copying 行为决定RAII对象的 copying 行为。 处理方法根据对象及其资源的特点决定。 禁止复制 123class Lock: private Uncopyable &#123; ...&#125; 对资源进行引用计数。使用 std::shared_ptr（同时可以用 deleter 参数传入 function object，控制计数为0时的行为）。 123456789class Lock&#123;public: explicit Lock(Mutex* pm): mutexPtr(pm, unlock)&#123; // unlock 为 deleter lock(mutexPtr.get()); &#125;private: std::shared_ptr&lt;Mutex&gt; mutexPtr;&#125; 深拷贝资源 转移资源拥有权，比如使用 std::auto_ptr 条款15：在资源管理类中提供对原始资源的访问 APIs往往要求访问原始资源（raw resources），所以每一个RAII class应该提供一个“访问原始资源”的办法。 对原始资源的访问可能经由显式转换或隐式转换。一般而言显式转换比较安全，隐式转换更灵活。 条款16：成对使用new和delete时要采取相同形式 当你使用 new，有两件事发生。第一，内存被分配出来（通过名为operator new的函数，见条款49和条款51）。第二，此内存区域会有一个或多个构造函数被调用。 当你使用 delete，也有两件事发生：第一，资源内存会有一个或多个析构函数被调用；第二，内存才被释放（通过名为operator delete的函数，见条款51）。 如果你在new表达式中使用[]，必须在相应的delete表达式中也使用[]。如果你在new表达式中不使用[]，一定不要在相应的delete表达式中使用[]。 12345678910string* p1 = new string;delete p1;string* p2 = new string[10];delete[] p2;typedef string Def[3];string* p3 = new Def;delete[] p3; 条款17：以独立语句将newed对象置入智能指针 理由是C++编译器处理事件顺序的不确定性。 比如，process传入Widget的ptr，和一个priority()函数： 1process(std::shared_ptr&lt;Widget&gt;(new Widget), priority()); 执行顺序中，在 new 和 shared_ptr 构造函数执行时，priority()的执行出现异常，那么new的对象可能导致资源泄露。 以独立语句将 newed对象存储于（置入）智能指针内。正确方法： 1234// 1std::shared_ptr&lt;Widget&gt; pw(new Widget);// 2process(pw, priority()); 四、设计与声明 条款18：让接口容易被正确使用，不易被误用 “促进正确使用”的办法包括：接口的一致性，与内置类型的行为兼容。 “阻止误用”的办法包括：建立新类型时限制类型上的不必要操作，不让使用者负责资源管理。 std::shared_ptr 支持定制 custom deleter。可被用来自动解除互斥锁（mutexes；见条款14）。 条款19：设计class犹如设计type 设计高效的classes必须了解你面对的问题： 真的需要一个新type吗？如果只是为既有的class添加一些功能，是否单纯定义一或多个non-member函数或templates，就能够达到目的？。 新type的对象应该如何被创建和销毁？即class的构造函数、析构函数、内存分配函数和释放函数（operator new，operator new[]，operator delete和operator delete[]）的设计。 对象的初始化和对象的赋值有什么样的差别？这决定了构造函数和赋值操作符（operator=）的行为差异。别混淆了“初始化”和“赋值”。 新type的对象如何被passed by value？即如何设计copy constructor。 考虑type成员变量的取值合法范围。 新type 的继承关系如何？是否继承自虚基类，是否会被新子类继承，析构函数是否需要为virtual？。 新type需要什么样的类型转换？若允许类型 T1 被隐式转换为T2，就必须在 class T1 内写一个类型转换操作符（operator T2）或在 class T2 内写一个 non-explicit-one-argument 的构造函数（即，Ctor(int arg1, int arg2=1):m_arg1(arg1),m_arg2(arg2) {}）。如果你只允许 explicit 构造函数存在，就得写出专门负责执行转换的构造函数，且不能是类型转换操作符（type conversion operators, 即operator T2）或 non-explicit-one-argument 构造函数。 什么样的操作符和函数对此新 type 而言是合理的？即，需要为class声明哪些member函数，哪些外部全局函数。 哪个成员为 public，哪个为protected，哪个为 private，哪一个 classes 和/或 functions 应该是friends？ 新type有是否需要是个class template？。 条款20：宁以pass by reference to const替换pass by value 默认情况下C++以pass by value方式（一个继承自C的方式）传递对象至函数。默认函数参数都是以实参的副本为初值，而调用端所获得的也是函数返回值的一个副本，由对象的copy 构造函数生成，这可能使得pass by value成为费时的操作。 pass by reference to const这种传递方式，没有任何构造函数或析构函数被调用，因为没有任何新对象被创建。 以by reference方式传递参数也可以避免slicing（对象切割）问题。当一个derived class对象以by value方式传递并被视为一个base class对象，base class的copy构造函数会被调用，而没有初始化derived class的部分。 尽量以pass-by-reference-to-const替换pass-by-value。前者通常比较高效，并可避免切割问题（slicing problem）。 以上规则并不适用于内置类型，以及 STL 的迭代器和函数对象。对它们而言，pass-by-value往往比较适当。STL的迭代器和函数是基于C指针实现的。 条款21：必须返回对象时，别返回其reference 绝对不要返回pointer或reference指向一个local stack对象 绝对不要返回reference指向一个heap-allocated对象 条款22：将成员变量声明为private 从封装的角度观之，其实只有两种访问权限：private（提供封装）和其他（不提供封装）。 切记将成员变量声明为private。 protected并不比public更具封装性。 条款23：宁以non-member、non-friend替换member函数 宁可拿non-member、non-friend函数替换member函数。这样做可以增加封装性、包装弹性（packaging flexibility）和可扩展性。 条款24：若所有参数皆需类型转换，请为此采用non-member函数 如果你需要为某个成员函数的所有参数（包括this指针参数）进行类型转换，那么这个函数必须是个non-member。const T operator*(const T&amp; lhs, const T&amp; rhs) 。 条款25：考虑写出一个不抛异常的swap函数 当std::swap对你的类型效率不高时，提供一个swap成员函数，并确定这个函数不抛出异常。 如果你提供一个member swap，也该提供一个non-member swap用来调用前者。 调用swap时应针对std::swap使用using声明式，然后调用swap并且不带任何“命名空间资格修饰”。 为“用户定义类型”进行std templates全特化是好的，但千万不要更改std::swap原来的实现。 五、实现 条款26：尽可能延后变量定义式的出现时间 尽可能延后变量定义式的出现，尽可能在使用变量前定义变量，尽可能在变量赋初值时定义变量。这样做可增加程序的清晰度并改善程序效率。 条款27：尽量少做转型动作 const_cast 通常被用来将对象的常量性转除（cast away the constness）。它也是唯一有此能力的C++-style转型操作符。 dynamic_cast 主要用来执行“安全向下转型”（safe downcasting），也就是用来决定某对象是否归属继承体系中的某个类型。它是唯一可能耗费重大运行成本的转型动作。 reinterpret_cast 意图执行低级转型，实际运行情况取决于编译器，这也就表示它不可移植。例如将一个pointer to int转型为一个int。 static_cast 用来强迫隐式转换（implicit conversions），例如将non-const 对象转为 const 对象（条款3），或将 int 转为 double 等等。将 void* 指针转为某类型 typed 指针，将pointer-to-base转为pointer-to-derived。但它无法将const转为non-const——这个只有const_cast才办得到。 请记住： 如果可以，尽量避免转型，特别是在注重效率的代码中避免 dynamic_casts。如果有个设计需要转型动作，试着发展无需转型的替代设计。 如果转型是必要的，试着将它包装成某个函数。客户随后可以调用该函数，而不需将转型放进他们自己的代码内。 宁可使用C++-style（新式）转型，不要使用旧式转型。 条款28：避免返回handles指向对象内部成分 如果const成员函数传出一个reference指向成员变量，函数运行结果又被存储于对象外部，那么这个函数的调用者就可以通过reference修改对象的内部成员。 避免返回handles（包括references、指针、迭代器）指向对象内部。遵守这个条款可增加封装性，帮助 const 成员函数的行为像个 const，并将发生 dangling handles 的可能性降至最低。 条款29：为“异常安全”而努力是值得的 异常安全函数（Exception-safe functions）提供以下三个保证之一： 基本型保证：如果异常被抛出，程序内的任何事物仍然保持在有效状态下。 强烈型保证：如果异常被抛出，程序状态不改变。如果函数成功，就没有异常出现；如果函数失败，程序会回退到“调用函数之前”的状态。 不抛掷（nothrow）保证：承诺绝不抛出异常，因为它们总是能够完成它们其设计的功能。 异常安全码（Exception-safe code）必须提供上述三种保证之一。如果它不这样做，它就不具备异常安全性。 强烈型异常安全的一种实践：copy and swap。原则很简单：为你打算修改的对象做出一份副本，然后对副本做一切必要修改。若有任何修改动作抛出异常，原对象仍保持未改变状态。待所有改变都成功后，再将副本和原对象在一个不抛出异常的操作中置换（swap）。 异常安全函数（Exception-safe functions）即使发生异常也不会泄漏资源或允许任何数据结构败坏。 “强烈保证”往往能够以 copy-and-swap 实现出来，但“强烈保证”并非对所有函数都可实现或具备现实意义。 条款30：透彻了解inline的里里外外 将inline限制在小型、被频繁调用的函数身上。这可使日后的调试过程和二进制升级（binary upgradability）更容易，也可使潜在的代码膨胀问题最小化，使程序的速度提升机会最大化。 不要只因为function templates出现在头文件，就将它们声明为inline。 !!! important!!!: 现代C、C++编译器，会自动优化代码，程序中 inline 已经只算是一种提示符，并不具备编译层面上的绝对含义。所以，忘了它也无妨。 条款31：将文件间的编译依存关系降至最低 支持“编译依存性最小化”的一般构想是：依赖声明，不要依赖定义。 程序库头文件应该以“完全且仅有声明式”（full and declaration-onlyforms）的形式存在。这种做法不论是否涉及templates都适用。 六、继承与面向对象设计 条款32：确定你的public继承塑模出is-a关系 public inheritance 意味 \"is-a\" 的关系。适用于base classes身上的每一件事情一定也适用于derived classes身上，因为每一个derived class对象也都是一个base class对象。 条款33：避免覆盖继承而来的名称 为了让被遮掩的名称再见天日，可使用 using 声明式或转交函数（forwarding functions）。 1234567891011121314151617181920212223242526272829303132333435class Base &#123; int x;public: virtual void f1() = 0; virtual void f1(int); virtual void f2(); void f3(); void f3(double); virtual void f5(); ...&#125;;class Derievd: public Base &#123;public: using Base::f1; using Base::f3; virtual void f1(); void f3(); void f4(); // fowarding function virtual void f5() &#123; Base::f5(); &#125; ...&#125;;Derived d;int x = 1;d.f1(); // Derived::f1d.f1(x); // Base::f1d.f2(); // Base::f2d.f3(); // Derived::f3d.f3(x); // Base::f3d.f4(); // Derived::f4d.f5(); // Derived::f5, 转到Base::f5() 条款34：区分接口继承和实现继承 函数接口（function interfaces）继承和函数实现（functionimplementations）继承。 声明一个pure virtual函数的目的是为了让derived classes只继承函数接口。 声明简朴的（非纯）impure virtual函数的目的，是让derived classes继承该函数的接口和缺省实现，必要情况下，缺省实现可以单独设计为一个成员函数，而接口设计为pure virtual函数，防止缺省实现被误用。 声明non-virtual函数的目的是为了令derived classes继承函数的接口及一份强制性实现，比如设计每个对象都相同且必要的ID生成方法。 条款35：考虑virtual函数以外的其他选择 virtual函数的替代方案包括 NVI(Non-Virtual Interface) 手法及Strategy设计模式的多种形式。NVI手法自身是一个特殊形式的Template Method设计模式。 1234567891011121314// NVI(Non-Virtual Interface) class A &#123;public: int score() const &#123; // non-virtual, 子类不重载 ... int val = doScore(); ... return val; &#125;private: virtual int doScore() const &#123; // 子类重载 ... &#125;&#125;; 将功能从成员函数移到class外部函数，带来的一个缺点是，非成员函数无法访问class的non-public成员。 std::function 对象的行为就像一般函数指针。这样的对象可接纳，函数签名（target signature）一致的所有可调用对象（callable entities）。 1234567891011121314151617181920212223242526272829303132// declareclass Person;int defaultLearnStrategy(const Person&amp; pp);// definationclass Person &#123;public: typedef std::function&lt;int(const Person&amp;)&gt; learnStrat; explicit Person(learnStrat lst=defaultLearnStrategy): stratFunc(lst) &#123; ... &#125; int score() const &#123; return stratFunc(*this); &#125;private: learnStrat stratFunc;&#125;;int readStrategy(const Person&amp;) &#123; ...&#125;;int writeStrategy(const Person&amp;) &#123; ...&#125;;...Person p1(readStrategy);Person p2(writeStrategy);int score1 = p1.score();int score2 = p2.score(); 条款36：绝不重新定义继承而来的non-virtual函数 任何情况下都不该重新定义一个继承而来的non-virtual函数。 条款37：绝不重新定义继承而来的缺省参数值 绝对不要重新定义一个继承而来的virtual成员函数缺省参数值，因为缺省参数值都是静态绑定（statically bound），而virtual函数——你唯一应该覆写的东西——却是动态绑定（dynamically bound）。 静态绑定的问题，当父类指针指向子类对象，其静态类型就为父类。动态绑定，则会根据所指对象，解析动态类型为子类。 若子类重新定义一个继承而来的virtual成员函数缺省参数值，会静态解析为父类中的缺省参数值，而不是子类中重新定义的值。可能会导致一些不易排查的错误。 一种解决方法，是使用non-virtual实现父类的带缺省参数值的成员函数，调用一个virtual的功能成员，传入缺省参数值。子类只需要重载virtual的功能成员。 条款38：通过复合塑模出has-a关系 复合（composition）是类型之间的一种关系，指某种类型的对象内含其他类型的对象。 12345class A &#123; B component1; C component2; ...&#125;; 当复合发生于应用域内的对象之间，表现出has-a的关系；当它发生于实现域内则是表现is-implemented-in-terms-of的关系。 应用域指逻辑上的关联，比如电脑由存储系统、IO系统、计算系统等组成。实现域指一个类的实现中使用了buffer、mutex、binary search tree等技术手段。 条款39：明智而审慎地使用private继承 Private 继承意味 implemented-in-terms-of（根据某物实现出）。如果你让class D以private形式继承class B，你的用意是为了使用class B内某些特性和方法，不是因为B对象和D对象存在有任何观念上的关系。 private继承纯粹只是一种实现技术（继承自一个private base class的每样东西在你的class内都是private，因为它们都只是实现的细枝末节而已）。 Private继承在软件设计层面上没有意义，其意义只在于软件实现层面。 Private继承意味is-implemented-in-terms of（根据某物实现出）。它通常比复合（composition）的级别低。 和复合（composition）不同，private继承可以造成empty base最优化。这对致力于“对象尺寸最小化”的程序库开发者而言，可能很重要。 因为 empty class 始终会占用 1 字节的空间。若使用复合，那么加上alignment的影响，类的空间会存在一些浪费。 123456789101112131415class Defs &#123; typedef ... ...&#125;;// sizeof(A) == 8class A &#123; int x; Defs df;&#125;;// sizeof(B) == 4class B: private Defs &#123; int x; &#125;; 条款40：明智而审慎地使用多重继承 多重继承比单一继承复杂。它可能导致新的歧义性，以及对virtual继承的需要。 virtual继承会增加大小、速度、初始化（及赋值）复杂度等等成本。如果virtual base classes不带任何数据，将是最具实用价值的情况。 多重继承有其用途。比如，同时“public继承某个interface class”和“private继承某个协助实现的class”。 七、模板与泛型编程 条款41：了解隐式接口和编译期多态 面向对象编程世界总是以显式接口（explicit interface）和运行期多态（runtime polymorphism）解决问题。 Templates 及泛型编程的世界，与面向对象有根本上的不同。泛型编程中显式接口和运行期多态仍然存在，但重要性降低。反倒是隐式接口（implicit interfaces）和编译期多态（compile-time polymorphism）得到重视。 class 和 template 都支持接口（interfaces）和多态（polymorphism）。 对 class 而言接口是显式的（explicit），以函数签名为中心。多态则是通过virtual函数发生于运行期。 对 template 参数而言，接口是隐式的（implicit），它取决于T的具现化类型及其实现。多态则是通过template具现化和函数重载解析（function overloading resolution）发生于编译期。 条款42：了解typename的双重意义 template内出现的名称如果依赖某个template参数，称之为从属名称（dependent names）。 如果解析器在template中遭遇一个嵌套从属名称，它便假设这名称不是个类型，除非你明确指明它是个类型。 任何时候当你想要在template中指涉一个嵌套从属类型名称，就必须在它前面加上关键字 typename。 typename 不可以出现在 base classes list 内的嵌套从属类型名称之前，也不可在member initialization list（成员初值列）中作为base class修饰符。 声明template参数时，前缀关键字class和typename可互换。 标识嵌套从属类型名称时，请使用关键字typename；但不得在base class lists（基类列）或member initialization list（成员初值列）内以它作为base class修饰符。 123456789template&lt;typename T&gt;class Derived: public Base&lt;T&gt;::Nested &#123; // base class list 不允许typenamepublic: // member initialization list 不允许typename explicit Derived(int x): Base&lt;T&gt;::Nested(x) &#123; typename Base&lt;T&gt;::Nested temp; // dependent names &#125; ...&#125;; 条款43：学习处理模板化基类内的名称 可在derived class templates内通过 \"this-&gt;；\" 指涉base class template内的成员，而不只是在特化的class template中寻找成员。例如： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960template&lt;typename Company&gt;class MsgSender &#123;public: void send(const MsgInfo&amp; info) &#123;...&#125; void encryptedSend(const MsgInfo&amp; info) &#123;...&#125;&#125;;template&lt;&gt;class MsgSender&lt;CompanyK&gt; &#123;public: // 没有send void encryptedSend(const MsgInfo&amp; info) &#123;...&#125;&#125;;template&lt;typename Company&gt;class LogSender: public MsgSender&lt;Company&gt; &#123;public: void sendLog(const MsgInfo&amp; info) &#123; ... // 当 Company 为 CompanyK，出错 // 因为没有send，只是在MsgSender&lt;CompanyK&gt;中找成员 send(info); ... &#125;&#125;;// Solutiontemplate&lt;typename Company&gt;class LogSender: public MsgSender&lt;Company&gt; &#123;public: void sendLog(const MsgInfo&amp; info) &#123; ... // 可在 template&lt;typename Company&gt; class MsgSender 中找成员 this-&gt;send(info); ... &#125;&#125;;// 或者template&lt;typename Company&gt;class LogSender: public MsgSender&lt;Company&gt; &#123;public: // 告诉编译器 using MsgSender&lt;Company&gt;::send; void sendLog(const MsgInfo&amp; info) &#123; ... send(info); ... &#125;&#125;;// 或者template&lt;typename Company&gt;class LogSender: public MsgSender&lt;Company&gt; &#123;public: void sendLog(const MsgInfo&amp; info) &#123; ... // 明确指出 MsgSender&lt;Company&gt;::send(info); ... &#125;&#125; 条款44：将与参数无关的代码抽离 Templates生成多个classes和多个函数，所以任何template代码都不该与某个造成膨胀的template参数产生依赖关系。 因非类型模板参数（non-type template parameters，比如 n）而造成的代码膨胀，往往可消除，做法是以函数参数或class成员变量替换template参数。 123456template&lt;typename T, std::size_t n&gt;class Matrix &#123;public: void invert(); ...&#125;; 使用以上Matrix，其 Matrix&lt;int, 5&gt; 和 Matrix&lt;int, 10&gt; 会产生两套处理n不同，其他都类似的 invert 代码，造成代码膨胀。 1234567891011121314151617181920212223242526272829template&lt;typename T&gt;class MatrixBase &#123;protected: MatrixBase(std::size_t n, T* pMem): msize(n), pData(pMem) &#123; ... &#125; void setDataPtr(T* ptr) &#123;pData = ptr;&#125; void invert(std::size_t fsize) &#123; ... &#125; ...private: std::size_t msize; T* pData;&#125;;template&lt;typename T, std::size_t n&gt;class Matrix: private MatrixBase&lt;T&gt; &#123; using MatrixBase&lt;T&gt;::invert; // 使用Base的invertpublic: Matrix(): MatrixBase&lt;T&gt;(n, nullptr), pData(new T[n * n]) &#123; this-&gt;setDataPtr(pData.get()); &#125; void invert() &#123; this-&gt;invert(n); &#125; ...private: boost::scoped_array&lt;T&gt; pData;&#125;; 以上就只有一份 invert 代码，是一种解决方式。并且使用指针传递数据地址，进一步与 n 参数分离。 因类型参数（type parameters，比如 int、long等）而造成的代码膨胀，往往可降低，做法是让带有完全相同二进制表述（binary representations）的具现类型（instantiation types）共享实现码。比如，STL中，vector、list等，在实现操作强类型指针 T* 的成员函数时，都调用了一个操作 void* 的成员函数，由后者完成实际工作，避免代码膨胀。 条款45：运用成员函数模板接受所有兼容类型 请使用member function templates（成员函数模板）生成“可接受所有兼容类型”的函数。 如果你声明 member templates 用于“泛化copy构造”或“泛化assignment操作”，你还是需要声明正常的copy构造函数和copy assignment操作符。因为泛化copy构造并不会阻止编译器生成默认的copy构造函数。 123456789101112131415template&lt;class T&gt;class shared_ptr &#123;public: // copy constructor shared_ptr(const shared_ptr&amp; rhs); // templated copy constructor template&lt;class Y&gt; shared_ptr(const shared_ptr&lt;Y&gt;&amp; rhs); // copy assignment shared_ptr&amp; operator=(const shared_ptr&amp; rhs); // templated copy assignment template&lt;class Y&gt; shared_ptr&amp; operator=(const shared_ptr&lt;Y&gt;&amp; rhs); ...&#125; 条款46：需要类型转换时请为模板定义非成员函数 当我们编写一个class template，实现一个外部函数 function template，其所有参数需要进行class template的隐式类型转换时，请将这个外部函数定义为 class template 内部的 friend 函数。 因为 function template 在对实参进行类型推导时，从不考虑通过构造函数进行的隐式类型转换。 这里 friend 的作用不再是为了外部函数访问 class 的 non-public 部分，而是创建一个 non-member function ，以此来完成实参的类型的隐式转换。 123456789101112131415161718template&lt;class T&gt;const Rational&lt;T&gt; doMultiply(const Rational&lt;T&gt;&amp; lhs, const Rational&lt;T&gt;&amp; rhs); // declaretemplate&lt;class T&gt;class Rational &#123;public: // 完成模板具体化 friend const Rational&lt;T&gt; operator*(const Rational&lt;T&gt;&amp; lhs, const Rational&lt;T&gt;&amp; rhs) &#123; return doMultiply(lhs, rhs); &#125;&#125;;template&lt;class T&gt;const Rational&lt;T&gt; doMultiply(const Rational&lt;T&gt;&amp; lhs, const Rational&lt;T&gt;&amp; rhs) &#123; ...&#125; 条款47：请使用traits classes表现类型信息 STL中 iterator 的一个示例： 123456789template&lt;typename T&gt;struct __list_iterator &#123; typedef bidirectional_iterator_tag iterator_category; typedef T value_type; typedef T* pointer; typedef T&amp; reference; typedef ptrdiff_t difference_type; ...&#125;; trait class常见设计如下： 1234567891011121314151617181920212223242526272829303132//使用iterator提供的类型信息template&lt;typename Iterator&gt;struct iterator_traits&#123; typedef typename Iterator::iterator_category iterator_category; typedef typename Iterator::value_type value_typep; typedef typename Iterator::difference_type difference_type; typedef typename Iterator::pointer pointer; typedef typename Iterator::reference reference;&#125;;// 指针偏特化。template&lt;typename T&gt;struct iterator_traits&lt;T *&gt;&#123; typedef random_access_iterator_tag iterator_category; typedef T value_type; typedef ptrdiff_t difference_type; typedef T* pointer; typedef T&amp; reference;&#125;;// const指针偏特化template&lt;typename T&gt;struct iterator_traits&lt;const T *&gt;&#123; typedef random_access_iterator_tag iterator_category; typedef T value_type; typedef ptrdiff_t difference_type; typedef const T* pointer; typedef const T&amp; reference;&#125;; 常见使用： 1typedef typename iterator_traits&lt;Iterator&gt;::iterator_category category; iterator_traits 在编译期获取 iterator_category 等信息。 请记住： Traits classes使得“类型相关信息”在编译期可用。它们通过 templates 和 templates特化实现。 整合重载技术（overloading）后，traits classes 有可能在编译期对类型执行if...else测试。 条款48：认识template元编程 Template metaprogramming（TMP，模板元编程）是编写template-based C++程序并执行于编译期的过程。一旦TMP程序结束执行，其输出，也就是从 templates 具体生成的若干C++源码，便会一如往常地被编译。 一个 TMP 递归程序： 123456789101112template&lt;unsigned n&gt;struct Factorial &#123; enum &#123; value = n * Factorial&lt;n - 1&gt;::value &#125;;&#125;;template&lt;&gt;struct Factorial&lt;0&gt; &#123; enum &#123; value = 1 &#125;;&#125;;...int f6 = Factorial&lt;6&gt;::value; Template metaprogramming（TMP，模板元编程）可将工作由运行期移往编译期，因而得以实现早期错误侦测和更高的执行效率。 TMP 可被用来生成“基于策略选择组合”（based on combinations of policy choices）的定制代码，可用于实现多种设计模式，也可用来避免生成对某些特殊类型并不适合的代码。 八、定制new和delete 条款49：了解new-handler的行为 set_new_handler 允许客户指定一个函数，在内存分配无法获得满足时被调用。 nothrow new 是一个颇为局限的工具，因为它只适用于内存分配阶段；后续的构造函数调用还是可能抛出异常。 123456789101112void outOfMem() &#123; std::cerr &lt;&lt; \"Out of memory.\" &lt;&lt; std::endl; std::abort();&#125;int main() &#123; std::set_new_handler(outOfMem); // 失败返回 0 int *noArr = new(std::nothrow) int[100000000000L]; // outOfMem() 触发 int *arr = new int[100000000000L];&#125; 条款50：了解new和delete的合理替换时机 合理替换时机： 用来检测运用上的错误。如果我们自行定义一个 operator new，在申请的内存中写入特定的签名signatures。operator delete 检查上述签名是否原封不动，若否就表示在分配的内存区域中发生了 overrun 或 underrun，并记录log信息。 为了强化效能。对某些应用程序而言，将编译器自带的new和delete替换为定制版本，是提升效率的办法之一。 为了收集使用上的统计数据。 为了优化内存空间的分配、内存对齐优化等。 为了将关联数据结构尽量保存在连续的更少的内存页上，减少page fault。 条款51：编写new和delete时需固守常规 operator new 应该内含一个无穷循环，并在其中尝试分配内存，如果它无法满足内存需求，就该调用new_handler。它也应该有能力处理0 bytes申请。Class专属版本则还应该处理申请内存大小和class大小不匹配的情况，这通常是 derived class 没有实现 operator new 而调用了 base class 的 operator new 的情况。 operator delete 应该在收到null指针时不做任何事。Class专属版本则还应该处理申请内存大小和class大小不匹配的情况。 12345678910111213141516171819202122232425262728293031323334class Base &#123;public: static void* operator new(std::size_t size) throw(std::bad_alloc); static void operator delete(void* rawMem, std::size_t size) throw(); ...&#125;;void* Base::operator new(std::size_t size) throw(std::bad_alloc) &#123; if (size != sizeof(Base)) return ::operator new(size); //使用std中标准new if (size == 0) size = 1; // 一种处理方法，始终返回合法指针 while (true) &#123; 分配内存; if 分配成功 return 指针 // 以下只是为了取得 new_handler 函数指针 std::new_handler gHandler = std::set_new_handler(0); std::set_new_handler(gHandler); if (gHandler) (*gHandler)(); else throw std::bad_alloc(); &#125;&#125;void Base::operator delete(void* rawMem, std::size_t size) throw() &#123; if (rawMem == 0) return; if (size != sizeof(Base)) &#123; ::operator delete(rawMem); return; &#125; 回收内存; return;&#125; 条款52：写了placement new也要写placement delete 如果operator new接受的参数不止size_t，那就是 placement new。众多 placement new 版本中特别有用的一个是“接受一个指针指向对象该被构造之处”。 一个带额外参数的 operator new，需要带相同额外参数的对应版operator delete。 要防止内存泄漏，必须同时提供一个正常的operator delete，用于构造期间无任何异常被抛出，和一个 placement delete 版本，用于构造期间有异常被抛出。后者的额外参数必须和operator new一样。 当你写一个 placement operator new，也需要写对应的 placement operator delete。如果没有，你的程序可能会发生内存泄漏。 当你声明 placement new 和placement delete，考虑是否有必要覆盖它们的正常（全局默认）版本。 1234567891011121314151617181920212223242526272829303132333435class StandardNewDelete &#123;public: // 使用全局默认的 new/delete static void* operator new(std::size_t size) throw(std::bad_alloc) &#123; return ::operator new(size); &#125; static void operator delete(void* pMem) throw() &#123; ::operator delete(pMem); &#125; // 使用全局默认的 placement new/placement delete static void* operator new(std::size_t size, void* ptr) throw() &#123; return ::operator new(size, ptr); &#125; static void operator delete(void* pMem, void* ptr) throw() &#123; ::operator delete(pMem, ptr); &#125; // 使用全局默认的 nothrow new/nothrow delete static void* operator new(std::size_t size, const std::nothrow_t&amp; nt) throw() &#123; return ::operator new(size, nt); &#125; static void operator delete(void* pMem, const std::nothrow_t&amp; nt) throw() &#123; ::operator delete(pMem); &#125;&#125;// 增加自定义形式class Derived: public StandardNewDelete &#123;public: // 防止标准 new/delete 被覆盖 using StandardNewDelete::operator new; using StandardNewDelete::operator delete; // 追加自定义 placement new/placement delete static void* operator new(std::size_t size, std::ostream&amp; logStream) throw(std::bad_alloc); static void operator delete(void* pMem, std::ostream&amp; logStream) throw();&#125; 九、杂项讨论 条款53：不要轻忽编译器的警告 严肃对待编译器发出的警告信息。努力在编译器的最高警告级别下争取“无任何警告”。 不同的编译器处理方式并不相同。 条款54：让自己熟悉标准程序库 C++标准程序库的主要功能由STL、iostreams、locales组成。 熟悉智能指针、函数指针、hash-based容器、正则表达式（regular expressions）等。 条款55：让自己熟悉Boost Boost致力于免费、源码开放、同僚复审的C++程序库开发。Boost在C++标准化过程中扮演深具影响力的角色。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Book","slug":"Book","permalink":"https://racleray.github.io/tags/Book/"}],"author":"HeRui"},{"title":"epoll简述","slug":"epoll简述","date":"2022-03-02T16:02:59.000Z","updated":"2024-01-09T03:11:52.029Z","comments":true,"path":"posts/d207ef0b.html","link":"","permalink":"https://racleray.github.io/posts/d207ef0b.html","excerpt":"一图流简易epoll实现机制","text":"epoll实现机制粗糙版总结。 Level-triggered VS Edge-triggered 两种模式在实现中的不同之处在于： 在 ep_send_events_proc 函数（ep_send_events中的一个回调函数）的中，如果是 level-triggered 模式，当前的 epoll_item 对象被重新加到 eventpoll 的就绪列表 ready list 中，这样在下一次 epoll_wait 调用时，这些 epoll_item 对象就会被重新处理。而 edge-triggered 模式，不会重新加入。 另外，引用一篇优质blog select、poll、epoll - IO模型超详解。基础概念和基础使用等的信息，直接搜索，不再重复写这里了。 在上面引用的blog中，有一张关于 epoll 机制更抽象一点的图，放一起方便对比。但是该作者有一处错误，关于mmap说明。 这里对 mmap 得说明是错误的。并不是只有块设备才可以进行 mmap 内存映射。 所以上图中的解释是错误的。按我的认知来讲，这里没有将事件链表进行内存映射，至少不是设备类型的原因。 rdlist 链表中的事件是会被复制到用户空间的。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Computer Network","slug":"Notes/Computer-Network","permalink":"https://racleray.github.io/categories/Notes/Computer-Network/"}],"tags":[{"name":"epoll","slug":"epoll","permalink":"https://racleray.github.io/tags/epoll/"}],"author":"HeRui"},{"title":"C/C++测试框架","slug":"C-Cpp测试框架","date":"2022-01-23T12:59:29.000Z","updated":"2023-09-23T13:57:26.173Z","comments":true,"path":"posts/aeaec6fb.html","link":"","permalink":"https://racleray.github.io/posts/aeaec6fb.html","excerpt":"测试框架：catch2、doctes、googletest、cmocka","text":"框架 Head only catch2 doctest Compile googletest C 测试框架： cmocka doctest 轻量 编译速度快（相比于 catch2） API友好 功能丰富，支持对模板批量测试 使用 CMake 配置时，确保编译的目标文件找得到 doctest 的头文件即可。 1234include_directories('path to doctest.h')# 或者target_include_directories($&#123;target_name&#125; PUBLIC 'path to doctest.h') 断言宏等级划分： REQUIRE：这个等级算是最高的，如果断言失败，不仅会标记为测试不通过，而且会强制退出测试。 CHECK：如果断言失败，标记为测试不通过，但不会强制退出，会继续执行。 WARN：如果断言失败，不会标记为测试不通过，也不会强制退出，但是会给出对应的提示。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// build test subcases like a tree.// run: 1--2--end 1--3--end two subroutines.TEST_CASE(\"vectors can be sized and resized\") &#123; std::vector&lt;int&gt; v(5); // 1 REQUIRE(v.size() == 5); REQUIRE(v.capacity() &gt;= 5); SUBCASE(\"adding to the vector increases it's size\") &#123; // 2 v.push_back(1); CHECK(v.size() == 6); CHECK(v.capacity() &gt;= 6); &#125; SUBCASE(\"reserving increases just the capacity\") &#123; // 3 v.reserve(6); CHECK(v.size() == 5); CHECK(v.capacity() &gt;= 6); &#125;&#125;// Group the test cases.TEST_SUITE(\"math\") &#123; TEST_CASE(\"\") &#123;&#125; // part of the math test suite TEST_CASE(\"\") &#123;&#125; // part of the math test suite&#125;// Test template.TEST_CASE_TEMPLATE(\"test std::any as integer\", T, char, short, int, long long int) &#123; auto v = T(); std::any var = T(); CHECK(std::any_cast&lt;T&gt;(var) == v);&#125;TEST_CASE_TEMPLATE(\"test std::any as string\", T, const char *, std::string_view, std::string) &#123; T v = \"hello world\"; std::any var = v; CHECK(std::any_cast&lt;T&gt;(var) == v);&#125;TEST_CASE(\"infos\") &#123; REQUIRE(\"foobar\" == doctest::Contains(\"foo\")); CHECK_MESSAGE(2 == 1, \"not valid\"); REQUIRE(22.0 / 7 == doctest::Approx(3.141).epsilon(0.01)); // allow for a 1% error&#125; nanobench nanobench 12345678910111213141516include(FetchContent)FetchContent_Declare( nanobench GIT_REPOSITORY https://github.com/martinus/nanobench.git GIT_TAG v4.1.0 GIT_SHALLOW TRUE)FetchContent_MakeAvailable(nanobench)...# 目标文件链接target_link_libraries($&#123;target_name&#125; PRIVATE nanobench) 12345678910#include &lt;nanobench.h&gt;#include &lt;atomic&gt;int main() &#123; int y = 0; std::atomic&lt;int&gt; x(0); ankerl::nanobench::Bench().run(\"compare_exchange_strong\", [&amp;] &#123; x.compare_exchange_strong(y, 0); &#125;);&#125; 输出结果 ns/op op/s err% total benchmark 9.07 110,254,890.34 0.0% 0.00 compare_exchange_strong ns/op：每个bench内容需要经历的时间（ns为单位） op/s：每秒可以执行多少次操作 err%：运行多次测试的波动情况（误差） ins/op：每次操作需要多少条指令 cyc/op：每次操作需要多少次时钟周期 bra/op：每次操作有多少次分支预判 miss%：分支预判的miss率 total：本次消耗的总时间 benchmark：\"compare_exchange_strong\" 自定义测试名称 可测 BigO 时间复杂度： 123456789101112131415161718192021222324252627282930313233343536#define DOCTEST_CONFIG_IMPLEMENT_WITH_MAIN#include &lt;doctest.h&gt;#include &lt;nanobench.h&gt;#include &lt;iostream&gt;#include &lt;atomic&gt;#include &lt;set&gt;TEST_CASE(\"tutorial_complexity_set_find\") &#123; // Create a single benchmark instance that is used in multiple benchmark // runs, with different settings for complexityN. ankerl::nanobench::Bench bench; // a RNG to generate input data ankerl::nanobench::Rng rng; std::set&lt;uint64_t&gt; set; // Running the benchmark multiple times, with different number of elements for (auto setSize : &#123;10U, 20U, 50U, 100U, 200U, 500U, 1000U, 2000U, 5000U, 10000U&#125;) &#123; // fill up the set with random data while (set.size() &lt; setSize) &#123; set.insert(rng()); &#125; // Run the benchmark, provide setSize as the scaling variable. bench.complexityN(set.size()).run(\"std::set find\", [&amp;] &#123; ankerl::nanobench::doNotOptimizeAway(set.find(rng())); &#125;); &#125; // calculate BigO complexy best fit and print the results std::cout &lt;&lt; bench.complexityBigO() &lt;&lt; std::endl;&#125; 可使用 pyperf 解析结果： 12345678910111213141516171819202122232425262728#define DOCTEST_CONFIG_IMPLEMENT_WITH_MAIN#include &lt;doctest.h&gt;#include &lt;nanobench.h&gt;#include &lt;algorithm&gt;#include &lt;fstream&gt;#include &lt;atomic&gt;#include &lt;random&gt;TEST_CASE(\"shuffle_pyperf\") &#123; std::vector&lt;uint64_t&gt; data(500, 0); // input data for shuffling // NOLINTNEXTLINE(cert-msc32-c,cert-msc51-cpp) std::default_random_engine defaultRng(123); std::ofstream fout1(\"pyperf_shuffle_std.json\"); ankerl::nanobench::Bench() .epochs(100) .run(\"std::shuffle with std::default_random_engine\", [&amp;]() &#123; std::shuffle(data.begin(), data.end(), defaultRng); &#125;) .render(ankerl::nanobench::templates::pyperf(), fout1); std::ofstream fout2(\"pyperf_shuffle_nanobench.json\"); ankerl::nanobench::Rng rng(123); ankerl::nanobench::Bench() .epochs(100) .run(\"ankerl::nanobench::Rng::shuffle\", [&amp;]() &#123; rng.shuffle(data); &#125;) .render(ankerl::nanobench::templates::pyperf(), fout2);&#125; 1python3 -m pyperf stats pyperf_shuffle_std.json cmocka API文档。但是建议结合 source code 中的示例程序，了解 cmocka 的使用。 项目下新建文件夹 cmocka ，添加以下 .cmake 文件： 12345678910111213141516include(FetchContent)FetchContent_Declare( cmocka GIT_REPOSITORY https://git.cryptomilk.org/projects/cmocka.git GIT_TAG cmocka-1.1.7 GIT_SHALLOW 1)set(WITH_STATIC_LIB ON CACHE BOOL \"CMocka: Build with a static library\" FORCE)set(WITH_CMOCKERY_SUPPORT OFF CACHE BOOL \"CMocka: Install a cmockery header\" FORCE)set(WITH_EXAMPLES OFF CACHE BOOL \"CMocka: Build examples\" FORCE)set(UNIT_TESTING ON CACHE BOOL \"CMocka: Build with unit testing\" FORCE)set(PICKY_DEVELOPER OFF CACHE BOOL \"CMocka: Build with picky developer flags\" FORCE)FetchContent_MakeAvailable(cmocka) 项目根目录下的 CMakeLists.txt 中： 123456789include(cmake/FetchCMocka.cmake)# 添加编译目标add_executable(CMockaExample test.c)target_compile_features(CMockaExample PRIVATE c_std_99)target_link_libraries(CMockaExample PRIVATE cmocka-static)enable_testing()add_test(NAME CMockaExample COMMAND CMockaExample) 12345678910111213141516171819#include &lt;stdarg.h&gt;#include &lt;setjmp.h&gt;#include &lt;stddef.h&gt;#include &lt;cmocka.h&gt;static void test(void **state)&#123; assert_int_equal(2, 2);&#125;int main()&#123; const struct CMUnitTest tests[] = &#123; cmocka_unit_test(test), &#125;; return cmocka_run_group_tests(tests, NULL, NULL);&#125;","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"C","slug":"Tools/C","permalink":"https://racleray.github.io/categories/Tools/C/"}],"tags":[{"name":"C","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"测试框架","slug":"测试框架","permalink":"https://racleray.github.io/tags/%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6/"}],"author":"HeRui"},{"title":"对比学习损失使用","slug":"对比学习损失使用","date":"2021-11-03T14:52:53.000Z","updated":"2023-08-07T12:02:23.619Z","comments":true,"path":"posts/39bd8d48.html","link":"","permalink":"https://racleray.github.io/posts/39bd8d48.html","excerpt":"简单记录两种对比学习损失的简单使用，以及计算方法。","text":"对于自监督学习，一般分为两种。一种是AutoEncoder这种通过一个表征向量，从自己到自己的还原过程，这类称为生成式自监督学习。一种是以学习区分两种不同类事物的关键特征为目标，通过构建正负例子，学习表征向量的方法，这类称为判别式自监督学习，也叫做对比学习。 对比学习的通过互信息，衡量一个表征的好坏，与正例相似而远离负例。这里记录两个常用的对比学习损失。 NTXentLoss NTXentLoss也就是InfoNCE使用的损失： \\[ L = -log \\frac{exp(q \\cdot k_+ / \\tau)}{\\sum^{K}_{i=0}exp(q \\cdot k_i / \\tau)} \\] 分子在最小化损失函数时，会使表征 \\(q\\) 与正例 \\(k_+\\) 的相似度增加。 可以直接使用 PyTorch Metric Learning 包调用损失函数类。 12345from pytorch_metric_learning.losses import NTXentLoss...loss_func = NTXentLoss(temperature=temperature)... 其基本流程如下： 123456789for anchor, positive in pos_pairs: numerator = torch.exp(torch.matmul(anchor, positive) / (temperature * torch.norm(anchor) * torch.norm(positive))) denominator = numerator.clone() for (candidate, negetive) in neg_pairs: tmp = torch.exp(torch.matmul(anchor, negative) / (temperature * torch.norm(anchor) * torch.norm(negative))) denominator += tmp total_loss += -torch.log(numerator / denominator) SupConLoss SupConLoss（Supervised Contrastive）是在监督数据中使用对比学习的损失函数。由于有监督数据的支撑，正例不再来源于样本自身，而且可以来自监督标签中属于同一类的样本。其计算公式的区别也在于多了监督标签的部分。 \\[ L^{sup}= \\sum_{i \\in I} \\frac{-1}{|P(i)|} \\sum_{p \\in P(i)} log \\frac{exp(z_p \\cdot z_i / \\tau)}{ \\sum_{a \\in A(i)} exp(z_a \\cdot z_i / \\tau)} \\] 对每个正例除以包含该正例的positive pairs的数量。具体看代码比较直接。这个公式有两种形式，还有一种是将对 \\(|P(i)|\\) 求平均的操作放置于 log 之内。 可以直接使用 PyTorch Metric Learning 包调用损失函数类。 123from pytorch_metric_learning.losses import SupConLossloss_func = SupConLoss(temperature=temperature) 其基本流程如下： 1234567891011121314losses = torch.zeros(num_of_classes, dtype=torch.float64)for anchor, positive in pos_pairs: numerator = torch.exp(torch.matmul(anchor, positive) / (temperature * torch.norm(anchor) * torch.norm(positive))) denominator = numerator.clone() for (candidate, negetive) in neg_pairs: tmp = torch.exp(torch.matmul(anchor, negative) / (temperature * torch.norm(anchor) * torch.norm(negative))) denominator += tmp losses[anchor_idx] += -torch.log(numerator / denominator)total_loss = torch.mean(losses / num_of_positive_pairs_per_anchor) Gather操作 和之前主题无关，只是记在一起。 1output = tensor.gather(dim, index) tensor与index是两个维度相同的张量。 output中的下标为 (i, j) 的值来自： dim = 0，从tensor中取值时，0维的坐标值来自index张量的 (i, j) 位置的值，1维的坐标值就是 j （output中本来的坐标值）。 dim = 1，从tensor中取值时，1维的坐标值来自index张量的 (i, j) 位置的值，0维的坐标值就是 i （output中本来的坐标值）。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"},{"name":"contrastive learning","slug":"contrastive-learning","permalink":"https://racleray.github.io/tags/contrastive-learning/"}],"author":"HeRui"},{"title":"设计模式Notes","slug":"设计模式Notes","date":"2021-10-25T12:14:37.000Z","updated":"2023-08-07T11:54:31.049Z","comments":true,"path":"posts/97f88c07.html","link":"","permalink":"https://racleray.github.io/posts/97f88c07.html","excerpt":"GOF中23中设计模式整理。","text":"计算机科学中有两种思考方式： 底层思维：向下，把握机器底层从微观理解对象构造。 抽象思维：向上，将问题处理过程抽象为程序代码。 设计模式通过抽象，分离职责，提高复用性。 第一个示例 绘制点或者线，实现方式一，点是点，线是线。 123456789101112131415161718192021222324252627class Point &#123;...&#125;;class Line &#123;...&#125;;class MainForm : public Form &#123; ...;private: vector&lt;Line&gt; lineVector; vector&lt;Rect&gt; rectVector;protected: ... virtual void OnMouseUp(const MouseEventArgs&amp; e); virtual void OnPaint(const PaintEventArgs&amp; e);&#125;;void MainForm::OnMouseUp(const MouseEventArgs&amp; e)&#123; // Point处理代码 ...; // Line处理代码 ...;&#125;void MainForm::OnPaint(const PaintEventArgs&amp; e)&#123; // Point处理代码 ...; // Line处理代码 ...;&#125; 方式二，抽象出Shape基类，在MainForm中统一接口调用。 123456789101112131415161718192021222324252627282930313233343536373839404142434445class Shape&#123;public: virtual void Draw(const Graphics&amp; g)=0; virtual ~Shape() &#123; &#125;&#125;;class Point: public Shape&#123; // 画点 virtual void Draw(const Graphics&amp; g)&#123; ... &#125;;&#125;;class Line: public Shape&#123; // 画线 virtual void Draw(const Graphics&amp; g)&#123; ... &#125;;&#125;;class MainForm : public Form &#123; ...;private: // 抽象 vector&lt;Shape *&gt; shapes;protected: ... virtual void OnMouseUp(const MouseEventArgs&amp; e); virtual void OnPaint(const PaintEventArgs&amp; e);&#125;;void MainForm::OnMouseUp(const MouseEventArgs&amp; e)&#123; // Shape处理代码 ...;&#125;void MainForm::OnPaint(const PaintEventArgs&amp; e)&#123; // Shape处理代码 ...; for (int i = 0; i &lt; shapes.size(); i++)&#123; shapes[i]-&gt;Draw(e.Graphics); //多态调用 &#125;&#125; 概念 设计模式要解决的能解决的问题，是程序同时有“稳定不变的部分”和“可能发生变化的部分”时，如何提高代码重用性。目标是将变化的部分规约到一起，并使扩展功能变得容易一些。如果只有稳定的部分，不需要设计模式。如果全是变化的部分，使用设计模式并不能到达目的。 面向对象的理解 隔离变化 从宏观层面来看，面向对象的构建方式更能适应软件的变化，能将变化所带来的影响减为最小 各司其职 从微观层面来看，面向对象的方式更强调各个类的“责任” 由于需求变化导致的新增类型不应该影响原来类型的实现——是所谓各负其责 对象是什么 从语言实现层面来看，对象封装了代码和数据。 从规格层面讲，对象是一系列可被使用的公共接口。 从概念层面讲，对象是某种拥有责任的抽象。 一般术语的含义 运行时：程序已经被编译，加载到内存中的二进制形式。 扩展：一般来讲，就是建立新的子类，override父类中提供变化的接口方法。 稳定：一般指代码被编译成二进制之后，不会再改变（或者说很少改变）。不是说一个代码文件中，没有改变的代码片段。 变化：一般时程序中，会随着需求、场景、时间等频繁切换或者改变。这部分会经常要求重新编译。 不可修改：一般就是指源代码不会更改。 接口：一个类（抽象基类）对外开放的方法，一般会有统一的设计准则。 绑定：一般就是指调用关系，一个类中的方法会调用到另一个类中的方法实现。类间可以为父子关系。 原则 依赖倒置原则（DIP） 高层模块(稳定)不应该依赖于低层模块(变化)，二者都应该依赖于抽象(稳定) 。 抽象(稳定)不应该依赖于实现细节(变化) ，实现细节应该依赖于抽象(稳定)。 开放封闭原则（OCP） 对扩展开放，对更改封闭 类模块应该是可扩展的，但是不可修改 单一职责原则（SRP） 一个类应该仅有一个引起它变化的原因 变化的方向隐含着类的责任 Liskov 替换原则（LSP） 子类必须能够替换它们的基类(IS-A) 继承表达类型抽象 接口隔离原则（ISP） 不应该强迫客户程序依赖它们不用的方法 接口应该小而完备 优先使用对象组合，而不是类继承 类继承通常为“白箱复用”，对象组合通常为“黑箱复用” 继承在某种程度上破坏了封装性，子类父类耦合度高 而对象组合则只要求被组合的对象具有良好定义的接口，耦合度低 封装变化点 使用封装来创建对象之间的分界层，让设计者可以在分界层的一侧进行修改，而不会对另一侧产生不良的影响，从而实现层次间的松耦合。 针对接口编程，而不是针对实现编程 不将变量类型声明为某个特定的具体类，而是声明为某个接口 客户程序无需获知对象的具体类型，只需要知道对象所具有的接口 减少系统中各部分的依赖关系，从而实现“高内聚、松耦合”的类型设计方案 很抽象的总结，结合具体模式体会。 分类 从目的来看： 创建型（Creational）模式：将对象的部分创建工作延迟到子类或者其他对象，从而应对需求变化为对象创建时具体类型实现引来的冲击。 结构型（Structural）模式：通过类继承或者对象组合获得更灵活的结构，从而应对需求变化为对象的结构带来的冲击。 行为型（Behavioral）模式：通过类继承或者对象组合来划分类与对象间的职责，从而应对需求变化为多个交互的对象带来的冲击。 从范围来看： 类模式处理类与子类的静态关系。 对象模式处理对象间的动态关系。 从封装变化角度对模式分类： 组件协作： • Template Method • Observer / Event • Strategy 单一职责： • Decorator • Bridge 对象创建: • Factory Method • Abstract Factory • Prototype • Builder 对象性能： • Singleton • Flyweight 接口隔离: • Façade • Proxy • Mediator • Adapter 状态变化： • Memento • State 数据结构： • Composite • Iterator • Chain of Responsibility 行为变化： • Command • Visitor 领域问题： • Interpreter 现代软件设计的特征是“需求的频繁变化”。设计模式的要点是“寻找变化点，然后在变化点处应用设计模式，从而来更好地应对需求的变化”。“什么时候、什么地点应用设计模式”比“理解设计模式结构本身”更为重要。 设计模式的应用不宜先入为主，一上来就使用设计模式是对设计模式的最大误用。没有一步到位的设计模式。敏捷软件开发实践提倡的“Refactoring to Patterns”是目前普遍公认的最好的使用设计模式的方法。 重构技法 静态 转 动态 早绑定 转 晚绑定 继承 转 组合 编译时依赖 转 运行时依赖 紧耦合 转 松耦合 虽然表述上不同，但是实质上意思是类似的。 组件协作相关模式 现代软件专业分工之后的第一个结果是“框架与应用程序的划分”，“组件协作”模式通过晚绑定（父类中调用子类的方法实现，虚函数实现），来实现框架与应用程序之间的松耦合，是二者之间协作时常用的模式。 典型模式： Template Method Observer / Event Strategy Template Method 对于某一项任务，它常常有稳定的整体操作结构，但各个子步骤却有很多改变的需求，或者由于固有的原因（比如框架与应用之间的关系）而无法和任务的整体结构同时实现。 在确定稳定操作结构的前提下，来灵活应对各个子步骤的变化或者晚期实现需求。 示例2 设计一个library，支持application在使用时可以自定义框架中的某些步骤。 实现一，Application实现过程还需要完成main函数中调用library，并完成算法逻辑的过程。 123456789101112131415161718192021222324252627class Library&#123;public: void Step1()&#123;...&#125; void Step3()&#123;...&#125; void Step5()&#123;...&#125;&#125;;class Application&#123;public: bool Step2()&#123;...&#125; void Step4()&#123;...&#125;&#125;;int main()&#123; Library lib(); Application app(); lib.Step1(); if (app.Step2())&#123; lib.Step3(); &#125; for (int i = 0; i &lt; 4; i++)&#123; app.Step4(); &#125; lib.Step5();&#125; 实现二，利用虚函数，将固定的算法逻辑在lib中实现，运行时调用App中实现的override的自定义步骤。 12345678910111213141516171819202122232425262728293031323334353637383940class Library&#123;public: //稳定 template method void Run()&#123; Step1(); if (Step2()) Step3(); for (int i = 0; i &lt; 4; i++) Step4(); Step5(); &#125; virtual ~Library()&#123; &#125;protected: void Step1() &#123; 稳定 &#125; void Step3() &#123; 稳定 &#125; void Step5() &#123; 稳定 &#125; // 一般设置为protected virtual bool Step2() = 0;//变化,虚函数的多态调用 virtual void Step4() =0; //变化&#125;;class Application : public Library &#123;protected: virtual bool Step2()&#123; //... 子类重写实现 &#125; virtual void Step4() &#123; //... 子类重写实现 &#125;&#125;;int main()&#123; Library* pLib=new Application(); lib-&gt;Run(); delete pLib;&#125; 实际上就是个虚函数的应用。这里的第一种为早绑定，在app程序中，实现固定不变的流程，并调用lib中的方法。第二种是晚绑定，固定流程实现在lib中，app只关心变化的部分，实现自定义的方法即可，从lib中延迟调用app实现的自定义方法。 ConcreteClass靠AbstractClass来实现算法中不变的步骤。 抽象代码结构 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;/* * AbstractClass * implements a template method defining the skeleton of an algorithm */class AbstractClass&#123;public: virtual ~AbstractClass() &#123;&#125; void templateMethod() &#123; // ... primitiveOperation1(); // ... primitiveOperation2(); // ... &#125; virtual void primitiveOperation1() = 0; virtual void primitiveOperation2() = 0; // ...&#125;;/* * Concrete Class * implements the primitive operations to carry out specific steps * of the algorithm, there may be many Concrete classes, each implementing * the full set of the required operation */class ConcreteClass : public AbstractClass&#123;public: ~ConcreteClass() &#123;&#125; void primitiveOperation1() &#123; std::cout &lt;&lt; \"Primitive operation 1\" &lt;&lt; std::endl; // ... &#125; void primitiveOperation2() &#123; std::cout &lt;&lt; \"Primitive operation 2\" &lt;&lt; std::endl; // ... &#125; // ...&#125;;int main()&#123; AbstractClass *tm = new ConcreteClass; tm-&gt;templateMethod(); delete tm; return 0;&#125; Strategy 在软件构建过程中，某些对象使用的算法可能多种多样，经常改变，如果将这些算法都编码到对象中，将会使对象变得异常复杂；而且有时候支持不使用的算法也是一个性能负担。 Strategy将算法与对象本身解耦。 Strategy表述为： 定义一系列算法，把它们一个个封装起来，并且使它们可互相替换（变化）。该模式使得算法可独立于使用它的客户程序(稳定)而变化（扩展，子类化）。 示例3 设计一个计税程序，根据不同国家税法，进行计算。 方法一，使用 if else 结构，将不同方法整合再一个对象中。 12345678910111213141516171819202122enum TaxBase &#123; CN_Tax, US_Tax, DE_Tax, FR_Tax //扩展&#125;;class SalesOrder&#123; TaxBase tax;public: double CalculateTax()&#123; ... if (tax == CN_Tax)&#123;...&#125; else if (tax == US_Tax)&#123;...&#125; else if (tax == DE_Tax)&#123;...&#125; // 扩展更多情况，需要修改源代码，重新编译 else if (tax == FR_Tax)&#123; ... &#125; ... &#125;&#125;; 方法二，使用类实现不同策略，在应用程序部分实现不变化的部分。 12345678910111213141516171819202122232425262728// 策略基类class TaxStrategy&#123;public: virtual double Calculate(const Context&amp; context)=0; virtual ~TaxStrategy()&#123;&#125;&#125;;class CNTax : public TaxStrategy&#123;public: virtual double Calculate(const Context&amp; context)&#123;...&#125;&#125;;class USTax : public TaxStrategy&#123;public: virtual double Calculate(const Context&amp; context)&#123;...&#125;&#125;;class DETax : public TaxStrategy&#123;public: virtual double Calculate(const Context&amp; context)&#123;...&#125;&#125;;//扩展新策略class FRTax : public TaxStrategy&#123;public: virtual double Calculate(const Context&amp; context)&#123;...&#125;&#125;; 1234567891011121314151617181920// 稳定的流程部分class SalesOrder&#123;private: TaxStrategy* strategy;public: SalesOrder(StrategyFactory* strategyFactory)&#123; this-&gt;strategy = strategyFactory-&gt;NewStrategy(); &#125; ~SalesOrder()&#123; delete this-&gt;strategy; &#125; public double CalculateTax()&#123; //... Context context(); double val = strategy-&gt;Calculate(context); //多态调用 //... &#125;&#125;; 实现不同策略类，通过工厂模式传入策略，保持了稳定的流程部分不会改变。 这里的代码整合到了一起，实际上是在不同的文件中。 一般if else涉及多种场景切换且可能出现扩展需求的地方，都可以考虑使用strategy模式。更多的strategy对象，可能存在一些额外的开销，可以考虑设计为singleton模式。这样不同的context也可以共享一个strategy对象，节省了开销。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687#include &lt;iostream&gt;/* * Strategy * declares an interface common to all supported algorithms */class Strategy&#123;public: virtual ~Strategy() &#123; /* ... */ &#125; virtual void algorithmInterface() = 0; // ...&#125;;/* * Concrete Strategies * implement the algorithm using the Strategy interface */class ConcreteStrategyA : public Strategy&#123;public: ~ConcreteStrategyA() &#123; /* ... */ &#125; void algorithmInterface() &#123; std::cout &lt;&lt; \"Concrete Strategy A\" &lt;&lt; std::endl; &#125; // ...&#125;;class ConcreteStrategyB : public Strategy&#123;public: ~ConcreteStrategyB() &#123; /* ... */ &#125; void algorithmInterface() &#123; std::cout &lt;&lt; \"Concrete Strategy B\" &lt;&lt; std::endl; &#125; // ...&#125;;class ConcreteStrategyC : public Strategy&#123;public: ~ConcreteStrategyC() &#123; /* ... */ &#125; void algorithmInterface() &#123; std::cout &lt;&lt; \"Concrete Strategy C\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Context * maintains a reference to a Strategy object */class Context&#123;public: Context( Strategy* const s ) : strategy( s ) &#123;&#125; ~Context() &#123; delete strategy; &#125; void contextInterface() &#123; strategy-&gt;algorithmInterface(); &#125; // ...private: Strategy *strategy; // ...&#125;;int main()&#123; Context context( new ConcreteStrategyA() ); context.contextInterface(); return 0;&#125; Observer 在软件构建过程中，我们需要为某些对象建立一种“通知依赖关系” ——一个对象（目标对象）的状态发生改变，所有的依赖对象（观察者对象）都将得到通知。 定义描述： 定义对象间的一种一对多（变化）的依赖关系，以便当一个对象(Subject)的状态发生改变时，所有依赖于它的对象都得到通知并自动更新。 比如，当用户改变表格中的信息时, 柱状图能立即反映这一变化。 示例4 在一个大文件切分为小文件储存的过程中，增加不同的进度条显示。 方法一，直接在FileSpliter类中，调用进度条管理对象。 1234567891011121314151617181920212223class FileSplitter&#123; string m_filePath; int m_fileNumber; ProgressBar* m_progressBar; // 直接调用具体的进度条管理对象public: FileSplitter(const string&amp; filePath, int fileNumber, ProgressBar* progressBar) : m_filePath(filePath), m_fileNumber(fileNumber), m_progressBar(progressBar)&#123;...&#125; void split()&#123; ... // 直接调用具体对象方法，更新进度条 for (int i = 0; i &lt; m_fileNumber; i++)&#123; //... progressValue = (i + 1) / (float)progressValue; m_progressBar-&gt;setValue(progressValue); &#125; ... &#125;&#125;; 1234567891011121314151617class MainForm : public Form&#123; TextBox* txtFilePath; TextBox* txtFileNumber; ProgressBar* progressBar;public: void Button1_Click()&#123; string filePath = txtFilePath-&gt;getText(); int number = atoi(txtFileNumber-&gt;getText().c_str()); // 具体对象传入 FileSplitter splitter(filePath, number, progressBar); splitter.split(); &#125;&#125;; 以上实现，FileSplitter不能传入其它类型的进度条对象。不符合依赖倒置原则。 高层模块(稳定)不应该依赖于低层模块(变化)，二者都应该依赖于抽象(稳定) 。 抽象(稳定)不应该依赖于实现细节(变化) ，实现细节应该依赖于抽象(稳定)。 要改，就是将具体进度条对象，想办法变成一个抽象的父类对象。 方法二，使用observer，抽象一个IProgress基类对象，让FileSpliter依赖它。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152// observerclass IProgress&#123;public: // update virtual void DoProgress(float value)=0; virtual ~IProgress()&#123;&#125;&#125;;// subjectclass FileSplitter&#123; string m_filePath; int m_fileNumber; List&lt;IProgress*&gt; m_iprogressList; // 抽象通知机制，支持多个观察者 public: FileSplitter(const string&amp; filePath, int fileNumber) : m_filePath(filePath), m_fileNumber(fileNumber)&#123;...&#125; // 调用notifier: 所有具体observer，更新状态，此处不依赖具体对象 void split()&#123; ... for (int i = 0; i &lt; m_fileNumber; i++)&#123; progressValue = (i + 1) / (float)progressValue; onProgress(progressValue); //发送通知 &#125; ... &#125; // attach observer void addIProgress(IProgress* iprogress)&#123; m_iprogressList.push_back(iprogress); &#125; // detach observer void removeIProgress(IProgress* iprogress)&#123; m_iprogressList.remove(iprogress); &#125;protected: // notify virtual void onProgress(float value)&#123; List&lt;IProgress*&gt;::iterator itor=m_iprogressList.begin(); while (itor != m_iprogressList.end() ) (*itor)-&gt;DoProgress(value); //调用observer更新进度条 itor++; &#125; &#125;&#125;; 123456789101112131415161718192021222324252627282930313233343536373839404142// 一种具体observerclass ConsoleObserver: public IProgress &#123;public: virtual void DoProgress(float value)&#123; cout &lt;&lt; \".\"; &#125;&#125;;// 具体对象使用observer进行通知class MainForm : public Form, public IProgress&#123; TextBox* txtFilePath; TextBox* txtFileNumber; ProgressBar* progressBar;public: void Button1_Click()&#123; string filePath = txtFilePath-&gt;getText(); int number = atoi(txtFileNumber-&gt;getText().c_str()); // 具体observer ConsoleObserver ob1; // 具体subject FileSplitter splitter(filePath, number); // 添加observer splitter.addIProgress(this); //订阅通知 splitter.addIProgress(&amp;ob1)； //订阅通知 // observer将根据subject状态的改变，更新自己状态 splitter.split(); splitter.removeIProgress(&amp;ob1); splitter.removeIProgress(this); &#125; // 在mainform中更新 virtual void DoProgress(float value)&#123; progressBar-&gt;setValue(value); &#125;&#125;; 以上抽象出一个observer基类，由subject进行状态的传递，解耦了进度条对象大的设计。 Subject（目标） —目标知道它的观察者。可以有任意多个观察者观察同一个目标。 —提供注册和删除观察者对象的接口。 Observer（观察者） —为那些在目标发生改变时需获得通知的对象定义一个更新接口。 ConcreteSubject（具体目标） —将有关状态存入各ConcreteObserver对象。 —当它的状态发生改变时,向它的各个观察者发出通知。 ConcreteObserver（具体观察者） —维护一个指向ConcreteSubject对象的引用。 —存储有关状态，这些状态应与目标的状态保持一致。 —实现Observer的更新接口以使自身状态与目标的状态保持一致。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#include &lt;iostream&gt;#include &lt;vector&gt;class Subject;/* * Observer * defines an updating interface for objects that should be notified * of changes in a subject */class Observer&#123;public: virtual ~Observer() &#123;&#125; virtual int getState() = 0; virtual void update( Subject *subject ) = 0; // ...&#125;;/* * Concrete Observer * stores state of interest to ConcreteObserver objects and * sends a notification to its observers when its state changes */class ConcreteObserver : public Observer&#123;public: ConcreteObserver( const int state ) : observer_state( state ) &#123;&#125; ~ConcreteObserver() &#123;&#125; int getState() &#123; return observer_state; &#125; void update( Subject *subject ); // ...private: int observer_state; // ...&#125;;/* * Subject * knows its observers and provides an interface for attaching * and detaching observers */class Subject&#123;public: virtual ~Subject() &#123;&#125; void attach( Observer *observer ) &#123; observers.push_back(observer); &#125; void detach( const int index ) &#123; observers.erase( observers.begin() + index ); &#125; void notify() &#123; for ( unsigned int i = 0; i &lt; observers.size(); i++ ) &#123; observers.at( i )-&gt;update( this ); &#125; &#125; virtual int getState() = 0; virtual void setState( const int s ) = 0; // ...private: std::vector&lt;Observer*&gt; observers; // ...&#125;;/* * Concrete Subject * stores state that should stay consistent with the subject's */class ConcreteSubject : public Subject&#123;public: ~ConcreteSubject() &#123;&#125; int getState() &#123; return subject_state; &#125; void setState( const int s ) &#123; subject_state = s; &#125; // ... private: int subject_state; // ...&#125;;void ConcreteObserver::update( Subject *subject )&#123; observer_state = subject-&gt;getState(); std::cout &lt;&lt; \"Observer state updated.\" &lt;&lt; std::endl;&#125;int main()&#123; ConcreteObserver observer1( 1 ); ConcreteObserver observer2( 2 ); std::cout &lt;&lt; \"Observer 1 state: \" &lt;&lt; observer1.getState() &lt;&lt; std::endl; std::cout &lt;&lt; \"Observer 2 state: \" &lt;&lt; observer2.getState() &lt;&lt; std::endl; Subject *subject = new ConcreteSubject(); subject-&gt;attach( &amp;observer1 ); subject-&gt;attach( &amp;observer2 ); subject-&gt;setState( 10 ); subject-&gt;notify(); std::cout &lt;&lt; \"Observer 1 state: \" &lt;&lt; observer1.getState() &lt;&lt; std::endl; std::cout &lt;&lt; \"Observer 2 state: \" &lt;&lt; observer2.getState() &lt;&lt; std::endl; delete subject; return 0;&#125; 单一职责相关模式 在软件组件的设计中，如果责任划分的不清晰，使用继承得到的结果往往是随着需求的变化，子类急剧膨胀，同时充斥着重复代码，这时候的关键是划清责任。 典型模式 Decorator Bridge Decorator 使用继承来扩展对象的功能，为类型引入的静态特质，使得这种扩展方式缺乏灵活性； 并且随着子类的增多（扩展功能的增多），各种子类的组合（扩展功能的组合）会导致更多子类的膨胀。 动态（组合）地给一个对象增加一些额外的职责。就增加功能而言，Decorator模式比生成子类（继承）更为灵活（消除重复代码 &amp; 减少子类个数）。 示例5 设计一个数据流处理系统，后续在基础系统之上，扩展不同类型的流，以及增加加密、缓存功能。 实现一，使用类继承的方式： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869// 基类class Stream&#123;public： virtual char Read(int number)=0; virtual void Seek(int position)=0; virtual void Write(char data)=0; virtual ~Stream()&#123;&#125;&#125;;// 2种流数据class FileStream: public Stream&#123;public: virtual char Read(int number)&#123;读文件流&#125; virtual void Seek(int position)&#123;定位文件流&#125; virtual void Write(char data)&#123;写文件流&#125;&#125;;class NetworkStream :public Stream&#123;public: virtual char Read(int number)&#123;读网络流&#125; virtual void Seek(int position)&#123;定位网络流&#125; virtual void Write(char data)&#123;写网络流&#125;&#125;;// 扩展功能class CryptoFileStream :public FileStream&#123;public: virtual char Read(int number)&#123; //额外的加密操作... FileStream::Read(number);//读文件流 &#125; virtual void Seek(int position)&#123; //额外的加密操作... FileStream::Seek(position);//定位文件流 //额外的加密操作... &#125; virtual void Write(byte data)&#123; //额外的加密操作... FileStream::Write(data);//写文件流 //额外的加密操作... &#125;&#125;;class CryptoNetworkStream : :public NetworkStream&#123;public: virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;;class BufferedFileStream : public FileStream&#123; virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;;class BufferedNetworkStream : public NetworkStream&#123; virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;;class CryptoBufferedFileStream :public FileStream&#123;public: virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;; 大量重复代码，类数量随功能数增长很快。 实现二，使用组合而不是继承。将功能抽象成一种装饰类。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// 基类class Stream&#123;public： virtual char Read(int number)=0; virtual void Seek(int position)=0; virtual void Write(char data)=0; virtual ~Stream()&#123;&#125;&#125;;// 2种流数据class FileStream: public Stream&#123;public: virtual char Read(int number)&#123;读文件流&#125; virtual void Seek(int position)&#123;定位文件流&#125; virtual void Write(char data)&#123;写文件流&#125;&#125;;class NetworkStream :public Stream&#123;public: virtual char Read(int number)&#123;读网络流&#125; virtual void Seek(int position)&#123;定位网络流&#125; virtual void Write(char data)&#123;写网络流&#125;&#125;;// 组合而不是继承// public Stream继承是为了规范接口；而Stream* stream成员是装饰组合的关键class CryptoStream: public Stream &#123;private: Stream* stream;public: CryptoStream(Stream* stream):stream(stream)&#123;...&#125; virtual char Read(int number)&#123; //额外的加密操作... stream-&gt;Read(number);//读文件流 &#125; virtual void Seek(int position)&#123; //额外的加密操作... stream::Seek(position);//定位文件流 //额外的加密操作... &#125; virtual void Write(byte data)&#123; //额外的加密操作... stream::Write(data);//写文件流 //额外的加密操作... &#125;&#125;;class BufferedStream : public Stream&#123;private: Stream* stream; public: BufferedStream(Stream* stream):stream(stream)&#123;&#125; virtual char Read(int number)&#123; //额外的缓冲操作... stream-&gt;Read(number);//读文件流 &#125; virtual void Seek(int position)&#123; //额外的缓冲操作... stream::Seek(position);//定位文件流 //额外的缓冲操作... &#125; virtual void Write(byte data)&#123; //额外的缓冲操作... stream::Write(data);//写文件流 //额外的缓冲操作... &#125;&#125;;int main()&#123; FileStream* s1=new FileStream(); CryptoStream* s2=new CryptoStream(s1); BufferedStream* s3=new BufferedStream(s1); BufferedStream* bufferedCryptoStm=new BufferedStream(s2);&#125; 实现三，将拥有共同成员的BufferedStream和CryptoStream再抽象出一个父类。 123456789101112131415161718192021222324252627282930313233343536// 基类class Stream&#123; ...&#125;;// 2种流数据class FileStream: public Stream&#123; ...&#125;;class NetworkStream :public Stream&#123; ...&#125;;// 抽象一个父类DecoratorStream: public Stream&#123;protected: Stream* stream; DecoratorStream(Stream * stm):stream(stm)&#123;...&#125;&#125;;class CryptoStream: public DecoratorStream &#123;public: CryptoStream(Stream* stream):DecoratorStream(stream)&#123;...&#125; virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;;class BufferedStream : public DecoratorStream&#123;public: BufferedStream(Stream* stream):DecoratorStream(stream)&#123;&#125; virtual char Read(int number)&#123;...&#125; virtual void Seek(int position)&#123;...&#125; virtual void Write(byte data)&#123;...&#125;&#125;; 以下情况使用 Decorator模式 在不影响其他对象的情况下，以动态、透明的方式给单个对象添加职责。 处理那些可以撤消的职责。 当不能采用生成子类的方法进行扩充时。一种情况是，可能有大量独立的扩展，为支持每一种组合将产生大量的子类，使得子类数目呈爆炸性增长。另一种情况可能是因为类定义被隐藏，或类定义不能用于生成子类。 抽象结构如下： 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#include &lt;iostream&gt;/* * Component * defines an interface for objects that can have responsibilities * added to them dynamically */class Component&#123;public: virtual ~Component() &#123;&#125; virtual void operation() = 0; // ...&#125;;/* * Concrete Component * defines an object to which additional responsibilities * can be attached */class ConcreteComponent : public Component&#123;public: ~ConcreteComponent() &#123;&#125; void operation() &#123; std::cout &lt;&lt; \"Concrete Component operation\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Decorator * maintains a reference to a Component object and defines an interface * that conforms to Component's interface */class Decorator : public Component&#123;public: ~Decorator() &#123;&#125; Decorator( Component *c ) : component( c ) &#123;&#125; virtual void operation() &#123; component-&gt;operation(); &#125; // ...private: Component *component;&#125;;/* * Concrete Decorators * add responsibilities to the component (can extend the state * of the component) */class ConcreteDecoratorA : public Decorator&#123;public: ConcreteDecoratorA( Component *c ) : Decorator( c ) &#123;&#125; void operation() &#123; Decorator::operation(); std::cout &lt;&lt; \"Decorator A\" &lt;&lt; std::endl; &#125; // ...&#125;;class ConcreteDecoratorB : public Decorator&#123;public: ConcreteDecoratorB( Component *c ) : Decorator( c ) &#123;&#125; void operation() &#123; Decorator::operation(); std::cout &lt;&lt; \"Decorator B\" &lt;&lt; std::endl; &#125; // ...&#125;;int main()&#123; ConcreteComponent *cc = new ConcreteComponent(); ConcreteDecoratorB *db = new ConcreteDecoratorB( cc ); ConcreteDecoratorA *da = new ConcreteDecoratorA( db ); Component *component = da; component-&gt;operation(); delete da; delete db; delete cc; return 0;&#125; 通过采用组合而非继承的手法， Decorator模式实现了在运行时动态扩展对象功能的能力，而且可以根据需要扩展多个功能。 Decorator类在接口上表现为is-a Component的继承关系，即Decorator类继承了Component类所具有的接口。但在实现上又表现为has-a Component的组合关系，即Decorator类又使用了另外一个Component类。 Bridge 将抽象部分(业务功能)与实现部分(平台实现)分离，使它们都可以独立地变化。 简单来讲，就是将变化划分成不同的类别，通过父类统一某一类变化的接口。通过组合抽象父类的指针成员，达到简化代码的目的。有点抽象，看示例。 示例6 实现一个消息通知程序，再不同的使用平台，有不同的表现形式。 实现一，通过类继承，达到适应不同平台的目的。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566// 纯虚基类class Messager&#123;public: virtual void Login(string username, string password)=0; virtual void SendMessage(string message)=0; virtual void SendPicture(Image image)=0; virtual void PlaySound()=0; virtual void DrawShape()=0; virtual void WriteText()=0; virtual void Connect()=0; virtual ~Messager()&#123;&#125;&#125;;// 平台实现// 注意这里 PCMessagerBase 依然有纯虚函数(Login等)，不能实例化class PCMessagerBase : public Messager&#123;public: virtual void PlaySound()&#123;...&#125; virtual void DrawShape()&#123;...&#125; virtual void WriteText()&#123;...&#125; virtual void Connect()&#123;...&#125;&#125;;class MobileMessagerBase : public Messager&#123;public: virtual void PlaySound()&#123;...&#125; virtual void DrawShape()&#123;...&#125; virtual void WriteText()&#123;...&#125; virtual void Connect()&#123;...&#125;&#125;;// 业务抽象// 这里的类数，在平台实现类数基础上成倍增长class PCMessagerLite : public PCMessagerBase &#123;public: virtual void Login(string username, string password)&#123; PCMessagerBase::Connect(); &#125; virtual void SendMessage(string message)&#123; PCMessagerBase::WriteText(); &#125; virtual void SendPicture(Image image)&#123; PCMessagerBase::DrawShape(); &#125;&#125;;class PCMessagerPerfect : public PCMessagerBase &#123;public: virtual void Login(string username, string password)&#123; PCMessagerBase::PlaySound(); PCMessagerBase::Connect(); &#125; virtual void SendMessage(string message)&#123; PCMessagerBase::PlaySound(); PCMessagerBase::WriteText(); &#125; virtual void SendPicture(Image image)&#123; PCMessagerBase::PlaySound(); PCMessagerBase::DrawShape(); &#125;&#125;;class MobileMessagerLite : public MobileMessagerBase &#123;...&#125;;class MobileMessagerPerfect : public MobileMessagerBase &#123;...&#125;; 实现二，使用组合而不是继承，同时分离基类中平台实现和业务逻辑部分，保证两部分的派生类都可以分别实例化。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// 分离基类中平台实现和业务逻辑部分class Messager&#123;protected: MessagerImp* messagerImp; // 组合 Messager(MessagerImp* imp): messagerImp(imp) &#123;...&#125;public: virtual void Login(string username, string password)=0; virtual void SendMessage(string message)=0; virtual void SendPicture(Image image)=0; virtual ~Messager()&#123;&#125;&#125;;class MessagerImp&#123;public: virtual void PlaySound()=0; virtual void DrawShape()=0; virtual void WriteText()=0; virtual void Connect()=0; virtual MessagerImp()&#123;&#125;&#125;;// 平台实现// 这里没有纯虚函数，能实例化class PCMessagerImp : public MessagerImp&#123;public: virtual void PlaySound()&#123;...&#125; virtual void DrawShape()&#123;...&#125; virtual void WriteText()&#123;...&#125; virtual void Connect()&#123;...&#125;&#125;;class MobileMessagerImp : public MessagerImp&#123;public: virtual void PlaySound()&#123;...&#125; virtual void DrawShape()&#123;...&#125; virtual void WriteText()&#123;...&#125; virtual void Connect()&#123;...&#125;&#125;;// 业务抽象 m// 基类中MessagerImp* 成员，利用多态，实现不同平台实现的组合class MessagerLite:public Messager &#123;public: MessagerLite(MessagerImp *imp): Messager(imp) &#123;...&#125; virtual void Login(string username, string password)&#123; messagerImp-&gt;Connect(); &#125; virtual void SendMessage(string message)&#123; messagerImp-&gt;WriteText(); &#125; virtual void SendPicture(Image image)&#123; messagerImp-&gt;DrawShape(); &#125;&#125;;class MessagerPerfect:public Messager &#123;public: MessagerPerfect(MessagerImp *imp): Messager(imp) &#123;...&#125; virtual void Login(string username, string password)&#123; messagerImp-&gt;PlaySound(); messagerImp-&gt;Connect(); &#125; virtual void SendMessage(string message)&#123; messagerImp-&gt;PlaySound(); messagerImp-&gt;WriteText(); &#125; virtual void SendPicture(Image image)&#123; messagerImp-&gt;PlaySound(); messagerImp-&gt;DrawShape(); &#125;&#125;; 桥模式相比于Decorator模式，主要是针对基类，分离了不同变化方向（有点抽象，就像x轴y轴代表不同维度）的成员，分离成多个类。 抽象代码结构 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#include &lt;iostream&gt;/* * Implementor * defines the interface for implementation classes */class Implementor&#123;public: virtual ~Implementor() &#123;&#125; virtual void action() = 0; // ...&#125;;/* * Concrete Implementors * implement the Implementor interface and define concrete implementations */class ConcreteImplementorA : public Implementor&#123;public: ~ConcreteImplementorA() &#123;&#125; void action() &#123; std::cout &lt;&lt; \"Concrete Implementor A\" &lt;&lt; std::endl; &#125; // ...&#125;;class ConcreteImplementorB : public Implementor&#123;public: ~ConcreteImplementorB() &#123;&#125; void action() &#123; std::cout &lt;&lt; \"Concrete Implementor B\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Abstraction * defines the abstraction's interface */class Abstraction&#123;public: virtual ~Abstraction() &#123;&#125; virtual void operation() = 0; // ...&#125;;/* * RefinedAbstraction * extends the interface defined by Abstraction */class RefinedAbstraction : public Abstraction&#123;public: ~RefinedAbstraction() &#123;&#125; RefinedAbstraction(Implementor *impl) : implementor(impl) &#123;&#125; void operation() &#123; implementor-&gt;action(); &#125; // ...private: Implementor *implementor;&#125;;int main()&#123; Implementor *ia = new ConcreteImplementorA; Implementor *ib = new ConcreteImplementorB; Abstraction *abstract1 = new RefinedAbstraction(ia); abstract1-&gt;operation(); Abstraction *abstract2 = new RefinedAbstraction(ib); abstract2-&gt;operation(); delete abstract1; delete abstract2; delete ia; delete ib; return 0;&#125; 对象创建相关模式 通过“对象创建” 模式绕开new，来避免对象创建（new）过程中所导致的紧耦合（依赖具体类），从而支持对象创建的稳定。它是接口抽象之后的第一步工作。 面向接口，可以视为依赖抽象基类，调用抽象基类方法。 典型模式: Factory Method Abstract Factory Prototype Builder Factory Method 在软件系统中，经常面临着创建对象的工作；由于需求的变化，需要创建的对象的具体类型经常变化。 将程序中，对具体对象的依赖，转变为对抽象的创建对象的接口的依赖。 定义一个用于创建对象的接口，让子类决定实例化哪一个类。Factory Method使得一个类的实例化延迟（目的：解耦，手段：虚函数）到子类。 示例1 实现一个能够切分不同类型文件的切分程序。 实现一，直接建立不同的具体切分对象。 123456789class ISplitter&#123;public: virtual void split()=0; virtual ~ISplitter()&#123;&#125;&#125;;class BinarySplitter : public ISplitter&#123;...&#125;;class TxtSplitter: public ISplitter&#123;...&#125;; 1234567class MainForm : public Form &#123;public: void Button_Click()&#123; ISplitter * splitter = new BinarySplitter();//依赖具体类 splitter-&gt;split(); &#125;&#125;; 显然，new BinarySplitter()需要根据不同的文件类型，进行修改。一旦修改，源文件就需要重新编译。 实现二，使用抽象的工厂，提供对象创建的接口。 123456789101112131415161718192021222324252627282930313233//抽象类class ISplitter&#123;public: virtual void split()=0; virtual ~ISplitter()&#123;&#125;&#125;;//工厂基类class SplitterFactory&#123;public: virtual ISplitter* CreateSplitter()=0; virtual ~SplitterFactory()&#123;&#125;&#125;;//具体类class BinarySplitter : public ISplitter&#123;...&#125;;class TxtSplitter: public ISplitter&#123;...&#125;;//具体工厂class BinarySplitterFactory: public SplitterFactory&#123;public: virtual ISplitter* CreateSplitter()&#123; return new BinarySplitter(); &#125;&#125;;class TxtSplitterFactory: public SplitterFactory&#123;public: virtual ISplitter* CreateSplitter()&#123; return new TxtSplitter(); &#125;&#125;; 123456789101112class MainForm : public Form&#123; SplitterFactory* factory;//工厂public: MainForm(SplitterFactory* factory)&#123; this-&gt;factory=factory; &#125; void Button_Click()&#123; ISplitter * splitter=factory-&gt;CreateSplitter(); //多态new splitter-&gt;split(); &#125;&#125;; 现在，创建不同对象，只需要输入不同的工厂对象。虽然依旧是要创建对象，但是，在编译单元层面，以上代码是不用重新编译的。有新的类型需要创建，直接增加一个新的源文件即可。 Factory Method模式用于隔离类对象的使用者和具体类型之间的耦合关系。面对一个经常变化的具体类型，紧耦合关系(new)会导致软件的脆弱。 将所要创建的具体对象工作延迟到子类（BinarySplitterFactory、TxtSplitterFactory），从而实现一种扩展（而非更改）的策略。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111#include &lt;iostream&gt;#include &lt;string&gt;/* * Product * products implement the same interface so that the classes can refer * to the interface not the concrete product */class Product&#123;public: virtual ~Product() &#123;&#125; virtual std::string getName() = 0; // ...&#125;;/* * Concrete Product * define product to be created */class ConcreteProductA : public Product&#123;public: ~ConcreteProductA() &#123;&#125; std::string getName() &#123; return \"type A\"; &#125; // ...&#125;;/* * Concrete Product * define product to be created */class ConcreteProductB : public Product&#123;public: ~ConcreteProductB() &#123;&#125; std::string getName() &#123; return \"type B\"; &#125; // ...&#125;;/* * Creator * contains the implementation for all of the methods * to manipulate products except for the factory method */class Creator&#123;public: virtual ~Creator() &#123;&#125; virtual Product* createProductA() = 0; virtual Product* createProductB() = 0; virtual void removeProduct( Product *product ) = 0; // ...&#125;;/* * Concrete Creator * implements factory method that is responsible for creating * one or more concrete products ie. it is class that has * the knowledge of how to create the products */class ConcreteCreator : public Creator&#123;public: ~ConcreteCreator() &#123;&#125; Product* createProductA() &#123; return new ConcreteProductA(); &#125; Product* createProductB() &#123; return new ConcreteProductB(); &#125; void removeProduct( Product *product ) &#123; delete product; &#125; // ...&#125;;int main()&#123; Creator *creator = new ConcreteCreator(); Product *p1 = creator-&gt;createProductA(); std::cout &lt;&lt; \"Product: \" &lt;&lt; p1-&gt;getName() &lt;&lt; std::endl; creator-&gt;removeProduct( p1 ); Product *p2 = creator-&gt;createProductB(); std::cout &lt;&lt; \"Product: \" &lt;&lt; p2-&gt;getName() &lt;&lt; std::endl; creator-&gt;removeProduct( p2 ); delete creator; return 0;&#125; 以上代码的 creator 将不同创建方法整合到一个类中。也可以按照示例1的方法，creator 转化为不同的子类。 Abstract Factory 类似Factory Method的作用，不过需要创建的对象是“一系列相互关联的对象”。这个时候，将这一系列对象的创建，汇集到一个工厂类中。 示例2 实现一个可以使用不同的数据库的类，可进行数据库的连接、SQL语句执行。 实现一，根据不同功能，建立不同的工厂。 12345678910111213141516171819202122232425262728//Connection基类class IDBConnection&#123;...&#125;;class IDBConnectionFactory&#123;public: virtual IDBConnection* CreateDBConnection()=0;&#125;;//Command基类class IDBCommand&#123;&#125;;class IDBCommandFactory&#123;public: virtual IDBCommand* CreateDBCommand()=0;&#125;;//Reader基类class IDataReader&#123;&#125;;class IDataReaderFactory&#123;public: virtual IDataReader* CreateDataReader()=0;&#125;;//SQL数据库类工厂 3 个class SqlConnection: public IDBConnection&#123;&#125;;class SqlConnectionFactory:public IDBConnectionFactory&#123;&#125;;class SqlCommand: public IDBCommand&#123;&#125;;class SqlCommandFactory:public IDBCommandFactory&#123;&#125;;class SqlDataReader: public IDataReader&#123;&#125;;class SqlDataReaderFactory:public IDataReaderFactory&#123;&#125;; 12345678910111213141516171819202122// 使用3个工厂class EmployeeDAO&#123; IDBConnectionFactory* dbConnectionFactory; IDBCommandFactory* dbCommandFactory; IDataReaderFactory* dataReaderFactory; public: // 构造函数传入Factory指针 ... vector&lt;EmployeeDO&gt; GetEmployees()&#123; IDBConnection* connection = dbConnectionFactory-&gt;CreateDBConnection(); connection-&gt;ConnectionString(\"...\"); IDBCommand* command = dbCommandFactory-&gt;CreateDBCommand(); command-&gt;CommandText(\"...\"); command-&gt;SetConnection(connection); //关联性,使用connection IDBDataReader* reader = command-&gt;ExecuteReader(); while (reader-&gt;Read())&#123;...&#125; &#125;&#125;; 以上代码，除了不够简洁，还有可能传入不匹配的 Connection 对象和 Command 对象（比如 SQL的Connection 和 MongoDB的Command）。 实现二，使用一个工厂，完成对象创建。 12345678910111213141516171819202122232425262728293031//数据库访问有关的基类class IDBConnection&#123;...&#125;;class IDBCommand&#123;...&#125;;class IDataReader&#123;...&#125;;// Factory基类class IDBFactory&#123;public: ... virtual IDBConnection* CreateDBConnection()=0; virtual IDBCommand* CreateDBCommand()=0; virtual IDataReader* CreateDataReader()=0; ...&#125;;// SQL数据库的相关类class SqlConnection: public IDBConnection&#123;...&#125;;class SqlCommand: public IDBCommand&#123;...&#125;;class SqlDataReader: public IDataReader&#123;...&#125;;// SQL数据库各个相关类的创建工厂class SqlDBFactory:public IDBFactory&#123;public: ... virtual IDBConnection* CreateDBConnection()=0; virtual IDBCommand* CreateDBCommand()=0; virtual IDataReader* CreateDataReader()=0; ...&#125;; 12345678910111213141516class EmployeeDAO&#123; IDBFactory* dbFactory; public: vector&lt;EmployeeDO&gt; GetEmployees()&#123; IDBConnection* connection = dbFactory-&gt;CreateDBConnection(); connection-&gt;ConnectionString(\"...\"); IDBCommand* command = dbFactory-&gt;CreateDBCommand(); command-&gt;CommandText(\"...\"); command-&gt;SetConnection(connection); IDBDataReader* reader = command-&gt;ExecuteReader(); while (reader-&gt;Read())&#123;...&#125; &#125;&#125;; 使用一个类管理相关对象的创建。Abstract Factory模式主要在于应对“新系列”的需求变动。其缺点在于难以应对“新对象”的需求变动，这就是Factory Method的事情了。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156#include &lt;iostream&gt;/* * Product A * products implement the same interface so that the classes can refer * to the interface not the concrete product */class ProductA&#123;public: virtual ~ProductA() &#123;&#125; virtual const char* getName() = 0; // ...&#125;;/* * ConcreteProductAX and ConcreteProductAY * define objects to be created by concrete factory */class ConcreteProductAX : public ProductA&#123;public: ~ConcreteProductAX() &#123;&#125; const char* getName() &#123; return \"A-X\"; &#125; // ...&#125;;class ConcreteProductAY : public ProductA&#123;public: ~ConcreteProductAY() &#123;&#125; const char* getName() &#123; return \"A-Y\"; &#125; // ...&#125;;/* * Product B * same as Product A, Product B declares interface for concrete products * where each can produce an entire set of products */class ProductB&#123;public: virtual ~ProductB() &#123;&#125; virtual const char* getName() = 0; // ...&#125;;/* * ConcreteProductBX and ConcreteProductBY * same as previous concrete product classes */class ConcreteProductBX : public ProductB&#123;public: ~ConcreteProductBX() &#123;&#125; const char* getName() &#123; return \"B-X\"; &#125; // ...&#125;;class ConcreteProductBY : public ProductB&#123;public: ~ConcreteProductBY() &#123;&#125; const char* getName() &#123; return \"B-Y\"; &#125; // ...&#125;;/* * Abstract Factory * provides an abstract interface for creating a family of products */class AbstractFactory&#123;public: virtual ~AbstractFactory() &#123;&#125; virtual ProductA *createProductA() = 0; virtual ProductB *createProductB() = 0;&#125;;/* * Concrete Factory X and Y * each concrete factory create a family of products and client uses * one of these factories so it never has to instantiate a product object */class ConcreteFactoryX : public AbstractFactory&#123;public: ~ConcreteFactoryX() &#123;&#125; ProductA *createProductA() &#123; return new ConcreteProductAX(); &#125; ProductB *createProductB() &#123; return new ConcreteProductBX(); &#125; // ...&#125;;class ConcreteFactoryY : public AbstractFactory&#123;public: ~ConcreteFactoryY() &#123;&#125; ProductA *createProductA() &#123; return new ConcreteProductAY(); &#125; ProductB *createProductB() &#123; return new ConcreteProductBY(); &#125; // ...&#125;;int main()&#123; ConcreteFactoryX *factoryX = new ConcreteFactoryX(); ConcreteFactoryY *factoryY = new ConcreteFactoryY(); ProductA *p1 = factoryX-&gt;createProductA(); std::cout &lt;&lt; \"Product: \" &lt;&lt; p1-&gt;getName() &lt;&lt; std::endl; ProductA *p2 = factoryY-&gt;createProductA(); std::cout &lt;&lt; \"Product: \" &lt;&lt; p2-&gt;getName() &lt;&lt; std::endl; delete p1; delete p2; delete factoryX; delete factoryY; return 0;&#125; 结构和示例2稍有不同。 Prototype 和Factory Method一样，用于创造对象，但是是通过拷贝原型来创建对象。类的拷贝构造函数需要指定正确。拷贝指的是深拷贝。 和Factory Method不同的是，Prototype更关注对象初始状态的变化，可以创建几种不同的状态的原型，供程序使用。 示例3 示例和Factory Method一样，在Factory Method基础上做了改动，变成Prototype模式。 12345678910111213141516171819202122//将抽象类和工厂基类合并为一个类class ISplitter&#123;public: virtual void split()=0; virtual ISplitter* clone()=0; //通过克隆自己来创建对象 virtual ~ISplitter()&#123;&#125;&#125;;//具体类class BinarySplitter : public ISplitter&#123;public: virtual ISplitter* clone()&#123; return new BinarySplitter(*this); //通过克隆自己来创建对象 &#125;&#125;;class TxtSplitter: public ISplitter&#123;public: virtual ISplitter* clone()&#123; return new TxtSplitter(*this); //通过克隆自己来创建对象 &#125;&#125;; 1234567891011class MainForm : public Form &#123; ISplitter* prototype;//原型对象public: MainForm(ISplitter* prototype)&#123; this-&gt;prototype=prototype; &#125; void Button_Click()&#123; ISplitter * splitter=prototype-&gt;clone(); //克隆原型 splitter-&gt;split(); &#125;&#125;; 当一个系统应该独立于它的产品创建、构成和表示时，要使用 Prototype模式。当一个类的实例只能有几个不同状态组合中的一种时。建立相应数目的原型并克隆它们 可能比每次用合适的状态手工实例化该类更方便一些。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#include &lt;iostream&gt;#include &lt;string&gt;/* * Prototype * declares an interface for cloning itself */class Prototype&#123;public: virtual ~Prototype() &#123;&#125; virtual Prototype* clone() = 0; virtual std::string type() = 0; // ...&#125;;/* * Concrete Prototype A and B * implement an operation for cloning itself */class ConcretePrototypeA : public Prototype&#123;public: ~ConcretePrototypeA() &#123;&#125; Prototype* clone() &#123; return new ConcretePrototypeA(); &#125; std::string type() &#123; return \"type A\"; &#125; // ...&#125;;class ConcretePrototypeB : public Prototype&#123;public: ~ConcretePrototypeB() &#123;&#125; Prototype* clone() &#123; return new ConcretePrototypeB(); &#125; std::string type() &#123; return \"type B\"; &#125; // ...&#125;;/* * Client * creates a new object by asking a prototype to clone itself */class Client&#123;public: static void init() &#123; types[ 0 ] = new ConcretePrototypeA(); types[ 1 ] = new ConcretePrototypeB(); &#125; static void remove() &#123; delete types[ 0 ]; delete types[ 1 ]; &#125; static Prototype* make( const int index ) &#123; if ( index &gt;= n_types ) &#123; return nullptr; &#125; return types[ index ]-&gt;clone(); &#125; // ...private: static Prototype* types[ 2 ]; // declaration static int n_types;&#125;;Prototype* Client::types[ 2 ]; // definitionint Client::n_types = 2;int main()&#123; Client::init(); Prototype *prototype1 = Client::make( 0 ); std::cout &lt;&lt; \"Prototype: \" &lt;&lt; prototype1-&gt;type() &lt;&lt; std::endl; delete prototype1; Prototype *prototype2 = Client::make( 1 ); std::cout &lt;&lt; \"Prototype: \" &lt;&lt; prototype2-&gt;type() &lt;&lt; std::endl; delete prototype2; Client::remove(); return 0;&#125; Builder 在软件系统中，有时候面临着“一个复杂对象”的创建工作，它的创建过程是一套固定的组合方法，和多个不同的被组合的子对象。 Builder将一个复杂对象的构建与其表示相分离，使得同样的构建过程(稳定)可以创建不同的表示(变化)。 这个类似Template Method，不过Builder是针对对象的创建来进行设计的。 示例4 实现一个程序，可以创建出不同种类的房子图像。 1234567891011121314151617181920212223242526272829303132333435363738// House抽象基类class House&#123; //...&#125;;// House建房配置的基类class HouseBuilder &#123;public: House* GetResult()&#123; return pHouse; &#125; virtual ~HouseBuilder()&#123;&#125;protected: House* pHouse; virtual void BuildPart1()=0; virtual void BuildPart2()=0; virtual void BuildPart3()=0; virtual void BuildPart4()=0; virtual void BuildPart5()=0;&#125;;// 通用图形绘制逻辑，稳定class HouseDirector&#123; HouseBuilder* pHouseBuilder;public: // 注意不要在C++的构造函数中调用虚函数 HouseDirector(HouseBuilder* pHouseBuilder)&#123; this-&gt;pHouseBuilder=pHouseBuilder; &#125; House* Construct()&#123; pHouseBuilder-&gt;BuildPart1(); for (int i = 0; i &lt; 4; i++)&#123; pHouseBuilder-&gt;BuildPart2(); &#125; bool flag=pHouseBuilder-&gt;BuildPart3(); if(flag)&#123; pHouseBuilder-&gt;BuildPart4(); &#125; pHouseBuilder-&gt;BuildPart5(); return pHouseBuilder-&gt;GetResult(); &#125;&#125;; 12345678910111213// 具体类，StoneHouse的参数类class StoneHouse: public House&#123;...&#125;;// 具体类，构建StoneHouse各个部分的具体算法实现class StoneHouseBuilder: public HouseBuilder&#123;protected: virtual void BuildPart1()&#123;...&#125; virtual void BuildPart2()&#123;...&#125; virtual void BuildPart3()&#123;...&#125; virtual void BuildPart4()&#123;...&#125; virtual void BuildPart5()&#123;...&#125; &#125;; 这样如果有其它种类的House，可以新创建House和HouseBuilder的子类即可。 Builder 模式主要用于“分步骤构建一个复杂的对象”。在这其中“分步骤”是一个稳定的算法，而复杂对象的各个部分则经常变化。 变化点在哪里，封装哪里—— Builder模式主要在于应对“复杂对象各个部分”的频繁需求变动。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159#include &lt;iostream&gt;#include &lt;string&gt;/* * Product * the final object that will be created using Builder */class Product&#123;public: void makeA( const std::string &amp;part ) &#123; partA = part; &#125; void makeB( const std::string &amp;part ) &#123; partB = part; &#125; void makeC( const std::string &amp;part ) &#123; partC = part; &#125; std::string get() &#123; return (partA + \" \" + partB + \" \" + partC); &#125; // ... private: std::string partA; std::string partB; std::string partC; // ...&#125;;/* * Builder * abstract interface for creating products */class Builder&#123;public: virtual ~Builder() &#123;&#125; Product get() &#123; return product; &#125; virtual void buildPartA() = 0; virtual void buildPartB() = 0; virtual void buildPartC() = 0; // ...protected: Product product;&#125;;/* * Concrete Builder X and Y * create real products and stores them in the composite structure */class ConcreteBuilderX : public Builder&#123;public: void buildPartA() &#123; product.makeA( \"A-X\" ); &#125; void buildPartB() &#123; product.makeB( \"B-X\" ); &#125; void buildPartC() &#123; product.makeC( \"C-X\" ); &#125; // ...&#125;;class ConcreteBuilderY : public Builder&#123;public: void buildPartA() &#123; product.makeA( \"A-Y\" ); &#125; void buildPartB() &#123; product.makeB( \"B-Y\" ); &#125; void buildPartC() &#123; product.makeC( \"C-Y\" ); &#125; // ...&#125;;/* * Director * responsible for managing the correct sequence of object creation */class Director &#123;public: Director() : builder() &#123;&#125; ~Director() &#123; if ( builder ) &#123; delete builder; &#125; &#125; void set( Builder *b ) &#123; if ( builder ) &#123; delete builder; &#125; builder = b; &#125; Product get() &#123; return builder-&gt;get(); &#125; void construct() &#123; builder-&gt;buildPartA(); builder-&gt;buildPartB(); builder-&gt;buildPartC(); // ... &#125; // ...private: Builder *builder;&#125;;int main()&#123; Director director; director.set( new ConcreteBuilderX ); director.construct(); Product product1 = director.get(); std::cout &lt;&lt; \"1st product parts: \" &lt;&lt; product1.get() &lt;&lt; std::endl; director.set( new ConcreteBuilderY ); director.construct(); Product product2 = director.get(); std::cout &lt;&lt; \"2nd product parts: \" &lt;&lt; product2.get() &lt;&lt; std::endl; return 0;&#125; 对象性能相关模式 处理面向对象编程所产生的额外代价，比如太多的对象创建的资源消耗等问题。 典型模式： Singleton Flyweight Singleton 处理一个类，只允许一个实例存在，或者只需要存在一个对象即可。 Singleton保证一个类仅有一个实例，并提供一个访问它的全局访问点。 示例5 单线程环境下实现 123456789101112131415161718class Singleton&#123;private: static Singleton* m_instance; Singleton(); Singleton(const Singleton&amp; other);public: static Singleton* getInstance();&#125;;Singleton* Singleton::m_instance=nullptr;//线程非安全版本Singleton* Singleton::getInstance() &#123; if (m_instance == nullptr) &#123; m_instance = new Singleton(); &#125; return m_instance;&#125; 版本二，代价过高（相对而言） 12345678//线程安全版本，但锁的代价过高. 读变量，不需要获取锁，只有写才需要加锁Singleton* Singleton::getInstance() &#123; Lock lock; if (m_instance == nullptr) &#123; m_instance = new Singleton(); &#125; return m_instance;&#125; 双检查，第一个判断保证读变量不会获取锁。第二个判断，保证当两个或多个线程都进入了第一个判断内，不会创建多个对象。 12345678910//双检查锁，但由于内存读写reorder不安全，直接以下代码是不能应用的Singleton* Singleton::getInstance() &#123; if(m_instance==nullptr)&#123; Lock lock; if (m_instance == nullptr) &#123; m_instance = new Singleton(); &#125; &#125; return m_instance;&#125; 但是，m_instance = new Singleton() 在cpu指令执行的时候，可能会出现指令reorder的情况。因为编译器会对汇编代码进行优化。比如，指令顺序为（分配内存--调用构造器--将对象赋值到变量内存），可能变成（分配内存--将对象赋值到变量内存--调用构造器）。 因此，当指令执行顺序是（分配内存--将对象赋值到变量内存--调用构造器）时，线程1可能处在（分配内存--将对象赋值到变量内存）阶段，此时 m_instance != nullptr。如果此时线程2，开始第一个判断，会直接跳转到 return m_instance，但是 m_instance 并没有调用构造器，并不是一个可用的对象。问题就出现了。 C++ 11版本之后的跨平台实现双检查： 123456789101112131415161718//C++ 11版本之后的跨平台实现 (Java 中使用 volatile 禁止cpu指令reorder)std::atomic&lt;Singleton*&gt; Singleton::m_instance;std::mutex Singleton::m_mutex;Singleton* Singleton::getInstance() &#123; Singleton* tmp = m_instance.load(std::memory_order_relaxed); std::atomic_thread_fence(std::memory_order_acquire);//获取内存fence if (tmp == nullptr) &#123; std::lock_guard&lt;std::mutex&gt; lock(m_mutex); tmp = m_instance.load(std::memory_order_relaxed); if (tmp == nullptr) &#123; tmp = new Singleton; std::atomic_thread_fence(std::memory_order_release);//释放内存fence m_instance.store(tmp, std::memory_order_relaxed); &#125; &#125; return tmp;&#125; Singleton的构造器可以设置为protected，允许子类派生。一般不要支持拷贝构造函数，或者clone接口。 抽象代码结构 单线程版 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;/* * Singleton * has private static variable to hold one instance of the class * and method which gives us a way to instantiate the class */class Singleton&#123;public: // The copy constructor and assignment operator // are defined as deleted, which means that you // can't make a copy of singleton. // // Note: you can achieve the same effect by declaring // the constructor and the operator as private Singleton( Singleton const&amp; ) = delete; Singleton&amp; operator=( Singleton const&amp; ) = delete; static Singleton* get() &#123; if ( !instance ) &#123; instance = new Singleton(); &#125; return instance; &#125; static void restart() &#123; if ( instance ) &#123; delete instance; &#125; &#125; void tell() &#123; std::cout &lt;&lt; \"This is Singleton.\" &lt;&lt; std::endl; // ... &#125; // ...private: Singleton() &#123;&#125; static Singleton *instance; // ...&#125;;Singleton* Singleton::instance = nullptr;int main()&#123; Singleton::get()-&gt;tell(); Singleton::restart(); return 0;&#125; Flyweight 对于大量存在的对象，会产生较高的内存上代价。 Flyweight（享元）运用共享技术有效地支持大量细粒度的对象。 原理就是将对象以一个全局唯一key，储存在一个对象记录数据结构中。当下一次使用同一对象，直接查找记录，返回已经存在的对象。 示例6 实现能够显示每种字体属性的程序。 12345678910111213141516171819202122232425262728293031323334class Font &#123;private: //unique object key string key; //object state //...public: Font(const string&amp; key)&#123;...&#125;&#125;;class FontFactory&#123;private: // 使用map作为一个对象记录 map&lt;string,Font* &gt; fontPool; public: Font* GetFont(const string&amp; key)&#123; map&lt;string,Font*&gt;::iterator item=fontPool.find(key); if(item!=footPool.end())&#123; return fontPool[key]; &#125; else&#123; Font* font = new Font(key); fontPool[key]= font; return font; &#125; &#125; void clear()&#123; //... &#125;&#125;; 享元，在对象数量很多，并且因此造成性能负担的时候，可以考虑使用。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#include &lt;iostream&gt;#include &lt;map&gt;/* * Flyweight * declares an interface through which flyweights can receive * and act on extrinsic state */class Flyweight&#123;public: virtual ~Flyweight() &#123;&#125; virtual void operation() = 0; // ...&#125;;/* * UnsharedConcreteFlyweight * not all subclasses need to be shared */class UnsharedConcreteFlyweight : public Flyweight&#123;public: UnsharedConcreteFlyweight( const int intrinsic_state ) : state( intrinsic_state ) &#123;&#125; ~UnsharedConcreteFlyweight() &#123;&#125; void operation() &#123; std::cout &lt;&lt; \"Unshared Flyweight with state \" &lt;&lt; state &lt;&lt; std::endl; &#125; // ... private: int state; // ...&#125;;/* * ConcreteFlyweight * implements the Flyweight interface and adds storage * for intrinsic state */class ConcreteFlyweight : public Flyweight&#123;public: ConcreteFlyweight( const int all_state ) : state( all_state ) &#123;&#125; ~ConcreteFlyweight() &#123;&#125; void operation() &#123; std::cout &lt;&lt; \"Concrete Flyweight with state \" &lt;&lt; state &lt;&lt; std::endl; &#125; // ... private: int state; // ...&#125;;/* * FlyweightFactory * creates and manages flyweight objects and ensures * that flyweights are shared properly */class FlyweightFactory&#123;public: ~FlyweightFactory() &#123; for ( auto it = flies.begin(); it != flies.end(); it++ ) &#123; delete it-&gt;second; &#125; flies.clear(); &#125; Flyweight *getFlyweight( const int key ) &#123; if ( flies.find( key ) != flies.end() ) &#123; return flies[ key ]; &#125; Flyweight *fly = new ConcreteFlyweight( key ); flies.insert( std::pair&lt;int, Flyweight *&gt;( key, fly ) ); return fly; &#125; // ...private: std::map&lt;int, Flyweight*&gt; flies; // ...&#125;;int main()&#123; FlyweightFactory *factory = new FlyweightFactory; factory-&gt;getFlyweight(1)-&gt;operation(); factory-&gt;getFlyweight(2)-&gt;operation(); delete factory; return 0;&#125; 接口隔离相关模式 在软件构建过程中，某些接口之间的“直接”依赖可能会带来很多问题，甚至无法实现。这时候，往往采用添加一层“间接”的稳定接口层，来实现间接的依赖。 间接的中间层思想，很常见，比如指针就是一种间接层，操作系统是用户程序和机器硬件之间的中间层。 典型模式： Facade (c不是英文字符，整个词是个法文，不过这不重要) Proxy Adapter Mediator Facade 为子系统中的一组接口提供一个一致的界面， Facade模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。 目标效果如图： Facade模式更注重架构层次，偏向架构设计的模式。实现了内部组件和外部客户程序的解耦。其内部组件一般是相互依赖的一系列类型。 Facade模式为一个复杂子系统提供一个简单接口时。子系统往往因为不断演化而变得越来越复杂。 抽象代码结构 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#include &lt;iostream&gt;/* * Subsystems * implement more complex subsystem functionality * and have no knowledge of the facade */class SubsystemA&#123;public: void suboperation() &#123; std::cout &lt;&lt; \"Subsystem A method\" &lt;&lt; std::endl; // ... &#125; // ...&#125;;class SubsystemB&#123;public: void suboperation() &#123; std::cout &lt;&lt; \"Subsystem B method\" &lt;&lt; std::endl; // ... &#125; // ...&#125;;class SubsystemC&#123;public: void suboperation() &#123; std::cout &lt;&lt; \"Subsystem C method\" &lt;&lt; std::endl; // ... &#125; // ...&#125;;/* * Facade * delegates client requests to appropriate subsystem object * and unified interface that is easier to use */class Facade&#123;public: Facade() : subsystemA(), subsystemB(), subsystemC() &#123;&#125; void operation1() &#123; subsystemA-&gt;suboperation(); subsystemB-&gt;suboperation(); // ... &#125; void operation2() &#123; subsystemC-&gt;suboperation(); // ... &#125; // ... private: SubsystemA *subsystemA; SubsystemB *subsystemB; SubsystemC *subsystemC; // ...&#125;;int main()&#123; Facade *facade = new Facade(); facade-&gt;operation1(); facade-&gt;operation2(); delete facade; return 0;&#125; Proxy 在面向对象的系统中，有些对象可能由于，对象创建开销很大、需要额外安全控制、需要进程外的访问操作等，直接访问对象会麻烦。 比如说，加载一个word文档，如果有很多图片，肯定不会希望在打开文件时就完成对所有图片的加载工作，那样会很慢。这时候，可以转化为加载一个图片的proxy代理，当浏览到该图片是，再通过proxy加载相应图片。 Proxy为其他对象提供一种代理以控制对这个对象的访问。 通常Proxy会保持原对象的接口，这个被叫做“透明操作”。但是也不一定完全保持原对象接口。 Proxy可能会作用于一个对象，也可能作用于一个大的系统，实现起来可能会很复杂。 另一个使用场景是对用户隐藏另一种称之为 copy-on-write的优化方式，该优化与根据需要创建对象有关。拷贝一个庞大而复杂的对象是一种开销很大的操作，如果这个拷贝根本没有被修改，那么这些开销就没有必要。用代理延迟这一拷贝过程，我们可以保证只有当这个对象被修改的时候才对它进行拷贝。 在实现 Copy-on-write时必须对实体进行引用计数。拷贝代理仅会增加引用计数。只有当用户请求一个修改该实体的操作时，代理才会真正的拷贝它。在这种情况下，代理还必须减少实体的引用计数。当引用的数目为零时，这个实体将被删除。 Copy-on-Write可以大幅度的降低拷贝庞大实体时的开销。 下面是一些可以使用 Proxy模式常见情况： 远程代理（Remote Proxy）为一个对象在不同的地址空间提供局部代表。 虚代理（Virtual Proxy）根据需要创建开销很大的对象。 保护代理（Protection Proxy）控制对原始对象的访问。保护代理用于对象应该有不同的访问权限的时候。 智能指引 （Smart Reference）取代了简单的指针，它在访问对象时执行一些附加操作。它的典型用途包括： 对指向实际对象的引用计数，这样当该对象没有引用时，可以自动释放它 (也称为 Smart Pointers )。 当第一次引用一个持久对象时，将它装入内存。 在访问一个实际对象前，检查是否已经锁定了它，以确保其他对象不能改变它。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#include &lt;iostream&gt;/* * Subject * defines the common interface for RealSubject and Proxy * so that a Proxy can be used anywhere a RealSubject is expected */class Subject&#123;public: virtual ~Subject() &#123; /* ... */ &#125; virtual void request() = 0; // ...&#125;;/* * Real Subject * defines the real object that the proxy represents */class RealSubject : public Subject&#123;public: void request() &#123; std::cout &lt;&lt; \"Real Subject request\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Proxy * maintains a reference that lets the proxy access the real subject */class Proxy : public Subject&#123;public: Proxy() &#123; subject = new RealSubject(); &#125; ~Proxy() &#123; delete subject; &#125; void request() &#123; subject-&gt;request(); &#125; // ...private: RealSubject *subject;&#125;;int main()&#123; Proxy *proxy = new Proxy(); proxy-&gt;request(); delete proxy; return 0;&#125; Adapter 在软件系统中，有时会需要将 一些现有的对象 放在新的环境中使用。但是此时的接口发生了变化。 Adapter将一个类的接口转换成客户希望的另外一个接口。 Adapter模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。也被称为Wrapper。 模式表达如下： Adapter继承Target保持了形同接口，同时组合了一个Adaptee对象，可以调用Adaptee的方法。 抽象代码结构 使用多继承的方式，类adapter。但是依然推荐组合而不是继承。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;iostream&gt;/* * Target * defines specific interface that Client uses */class Target&#123;public: virtual ~Target() &#123;&#125; virtual void request() = 0; // ...&#125;;/* * Adaptee * defines an existing interface that needs adapting and thanks * to Adapter it will get calls that client makes on the Target * */class Adaptee&#123;public: void specificRequest() &#123; std::cout &lt;&lt; \"specific request\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Adapter * implements the Target interface and when it gets a method call it * delegates the call to a Adaptee */class Adapter : public Target&#123;public: Adapter() : adaptee() &#123;&#125; ~Adapter() &#123; delete adaptee; &#125; void request() &#123; adaptee-&gt;specificRequest(); // ... &#125; // ...private: Adaptee *adaptee; // ...&#125;;int main()&#123; Target *t = new Adapter(); t-&gt;request(); delete t; return 0;&#125; 使用组合对象，对象adapter 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#include &lt;iostream&gt;/* * Target * defines specific interface that Client uses */class Target&#123;public: virtual ~Target() &#123;&#125; virtual void request() = 0; // ...&#125;;/* * Adaptee * all requests get delegated to the Adaptee which defines * an existing interface that needs adapting */class Adaptee&#123;public: ~Adaptee() &#123;&#125; void specificRequest() &#123; std::cout &lt;&lt; \"specific request\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Adapter * implements the Target interface and lets the Adaptee respond * to request on a Target by extending both classes * ie adapts the interface of Adaptee to the Target interface */class Adapter : public Target, private Adaptee&#123;public: virtual void request() &#123; specificRequest(); &#125; // ...&#125;;int main()&#123; Target *t = new Adapter(); t-&gt;request(); delete t; return 0;&#125; Mediator 处理多个对象相互关联，并且其引用关系复杂，使得改变变得不那么容易。 Mediator用一个中介对象来封装一系列的对象交互。中介者使各对象不需要显式地相互引用，从而使其耦合松散，而且可以独立地改变它们之间的交互。 和Facade很相似，但是Mediator是作用于内部对象之间的，双向的关系。Facade是解耦内部对象和外部环境的，单向的关系。 图中忽略了Mediator与Colleagu之间的通信实现，这个往往是复杂的。其中可以用到Observer模式，来自动检测不同对象间的通信转换。 Mediator中有Colleague的指针成员，Colleague中有Mediator的指针成员。方便两者之间的相互调用。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;string&gt;class Mediator;/* * Colleague classes * each colleague communicates with its mediator whenever * it would have otherwise communicated with another colleague */class Colleague&#123;public: Colleague( Mediator* const m, const unsigned int i ) : mediator( m ), id( i ) &#123;&#125; virtual ~Colleague() &#123;&#125; unsigned int getID() &#123; return id; &#125; virtual void send( std::string ) = 0; virtual void receive( std::string ) = 0;protected: Mediator *mediator; unsigned int id;&#125;;class ConcreteColleague : public Colleague&#123;public: ConcreteColleague( Mediator* const m, const unsigned int i ) : Colleague( m, i ) &#123;&#125; ~ConcreteColleague() &#123;&#125; void send( std::string msg ); void receive( std::string msg ) &#123; std::cout &lt;&lt; \"Message '\" &lt;&lt; msg &lt;&lt; \"' received by Colleague \" &lt;&lt; id &lt;&lt; std::endl; &#125;&#125;;/* * Mediator * defines an interface for communicating with Colleague objects */class Mediator&#123;public: virtual ~Mediator() &#123;&#125; virtual void add( Colleague* const c ) = 0; virtual void distribute( Colleague* const sender, std::string msg ) = 0;protected: Mediator() &#123;&#125;&#125;;/* * Concrete Mediator * implements cooperative behavior by coordinating Colleague objects * and knows its colleagues */class ConcreteMediator : public Mediator&#123;public: ~ConcreteMediator() &#123; for ( unsigned int i = 0; i &lt; colleagues.size(); i++ ) &#123; delete colleagues[ i ]; &#125; colleagues.clear(); &#125; void add( Colleague* const c ) &#123; colleagues.push_back( c ); &#125; void distribute( Colleague* const sender, std::string msg ) &#123; for ( unsigned int i = 0; i &lt; colleagues.size(); i++ ) &#123; if ( colleagues.at( i )-&gt;getID() != sender-&gt;getID() ) &#123; colleagues.at( i )-&gt;receive( msg ); &#125; &#125; &#125;private: std::vector&lt;Colleague*&gt; colleagues;&#125;;void ConcreteColleague::send( std::string msg )&#123; std::cout &lt;&lt; \"Message '\"&lt;&lt; msg &lt;&lt; \"' sent by Colleague \" &lt;&lt; id &lt;&lt; std::endl; mediator-&gt;distribute( this, msg );&#125;int main()&#123; Mediator *mediator = new ConcreteMediator(); Colleague *c1 = new ConcreteColleague( mediator, 1 ); Colleague *c2 = new ConcreteColleague( mediator, 2 ); Colleague *c3 = new ConcreteColleague( mediator, 3 ); mediator-&gt;add( c1 ); mediator-&gt;add( c2 ); mediator-&gt;add( c3 ); c1-&gt;send( \"Hi!\" ); c3-&gt;send( \"Hello!\" ); delete mediator; return 0;&#125; 状态变化相关模式 关注对象状态的改变，对这些 变化的状态进行管理，维持更高层模块的稳定。 典型模式： State Memento State 某些对象的状态如果发生改变，那么它的行为也会发生改变。比如文件的读写状态变化。 State模式允许一个对象在其内部状态改变时改变它的行为。对象看起来似乎修改了它的类。 类似Strategy模式，但是这里关注状态的变化。 示例1 实现对网络连接状态的跟踪和处理。 实现一，使用条件判断，处理和转换状态。 12345678910111213141516171819202122enum NetworkState &#123; Network_Open, Network_Close, Network_Connect,&#125;;class NetworkProcessor&#123; NetworkState state;public: void Operation()&#123; if (state == Network_Open)&#123; ... state = Network_Close; &#125; else if (state == Network_Close)&#123; ... state = Network_Connect; &#125; else if (state == Network_Connect)&#123; ... state = Network_Open; &#125; &#125;&#125;; 实现二，抽象出状态对象，状态对象自己处理自己的转换关系。 12345678910111213141516171819202122232425262728293031323334353637383940414243// 状态对象基类class NetworkState&#123;public: NetworkState* pNext; virtual void Operation1()=0; virtual void Operation2()=0; virtual void Operation3()=0; virtual ~NetworkState()&#123;&#125;&#125;;// 具体状态对象，可设计为Singletonclass OpenState :public NetworkState&#123; static NetworkState* m_instance;public: static NetworkState* getInstance()&#123; if (m_instance == nullptr) &#123; m_instance = new OpenState(); &#125; return m_instance; &#125; // 改变 pNext 指针，抽象转换对象状态。 void Operation1()&#123; ... pNext = CloseState::getInstance(); &#125; void Operation2()&#123; ... pNext = ConnectState::getInstance(); &#125; void Operation3()&#123; ... pNext = OpenState::getInstance(); &#125;&#125;;// 更多的具体状态对象，可扩展class CloseState:public NetworkState&#123; ... &#125;... 12345678910111213141516171819202122232425// 应用类，稳定，一般可保持不变class NetworkProcessor&#123; NetworkState* pState; public: NetworkProcessor(NetworkState* pState)&#123; this-&gt;pState = pState; &#125; // 通过 状态对象 处理状态转换 void Operation1()&#123; pState-&gt;Operation1(); pState = pState-&gt;pNext; &#125; void Operation2()&#123; pState-&gt;Operation2(); pState = pState-&gt;pNext; &#125; void Operation3()&#123; pState-&gt;Operation3(); pState = pState-&gt;pNext; &#125;&#125;; State模式将状态的改变，实现为状态对象的改变，通过抽象基类，不再依赖具体对象。而且，状态的转换更加明确，不易出错。 抽象代码结构 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#include &lt;iostream&gt;/* * State * defines an interface for encapsulating the behavior associated * with a particular state of the Context */class State&#123;public: virtual ~State() &#123; /* ... */ &#125; virtual void handle() = 0; // ...&#125;;/* * Concrete States * each subclass implements a behavior associated with a state * of the Context */class ConcreteStateA : public State&#123;public: ~ConcreteStateA() &#123; /* ... */ &#125; void handle() &#123; std::cout &lt;&lt; \"State A handled.\" &lt;&lt; std::endl; &#125; // ...&#125;;class ConcreteStateB : public State&#123;public: ~ConcreteStateB() &#123; /* ... */ &#125; void handle() &#123; std::cout &lt;&lt; \"State B handled.\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Context * defines the interface of interest to clients */class Context&#123;public: Context() : state() &#123; /* ... */ &#125; ~Context() &#123; delete state; &#125; void setState( State* const s ) &#123; if ( state ) &#123; delete state; &#125; state = s; &#125; void request() &#123; state-&gt;handle(); &#125; // ...private: State *state; // ...&#125;;int main()&#123; Context *context = new Context(); context-&gt;setState( new ConcreteStateA() ); context-&gt;request(); context-&gt;setState( new ConcreteStateB() ); context-&gt;request(); delete context; return 0;&#125; Memento 当一个对象，需要保存一个或者多个内存快照，用于未来恢复到当前对象状态。这时，会出现一个潜在问题，对象的实现细节可能会暴露。 Memento的想法，就是找另一个封闭的对象B，保存当前对象A的状态，并在需要的时候用于恢复对象A的状态。 Memento在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态。这样以后就可将该对象恢复到原先保存的状态。 这个模式就是为了隐藏对象信息，而不是显示地将一个一个成员变量导出保存。 但是，这个模式实现地内存快照方法，已经过时了，现在往往会采用效率更高、更简洁地序列化方法来实现。 抽象代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137#include &lt;iostream&gt;#include &lt;vector&gt;/* * Memento * stores internal state of the Originator object and protects * against access by objects other than the originator */class Memento&#123;private: // accessible only to Originator friend class Originator; Memento( const int s ) : state( s ) &#123;&#125; void setState( const int s ) &#123; state = s; &#125; int getState() &#123; return state; &#125; // ...private: int state; // ...&#125;;/* * Originator * creates a memento containing a snapshot of its current internal * state and uses the memento to restore its internal state */class Originator&#123;public: // implemented only for printing purpose void setState( const int s ) &#123; std::cout &lt;&lt; \"Set state to \" &lt;&lt; s &lt;&lt; \".\" &lt;&lt; std::endl; state = s; &#125; // implemented only for printing purpose int getState() &#123; return state; &#125; void setMemento( Memento* const m ) &#123; state = m-&gt;getState(); &#125; Memento *createMemento() &#123; return new Memento( state ); &#125;private: int state; // ...&#125;;/* * CareTaker * is responsible for the memento's safe keeping */class CareTaker&#123;public: CareTaker( Originator* const o ) : originator( o ) &#123;&#125; ~CareTaker() &#123; for ( unsigned int i = 0; i &lt; history.size(); i++ ) &#123; delete history.at( i ); &#125; history.clear(); &#125; void save() &#123; std::cout &lt;&lt; \"Save state.\" &lt;&lt; std::endl; history.push_back( originator-&gt;createMemento() ); &#125; void undo() &#123; if ( history.empty() ) &#123; std::cout &lt;&lt; \"Unable to undo state.\" &lt;&lt; std::endl; return; &#125; Memento *m = history.back(); originator-&gt;setMemento( m ); std::cout &lt;&lt; \"Undo state.\" &lt;&lt; std::endl; history.pop_back(); delete m; &#125; // ...private: Originator *originator; std::vector&lt;Memento*&gt; history; // ...&#125;;int main()&#123; Originator *originator = new Originator(); CareTaker *caretaker = new CareTaker( originator ); originator-&gt;setState( 1 ); caretaker-&gt;save(); originator-&gt;setState( 2 ); caretaker-&gt;save(); originator-&gt;setState( 3 ); caretaker-&gt;undo(); std::cout &lt;&lt; \"Actual state is \" &lt;&lt; originator-&gt;getState() &lt;&lt; \".\" &lt;&lt; std::endl; delete originator; delete caretaker; return 0;&#125; 数据结构相关模式 假如存在一些数据结构，如果让客户程序直接依赖它们，会破坏组件的复用性。比如说接口的不统一，导致源代码需要频繁改变。 这时候，对这些数据结构进行封装，对外提供统一的接口，来实现与数据结构无关的访问。 典型模式： Composite Iterator Chain of Responsibility Composite 处理对内部数据结构复杂实现的依赖，Composite将对象组合成树形结构以表示“部分-整体”的层次结构。 Composite使得用户对单个对象和组合对象的使用具有一致性。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105#include &lt;iostream&gt;#include &lt;vector&gt;/* * Component * defines an interface for all objects in the composition * both the composite and the leaf nodes */class Component&#123;public: virtual ~Component() &#123;&#125; virtual Component *getChild( int ) &#123; return 0; &#125; virtual void add( Component * ) &#123; /* ... */ &#125; virtual void remove( int ) &#123; /* ... */ &#125; virtual void operation() = 0;&#125;;/* * Composite * defines behavior of the components having children * and store child components */class Composite : public Component&#123;public: ~Composite() &#123; for ( unsigned int i = 0; i &lt; children.size(); i++ ) &#123; delete children[ i ]; &#125; &#125; Component *getChild( const unsigned int index ) &#123; return children[ index ]; &#125; void add( Component *component ) &#123; children.push_back( component ); &#125; void remove( const unsigned int index ) &#123; Component *child = children[ index ]; children.erase( children.begin() + index ); delete child; &#125; void operation() &#123; for ( unsigned int i = 0; i &lt; children.size(); i++ ) &#123; children[ i ]-&gt;operation(); &#125; &#125; private: std::vector&lt;Component*&gt; children;&#125;;/* * Leaf * defines the behavior for the elements in the composition, * it has no children */class Leaf : public Component&#123;public: Leaf( const int i ) : id( i ) &#123;&#125; ~Leaf() &#123;&#125; void operation() &#123; std::cout &lt;&lt; \"Leaf \"&lt;&lt; id &lt;&lt;\" operation\" &lt;&lt; std::endl; &#125;private: int id;&#125;;int main()&#123; Composite composite; for ( unsigned int i = 0; i &lt; 5; i++ ) &#123; composite.add( new Leaf( i ) ); &#125; composite.remove( 0 ); composite.operation(); return 0;&#125; Composite的operation()中处理到了Composite类型对象，会继续处理其children。 典型的 Composite对象结构如下图： Iterator 不管内部的数据结构如何定义，Iterator使用统一接口，隐藏内部实现，供外部使用各种数据结构。 Iterator提供一种方法顺序访问一个聚合对象中各个元素 , 而又不需暴露该对象的内部表示。 这个模式在C++中，并不是实现目的的最优选择。STL中泛型编程实现的Iterator比面向对象设计模式实现的Iterator，更高效。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141#include &lt;iostream&gt;#include &lt;stdexcept&gt;#include &lt;vector&gt;class Iterator;class ConcreteAggregate;/* * Aggregate * defines an interface for aggregates and it decouples your * client from the implementation of your collection of objects */class Aggregate&#123;public: virtual ~Aggregate() &#123;&#125; virtual Iterator *createIterator() = 0; // ...&#125;;/* * Concrete Aggregate * has a collection of objects and implements the method * that returns an Iterator for its collection * */class ConcreteAggregate : public Aggregate&#123;public: ConcreteAggregate( const unsigned int size ) &#123; list = new int[size](); count = size; &#125; ~ConcreteAggregate() &#123; delete[] list; &#125; Iterator *createIterator(); unsigned int size() const &#123; return count; &#125; int at( unsigned int index ) &#123; return list[ index ]; &#125; // ...private: int *list; unsigned int count; // ...&#125;;/* * Iterator * provides the interface that all iterators must implement and * a set of methods for traversing over elements */class Iterator&#123;public: virtual ~Iterator() &#123; /* ... */ &#125; virtual void first() = 0; virtual void next() = 0; virtual bool isDone() const = 0; virtual int currentItem() const = 0; // ...&#125;;/* * Concrete Iterator * implements the interface and is responsible for managing * the current position of the iterator */class ConcreteIterator : public Iterator&#123;public: ConcreteIterator( ConcreteAggregate *l ) : list( l ), index( 0 ) &#123;&#125; ~ConcreteIterator() &#123;&#125; void first() &#123; index = 0; &#125; void next() &#123; index++; &#125; bool isDone() const &#123; return ( index &gt;= list-&gt;size() ); &#125; int currentItem() const &#123; if ( isDone() ) &#123; return -1; &#125; return list-&gt;at(index); &#125; // ...private: ConcreteAggregate *list; unsigned int index; // ...&#125;;Iterator *ConcreteAggregate::createIterator()&#123; return new ConcreteIterator( this );&#125;int main()&#123; unsigned int size = 5; ConcreteAggregate list = ConcreteAggregate( size ); Iterator *it = list.createIterator(); for ( ; !it-&gt;isDone(); it-&gt;next()) &#123; std::cout &lt;&lt; \"Item value: \" &lt;&lt; it-&gt;currentItem() &lt;&lt; std::endl; &#125; delete it; return 0;&#125; 面向对象的Iterator，相比STL的Iterator，其代价在于ConcreteIterator这种子类调用虚函数的代价。 这种通过虚函数实现多态调用的方法，是运行时多态。 STL通过模板支持不同的数据结构，是编译时多态，由编译器直接推断出具体类型。显然，就程序运行时效率而言，泛型编程的方式效率会更高。 然而在一些不支持泛型编程的语言种，Iterator设计模式还是有应用的。 Chain of Responsibility 当一个请求可以被多个对象处理，但是每次仅会有一个对象完成处理操作，此时，如果显示的一种情况一种情况地去指定处理对象，那么实现会很复杂。 Chain of Responsibility使多个对象都有机会处理请求，从而避免请求的发送者和接收者之间的耦合关系。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它为止。 一个链表中所有对象，依次处理一个请求。最后没有被处理的请求，应当设置默认的响应机制。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#include &lt;iostream&gt;/* * Handler * defines an interface for handling requests and * optionally implements the successor link */class Handler&#123;public: virtual ~Handler() &#123;&#125; virtual void setHandler( Handler *s ) &#123; successor = s; &#125; virtual void handleRequest() &#123; if (successor != 0) &#123; successor-&gt;handleRequest(); &#125; &#125; // ...private: // 相当于链表的next指针 Handler *successor;&#125;;/* * Concrete Handlers * handle requests they are responsible for */class ConcreteHandler1 : public Handler&#123;public: ~ConcreteHandler1() &#123;&#125; bool canHandle() &#123; // ... return false; &#125; virtual void handleRequest() &#123; if ( canHandle() ) &#123; std::cout &lt;&lt; \"Handled by Concrete Handler 1\" &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; \"Cannot be handled by Handler 1\" &lt;&lt; std::endl; Handler::handleRequest(); &#125; // ... &#125; // ...&#125;;class ConcreteHandler2 : public Handler&#123;public: ~ConcreteHandler2() &#123;&#125; bool canHandle() &#123; // ... return true; &#125; virtual void handleRequest() &#123; if ( canHandle() ) &#123; std::cout &lt;&lt; \"Handled by Handler 2\" &lt;&lt; std::endl; &#125; else &#123; std::cout &lt;&lt; \"Cannot be handled by Handler 2\" &lt;&lt; std::endl; Handler::handleRequest(); &#125; // ... &#125; // ...&#125;;int main()&#123; ConcreteHandler1 handler1; ConcreteHandler2 handler2; // 设置链式处理顺序 handler1.setHandler( &amp;handler2 ); handler1.handleRequest(); return 0;&#125; 行为变化相关模式 在组件构建过程中，组件行为会发生变化，并且因此导致组件不得不重新实现。行为一般就是具体的成员方法实现。 行为变化相关模式，就是将组件的行为和组件自身进行解耦。分隔出变化的行为部分。 典型模式： Command Visitor Command 将行为抽象为对象，有点类似C++的函数对象。 Command模式将一个请求封装为一个对象，从而使你可用不同的请求对客户进行参数化；对请求（此时是一个可储存的对象）排队或记录请求日志，以及支持可撤消的操作。 Command模式也被称为动作( Action )或事务( Transaction )模式。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#include &lt;iostream&gt;/* * Receiver * knows how to perform the operations associated * with carrying out a request */class Receiver&#123;public: void action() &#123; std::cout &lt;&lt; \"Receiver: execute action\" &lt;&lt; std::endl; &#125; // ...&#125;;/* * Command * declares an interface for all commands */class Command&#123;public: virtual ~Command() &#123;&#125; virtual void execute() = 0; // ...protected: Command() &#123;&#125;&#125;;/* * Concrete Command * implements execute by invoking the corresponding * operation(s) on Receiver */class ConcreteCommand : public Command&#123;public: ConcreteCommand( Receiver *r ) : receiver( r ) &#123;&#125; ~ConcreteCommand() &#123; if ( receiver ) &#123; delete receiver; &#125; &#125; void execute() &#123; receiver-&gt;action(); &#125; // ... private: Receiver *receiver; // ...&#125;;/* * Invoker * asks the command to carry out the request */class Invoker&#123;public: void set( Command *c ) &#123; command = c; &#125; void confirm() &#123; if ( command ) &#123; command-&gt;execute(); &#125; &#125; // ...private: Command *command; // 可以设置为多个command的容器 // ...&#125;;int main()&#123; ConcreteCommand command( new Receiver() ); Invoker invoker; invoker.set( &amp;command ); invoker.confirm(); return 0;&#125; Command模式有点类似C++的函数对象，但是两者也有明显不同。 面向对象的Command模式对于接口的定义更加规范灵活，但是性能会低一些。C++函数对象加上模板，是通过函数签名来定义接口，另外运行时性能会更高一些。 在不能使用模板和函数对象的语言中，Command模式会比较常见一些。 Visitor 在某些层次结构的类中，比如一个抽象父类下有多个子类，如果需要增加一个新的方法到所有子类中，直接从基类开始修改的话，无疑会很麻烦，而且不符合对修改封闭的设计原则。 Visitor模式表示一个作用于某对象结构中的各元素的操作。它使你可以在不改变各元素的类的前提下定义作用于这些元素的新操作。 使用Visitor类来修改原来的类中的方法。 比如以下代码 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;using namespace std;class Element&#123;public: virtual void Func1() = 0; virtual void Func2(int data)=0; //新增方法 virtual void Func3(int data)=0; virtual ~Element()&#123;&#125;&#125;;class ElementA : public Element &#123;public: void Func1() override&#123;...&#125; void Func2(int data) override&#123;...&#125; // 需要新增方法&#125;;class ElementB : public Element&#123;public: void Func1() override&#123;...&#125; void Func2(int data) override &#123;...&#125; // 需要新增方法&#125;; 新增方法3，需要更改所有Element类。 使用Visitor模式，结构如下： 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;using namespace std;class Element&#123;public: virtual void accept(Visitor&amp; visitor) = 0; //第一次多态辨析 virtual ~Element()&#123;&#125;&#125;;class ElementA : public Element&#123;public: void accept(Visitor &amp;visitor) override &#123; visitor.visitElementA(*this); &#125; ...&#125;;class ElementB : public Element&#123;public: void accept(Visitor &amp;visitor) override &#123; visitor.visitElementB(*this); //第二次多态辨析 &#125; ...&#125;;class Visitor&#123;public: virtual void visitElementA(ElementA&amp; element) = 0; virtual void visitElementB(ElementB&amp; element) = 0; virtual ~Visitor()&#123;&#125;&#125;; 1234567891011121314151617181920212223242526272829303132333435//扩展方法，通过visitElementA、visitElementB实现class Visitor1 : public Visitor&#123;public: void visitElementA(ElementA&amp; element) override&#123; cout &lt;&lt; \"Visitor1 is processing ElementA\" &lt;&lt; endl; &#125; void visitElementB(ElementB&amp; element) override&#123; cout &lt;&lt; \"Visitor1 is processing ElementB\" &lt;&lt; endl; &#125;&#125;;//扩展另一种方法class Visitor2 : public Visitor&#123;public: void visitElementA(ElementA&amp; element) override&#123; cout &lt;&lt; \"Visitor2 is processing ElementA 2\" &lt;&lt; endl; &#125; void visitElementB(ElementB&amp; element) override&#123; cout &lt;&lt; \"Visitor2 is processing ElementB 2\" &lt;&lt; endl; &#125;&#125;;int main() &#123; Visitor2 visitor; ElementB elementB; elementB.accept(visitor); ElementA elementA; elementA.accept(visitor); return 0;&#125; 使用Visitor模式之后，只需要扩展新的Visitor子类，就能增加新的方法。 这个过程中，有一种被称为 double dispatch 的实现方法。第一次 dispatch ，Element类中的accept方法，辨析时哪一个扩展的Visitor子类。第二次 dispatch ，accept方法内visitor的成员方法visitElementX方法，辨析当前类是哪一种Element子类。 Visitor模式的使用有一个严格的条件，Element子类的数目必须是已知的，且不会发生变化。否则，整个过程将不再稳定，Visitor模式不如不用。 领域规则相关模式 在特定领域中，可以将变化模式抽象为一些规则，将这些规则通过设计语法实现，就能解决一般性的问题。 典型模式： Interpreter Interpreter 如果在软件构建过程中，某些结构不断地重复出现。构建一种规则，使得问题可以被表达，并通过一个解释器，来解释还原这个表达。 Interpreter模式给定一个语言，定义它的文法的一种表示，并定义一个解释器，这个解释器使用该表示来解释语言中的句子。 定义比较抽象。举例来说，实现一个加减法表达式运算，可以抽向出加法运算文法地类、减法运算文法地类等，实现一个解释器完成加减运算优先级的处理，调用不同的文法处理操作数。 抽象代码结构 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115#include &lt;iostream&gt;#include &lt;map&gt;/* * Context * contains information that's global to the interpreter */class Context&#123;public: void set( const std::string&amp; var, const bool value) &#123; vars.insert( std::pair&lt;std::string, bool&gt;( var, value ) ); &#125; bool get( const std::string&amp; exp ) &#123; return vars[ exp ]; &#125; // ...private: std::map&lt;std::string, bool&gt; vars; // ...&#125;;/* * Abstract Expression * declares an abstract Interpret operation that is common to all nodes * in the abstract syntax tree */class AbstractExpression&#123;public: virtual ~AbstractExpression() &#123;&#125; virtual bool interpret( Context* const ) &#123; return false; &#125; // ...&#125;;/* * Terminal Expression * implements an Interpret operation associated with terminal symbols * in the grammar (an instance is required for every terminal symbol * in a sentence) */class TerminalExpression : public AbstractExpression&#123;public: TerminalExpression( const std::string&amp; val ) : value( val ) &#123;&#125; ~TerminalExpression() &#123;&#125; bool interpret( Context* const context ) &#123; return context-&gt;get( value ); &#125; // ... private: std::string value; // ...&#125;;/* * Nonterminal Expression * implements an Interpret operation for nonterminal symbols * in the grammar (one such class is required for every rule in the grammar) */class NonterminalExpression : public AbstractExpression&#123;public: NonterminalExpression( AbstractExpression *left, AbstractExpression *right ) : lop( left ), rop( right ) &#123;&#125; ~NonterminalExpression() &#123; delete lop; delete rop; &#125; bool interpret( Context *const context ) &#123; return lop-&gt;interpret( context ) &amp;&amp; rop-&gt;interpret( context ); &#125; // ... private: AbstractExpression *lop; AbstractExpression *rop; // ...&#125;;int main()&#123; // An example of very simple expression tree // that corresponds to expression (A AND B) AbstractExpression *A = new TerminalExpression(\"A\"); AbstractExpression *B = new TerminalExpression(\"B\"); AbstractExpression *exp = new NonterminalExpression( A, B ); Context context; context.set( \"A\", true ); context.set( \"B\", false ); std::cout &lt;&lt; context.get( \"A\" ) &lt;&lt; \" AND \" &lt;&lt; context.get( \"B\" ); std::cout &lt;&lt; \" = \" &lt;&lt; exp-&gt;interpret( &amp;context ) &lt;&lt; std::endl; delete exp; return 0;&#125; 实现这个模式挺复杂的，处理一些简单情况还行。太复杂的情况，面向对象的方式本身代价也比较大，此时可以考虑使用一些语法分析生成器标准工具。 设计模式小结 设计模式的目标是，提高复用性，管理分离变化的部分。所以，对复用性没有要求的时候，设计模式也没那么必要。 什么时候不用模式？代码可读性很差时，不用模式。需求理解不充分时，不用模式。程序中变化的部分没有显现时，不用模式。不是系统的关键依赖点的地方，不用模式。项目没有复用价值时，不用模式。项目将要发布时，不用模式。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Design Patterns","slug":"Notes/Design-Patterns","permalink":"https://racleray.github.io/categories/Notes/Design-Patterns/"}],"tags":[{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"},{"name":"design patterns","slug":"design-patterns","permalink":"https://racleray.github.io/tags/design-patterns/"}],"author":"HeRui"},{"title":"数据结构整理-part3","slug":"数据结构整理-part3","date":"2021-10-08T13:30:33.000Z","updated":"2023-08-07T11:54:31.043Z","comments":true,"path":"posts/3626ce4c.html","link":"","permalink":"https://racleray.github.io/posts/3626ce4c.html","excerpt":"常见基本数据结构整理第三部分，使用C语言编写。","text":"堆与优先队列 堆可以看成是一棵完全二叉树，除最后一层以外，它的每一层都是填满的，最后一层从左到右依次填入。 根结点权值大于等于树中任意结点权值的堆称为大根堆。 根结点权值小于等于树中任意结点权值的堆则称为小根堆。 并不需要真的维护一棵完全二叉树，而只需用一个数组来存储，堆按从上到下，从左到右的顺序，依次存储在下标从 1 开始的数组里。 元素插入，先添加到完全二叉树的最后一位之后，然后对比与父节点的关系，判断是否交换与父节点的位置。 元素删除，先对调堆顶元素与完全二叉树最后一个元素，删除最后一个元素，然后从堆顶开始调整二叉树的堆序性。 堆排序，和元素删除类似，只是不删除对调之后的最后一个元素，只是将序列尾部 index 减一。直到 index 等于 0。 总而言之，挺简单的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Heap &#123; int *data, size;&#125; Heap;void init(Heap *h, int length_input) &#123; h-&gt;data = (int *)malloc(sizeof(int) * length_input); h-&gt;size = 0;&#125;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void push(Heap *h, int value) &#123; h-&gt;data[h-&gt;size] = value; int cur = h-&gt;size; int parent = (cur - 1) / 2; while (h-&gt;data[cur] &gt; h-&gt;data[parent]) &#123; swap(&amp;h-&gt;data[cur], &amp;h-&gt;data[parent]); cur = parent; parent = (cur - 1) / 2; &#125; h-&gt;size++;&#125;void output(Heap *h) &#123; for (int i = 0; i &lt; h-&gt;size; i++) &#123; (i &gt; 0) &amp;&amp; printf(\" \"); printf(\"%d \", h-&gt;data[i]); &#125; printf(\"\\n\");&#125;int top(Heap *h) &#123; return h-&gt;data[0];&#125;void update(Heap *h, int pos, int n) &#123; int lchild = 2 * pos + 1, rchild = 2 * pos + 2; int max_idx = pos; if (h-&gt;data[lchild] &gt; h-&gt;data[max_idx] &amp;&amp; lchild &lt; n) max_idx = lchild; if (rchild &lt; n &amp;&amp; h-&gt;data[rchild] &gt; h-&gt;data[max_idx]) max_idx = rchild; if (max_idx != pos) &#123; swap(&amp;h-&gt;data[max_idx], &amp;h-&gt;data[pos]); update(h, max_idx, n); &#125; return;&#125;void pop(Heap *h) &#123; swap(&amp;h-&gt;data[0], &amp;h-&gt;data[--h-&gt;size]); update(h, 0, h-&gt;size);&#125;void heap_sort(Heap *h) &#123; for (int i = h-&gt;size - 1; i &gt; 0; i--) &#123; swap(&amp;h-&gt;data[i], &amp;h-&gt;data[0]); update(h, 0, i); &#125;&#125;void clear(Heap *h) &#123; free(h-&gt;data); free(h);&#125;int main() &#123; Heap *h = (Heap *)malloc(sizeof(Heap)); init(h, 105); int n; scanf(\"%d\", &amp;n); for (int i = 0, val; i &lt; n; i++) &#123; scanf(\"%d\", &amp;val); push(h, val); &#125; int m; scanf(\"%d\", &amp;m); while (m--) &#123; printf(\"%d\\n\", top(h)); pop(h); &#125; output(h); heap_sort(h); output(h); clear(h); return 0;&#125; 优先队列 在 C++ 的 STL 里，有封装好的优先队列priority_queue，它包含在头文件&lt;queue&gt;里。优先级可以自己定义，默认优先级是权值大的元素优先级高。 字符串Huffman编码 建立Huffman树的方法，在总结 树 的部分有说明。 首先统计每个字母在字符串里出现的频率，把每个字母看成一个结点，结点的权值即是字母出现的频率。 把每个结点看成一棵只有根结点的二叉树，一开始把所有二叉树（结点）都放在一个集合里，接下来开始如下编码： 步骤一：从集合里取出两个根结点权值最小的树a和b，构造出一棵新的二叉树c，二叉树c的根结点的权值为a和b的根结点权值和，二叉树c的左右子树分别是a和b。（合并） 步骤二：将二叉树a和b从集合里删除，把二叉树c加入集合里。（更新候选） 重复以上两个步骤，直到集合里只剩下一棵二叉树，最后剩下的就是哈夫曼树了。 规定每个有孩子的结点，到左孩子的路径为 0，到右孩子的路径为 1。每个字母的编码就是根结点到字母对应结点的路径。 计算字符串进行哈夫曼编码后的长度，即哈夫曼树的带权路径长度 WPL（Weighted Path Length），也就是每个叶子结点到根结点的距离乘以叶子结点权值结果之和。 一种简单的计算方法是：当哈夫曼树上结点总个数大于 1 时，哈夫曼树的 WPL，等于树上除根结点之外的所有结点的权值之和。如果结点总个数为 1，则哈夫曼树的 WPL 即为根结点权值。 想想Huffman树建立的过程，这种简单的计算WPL的方法，挺直观的。 利用小根堆计算Huffman树的WPL 不考虑建立二叉树的步骤，只累加计算每次作为插入结点的权值之和就能得到结果。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Heap &#123; int *data, size;&#125; Heap;void init(Heap *h, int length_input) &#123; h-&gt;data = (int *)malloc(sizeof(int) * length_input); h-&gt;size = 0;&#125;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void push(Heap *h, int value) &#123; h-&gt;data[h-&gt;size] = value; int current = h-&gt;size; int father = (current - 1) / 2; while (h-&gt;data[current] &lt; h-&gt;data[father]) &#123; swap(&amp;h-&gt;data[current], &amp;h-&gt;data[father]); current = father; father = (current - 1) / 2; &#125; h-&gt;size++;&#125;int top(Heap *h) &#123; return h-&gt;data[0];&#125;void update(Heap *h, int pos, int n) &#123; int lchild = 2 * pos + 1, rchild = 2 * pos + 2; int max_value = pos; if (lchild &lt; n &amp;&amp; h-&gt;data[lchild] &lt; h-&gt;data[max_value]) &#123; max_value = lchild; &#125; if (rchild &lt; n &amp;&amp; h-&gt;data[rchild] &lt; h-&gt;data[max_value]) &#123; max_value = rchild; &#125; if (max_value != pos) &#123; swap(&amp;h-&gt;data[pos], &amp;h-&gt;data[max_value]); update(h, max_value, n); &#125;&#125;void pop(Heap *h) &#123; swap(&amp;h-&gt;data[0], &amp;h-&gt;data[h-&gt;size - 1]); h-&gt;size--; update(h, 0, h-&gt;size);&#125;int heap_size(Heap *h) &#123; return h-&gt;size;&#125;void clear(Heap *h) &#123; free(h-&gt;data); free(h);&#125;int main() &#123; int n, value, ans = 0; scanf(\"%d\", &amp;n); Heap *heap = (Heap *)malloc(sizeof(Heap)); init(heap, n); for (int i = 1; i &lt;= n; i++) &#123; scanf(\"%d\", &amp;value); push(heap, value); &#125; if (n == 1) &#123; ans = ans + top(heap); &#125; while (heap_size(heap) &gt; 1) &#123; int a = top(heap); pop(heap); int b = top(heap); pop(heap); ans = ans + a + b; push(heap, a + b); &#125; printf(\"%d\\n\", ans); clear(heap); return 0;&#125; 并查集 在数据结构里，森林是指若干棵互不相交的树的集合。 并查集（Merge-Find Set），也被称为不相交集合（Disjoint Set），是用于解决若干的不相交集合检索关系的数据结构。 一般并查集有以下几种操作： MAKE-SET(x)：初始化操作，建立一个只包含元素 x 的集合。 UNION(x, y)：合并操作，将包含 x 和 y 的集合合并为一个新的集合。 FIND-SET(x)：查询操作，计算 x 所在的集合。 “并查集”这个词通常既可以指代不相交集合的数据结构，也可以表示其对应的算法。其在有些教材中的英文名称也叫做 Disjoint Set Union，表示用于求不相交集合并集的相关算法。 并查集用有根树来表示集合，树中的每一个结点都对应集合的一个成员，每棵树表示一个集合。 每个成员都有一条指向父结点的边，整个有根树通过这些指向父结点的边来维护。 每棵树的根就是这个集合的代表，并且根的父结点是它自己。 并查集的查询操作，指的是查找出指定元素所在有根树的根结点是谁。 并查集的合并操作需要用到查询操作的结果。合并两个元素所在的集合，需要首先求出两个元素所在集合的根。接下来将其中一个根结点的父亲设置为另一个根结点。 优化方法 并查集的查询操作最坏情况下的时间复杂度为 \\(O(n)\\) 退化成一个单链表。 其优化方法是，在合并时，将高度较低的树接到高度较高的树根上，可以防止树退化成一条链。 利用一个数组保存每个节点的所在树的节点总数，即保存每个节点的秩（可视为height）。 分别获得传入的两个节点所在的树的根节点。 比较两个根节点是否，相同则返回 false，结束合并操作、 若两个根节点的秩不同，比较他们的秩的大小。 将秩较小的根节点的父指针指向秩较大的跟节点。 更新合并后的根节点的秩，返回 true，结束合并操作。 通过路径压缩的方法可以进一步减少均摊复杂度，此时同一个集合内的节点可以一步找到根节点。同时使用这两种优化方法，可以将每次操作的时间复杂度优化至接近常数级。 路径压缩指，在进行路径压缩优化时只需在查找根节点时，将待查找的节点的父指针指向它所在的树的根节点。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct DisjointSet&#123; int *father, *rank;&#125; DisjointSet;void init(DisjointSet *s, int size) &#123; s-&gt;father = (int *)malloc(sizeof(int) * size); s-&gt;rank = (int *)malloc(sizeof(int) * size); for (int i = 0; i &lt; size; ++i) &#123; s-&gt;father[i] = i; s-&gt;rank[i] = 1; &#125;&#125;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;int max(int a, int b) &#123; return a &gt; b ? a : b;&#125;int find_set(DisjointSet *s, int node) &#123; if (s-&gt;father[node] != node) &#123; // 递归找到根节点，node的father指向根节点 s-&gt;father[node] = find_set(s, s-&gt;father[node]); &#125; // 回溯过程中，路径上node的father都指向根 return s-&gt;father[node];&#125;// 根据高度合并int merge(DisjointSet *s, int node1, int node2) &#123; int ancestor1 = find_set(s, node1); int ancestor2 = find_set(s, node2); if (ancestor1 != ancestor2) &#123; if (s-&gt;rank[ancestor1] &gt; s-&gt;rank[ancestor2]) &#123; swap(&amp;ancestor1, &amp;ancestor2); &#125; s-&gt;father[ancestor1] = ancestor2; s-&gt;rank[ancestor2] = max(s-&gt;rank[ancestor2], s-&gt;rank[ancestor1] + 1); return 1; &#125; return 0;&#125;void clear(DisjointSet *s) &#123; free(s-&gt;father); free(s-&gt;rank); free(s);&#125;int main() &#123; DisjointSet *dsu = (DisjointSet *)malloc(sizeof(DisjointSet)); init(dsu, 100); int m, x, y; scanf(\"%d\", &amp;m); for (int i = 0; i &lt; m; ++i) &#123; scanf(\"%d%d\", &amp;x, &amp;y); int ans = merge(dsu, x, y); if (ans) &#123; printf(\"success\\n\"); &#125; else &#123; printf(\"failed\\n\"); &#125; &#125; clear(dsu); return 0;&#125; 连通分量 连通分量就是图 G 的最大连通子图。对于一个无向图，使用FloodFill算法可以求得连通分量。 找到一个没有染色的顶点，将其染为新的颜色 \\(Color_{new}\\)，如果没有则算法结束。 初始化一个空的队列，并将第一步的顶点插入队列。 不断获得队首元素的值并弹出，将和队首元素相邻的未染色顶点染为 \\(Color_{new}\\)，并将其加入队列。 重复执行第一步，直到所有顶点都被染色，算法结束。 FloodFill 的时间复杂度是 \\(O(V+E)\\)，其中广度优先遍历的部分可以替换成深度优先遍历，复杂度是一样的。通常考虑到递归调用的时间开销，往往广度优先遍历的效率要更高一些。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAX_N 1000typedef struct Graph &#123; int n; int color[MAX_N]; int mat[MAX_N][MAX_N];&#125; Graph;void init_Graph(Graph *g, int input_n) &#123; g-&gt;n = input_n; for (int i = 0; i &lt; g-&gt;n; i++) &#123; g-&gt;color[i] = 0; for (int j = 0; j &lt; g-&gt;n; j++) &#123; g-&gt;mat[i][j] = 0; &#125; &#125;&#125;void insert(Graph *g, int x, int y) &#123; g-&gt;mat[x][y] = 1; g-&gt;mat[y][x] = 1;&#125;void floodfill(Graph *g) &#123; int color_cnt = 0; int q[MAX_N]; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; if (g-&gt;color[i] == 0) &#123; color_cnt++; g-&gt;color[i] = color_cnt; int l = 0, r = 0; q[r++] = i; // 遍历相邻未染色节点，通过队列，广度优先染色 while (l &lt; r) &#123; int vertex = q[l++]; for (int j = 0; j &lt; g-&gt;n; j++) &#123; if (g-&gt;mat[vertex][j] &amp;&amp; g-&gt;color[j] == 0) &#123; g-&gt;color[j] = color_cnt; q[r++] = j; &#125; &#125; &#125; &#125; &#125; // output print for (int i = 0; i &lt; g-&gt;n; ++i) &#123; printf(\"%d %d\\n\", i, g-&gt;color[i]); &#125;&#125;int main() &#123; int n, m; scanf(\"%d %d\", &amp;n, &amp;m); Graph *g = (Graph *)malloc(sizeof(Graph)); init_Graph(g, n); for (int i = 0; i &lt; m; i++) &#123; int a, b; scanf(\"%d %d\", &amp;a, &amp;b); insert(g, a, b); &#125; floodfill(g); free(g); return 0;&#125; 最小生成树 从一个带权图中抽出一棵生成树，使得边权值和最小，这棵生成树就叫做最小生成树。 Prim 思路很简单，不断寻找距离当前生成树最近的顶点。逐步添加最短边到生成树中。 定义带权图 G 的顶点集合为 V，接着再定义最小生成树的顶点集合为 U，初始集合 U 为空。 任选一个顶点 \\(x\\)，加入集合 \\(U\\)，并记录每个顶点到当前最小生成树的最短距离。 选择一个距离当前最小生成树最近的，且不属于集合 \\(U\\) 的顶点 \\(v\\)（如果有多个顶点 \\(v\\)，任选其一），将顶点 \\(v\\) 加入集合 \\(U\\)，并更新所有与顶点 \\(v\\) 相连的顶点到当前最小生成树的最短距离记录。 3. 重复第二步操作，直至集合 \\(U\\) 等于集合 \\(V\\)。 贪心策略，意味着就很直白，没啥难的。 使用不同的数据结构实现，时间复杂度不同。 EXTRACT-MIN DECREASE-KEY Total array O(V) O(1) \\(O(V^2)\\) binary heap O(log V) O(log V) \\(O(E \\log V)\\) Fibonacci heap O(log V) O(1) \\(O(E + V\\log V)\\) 对于稠密图，E达到 \\(O(V^2)\\) 级别，使用 Fibonacci heap 实现的Prim算法比较合适。 下面的实现，起始有点模糊，抽象了一个 dist 数组。需要到实现过程中才能准确反映出他的意义。一个抽象的解释是，第 i 个节点到当前生成树的最小距离（权值最小的那条边的值）。将生成树看作一个整体的话，dist 数组中的有效值就是与生成树相连的所有边的权值。 通过 dist 数组，找到下一个生成树节点。然后更新加入新节点之后，新的 dist 数组。挺抽象的，但是还好，过程很短。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAX_N 1000const int INF = 0x3f3f3f3f;typedef struct Graph &#123; int n; int visited[MAX_N], dist[MAX_N]; int mat[MAX_N][MAX_N];&#125;Graph;void init(Graph *g, int input_n) &#123; g-&gt;n = input_n; for (int i = 0; i &lt; g-&gt;n; i++) &#123; for (int j = 0; j &lt; g-&gt;n; j++) &#123; g-&gt;mat[i][j] = INF; &#125; &#125;&#125;void insert(Graph *g, int x, int y, int weight) &#123; g-&gt;mat[x][y] = weight; g-&gt;mat[y][x] = weight;&#125;int prim(Graph *g, int v) &#123; int total_weight = 0; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; g-&gt;visited[i] = 0; g-&gt;dist[i] = INF; &#125; g-&gt;dist[v] = 0; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; int min_dist = INF, min_vertex; // 贪心搜索下一个生成树节点 for (int j = 0; j &lt; g-&gt;n; ++j) &#123; if (!g-&gt;visited[j] &amp;&amp; g-&gt;dist[j] &lt; min_dist) &#123; min_dist = g-&gt;dist[j]; min_vertex = j; &#125; &#125; total_weight += min_dist; g-&gt;visited[min_vertex] = 1; // 新加入节点后，更新dist数组 for (int j = 0; j &lt; g-&gt;n; ++j) &#123; if (!g-&gt;visited[j] &amp;&amp; g-&gt;mat[min_vertex][j] &lt; g-&gt;dist[j]) &#123; g-&gt;dist[j] = g-&gt;mat[min_vertex][j]; &#125; &#125; &#125; return total_weight;&#125;int main() &#123; int n, m; scanf(\"%d %d\", &amp;n, &amp;m); Graph *g = (Graph *)malloc(sizeof(Graph)); init(g, n); for (int i = 0; i &lt; m; i++) &#123; int a, b, c; scanf(\"%d %d %d\", &amp;a, &amp;b ,&amp;c); insert(g, a, b, c); &#125; printf(\"%d\\n\", prim(g, 0)); free(g); return 0;&#125; 实现中很容易发现，存在很多不必要的遍历，每次遍历都是从 0 到 n，显然存在优化的空间，以后遇到再说。 Kruskal 定义带权图 G 的边集合为 E，接着再定义最小生成树的边集合为 T，初始集合 T 都为空。 首先，把图 \\(G\\) 看成一个有 \\(n\\) 棵树的森林，图上每个顶点对应一棵树。 接着，将边集合 \\(E\\) 的每条边，按权值从小到大进行排序， 依次遍历每条边 \\(e = (u, v)\\)，记顶点 \\(u\\) 所在的树为 \\(T_u\\)，顶点 \\(v\\) 所在的树为 \\(T_v\\)，如果 \\(T_u\\) 和 \\(T_v\\) 不是同一棵树，则将边 \\(e\\) 加入集合 \\(T\\)，并将两棵树 \\(T_u\\) 和 \\(T_v\\) 进行合并。 算法执行完毕后，集合 \\(T\\) 记录了最小生成树的所有边。 贪心策略，每次都会选择一条两个顶点不在同一棵树且权值最小的边加入集合。 算法包括所有边权值排序和遍历所有边合并树。整体时间复杂度主要看排序部分 \\(O(E\\log(E))\\)。所以对于结点多但是边少的稀疏图，性能会比Prim算法好。 实现上，排序使用快速排序，合并验证使用并查集，。 最短路径 求一个起点到其余各个顶点的最短路径问题。 Dijkstra 定义带权图 G 所有顶点的集合为 V，接着再定义已确定最短路径的顶点集合为 U，初始集合 U 为空。 首先将起点 \\(x\\) 加入集合 \\(U\\)，并在数组 \\(A\\) 中记录起点 \\(x\\) 到各个点的最短路径（如果顶点到起点 \\(x\\) 有直接相连的边，则最短路径为边权值，否则为一个极大值）。 从数组 \\(A\\) 中选择一个距离起点 \\(x\\) 最近的，且不属于集合 \\(U\\) 的顶点 \\(v\\)（如果有多个顶点 \\(v\\)，任选其一即可），将顶点 \\(v\\) 加入集合 \\(U\\)，并更新所有与顶点 \\(v\\) 相连的顶点到起点 \\(x\\) 的最短路径。 重复第二步操作，直至集合 \\(U\\) 等于集合 \\(V\\)。 算法结束，数组 \\(A\\) 记录了起点 \\(x\\) 到其余 \\(n - 1\\) 个点的最短路径。 和Prim很类似，差别在于比较大小的目标是距离起点 x 的距离。同时算法不能处理有负权边的问题（这是Bellman Ford这类算法解决的问题）。 时间复杂度和Prim类似，在是使用 Fibonacci heap 优化后，为\\(O(E + V\\log V)\\)。 算法的关键就是更新当前最优结点的相邻结点的最短路径。（真的不喜欢贪心这个词，每次找到最值怎么就贪心了？贪心是已经得到最好，但是还是不满足的。） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define MAX_N 1000const int INF = 0x3f3f3f3f;typedef struct Graph &#123; int n; int visited[MAX_N], dist[MAX_N]; int mat[MAX_N][MAX_N];&#125;Graph;void init(Graph *g, int input_n) &#123; g-&gt;n = input_n; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; for (int j = 0; j &lt; g-&gt;n; j++) &#123; g-&gt;mat[i][j] = INF; &#125; &#125;&#125;void insert(Graph *g, int x, int y, int weight) &#123; g-&gt;mat[x][y] = weight; g-&gt;mat[y][x] = weight;&#125;void dijkstra(Graph *g, int v) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; g-&gt;visited[i] = 0; g-&gt;dist[i] = INF; &#125; g-&gt;dist[v] = 0; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; int min_d = INF, min_i; // i结点当前到起点的最短路径 for (int j = 0; j &lt; g-&gt;n; ++j) &#123; if (!g-&gt;visited[j] &amp;&amp; g-&gt;dist[j] &lt; min_d) &#123; min_d = g-&gt;dist[j]; min_i = j; &#125; &#125; g-&gt;visited[min_i] = 1; // 从已知最短路径的结点开始，计算相邻结点到起点的最短路径是否需要更新 for (int k = 0; k &lt; g-&gt;n; k++) &#123; if (!g-&gt;visited[k] &amp;&amp; g-&gt;mat[min_i][k] + min_d &lt; g-&gt;dist[k]) &#123; g-&gt;dist[k] = g-&gt;mat[min_i][k] + min_d; &#125; &#125; &#125;&#125;int main() &#123; int n, m; scanf(\"%d %d\", &amp;n, &amp;m); Graph *g = (Graph *)malloc(sizeof(Graph)); init(g, n); for (int i = 0; i &lt; m; i++) &#123; int a, b, c; scanf(\"%d %d %d\", &amp;a, &amp;b ,&amp;c); insert(g, a, b, c); &#125; int v; scanf(\"%d\", &amp;v); dijkstra(g, v); for (int i = 0; i &lt; n; i++) &#123; printf(\"%d: %d\\n\", i, g-&gt;dist[i]); &#125; free(g); return 0;&#125; 字符串匹配 这部分之前整理过，字符串匹配从KMP到AC自动机。但是这个东西就是容易忘。 基础的暴力搜索方法就不谈了，没啥可记的。 12345678910111213141516171819202122232425262728293031#include &lt;stdio.h&gt;#include &lt;string.h&gt;#define MAXLEN 1024int match(char *buffer, char *pattern) &#123; for (int i = 0; i &lt; strlen(buffer) - strlen(pattern) + 1; ++i) &#123; int j = 0; for (; j &lt; strlen(pattern); ++j) &#123; if (buffer[i + j] != pattern[j]) &#123; break; &#125; &#125; if (j == strlen(pattern)) &#123; return i; &#125; &#125; return -1;&#125;int main() &#123; char buffer[MAXLEN], pattern[MAXLEN]; scanf(\"%s%s\", buffer, pattern); int location = match(buffer, pattern); if (location != -1) &#123; printf(\"match success, location is %d\\n\", location); &#125; else &#123; printf(\"match failed!\\n\"); &#125; return 0;&#125; 注意 i &lt; strlen(buffer) - strlen(pattern) + 1，其中 + 1 才表示最后一段 pattern 长度的字符的起点index。 KMP 12S: aaaaaababaaaaaW: ababc 上例，abab匹配，c不匹配。暴力搜索会直接退回4个位置，从S中a的下一个b开始重新匹配W。 KMP的思想就是，不回退4，而是利用已经匹配过的字符，找到可以直接跳过，并且一定和W的一部分匹配的位置。比如上例中，第一次abab匹配，c不匹配，那么此时，根据W的规律，S中必然出现了abab。此时可以跳过S中a之后的第一个b，从第二个ab之后的位置开始匹配W的第一个ab之后的部分。 KMP就是分析W的规律，找到当每一个位置字符出现不匹配时，下一次匹配可以高效跳过的部分。 建立一个 Next 数组，记录在 W 中，拥有相同前缀的子串中最后一个字符的index值。就是下一次匹配开始的位置。注意其中一个子串是从 W 的第一个字符开始的，另一个字串位于 W 中间部分。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;stdio.h&gt;#include &lt;string.h&gt;#define MAX_LEN 100010void get_next(char *pat, int *next) &#123; next[0] = 0; for (int i = 1, j = 0; i &lt; strlen(pat); ++i) &#123; // j - 1 代表已经成功匹配的最后一个位置的index，j为待匹配的第一个位置 while (j &amp;&amp; pat[j] != pat[i]) &#123; j = next[j - 1]; &#125; if (pat[j] == pat[i]) &#123; j++; &#125; next[i] = j; &#125;&#125;int find(char *buffer, char *pat, int *next) &#123; // j 代表j - 1 及之前的部分已经匹配，匹配成功会等于 pat 的长度 for (int i = 0, j = 0; i &lt; strlen(buffer); ++i) &#123; while (j &amp;&amp; pat[j] != buffer[i]) &#123; j = next[j - 1]; &#125; if (pat[j] == buffer[i]) &#123; j++; &#125; if (j == strlen(pat)) &#123; return i - j + 1; &#125; &#125; return -1;&#125;int main() &#123; char buffer[MAX_LEN], pattern[MAX_LEN]; int next[MAX_LEN]; scanf(\"%s%s\", buffer, pattern); get_next(pattern, next); int location = find(buffer, pattern, next); if (location == -1) &#123; printf(\"No\\n\"); &#125; else &#123; printf(\"Yes\\n%d\\n\", location); &#125; return 0;&#125; Trie 多个模式字符串前缀树，处理一个输入串从多个模式字符串中进行匹配前缀的情况。 Trie 树有以下特点： Trie 树的根结点上不存储字符，其余结点上存且只存一个字符。 从根结点出发，到某一结点上经过的字符，即是该结点对应的前缀。 每个结点的孩子结点存储的字符各不相同。 Trie 树牺牲空间来换取时间，当数据量很大时，会占用很大空间。如果字符串均由小写字母组成，则每个结点最多会有 \\(26\\) 个孩子结点，则最多会有 \\(26^n\\) 个用于存储的结点，\\(n\\) 为字符串的最大长度。 Trie 树常用于字符串的快速检索，字符串的快速排序与去重，文本的词频统计等。查询效率对于长度为 n 的输入串，时间复杂为 O(n)。 下面程序利用Trie计算一段输入串 S 中不重复的字串个数。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;const int N = 100010;const int SIZE = 26;const char BASE = 'a';typedef struct TrieNode &#123; int is_terminal; struct TrieNode **childs; // 26个字母&#125; TrieNode, *Trie;TrieNode* new_node() &#123; Trie p = (Trie)malloc(sizeof(TrieNode)); p-&gt;childs = (Trie *)malloc(sizeof(Trie) *SIZE); for (int i = 0; i &lt; SIZE; i++) &#123; p-&gt;childs[i] = NULL; &#125; p-&gt;is_terminal = 0; return p;&#125;void clear(Trie t) &#123; if (t) &#123; for (int i = 0; i &lt; SIZE; ++i) &#123; if (t-&gt;childs[i]) &#123; clear(t-&gt;childs[i]); &#125; &#125; free(t-&gt;childs); free(t); &#125;&#125;// 增肌一个参数 res，累计一段字符串中不同字串的个数。void insert(Trie trie, char *pattern, int *res) &#123; TrieNode *p = trie; for (int i = 0; i &lt; strlen(pattern); ++i) &#123; if (p-&gt;childs[pattern[i] - BASE] == NULL) &#123; p-&gt;childs[pattern[i] - BASE] = new_node(); (*res)++; &#125; p = p-&gt;childs[pattern[i] - BASE]; &#125; p-&gt;is_terminal = 1;&#125;int find(Trie trie, char *buffer) &#123; TrieNode *p = trie; for (int i = 0; i &lt; strlen(buffer); i++) &#123; if (p-&gt;childs[buffer[i] - BASE] == NULL) &#123; return 0; &#125; p = p-&gt;childs[buffer[i] - BASE]; &#125; return p-&gt;is_terminal;&#125;int main() &#123; char s[100005]; scanf(\"%s\", s); // 计算累计一段字符串中不同字串的个数。 int res = 0; Trie root = new_node(); for (int i = 0; i &lt; strlen(s); ++i) &#123; insert(root, s + i, &amp;res); &#125; printf(\"%d\", res); clear(root); return 0;&#125; AC自动机 处理多个模式串，在一个输入串 S 中搜索出现的模式串的问题。 AC自动机约等于Trie + KMP，加速多pattern匹配过程。 构建 patterns 的 Trie 构建 fail 指针 开始匹配 构建 fail 指针 在Trie中，BFS遍历，第一层，fail指针都指向 root 第一层之后，每个节点的 fail 指针，指向 【该节点的父节点】 的 【fail指针指向的节点】 的 【儿子节点】中 【和该节点（自己）同字符的节点】。如果没有找到，【fail指针指向的节点】继续向上找 fail 节点，直到 root。 fail指针表示的是以当前字符作为开头的最长后缀所在的位置。 匹配过程 输入string s，trie从 root开始，进行匹配 当匹配失败，跳转到fail指针指向的节点，如果fail到 root，输入此位置之后的string s的部分，继续查找。 当匹配成功（Trie标记的节点），也跳转到fail指针指向的节点，如果此时跳转到 root，进行回溯到前一个最长的trie路径节点。 下面的程序，输入一个字符串，输出输入串中出现多少个已知pattern。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;const int SIZE = 26;const char BASE = 'a';const int MAX_SIZE = 20005;const int MAX_LEN = 25;typedef struct TrieNode &#123; int count; struct TrieNode** childs; struct TrieNode* fail;&#125; TrieNode, *Trie;TrieNode* new_node() &#123; TrieNode *p = (TrieNode *)malloc(sizeof(TrieNode)); p-&gt;childs = (TrieNode **)malloc(sizeof(TrieNode *) * SIZE); for (int i = 0; i &lt; SIZE; i++) &#123; p-&gt;childs[i] = NULL; &#125; p-&gt;fail = NULL; p-&gt;count = 0; return p;&#125;void clear(TrieNode *p) &#123; if (p != NULL) &#123; for (int i = 0; i &lt; SIZE; i++) &#123; if (p-&gt;childs[i] != NULL) &#123; clear(p-&gt;childs[i]); &#125; &#125; free(p-&gt;childs); free(p); &#125;&#125;// 构建Trie，并记录每个pattern出现次数，保存在每个node的count属性中void insert(Trie trie, char *buffer) &#123; TrieNode *p = trie; for (int i = 0; i &lt; strlen(buffer); i++) &#123; if (p-&gt;childs[buffer[i] - BASE] == NULL) &#123; p-&gt;childs[buffer[i] - BASE] = new_node(); &#125; p = p-&gt;childs[buffer[i] - BASE]; &#125; p-&gt;count++;&#125;// 构建fail指针void build_automaton(Trie root) &#123; root-&gt;fail = root; TrieNode *q[MAX_SIZE]; // 这里的BFS使用数组模拟queue int l = 0, r = 0; q[r++] = root; // BFS while (l &lt; r) &#123; TrieNode *now = q[l++]; for (int i = 0; i &lt; SIZE; ++i) &#123; if (now-&gt;childs[i] != NULL) &#123; TrieNode *child = now-&gt;childs[i]; if (now == root) &#123; // root指向自己 child-&gt;fail = root; &#125; else &#123; TrieNode *iter = now; // 找到fail指针中相同字符 while (iter != root &amp;&amp; iter-&gt;fail-&gt;childs[i] == NULL) &#123; iter = iter-&gt;fail; &#125; // 检查是否可以从fail指针的child开始向下匹配 if (iter-&gt;fail-&gt;childs[i] != NULL) &#123; child-&gt;fail = iter-&gt;fail-&gt;childs[i]; &#125; else &#123; child-&gt;fail = root; &#125; &#125; // BFS 每一层node q[r++] = child; &#125; &#125; &#125;&#125;int match_count(Trie root, const char *buffer) &#123; TrieNode *p = root; int total_count = 0; for (int i = 0; buffer[i]; ++i) &#123; // pattern不匹配，向fail指针回溯 while (p != root &amp;&amp; p-&gt;childs[buffer[i] - BASE] == NULL) &#123; p = p-&gt;fail; &#125; p = p-&gt;childs[buffer[i] - BASE]; if (p == NULL) &#123; p = root; &#125; // 累加匹配成功的Trie树路径上，有效pattern的数量，因为回溯停止时，停在最大匹配处 TrieNode *iter = p; while (iter != root) &#123; total_count += iter-&gt;count; iter = iter-&gt;fail; &#125; &#125; return total_count;&#125;int main() &#123; Trie root = new_node(); int n; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; ++i) &#123; char pattern[MAX_LEN]; scanf(\"%s\", pattern); insert(root, pattern); &#125; build_automaton(root); char str_buffer[100005]; scanf(\"%s\", str_buffer); printf(\"%d\\n\", match_count(root, str_buffer)); clear(root); return 0;&#125;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Data Structure","slug":"Notes/Data-Structure","permalink":"https://racleray.github.io/categories/Notes/Data-Structure/"}],"tags":[{"name":"data structure","slug":"data-structure","permalink":"https://racleray.github.io/tags/data-structure/"},{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"}],"author":"HeRui"},{"title":"数据结构整理-part2","slug":"数据结构整理-part2","date":"2021-10-08T13:30:33.000Z","updated":"2023-08-07T11:54:31.042Z","comments":true,"path":"posts/4121feda.html","link":"","permalink":"https://racleray.github.io/posts/4121feda.html","excerpt":"常见基本数据结构整理第二部分，使用C语言编写。","text":"哈希表 将key的值通过一个设计好的hash函数转换，在连续内存地址上寻址找到对应的值。 hash函数构造方法： 直接寻址法：将key值线性的映射到存储地址上。少量key适用。 除留余数法：将key值对整数 p 取的余数直接做为存储地址。一般选择不大于 p 的最大质数。 字符串hash的 BKD hash。 ... hash函数设计要求计算简单，值尽量均匀分布。 冲突处理 开放地址法 如果发生冲突，那么就使用某种策略寻找下一存储地址，直到找到一个不冲突的地址或者找到关键字，否则一直按这种策略继续寻找。如果冲突次数达到了上限则终止程序，表示关键字不存在哈希表里。 线性探测法，如果当前的冲突位置为 \\(d\\)，那么接下来几个探测地址为 \\(d + 1\\)，\\(d + 2\\)，\\(d + 3\\) 等，也就是从冲突地址往后面一个一个探测； 线性补偿探测法，它形成的探测地址为 \\(d + m\\)，\\(d + 2 \\times m\\)，\\(d + 3 \\times m\\) 等，与线性探测法不同，这里的查找单位不是 \\(1\\)，而是 \\(m\\)，为了能遍历到哈希表里所有位置，设置 \\(m\\) 和表长 \\(size\\) 互质； 随机探测法，这种方法和前两种方法类似，这里的查找单位不是一个固定值，而是一个随机序列。 二次探测法，它形成的探测地址为 \\(d + 1^2\\)，\\(d - 1^2\\)，\\(d + 2^2\\)，\\(d - 2^2\\) 等，这种方法在冲突位置左右跳跃着寻找探测地址。 规定，当冲突次数小于表长的一半时，就可以把字符串插入到哈希表中。而如果发生冲突次数大于表长的一半时，就需要调用重建函数去重建哈希表了，因为认为哈希表已经发生了“堆聚”现象，这种情况下要扩大哈希表的容量，提高查找和插入的效率。 开放地址法计算简单快捷，处理起来方便，但是也存在不少缺点。 线性探测法容易形成“堆聚”的情况，即很多记录就连在一块，而且一旦形成“堆聚”，记录会越聚越多。 另外，开放地址法都有一个缺点，删除操作显得十分复杂，不能直接删除关键字所在的记录，否则在查找删除位置后面的元素时，可能会出现找不到的情况，因为删除位置上已经成了空地址，查找到这里时会终止查找。 链地址法 该方法将所有哈希地址相同的结点构成一个单链表，单链表的头结点存在哈希数组里。链地址法常出现在经常插入和删除的情况下。 相比开放地址法，链地址法有以下优点：不会出现“堆聚”现象，哈希地址不同的关键字不会发生冲突；不需要重建哈希表，在开放地址法中，如果哈希表里存满关键字了就需要扩充哈希表然后重建哈希表，而在链地址法里，因为结点都是动态申请的，所以不会出现哈希表里存满关键字的情况；相比开放地址法，关键字删除更方便，只需要找到指定结点，删除该结点即可。 当然代价就是查找效率会稍低一点，但是对于hash来讲这一点效率影响在大多数非极端条件下，使用体验基本没有啥区别。 当关键字规模少的时候，开放地址法比链地址法更节省空间，因为用链地址法可能会存在哈希数组出现大量空地址的情况，而在关键字规模大的情况下，链地址法就比开放地址法更节省空间，链表产生的指针域可以忽略不计，关键字多，哈希数组里产生的空地址就少了。 实现线性探测法hash 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;typedef struct HashTable &#123; char **elem; int size;&#125; HashTable;void init(HashTable *h);int hash(HashTable *h, char value[]);int search(HashTable *h, char value[], int *pos, int *times);int insert(HashTable *h, char value[]);void recreate(HashTable *h);void init(HashTable *h) &#123; h-&gt;size = 2000; h-&gt;elem = (char **)malloc(sizeof(char *) * h-&gt;size); for (int i = 0; i &lt; h-&gt;size; i++) &#123; h-&gt;elem[i] = NULL; &#125;&#125;int hash(HashTable *h, char value[]) &#123; int code = 0; for (size_t i = 0; i &lt; strlen(value); i++) &#123; code = (code * 256 + value[i] + 128) % h-&gt;size; &#125; return code;&#125;int search(HashTable *h, char value[], int *pos, int *times) &#123; *pos = hash(h, value); *times = 0; while (h-&gt;elem[*pos] != NULL &amp;&amp; strcmp(h-&gt;elem[*pos], value) != 0) &#123; (*times)++; if (*times &lt; h-&gt;size) &#123; *pos = (*pos + 1) % h-&gt;size; &#125; else &#123; return 0; &#125; &#125; if (h-&gt;elem[*pos] != NULL &amp;&amp; strcmp(h-&gt;elem[*pos], value) == 0) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;int insert(HashTable *h, char value[]) &#123; int pos, times; // pos在search函数结束时时待插入元素的位置，times为冲突计数 if (search(h, value, &amp;pos, &amp;times)) &#123; return 2; &#125; else if (times &lt; h-&gt;size / 2) &#123; h-&gt;elem[pos] = (char *)malloc(strlen(value) + 1); strcpy(h-&gt;elem[pos], value); return 1; &#125; else &#123; recreate(h); insert(h, value); return 0; &#125;&#125;// 当插入值的冲突数大于size的一半时，增大哈希表。void recreate(HashTable *h) &#123; // 获取原哈希表中元素 char **temp_elem; temp_elem = (char **)malloc(sizeof(char *) * h-&gt;size); for (int i = 0; i &lt; h-&gt;size; i++) &#123; if (h-&gt;elem[i] != NULL) &#123; temp_elem[i] = (char *)malloc(strlen(h-&gt;elem[i]) + 1); strcpy(temp_elem[i], h-&gt;elem[i]); &#125; else &#123; temp_elem[i] = NULL; &#125; &#125; // 释放原哈希表中元素 for (int i = 0; i &lt; h-&gt;size; i++) &#123; if (h-&gt;elem[i] != NULL) &#123; free(h-&gt;elem[i]); &#125; &#125; free(h-&gt;elem); // 创建新哈希表 int copy_size = h-&gt;size; h-&gt;size = h-&gt;size * 2; h-&gt;elem = (char **)malloc(sizeof(char *) * h-&gt;size); for (int i = 0; i &lt; h-&gt;size; i++) &#123; h-&gt;elem[i] = NULL; &#125; for (int i = 0; i &lt; copy_size; i++) &#123; if (temp_elem[i] != NULL) &#123; insert(h, temp_elem[i]); &#125; &#125; // 释放内存 for (int i = 0; i &lt; copy_size; i++) &#123; if (temp_elem[i] != NULL) &#123; free(temp_elem[i]); &#125; &#125; free(temp_elem);&#125;void clear(HashTable *h) &#123; for (int i = 0; i &lt; h-&gt;size; ++i) &#123; if (h-&gt;elem[i] != NULL) &#123; free(h-&gt;elem[i]); &#125; &#125; free(h-&gt;elem); free(h);&#125;int main() &#123; HashTable *hashtable = (HashTable*)malloc(sizeof(HashTable)); init(hashtable); char buffer[1000]; int n; scanf(\"%d\", &amp;n); for (int i = 1; i &lt;= n; i++) &#123; scanf(\"%s\", buffer); int ans = insert(hashtable, buffer); if (ans == 0) &#123; printf(\"recreate while insert!\\n\"); &#125; else if (ans == 1) &#123; printf(\"insert success!\\n\"); &#125; else if (ans == 2) &#123; printf(\"It already exists!\\n\"); &#125; &#125; int temp_pos, temp_times; scanf(\"%s\", buffer); if (search(hashtable, buffer, &amp;temp_pos, &amp;temp_times)) &#123; printf(\"search success!\\n\"); &#125; else &#123; printf(\"search failed!\\n\"); &#125; clear(hashtable); return 0;&#125; 实现链地址法 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#include &lt;time.h&gt;typedef struct Node &#123; char *str; struct Node *next;&#125; Node;typedef struct HashTable &#123; Node **data; int size;&#125; HashTable;Node *initNode(const char *str) &#123; Node *n = (Node *)malloc(sizeof(Node)); //n-&gt;str = (char *)malloc(strlen(str) + 1); //strcpy(n-&gt;str, str); // 等价操作 n-&gt;str = strdup(str); n-&gt;next = NULL; return n;&#125;void freeNode(Node *n) &#123; if (!n) return; free(n-&gt;str); free(n); return;&#125;void freeList(Node *list) &#123; if (!list) return; Node *n; while (list) &#123; n = list; list = list-&gt;next; freeNode(n); &#125; return;&#125;HashTable *initHashTable(int size) &#123; HashTable *h = (HashTable *)malloc(sizeof(HashTable)); h-&gt;size = 2 * size; h-&gt;data = (Node **)calloc(h-&gt;size, sizeof(Node*));&#125;void freeHashTable(HashTable *h) &#123; if (!h) return; for (int i = 0; i &lt; h-&gt;size; i++) &#123; freeList(h-&gt;data[i]); &#125; free(h-&gt;data); free(h); return;&#125;Node *insertNode(Node *head, Node *p) &#123; p-&gt;next = head; return p;&#125;int BKDHash(const char *str) &#123; int seed = 11; // prime int hash = 0; while (*str) &#123; hash = hash * seed + str[0]; ++str; &#125; return hash &amp; 0x7fffffff;&#125;int insertHash(HashTable *h, const char *str) &#123; if (h == NULL) return 0; int hash = BKDHash(str); int idx = hash % h-&gt;size; h-&gt;data[idx] = insertNode(h-&gt;data[idx], initNode(str)); return 1;&#125;Node *searchList(Node *p, const char *str) &#123; while (p &amp;&amp; strcmp(p-&gt;str, str)) &#123; p = p-&gt;next; &#125; return p;&#125;Node *search(HashTable *h, const char *str) &#123; if (h == NULL) return NULL; int hash = BKDHash(str); int idx = hash % h-&gt;size; return searchList(h-&gt;data[idx], str);&#125;char *makeStr(char *str, int n) &#123; int len = rand() % (n - 1) + 1; char tmp; for (int i = 0; i &lt; len; i++) &#123; switch (rand() % 3) &#123; case 0: tmp = 'A' + rand() % 26; break; case 1: tmp = 'a' + rand() % 26; break; case 2: tmp = '0' + rand() % 10; break; &#125; str[i] = tmp; &#125; str[n] = '\\0'; // or str[n] = 0; return str;&#125;int main(void) &#123; srand(time(NULL)); #define N 10 char str[10]; int cnt = N; HashTable *h = initHashTable(N); while (cnt--) &#123; insertHash(h, makeStr(str, 10)); printf(\"%s \", str); &#125; putchar(10); // or putchar('\\n'); while (~scanf(\"%s\", str)) &#123; if (!strcmp(str, \"q\")) break; // q 退出 Node *node = search(h, str); if (node) printf(\"hash=%d addr=%p str=%s\\n\", BKDHash(str), node, node-&gt;str); else printf(\"%s not found\\n\", str); &#125; #undef N return 0;&#125; 查找表 用于查找的数据集合则称为 查找表（search table） 。查找表中的数据元素类型是一致的，并且有能够唯一标识出元素的 关键字（keyword） 。 通常对查找表有 4 种操作： 查找：在查找表中查看某个特定的记录是否存在 检索：查找某个特定记录的各种属性 插入：将某个不存在的数据元素插入到查找表中 删除：从查找表中删除某个特定元素 如果对查找表只执行前两种操作，则称这类查找表为 静态查找表（static search table） 。静态查找表建立以后，就不能再执行插入或删除操作，查找表也不再发生变化。比如顺序查找、折半查找、分块查找等。 对应的，如果对查找表还要执行后两种操作，则称这类查找表为 动态查找表（dynamic search table） 。对于动态查找表，往往使用二叉平衡树、B-树或哈希表来处理。 性能度量 使用 平均查找长度（average search length, ASL） 来衡量查找算法的性能。 假设每个元素时等概率分布，ASL就是所有元素被成功查找时，元素比较次数的期望。 通常平均查找长度，不考虑查找不成功的情况。 顺序查找 顺序查找（又称线性查找，sequential search） 是指在线性表中进行的查找算法。对于无序序列，查找成功的ASL为 \\(\\frac{n+1}{2}\\)。查找不成功ASL为 n。 对于有序序列，查找成功的ASL为 \\(\\frac{n+1}{2}\\)。查找不成功ASL为 \\(\\frac{n}{2} +\\frac{n}{n+1}\\)。 12345678910111213141516171819#include &lt;stdio.h&gt;int search(int *data, int length, int value) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (data[i] == value) &#123; return i; &#125; else if (data[i] &gt; value) &#123; return -1; &#125; &#125; return -1;&#125;int main() &#123; int a[5] = &#123;2, 4, 6, 8, 10&#125;; printf(\"%d\\n\", search(a, 5, 4)); printf(\"%d\\n\", search(a, 5, 5)); return 0;&#125; 折半查找 针对有序可随机寻址的顺序表结构，查找时间复杂度稳定下降为 \\(O(log(n))\\) 级别。 在定义端点下标时，决定了搜索区间的分布情况。右端定义为length - 1，搜索区间为闭区间。右端收缩为 mid - 1。右端定义为length，搜索区间为左闭右开，右端收缩为 mid。 1234567891011121314151617181920212223#include &lt;stdio.h&gt;int search(int *data, int length, int value) &#123; int left = 0, right = length - 1; while (left &lt;= right) &#123; int mid = (left + right) / 2; if (data[mid] == value) &#123; return mid; &#125; else if (data[mid] &lt; value) &#123; left = mid + 1; &#125; else &#123; right = mid - 1; &#125; &#125; return -1;&#125;int main() &#123; int a[5] = &#123;2, 4, 6, 8, 10&#125;; printf(\"%d\\n\", search(a, 5, 10)); printf(\"%d\\n\", search(a, 5, 5)); return 0;&#125; 搜索排好序的序列中某重复元素第一个出现位置的问题。 123456789101112int search(int *data, int length, int value) &#123; int left = 0, right = length; // 处理不存在目标情况 while (left &lt; right) &#123; int mid = (left + right) / 2; if (data[mid] == value) &#123; right = mid; // 收缩到第一个出现位置 &#125; else &#123; left = mid + 1; &#125; &#125; return right != length ? left : -1; // 处理不存在目标情况&#125; 搜索排好序的序列中某重复元素最后一个出现位置的问题。 123456789101112int search(int *data, int length, int value) &#123; int left = -1, right = length - 1; // left = -1 处理不存在目标情况 while (left &lt; right) &#123; int mid = (left + right + 1) / 2; // 保证left不会恒定为一个值，所以 + 1 if (data[mid] == value) &#123; left = mid; // 收缩到第一个出现位置 &#125; else &#123; right = mid - 1; &#125; &#125; return left; // 不存在返回 -1&#125; 三分查找 适合序列具有凸性的情况，某一点两侧分别是单调的。 首先将区间 \\([L, R]\\) 平均分成三部分：\\([L, m_1]\\)、\\([m_1, m_2]\\)、\\([m_2, R]\\)。 计算三等分点 \\(m_1\\) 和 \\(m_2\\) 对应的函数值 \\(f(m_1)\\) 和 \\(f(m_2)\\)。 比较 \\(f(m_1)\\) 和 \\(f(m_2)\\) 的大小。 如果 \\(f(m_1) &gt; f(m_2)\\)，则说明点 \\(T\\) 一定不在区间 \\([m_2, R]\\) 内，可以把右边界 \\(R\\) 更新成 \\(m_2\\)； 如果 \\(f(m_1) &lt; f(m_2)\\)，则说明点 \\(T\\) 一定不在区间 \\([L, m_1]\\) 内，可以把左边界 \\(L\\) 更新成 \\(m_1\\)； 如果 \\(f(m_1) = f(m_2)\\)，则说明点 \\(T\\) 一定落在区间 \\([m_1, m_2]\\) 内。另外，可以将这种情况归为上面两种情况的任一一种。 重复以上操作，不断缩小查找区间，直到在精度要求的范围内，左边界 \\(L\\) 等于右边界 \\(R\\)，这时的边界点 \\((L, f(L))\\) 或者 \\((R, f(R))\\) 即是查找的极大值点 \\(T\\)。 同理，如果求凹性函数的极小值点，只需要在第三步中，将大于号和小于号反向变化一下即可。 12345678910111213141516171819202122232425#include &lt;stdio.h&gt;int find_max(int *data, int length) &#123; int left = 0, right = length - 1; while (right - left &gt;= 2) &#123; int m1 = left + (right - left) / 3; int m2 = right - (right - left + 2) / 3; if (data[m1] &gt;= data[m2]) &#123; right = m2; &#125; else &#123; left = m1 + 1; &#125; &#125; if (data[left] &gt; data[right]) &#123; return left; &#125; else &#123; return right; &#125;&#125;int main() &#123; int a[5] = &#123;1, 2, 7, 5, 4&#125;; printf(\"%d\\n\", find_max(a, 5)); return 0;&#125; 分块查找 分块查找（Blocking Search）的基本思想是将一个线性表分成若干个子表，在查找时，先确定目标元素所在的子表再在该子表中去查找它。 分块查找也被叫做 索引顺序查找 ，在分块查找方法中，需要建立一个 索引表 。索引表中包含两类信息：关键字和指针。关键字指的每个子表中最大的关键字，指针则表示该子表中第一个元素在整个表中的下标。 分块查找要求整个线性表是分块有序的。第 i 个子表中最大的关键字小于第 i+1 个子表中最小的关键字。 而在每一个子表中，元素的排列是随意的，只能通过顺序查找的方法在子表中完成最终的查找工作。 分块查找的效率是基于顺序查找和折半查找之间的。先搜索引，再顺序搜索子表。 分块查找的优势在于，由于子块中的元素是随意排序的，只要找到对应的块就能直接进行插入和删除操作，而不用大量移动其它的元素，因此它适用于线性表需要频繁的动态变化的情况。 分块查找的缺点则在于它需要一定的内存空间来存放索引表并且要求对索引表进行排序。 C++ STL中的deque就是使用这种分块的“连续”结构设计思路。 排序 根据算法的时间复杂度，可以将排序算法分为复杂度为 \\(O(n^2),\\ O(n\\log(n)),\\ O(n)\\)等时间复杂度的排序算法。比如 \\(O(n)\\) 的 基数排序（radix sort）、 \\(O(n\\log(n))\\)的归并排序、\\(O(n^2)\\)的冒泡排序。 根据排序过程中元素是否完全保存在内存中，可以将算法分为 内部排序（internal sort） 和 外部排序（external sort） 。 对于一个排序算法，如果任意两个元素 \\(a_i\\) 和 \\(a_j\\) 在排序前的线性表中满足条件 i&lt;j 且 \\(a_i = a_j\\)，在排序后 \\(a_i\\) 仍在 \\(a_j\\) 之前，则称这个排序算法为 稳定排序（stable sort） (比如插入、冒泡、归并)，否则称这个排序算法为 不稳定排序(unstable sort) （选择、快速、堆、希尔排序）。 插入排序 将线性表分为已排序的前半部分和待排序的后半部分，从待排序部分选出第一个元素，插入到已排序部分的对应位置中，直到全部记录都插入到已排序部分中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#include &lt;stdio.h&gt;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void print(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", data[i]); &#125; printf(\"\\n\");&#125;void sort(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; for (int j = i - 1; j &gt;= 0; --j) &#123; if (data[j] &gt; data[j + 1]) &#123; swap(&amp;data[j], &amp;data[j + 1]); &#125; else &#123; break; &#125; &#125; &#125;&#125;void sort_better(int *data, int length) &#123; int cur; for (int i = 1; i &lt; length; i++) &#123; cur = data[i]; int j = i - 1; while (j &gt;= 0 &amp;&amp; data[j] &gt; cur) &#123; data[j + 1] = data[j]; j--; &#125; data[j + 1] = cur; &#125;&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); int arr[n]; for (int i = 0; i &lt; n; ++i) &#123; scanf(\"%d\", &amp;arr[i]); &#125; sort(arr, n); print(arr, n); return 0;&#125; 冒泡排序 从前往后两两比较相邻元素的关键字， 按照目标大小顺序进行交换元素，每趟交换以后最后一个元素一定是最大的，不再参与下一趟交换。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;stdio.h&gt;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void print(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", data[i]); &#125; printf(\"\\n\");&#125;void sort(int *data, int length) &#123; for (int i = 0; i &lt; length - 1; ++i) &#123; int swapped = 0; for (int j = 0; j &lt; length - i - 1; ++j) &#123; if (data[j] &gt; data[j + 1]) &#123; swap(&amp;data[j], &amp;data[j + 1]); swapped = 1; &#125; &#125; if (swapped == 0) &#123; break; &#125; &#125;&#125;void sort_better(int *data, int len) &#123; int isSorted = 0; do &#123; isSorted = 1; len--; for (int i=0; i&lt;len; ++i) &#123; if (data[i] &gt; data[i+1]) &#123; isSorted = 0; swap(&amp;data[i], &amp;data[i+1]); &#125; &#125; &#125; while (!isSorted);&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); int arr[n]; for (int i = 0; i &lt; n; ++i) &#123; scanf(\"%d\", &amp;arr[i]); &#125; sort(arr, n); print(arr, n); return 0;&#125; 归并排序 设法将两个有序的线性表组合成一个新的有序线性表。为了实现归并操作，每次合并都需要开辟额外的空间来临时保存合并后的排序结果，总共需要开辟 n 个元素的空间，所以归并排序的空间复杂度为 \\(O(n)\\)。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950#include &lt;stdio.h&gt;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void print(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", data[i]); &#125; printf(\"\\n\");&#125;void mergesort(int *data, int l, int r) &#123; if (l == r) return; int mid = (l + r) / 2; mergesort(data, l, mid); mergesort(data, mid + 1, r); int temp[r - l + 1]; // 或者从堆分配空间 int x = l, y = mid + 1, loc = 0; while (x &lt;= mid || y &lt;= r) &#123; if (x &lt;= mid &amp;&amp; (y &gt; r || data[x] &lt;= data[y])) temp[loc] = data[x++]; else temp[loc] = data[y++]; loc++; &#125; for (int i = l; i &lt;= r; i++) &#123; data[i] = temp[i - l]; &#125;&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); int arr[n]; for (int i = 0; i &lt; n; ++i) &#123; scanf(\"%d\", &amp;arr[i]); &#125; merge_sort(arr, 0, n - 1); print(arr, n); return 0;&#125; 选择排序 选择排序，每趟从线性表的待排序区域选取关键字最小的元素，将其放到已排序区域的最后。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;stdio.h&gt;inline void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void print(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", data[i]); &#125; printf(\"\\n\");&#125;void sort(int *data, int length) &#123; int temp; for (int i = 0; i &lt; length - 1; ++i) &#123; temp = i; for (int j = i + 1; j &lt; length; ++j) &#123; if (data[temp] &gt; data[j]) &#123; temp = j; &#125; &#125; if (i != temp) &#123; swap(&amp;data[i], &amp;data[temp]); &#125; &#125;&#125;void sort_better(int *data, int len) &#123; if (len &lt;= 1) return; register int *last = data + len - 1, *p, *minPtr; for ( ; data &lt; last; ++data) &#123; minPtr = data; for (p = data + 1; p &lt;= last; ++p) &#123; if (*p &lt; *minPtr) minPtr = p; &#125; swap(minPtr, data); &#125;&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); int arr[n]; for (int i = 0; i &lt; n; ++i) &#123; scanf(\"%d\", &amp;arr[i]); &#125; sort(arr, n); print(arr, n); return 0;&#125; 在每次查找关键字最小的元素时，可以使用堆对效率进行优化，使用堆来优化的选择排序就是堆排序。 快速排序 快速排序是目前应用最广泛的排序算法之一。它的基本思想是，每次从待排序区间选取一个元素（在后面的课程中都是选取第一个）作为基准记录，所有比基准记录小的元素都在基准记录的左边，而所有比基准记录大的元素都在基准记录的右边。之后分别对基准记录的左边和右边两个区间进行快速排序，直至将整个线性表排序完成。 quick_sort_better 实现了单边递归，将两个递归函数，优化成一个。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#include &lt;stdio.h&gt;void swap(int *a, int *b) &#123; int temp = *a; *a = *b; *b = temp;&#125;void print(int *data, int length) &#123; for (int i = 0; i &lt; length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", data[i]); &#125; printf(\"\\n\");&#125;void quick_sort(int *data, int left, int right) &#123; if (left &gt; right) &#123; return; &#125; int pivot = data[left], low = left, high = right; while (low &lt; high) &#123; while (low &lt; high &amp;&amp; data[high] &gt;= pivot) &#123; high--; &#125; data[low] = data[high]; while (low &lt; high &amp;&amp; data[low] &lt;= pivot) &#123; low++; &#125; data[high] = data[low]; &#125; data[low] = pivot; quick_sort(data, left, low - 1); quick_sort(data, low + 1, right);&#125;// quick sort 优化void quick_sort_better(int *data, int l, in r) &#123; if (l &gt;= r) return; while (l &lt; r) &#123; int x = l, y = r, pivot = data[(x + (y - x)) / 2]; while (x &lt;= y) &#123; while (data[x] &lt; pivot) ++x; while (data[y] &gt; pivot) --y; if (x &lt;= y) &#123; swap(&amp;data[x++], &amp;data[y--]); &#125; &#125; quick_sort_better(data, l, y); l = x; // 直接在当前函数内，快排后半部分 &#125; return;&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); int arr[n]; for (int i = 0; i &lt; n; ++i) &#123; scanf(\"%d\", &amp;arr[i]); &#125; quick_sort(arr, 0, n - 1); print(arr, n); return 0;&#125; 二叉查找树 对任意结点，如果左子树不为空，则左子树上所有结点的权值都小于该结点的权值；如果右子树不为空，则右子树上所有结点的权值都大于该结点的权值。 如果中序遍历二叉查找树，会得到一个从小到大的序列。 在二叉查找树的基础上可以加些优化，可以让其成为 AVL 树，红黑树，SBT，Splay 等等，这些高级的树结构解决了二叉树退化的问题。 插入过程： 根节点为空则新元素直接作为根节点，否则传入的参数 value 与根节点进行比较。 value 等于当前节点则直接返回，小于则跳转到步骤 3，而如果 value 大于当前节点时，跳转到步骤 4。 判断当前节点是否存在左孩子，如果存在则让其左孩子继续调用插入方法，回到步骤 2，如果不存在则将新元素插入到当前节点的左孩子位置上。 判断当前节点是否存在右孩子，存在则让其右子树继续调用插入方法，回到步骤 2，不存在则将新元素插入到当前节点的右孩子位置上。 简单实现插入和查找，输出中序遍历结果。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Node &#123; int data; struct Node *lchild, *rchild, *father;&#125;Node;Node* init(int data, Node *father) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;data = data; node-&gt;lchild = NULL; node-&gt;rchild = NULL; node-&gt;father = father; return node;&#125;Node* insert(int value, Node *tree) &#123; if (!tree) &#123; tree = init(value, NULL); return tree; &#125; if (value &gt; tree-&gt;data) &#123; if (!tree-&gt;rchild) tree-&gt;rchild = init(value, tree); else tree-&gt;rchild = insert(value, tree-&gt;rchild); &#125; else &#123; if (!tree-&gt;lchild) tree-&gt;lchild = init(value, tree); else tree-&gt;lchild = insert(value, tree-&gt;lchild); &#125; return tree;&#125;Node* search(int value, Node *tree) &#123; if (!tree || value == tree-&gt;data) return tree; if (value &gt; tree-&gt;data) &#123; if (!tree-&gt;rchild) return NULL; else return search(value, tree-&gt;rchild); &#125; else &#123; if (!tree-&gt;lchild) return NULL; else return search(value, tree-&gt;lchild); &#125;&#125;void inorder(Node *tree) &#123; if (!tree) return; inorder(tree-&gt;lchild); printf(\"%d \", tree-&gt;data); inorder(tree-&gt;rchild);&#125;void clear(Node *node) &#123; if (node != NULL) &#123; if (node-&gt;lchild != NULL) &#123; clear(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; clear(node-&gt;rchild); &#125; free(node); &#125;&#125;int main() &#123; Node *binarytree = NULL; init(100, binarytree); int n; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; ++i) &#123; int v; scanf(\"%d\", &amp;v); binarytree = insert(v, binarytree); &#125; int value; scanf(\"%d\", &amp;value); if (search(value, binarytree)) &#123; printf(\"search success!\\n\"); &#125; else &#123; printf(\"search failed!\\n\"); &#125; (void)inorder(binarytree); clear(binarytree); return 0;&#125; 删除操作 节点的前驱指的是值比它小的节点中最大的一个节点，后继是指值比它大的节点中最小的一个。 查找节点前驱的算法流程如下： 找到当前节点的左孩子，如果当前节点没有左孩子则不存在前驱，若存在，则找到其左孩子的右孩子。 若当前节点有右孩子则继续找到其右孩子，重复步骤 2，直至找到一个节点不存在右孩子时，那么它就是要查找的前驱。 删除操作的算法流程如下： 找到被删除的节点。 若它存在左孩子，则找到他的前驱，用前驱替换被删除节点的值，再调用删除节点的方法删除前驱。 若被删除节点不存在左孩子，则找到它的后继，用后继替换被删除节点的值，再调用删除节点的方法删除后继。 若被删除的节点不存在孩子节点，直接调用删除节点的的方法删除它。 实现删除 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct Node &#123; int data; struct Node *lchild, *rchild, *father;&#125;Node;Node* init(int _data, Node *_father) &#123; Node *node =(Node *)malloc(sizeof(Node)); node-&gt;data = _data; node-&gt;lchild = NULL; node-&gt;rchild = NULL; node-&gt;father = _father; return node;&#125;Node* insert(Node *node, int value) &#123; if (node == NULL) &#123; node = init(value, NULL); &#125; else if (value &gt; node-&gt;data) &#123; if (node-&gt;rchild == NULL) &#123; node-&gt;rchild = init(value, node); &#125; else &#123; node-&gt;rchild = insert(node-&gt;rchild, value); &#125; &#125; else if (value &lt; node-&gt;data) &#123; if (node-&gt;lchild == NULL) &#123; node-&gt;lchild = init(value, node); &#125; else &#123; node-&gt;lchild = insert(node-&gt;lchild, value); &#125; &#125; return node;&#125;Node* search(Node *node, int value) &#123; if (node == NULL || node-&gt;data == value) &#123; return node; &#125; else if (value &gt; node-&gt;data) &#123; if (node-&gt;rchild == NULL) &#123; return NULL; &#125; else &#123; return search(node-&gt;rchild, value); &#125; &#125; else &#123; if (node-&gt;lchild == NULL) &#123; return NULL; &#125; else &#123; return search(node-&gt;lchild, value); &#125; &#125;&#125;Node* predecessor(Node *node) &#123; if (!node-&gt;lchild) return NULL; Node *res = node-&gt;lchild; while (res &amp;&amp; res-&gt;rchild) &#123; res = res-&gt;rchild; &#125; return res;&#125;Node* successor(Node *node) &#123; if (!node-&gt;rchild) return NULL; Node *res = node-&gt;rchild; while (res &amp;&amp; res-&gt;lchild) &#123; res = res-&gt;lchild; &#125; return res;&#125;// 删除度为1或者0的节点，在delete_tree函数中使用。void remove_node(Node *node) &#123; if (!node) return; Node *tmp = NULL; if (node-&gt;lchild) &#123; tmp = node-&gt;lchild; tmp-&gt;father = node-&gt;father; &#125; if (node-&gt;rchild) &#123; tmp = node-&gt;rchild; tmp-&gt;father = node-&gt;father; &#125; if (node-&gt;father-&gt;lchild == node) &#123; node-&gt;father-&gt;lchild = tmp; &#125; else &#123; node-&gt;father-&gt;rchild = tmp; &#125; node-&gt;rchild = NULL; node-&gt;lchild = NULL; node-&gt;father = NULL; free(node);&#125;// 根据输入值删除节点int delete_tree(Node *tree, int value) &#123; if (!tree) return ERROR; Node *target = search(tree, value); if (!target) return ERROR; Node *tmp = NULL; if (target-&gt;lchild) &#123; tmp = predecessor(target); &#125; else if (target-&gt;rchild) &#123; tmp = successor(target); &#125; else &#123; tmp = target; &#125; target-&gt;data = tmp-&gt;data; remove_node(tmp); return OK;&#125;void clear(Node *node) &#123; if (node != NULL) &#123; if (node-&gt;lchild != NULL) &#123; clear(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; clear(node-&gt;rchild); &#125; free(node); &#125;&#125;int main() &#123; Node *binarytree = NULL; int arr[10] = &#123; 8, 9, 10, 3, 2, 1, 6, 4, 7, 5 &#125;; for (int i = 0; i &lt; 10; i++) &#123; binarytree = insert(binarytree, arr[i]); &#125; int value; scanf(\"%d\", &amp;value); if (search(binarytree, value) != NULL) &#123; printf(\"search success!\\n\"); &#125; else &#123; printf(\"search failed!\\n\"); &#125; scanf(\"%d\", &amp;value); if (delete_tree(binarytree, value)) &#123; printf(\"delete success!\\n\"); &#125; else &#123; printf(\"delete failed!\\n\"); &#125; clear(binarytree); return 0;&#125; 平衡二叉树 所有平衡二叉查找树基本由以下三个特征组成： 自平衡条件 旋转操作 旋转的触发 平衡二叉查找树通过设置合理的自平衡条件，使得二叉查找树的查找、插入等操作的性能不至于退化成 \\(O(n)\\). AVL 树是最早发明的一种平衡二叉查找树。 AVL AVL 树提出了一个概念： 平衡因子（balance factor） 。每个结点的平衡因子是指它左子树最大高度和右子树最大高度的差。 在 AVL 树中，平衡因子为 −1、 0、 1 的结点都被认为是平衡的，而平衡因子为 −2、 2 等其他值的结点被认为是不平衡的，需要对这个结点所在子树进行调整。 旋转 在 AVL 树中，一共有两种单旋操作：左旋和右旋。AVL 树通过一系列左旋和右旋操作，将不平衡的树调整为平衡二叉查找树。 通过进行左旋操作，使得原先的根 2 变成了其右孩子 4 的左孩子，而 4 原先的左孩子 3 变成了 2 的右孩子。 通过右旋操作，使得原先的根 5 变成了其左孩子 3 的右孩子，而 3 原先的右孩子变成了 5 的左孩子。 AVL 树中还有两种复合旋转操作（即“多旋”），由两次单旋操作组合而成。 左旋加右旋： 右旋加左旋： 旋转的触发 在插入一个元素后不断回溯的过程中，如果因此导致结点不平衡，则根据不平衡情况（一定是一边子树比另一边子树的高度大 2）进行对应的旋转： 左子树比右子树的高度大 2： 如果新增元素插入到左儿子的左子树中，则进行右旋操作。( LL 型调整 ) 如果新增元素插入到左儿子的右子树中，则进行左旋加右旋操作。（ LR 型调整 ） 右子树比左子树的高度大 2： 如果新增元素插入到右儿子的右子树中，则进行左旋操作。（ RR 型调整 ） 如果新增元素插入到右儿子的左子树中，则进行右旋加左旋操作。（ RL 型调整 ） 类似的，在删除一个元素后不断回溯的过程中，如果出现结点不平衡，则和插入操作采用相同的调整操作，确保在删除以后整棵树依然是平衡的。 Size Balanced Tree 对于 SBT，它的自平衡条件会显得稍微复杂一些：对于每个结点 t，同时满足： $$ size[right[t]]≥max(size[left[left[t]]],size[right[left[t]]]) \\ size[left[t]]≥max(size[left[right[t]]],size[right[right[t]]]) $$ size表示以该节点为根的子树中节点个数。 每个结点所在子树的结点个数，不小于其兄弟的两个孩子所在子树的结点个数的最大值。 旋转操作和 AVL 树的左旋右旋是完全一样的。 只是旋转的触发条件不同。 旋转的触发 在调整过程中，一共有 4 种会触发旋转的情况： LL 型（ size[left[left[t]]] &gt; size[right[t]] ）：首先对子树 t 执行右旋操作，旋转以后对 t 的右子树进行调整，之后再对子树 t 进行调整。 LR 型（ size[right[left[t]]] &gt; size[right[t]] ）：首先对 t 的左子树执行左旋操作，再对 t 进行右旋操作。之后分别调整结点 t 的左右子树，最终对结点 t进行调整。 RR 型（ size[right[right[t]]] &gt; size[left[t]] ）：首先对 t 执行左旋操作，旋转以后对 t 的左子树进行调整，之后再对 t 进行调整。 RL 型（ size[left[right[t]]] &gt; size[left[t]] ）：首先对结点 t 的右子树执行右旋操作，再对 t 进行左旋操作。之后分别调整 t 的左右子树，最终对 t 进行调整。 通过递归的进行调整，让不平衡的 SBTree 恢复平衡状态。 简化流程： 如果在处理左子树更高的情况： LL 型：右旋 t。 LR 型：左旋 t 的左子树，再右旋 t。 如果在处理右子树更高的情况： RR 型：左旋 t。 RL 型：右旋 t 的右子树，再左旋 t。 递归调整左子树中左子树的左子树更高的情况。 递归调整右子树中右子树的右子树更高的情况。 递归调整当前子树中左子树更高的情况。 递归调整当前子树中右子树更高的情况。 和 AVL 不太一样的是，SBTree 只有在插入时才可能触发调整，而不需要在删除结点以后进行调整 。 从理论上说，SBTree 和 AVL 树相比在均摊时间复杂度上没有区别，每次查询、插入和删除的时间复杂度都为 \\(O(\\log(n))\\)。 在实际运用中，SBTree 在查询操作较多的情况下会有效率上的优势。加之为了维护平衡性记录了每个结点所在子树大小（即子树内结点个数），相比其他平衡二叉查找树而言，更便于求解第 k 大元素、或求解元素的秩（rank）等类似问题。 实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct SBTNode &#123; int data, size; struct SBTNode *lchild, *rchild, *father;&#125;SBTNode;SBTNode* init(int init_data, int init_size, SBTNode *init_father);SBTNode *NIL;void init_NIL() &#123; NIL = (SBTNode *)malloc(sizeof(SBTNode)); NIL-&gt;data = 0; NIL-&gt;size = 0; NIL-&gt;lchild = NIL; NIL-&gt;rchild = NIL; NIL-&gt;father = NULL;&#125;SBTNode* init(int init_data, int init_size, SBTNode *init_father) &#123; SBTNode *node = (SBTNode *)malloc(sizeof(SBTNode)); node-&gt;data = init_data; node-&gt;size = init_size; node-&gt;lchild = NIL; node-&gt;rchild = NIL; node-&gt;father = init_father; return node;&#125;// 左旋实现，复杂一点的链表指向关系转移SBTNode* left_rotate(SBTNode *node) &#123; SBTNode *temp = node-&gt;rchild; node-&gt;rchild = temp-&gt;lchild; temp-&gt;lchild-&gt;father = node; temp-&gt;lchild = node; temp-&gt;father = node-&gt;father; node-&gt;father = temp; temp-&gt;size = node-&gt;size; node-&gt;size = node-&gt;lchild-&gt;size + node-&gt;rchild-&gt;size + 1; return temp;&#125;// 右旋实现，旋转操作的根节点为nodeSBTNode* right_rotate(SBTNode *node) &#123; SBTNode *temp = node-&gt;lchild; node-&gt;lchild = temp-&gt;rchild; temp-&gt;rchild-&gt;father = node; temp-&gt;rchild = node; temp-&gt;father = node-&gt;father; node-&gt;father = temp; temp-&gt;size = node-&gt;size; node-&gt;size = node-&gt;lchild-&gt;size + node-&gt;rchild-&gt;size + 1; return temp;&#125;// 递归调整平衡二叉树，以size为对比标准，flag指示子树哪一边size更大SBTNode* maintain(SBTNode *node, int flag) &#123; if (flag == 0) &#123; // 左子树中左子树size 大于 node的右子树，LL情况，使用右旋调整 if (node-&gt;lchild-&gt;lchild-&gt;size &gt; node-&gt;rchild-&gt;size) &#123; node = right_rotate(node); &#125; // LR, 先左旋再右旋 else if (node-&gt;lchild-&gt;rchild-&gt;size &gt; node-&gt;rchild-&gt;size) &#123; node-&gt;lchild = left_rotate(node-&gt;lchild); node = right_rotate(node); &#125; else &#123; return node; &#125; &#125; else &#123; // RR, 左旋 if (node-&gt;rchild-&gt;rchild-&gt;size &gt; node-&gt;lchild-&gt;size) &#123; node = left_rotate(node); &#125; // RL，先右旋再左旋 else if (node-&gt;rchild-&gt;lchild-&gt;size &gt; node-&gt;lchild-&gt;size) &#123; node-&gt;rchild = right_rotate(node-&gt;rchild); node = left_rotate(node); &#125; else &#123; return node; &#125; &#125; // 递归处理左右子树 node-&gt;lchild = maintain(node-&gt;lchild, 0); node-&gt;rchild = maintain(node-&gt;rchild, 1); // 当子树调整后，再处理 node 的平衡 node = maintain(node, 0); node = maintain(node, 1); return node;&#125;SBTNode* insert(SBTNode *node, int value) &#123; node-&gt;size++; if (value &gt; node-&gt;data) &#123; if (node-&gt;rchild == NIL) &#123; node-&gt;rchild = init(value, 1, node); &#125; else &#123; node-&gt;rchild = insert(node-&gt;rchild, value); &#125; &#125; else &#123; if (node-&gt;lchild == NIL) &#123; node-&gt;lchild = init(value, 1, node); &#125; else &#123; node-&gt;lchild = insert(node-&gt;lchild, value); &#125; &#125; return maintain(node, value &gt; node-&gt;data);&#125;SBTNode* search(SBTNode *node, int value) &#123; if (node == NIL || node-&gt;data == value) &#123; return node; &#125; else if (value &gt; node-&gt;data) &#123; if (node-&gt;rchild == NIL) &#123; return NIL; &#125; else &#123; return search(node-&gt;rchild, value); &#125; &#125; else &#123; if (node-&gt;lchild == NIL) &#123; return NIL; &#125; else &#123; return search(node-&gt;lchild, value); &#125; &#125;&#125;SBTNode* insert_node(SBTNode *node, int value) &#123; if (node == NULL) &#123; node = init(value, 1, NULL); return node; &#125; if (search(node, value) != NIL) &#123; return node; &#125; return insert(node, value);&#125;SBTNode* predecessor(SBTNode *node) &#123; SBTNode *temp = node-&gt;lchild; while (temp != NIL &amp;&amp; temp-&gt;rchild != NIL) &#123; temp = temp-&gt;rchild; &#125; return temp;&#125;SBTNode* successor(SBTNode *node) &#123; SBTNode *temp = node-&gt;rchild; while (temp != NIL &amp;&amp; temp-&gt;lchild != NIL) &#123; temp = temp-&gt;lchild; &#125; return temp;&#125;void remove_node(SBTNode *delete_node) &#123; SBTNode *temp = NIL; if (delete_node-&gt;lchild != NIL) &#123; temp = delete_node-&gt;lchild; temp-&gt;father = delete_node-&gt;father; delete_node-&gt;lchild = NIL; &#125; if (delete_node-&gt;rchild != NIL) &#123; temp = delete_node-&gt;rchild; temp-&gt;father = delete_node-&gt;father; delete_node-&gt;rchild = NIL; &#125; if (delete_node-&gt;father-&gt;lchild == delete_node) &#123; delete_node-&gt;father-&gt;lchild = temp; &#125; else &#123; delete_node-&gt;father-&gt;rchild = temp; &#125; temp = delete_node; while (temp != NULL) &#123; temp-&gt;size--; temp = temp-&gt;father; &#125; delete_node-&gt;lchild = NIL; delete_node-&gt;rchild = NIL; free(delete_node);&#125;int delete_tree(SBTNode *node, int value) &#123; SBTNode *delete_node, *current_node; current_node = search(node, value); if (current_node == NIL) &#123; return ERROR; &#125; if (current_node-&gt;lchild != NIL) &#123; delete_node = predecessor(current_node); &#125; else if (current_node-&gt;rchild != NIL) &#123; delete_node = successor(current_node); &#125; else &#123; delete_node = current_node; &#125; current_node-&gt;data = delete_node-&gt;data; remove_node(delete_node); return OK;&#125;void inorder(SBTNode *node) &#123; if (node == NIL) return; inorder(node-&gt;lchild); printf(\"%d \", node-&gt;data); inorder(node-&gt;rchild); return;&#125;void clear(SBTNode *node) &#123; if (node != NIL) &#123; if (node-&gt;lchild != NIL) &#123; clear(node-&gt;lchild); &#125; if (node-&gt;rchild != NIL) &#123; clear(node-&gt;rchild); &#125; free(node); &#125;&#125;int main() &#123; init_NIL(); SBTNode *binarytree = NULL; int arr[10] = &#123; 8, 9, 10, 3, 2, 1, 6, 4, 7, 5 &#125;; for (int i = 0; i &lt; 10; i++) &#123; binarytree = insert_node(binarytree, arr[i]); &#125; int value; scanf(\"%d\", &amp;value); if (search(binarytree, value) != NIL) &#123; printf(\"search success!\\n\"); &#125; else &#123; printf(\"search failed!\\n\"); &#125; scanf(\"%d\", &amp;value); if (delete_tree(binarytree, value)) &#123; printf(\"delete success!\\n\"); &#125; else &#123; printf(\"delete failed!\\n\"); &#125; clear(binarytree); return 0;&#125; 求解第k小元素 只需要在SBTree基础上，增加一个函数 select_a。SBTree记录了树的size，所以找到某个大小的元素比较便利。 12345678910111213141516171819202122232425...int select_a(SBTNode *node, int k) &#123; int rank = node-&gt;lchild-&gt;size + 1; if (rank == k) &#123; return node-&gt;data; &#125; else if (k &lt; rank) &#123; return select_a(node-&gt;lchild, k); &#125; else &#123; return select_a(node-&gt;rchild, k - rank); &#125;&#125;int main() &#123; init_NIL(); SBTNode *binarytree = NULL; int arr[10] = &#123; 8, 9, 10, 3, 2, 1, 6, 4, 7, 5 &#125;; for (int i = 0; i &lt; 10; i++) &#123; binarytree = insert_node(binarytree, arr[i]); &#125; int k; scanf(\"%d\", &amp;k); printf(\"%d\\n\", select_a(binarytree, k)); clear(binarytree); return 0;&#125; RBTree 红黑树（Red-Black Tree） 也是一种自平衡的二叉查找树。1972 年由 Rudolf Bayer 发明的，而“红黑树”的名字是由 Leo J. Guibas 和 Robert Sedgewick （写 《算法》的普林斯顿大佬）于 1978 年首次提出。 红黑树相比于 AVL，牺牲了部分平衡性以在插入、删除操作时减少旋转操作，整体性能优于 AVL，也正因如此，C++ STL 中的 map 就是用红黑树实现的。 红黑树和其他平衡二叉查找树相比，增加了一个 颜色 属性，用来标识树上的结点是红色还是黑色；并且如果一个结点没有子结点，则该结点的子节点对应的指针为 NIL，也就是说，红黑树的所有叶子都是 NIL。 红黑树是满足如下条件的二叉查找树： 每个结点要么是红色，要么是黑色； 根结点是黑色； 叶结点（NIL）是黑色； 如果一个结点是红色，则它的两个子节点都是黑色的； 从根结点出发到所有叶结点的路径上，均包含相同数目的黑色结点。 第五条规则是红黑树平衡性的核心。 因为第四条和第五条规则的限制，使得红黑树上从根结点到每个叶结点的最长路径最多是最短路径的两倍，这也确保了整棵二叉查找树是平衡的。 红黑树插入 插入分为四种情况，且插入节点默认为红色： 新结点 x 位于树的根 根据第二条规则，将新结点的颜色改为黑色。 x 的叔父结点（父节点的兄弟）是红色 此时 x 的祖父结点一定是黑色的。 将祖父结点的黑色改为红色，并将 x 的父结点和叔父结点改为黑色。之后将 x 的祖父结点作为 x 继续向上迭代（直到根节点）。 x 的叔父结点是黑色的，并且 x 是一个右孩子 对 x 的父结点进行左旋，原父结点仍为红色，叔父结点仍为黑色。进行第四种情况操作，且在第四种情况中，x代表左旋前的父节点。 x 的叔父结点是黑色的，并且 x 是一个左孩子 将 x 的父结点改为黑色，祖父结点改为红色，并对 x 的祖父结点进行右旋。 删除操作不写了，有点麻烦，也没那个兴趣从头实现，比较复杂的算法，照着伪代码实现挺耗时间的。 多路平衡二叉树 在数据量较大的工程应用（如数据库）中，由于树中的结点信息往往保存在磁盘而非内存里，维护一棵平衡二叉查找树会导致大量的磁盘寻址和读写操作。而磁盘存取的时间要比 CPU 运算的时间更长。 为了解决这个问题，可以通过每次存取连续多个数据项，来减少磁盘读写的时间开销。 平衡树是检索效率非常高的一类数据结构，但平衡树每个结点只能保存一个关键字。为了便于一次从磁盘中读入连续的多个数据，多路查找树来了。 将二叉查找改为多路查找，可以在降低查找深度的同时，在同一个结点内维护更多的信息，每次存取连续多个数据项，降低磁盘寻址和读写的时间开销，优化在磁盘上检索数据的性能。 多路查找树（Multi-way search tree） 是指树中结点最多有 M 个孩子。查找的时间效率依然可以保证为 \\(O(\\log(n))\\) 复杂度，并且树的深度更小。 2-3树 在 2-3 树中，有两种结点：2-node 和 3-node，表示每个结点有 2 个还是 3 个孩子。 树中的 所有叶子结点都在同一层 ，叶子结点可以包含一个或两个关键字。 2-node 一定 有两个孩子和一个关键字；3-node 一定 有三个孩子和两个关键字。 3-node有三个子树，两个关键字的值划分了三段连续的区间，三个子树分别位于这三个区间内。 B树 一棵最小度数为 t (t &gt;= 2) 的 B 树除了满足多路查找树的基本性质以外，还满足如下的性质： 根结点至少有一个关键字，以及两个孩子结点； 所有内部结点至少有 t 个孩子结点，至多有 2t 个孩子结点； 所有内部结点至少有 t−1 个关键字，至多有 2t−1 个关键字； 每个叶子结点没有孩子。 查找 123456789search(node, key) i = 0 while i &lt; node-&gt;count and key &gt; node-&gt;keys[i] i = i + 1 if i &lt; node-&gt;count and key == node-&gt;keys[i] return (node, i) else if node-&gt;is_leaf return NIL else return search(node-&gt;childs[i], key) 每个node有count个子节点。 插入 节点的子节点个数是可以变化的，所以当达到一个节点的最大度数（或者关键字个数）需要将子节点中合适的关键字提升到父节点中，若父节点也满了，就继续提升。划分出更小的节点，再进行插入。 删除 B 树的删除操作需要在递归过程中确保所在结点的关键字个数 不小于 最小度数 t。 B+ 树 B+ 树和 B 树的不同之处在于： 所有关键字都存放在叶结点中，非叶结点的关键字表示的是子树中所有关键字的最小值，这被称为 最小复写码 。也可以统一存储子树所有关键字的最大值。 叶结点包含了全部的关键字信息，并且叶结点之间按从小到大的顺序链接。 非叶子结点内并不需要真正保存关键字的具体信息，因此整体的磁盘读写开销会更小。 遍历叶子结点就可以从小到大遍历相邻的元素。 因此，现有的数据库索引大多采用 B+ 树作为索引数据结构。 B * 树 在 B* 树中，内部结点（非根、非叶子）也增加了一个指向兄弟的指针。并且每个结点的关键字个数至少为关键字个数上限的 \\(\\frac{2}{3}\\)。因为对下限的调整，所以 B * 树的空间使用率比 B 树和 B+ 树更高。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Data Structure","slug":"Notes/Data-Structure","permalink":"https://racleray.github.io/categories/Notes/Data-Structure/"}],"tags":[{"name":"data structure","slug":"data-structure","permalink":"https://racleray.github.io/tags/data-structure/"},{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"}],"author":"HeRui"},{"title":"数据结构整理-part1","slug":"数据结构整理-part1","date":"2021-10-03T13:30:33.000Z","updated":"2023-08-07T11:54:31.042Z","comments":true,"path":"posts/d828af60.html","link":"","permalink":"https://racleray.github.io/posts/d828af60.html","excerpt":"常见基本数据结构整理，使用C语言编写。","text":"导论 数据结构 是计算机存储、组织数据的方式，是指数据元素的集合以及数据元素之间存在的一种或者多种关系的集合 元素之间的关系包括数据的逻辑结构、数据的存储结构和数据的运算结构。 数据 是信息的载体，是可以被计算机识别存储并加工处理的描述客观事物的信息符号的总称。 数据元素 是数据的基本单位，在计算机程序中通常作为一个整体考虑。一个数据元素由若干个 数据项 组成。 数据项是数据结构中讨论的最小单位。有两类数据元素：如果数据元素不能再分，则称为 原子项 ；如果数据元素由若干个数据项组成，则称为 组合项 。 数据结构是一门研究非数值计算的学科，主要研究数据元素以及它们之间的关系和运算等，而且确保经过这些运算后所得到的新结构仍然是原来的结构类型。 数据结构有两个要素，一个是数据元素的集合，另一个是关系的集合。 集合结构。数据元素属于同一个集合。 线性结构。数据元素之间存在着一对一的关系。常见的有链表、队列、栈等。 树形结构。数据元素之间存在着一对多的关系。常见的有二叉树、二叉查找树、平衡二叉查找树等。 图形结构。数据元素之间存在着多对多的关系。 按照存储方式的不同，数据结构可以分为顺序存储结构和链式存储结构： 顺序存储结构，表示数据元素在存储器中是连续存储的，可以用相对位置来表示数据元素之间的逻辑结构，如顺序表、队列、栈等。 链式存储结构，每个数据元素里设置了一个指针用来指向另一个元素的存储地址，以此来表示数据元素之间的逻辑结构。 按照逻辑结构来分，数据结构可以分为线性结构和非线性结构 如果数据元素之间存在一对一的关系，则称为线性结构 否则称为非线性结构。集合结构、树形结构、图形结构都称为非线性结构。 算法（Algorithm）是对某一个或者某一类问题的解决方案的描述，根据问题的输入，在有限的计算时间里输出预期的结果。 算法有以下 5 个特征： 有穷性。算法必须在执行有限个操作后终止。 确切性。算法的每一个操作必须有明确的定义。 输入项。算法有零个或多个输入，描述算法的初始状态。 输出项。算法有一个或多个输出，没有输出的算法我们认为是没有意义的。 可行性。算法的每个计算操作都可以在有限时间内完成。 数据结构描述了数据元素之间的逻辑关系，算法描述了数据元素的操作步骤，数据结构和算法组成了程序世界。数据结构和算法之间是不可分割的关系，数据结构是程序的基础，算法将数据互相联系起来，形成了一套能解决具体问题的方案。 在解决问题时，一般我们会优先确定数据结构，然后再来完善算法，有时也会反过来，根据算法来选择合适的数据结构。选择一个合适的数据结构，可以降低算法的复杂度，提高算法的效率。 复杂度分析 时间复杂度 时间频度是指算法中语句的执行次数，用 T(n) 来表示， n 为问题的规模。 时间频度的表达方法有点复杂，我们需要更直观的表达方法，于是引入了时间复杂度的概念。 函数 f(n)，在 n 趋向于无穷大时， T(n)/f(n) 的极限值为不等于 0 的常数，则我们近似的将 f(n) 替代 T(n)，记为 T(n)=O(f(n))，称为算法的渐进时间复杂度。 时间复杂度只关心算法中最耗时的部分 常见复杂度级别 空间复杂度 空间复杂度是指运行该算法所占用的存储空间大小，记为 S(n) 预估出算法运行所需的存储空间，包括指令空间、数据空间、动态申请的内存空间等。 12345int *a = new int[n];int **b = new int*[n];for (int i = 0; i &lt; n; i++) &#123; b[i] = new int[n];&#125; S(n)=n+n^2，则空间复杂度为 O(n^2)。 内容 数据结构I 包含了一些基础的数据结构，一共分为三部分： 线性结构，包括顺序表、链表、队列、栈等； 树结构和图结构的入门，包括二叉树、图的存储方式等； 查找排序算法，包括哈希表、顺序查找、折半查找、三分查找等查找算法，和插入排序、冒泡排序、归并排序、选择排序和快速排序等排序算法。 数据结构II 包含了一些进阶的数据结构，一共分为两部分： 树结构，包括二叉查找树、平衡二叉查找树、堆与优先队列、森林与并查集等； 图结构，包括图的遍历、图的连通性、最短路和最小生成树等算法。 线性表 线性表是由 相同数据类型 的 n 个数据元素组成的有限序列。 线性表按照存储结构，可以分为顺序表和链表两种类型。 顺序表 实现顺序表的构造、插入、扩容、查找、删除、遍历这 6 种基本操作，并在本章最后用顺序表这个数据结构求解一道题目 顺序表是线性表的一种顺序存储形式。换句话说，线性表是逻辑结构，表示元素之间一对一的相邻关系；而顺序表是存储结构，是指用一组地址连续的存储单元，依次存储线性表中的数据元素，从而使得逻辑上相邻的两个元素在物理位置上也相邻。 顺序表在程序中通常用一维数组实现，一维数组可以是静态分配的，也可以是动态分配的。 在静态分配时，由于数组的大小和空间是固定的 在动态分配时，存储数组的空间在程序执行过程中会动态调整大小 支持随机访问 插入和删除操作需要移动大量的元素，从而保持逻辑上和物理上的连续性。 堆里数组，同时使用一段连续地址，储存相同类型的有限数据序列。 implement 实现一 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct Vector &#123; int size, length; int *data;&#125; Vector;void init(Vector *vector, int size) &#123; vector-&gt;size = size; vector-&gt;length = 0; vector-&gt;data = (int *)malloc(sizeof(int) * size);&#125;// 请在下面实现扩容函数 expandvoid expand(Vector *vector) &#123; int *old_data = vector-&gt;data; vector-&gt;size = vector-&gt;size * 2; vector-&gt;data = (int *)malloc(sizeof(int) * vector-&gt;size); for (int i = 0; i &lt; vector-&gt;length; ++i) &#123; vector-&gt;data[i] = old_data[i]; &#125; free(old_data);&#125;int insert(Vector *vector, int loc, int value) &#123; if (loc &lt; 0 || loc &gt; vector-&gt;length) &#123; return ERROR; &#125; if (vector-&gt;length &gt;= vector-&gt;size) &#123; // return ERROR; expand(vector); &#125; for (int i = vector-&gt;length; i &gt; loc; --i) &#123; vector-&gt;data[i] = vector-&gt;data[i - 1]; &#125; vector-&gt;data[loc] = value; vector-&gt;length++; return OK;&#125;int search(Vector *vector, int value) &#123; for (int i = 0; i &lt; vector-&gt;length; ++i) &#123; if (vector-&gt;data[i] == value) &#123; return i; &#125; &#125; return -1;&#125;int delete_node(Vector *vector, int index) &#123; if (index &lt; 0 || index &gt;= vector-&gt;length) &#123; return ERROR; &#125; for (int i = index + 1; i &lt; vector-&gt;length; ++i) &#123; vector-&gt;data[i - 1] = vector-&gt;data[i]; &#125; vector-&gt;length--; return OK;&#125;void print(Vector *vector) &#123; for (int i = 0; i &lt; vector-&gt;length; ++i) &#123; if (i &gt; 0) &#123; printf(\" \"); &#125; printf(\"%d\", vector-&gt;data[i]); &#125; printf(\"\\n\");&#125;void clear(Vector *vector) &#123; free(vector-&gt;data); free(vector);&#125;int main() &#123; Vector *a = (Vector *)malloc(sizeof(Vector)); init(a, 100); printf(\"%d\\n\", insert(a, 1, 0)); printf(\"%d\\n\", insert(a, 0, 1)); printf(\"%d\\n\", insert(a, 2, 1)); printf(\"%d\\n\", insert(a, 1, 2)); printf(\"%d\\n\", insert(a, 0, 3)); clear(a); return 0;&#125; 实现二 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define ERROR 0#define OK 1/*错误处理还不严谨，使用mem操作，比上一个for循环高效*/typedef struct Vector &#123; int s, l; int *d;&#125; Vector;void init(Vector *v, int size) &#123; if (!v) return; v-&gt;s = size; v-&gt;l = 0; v-&gt;d = (int *)malloc(size * sizeof(int));&#125;int expand(Vector *v) &#123; if (!v) return ERROR; int expandsize = v-&gt;s; int *tmp = NULL; while (expandsize) &#123; // realloc, append v-&gt;d. If necessary, copy to a bigger memory block. tmp = (int *)realloc(v-&gt;d, sizeof(int) * (v-&gt;s + expandsize)); if (tmp) break; expandsize &gt;&gt;= 2; &#125; if (!tmp) return ERROR; v-&gt;d = tmp; v-&gt;s += expandsize; // printf(\"Expand succeed\"); return OK;&#125;int insert(Vector *v, int loc, int value) &#123; if (!v) return ERROR; if (loc &lt; 0 || loc &gt; v-&gt;l) return ERROR; if (v-&gt;s &lt;= v-&gt;l) &#123; if (!expand(v)) return ERROR; &#125; memcpy(v-&gt;d + loc + 1, v-&gt;d + loc, sizeof(int) * (v-&gt;l - loc)); v-&gt;d[loc] = value; v-&gt;l++; return OK;&#125;int search(Vector *v, int target) &#123; if (!v) return ERROR; for (int i = 0; i &lt; v-&gt;l; ++i) &#123; if (*(v-&gt;d + i) == target) return i + 1; // return index begin from 1 &#125; return ERROR;&#125;int delete_node(Vector *v, int loc) &#123; if (!v) return ERROR; if (loc &lt; 0 || loc &gt;= v-&gt;l) return ERROR; memcpy(v-&gt;d + loc, v-&gt;d + loc + 1, sizeof(int) * (v-&gt;l - loc - 1)); v-&gt;l--; return OK;&#125;void print(Vector *v) &#123; int *tmp = v-&gt;d; int i = 0; while (i &lt; v-&gt;l) &#123; (i &gt; 0) &amp;&amp; printf(\" \"); printf(\"%d\", *tmp); ++i; ++tmp; &#125; printf(\"\\n\");&#125;void clear(Vector *v) &#123; free(v-&gt;d); free(v);&#125;int main() &#123; Vector *v = (Vector *)malloc(sizeof(Vector)); init(v, 20); // random test: use #include &lt;time.h&gt; srand(time(NULL)); op = rand() % 4; ... int n, op, loc, val; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; i++) &#123; scanf(\"%d\", &amp;op); switch (op) &#123; case 1: scanf(\"%d%d\", &amp;loc, &amp;val); if (insert(v, loc, val)) printf(\"success\\n\"); else printf(\"failed\\n\"); break; case 2: scanf(\"%d\", &amp;loc); if (delete_node(v, loc)) printf(\"success\\n\"); else printf(\"failed\\n\"); break; case 3: scanf(\"%d\", &amp;val); if (search(v, val)) printf(\"success\\n\"); else printf(\"failed\\n\"); break; case 4: print(v); break; &#125; &#125; clear(v); return 0;&#125; 链表 implement 注意没有单独定义 linked list 结构体，没有记录长度。 示例用代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Node&#123; int data; struct Node *next;&#125;Node, *LinkedList;LinkedList insert(LinkedList head, Node *node, int index) &#123; if (head == NULL) &#123; if (index != 0) &#123; return head; &#125; head = node; return head; &#125; if (index == 0) &#123; node-&gt;next = head; head = node; return head; &#125; Node *current_node = head; int count = 0; while (current_node-&gt;next != NULL &amp;&amp; count &lt; index - 1) &#123; current_node = current_node-&gt;next; count++; &#125; if (count == index - 1) &#123; node-&gt;next = current_node-&gt;next; current_node-&gt;next = node; &#125; return head;&#125;void output(LinkedList head) &#123; if (head == NULL) &#123; return; &#125; Node *current_node = head; while (current_node != NULL) &#123; printf(\"%d \", current_node-&gt;data); current_node = current_node-&gt;next; &#125; printf(\"\\n\");&#125;LinkedList delete_node(LinkedList head, int index) &#123; if (head == NULL) &#123; return head; &#125; Node *current_node = head; int count = 0; if (index == 0) &#123; head = head-&gt;next; free(current_node); return head; &#125; // count &lt; index - 1: index 比 链表长度 大很多，count == index - 1 不会成立，这里就是要保证找到目标位置时，count == index - 1 成立 while (current_node-&gt;next != NULL &amp;&amp; count &lt; index - 1) &#123; current_node = current_node-&gt;next; count++; &#125; // 停在要删除位置前一个，current_node-&gt;next != NULL 应该恒成立 if (count == index - 1 &amp;&amp; current_node-&gt;next != NULL) &#123; Node *delete_node = current_node-&gt;next; current_node-&gt;next = delete_node-&gt;next; free(delete_node); &#125; return head;&#125;LinkedList reverse(LinkedList head) &#123; if (head == NULL) &#123; return head; &#125; Node *next_node, *current_node; current_node = head-&gt;next; head-&gt;next = NULL; while (current_node != NULL) &#123; next_node = current_node-&gt;next; current_node-&gt;next = head; head = current_node; current_node = next_node; &#125; return head;&#125;void clear(LinkedList head) &#123; Node *current_node = head; while (current_node != NULL) &#123; Node *delete_node = current_node; current_node = current_node-&gt;next; free(delete_node); &#125;&#125;int main() &#123; LinkedList linkedlist = NULL; for (int i = 1; i &lt;= 10; i++) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;data = i; node-&gt;next = NULL; linkedlist = insert(linkedlist, node, i - 1); &#125; output(linkedlist); linkedlist = delete_node(linkedlist, 9); output(linkedlist); linkedlist = reverse(linkedlist); output(linkedlist); clear(linkedlist); return 0;&#125; 循环链表约瑟夫环 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Node&#123; int data; struct Node *next;&#125;Node, *LinkedList;LinkedList insert(LinkedList head, Node *node, int index) &#123; if (head == NULL) &#123; if (index != 0) &#123; return head; &#125; head = node; head-&gt;next = head; return head; &#125; if (index == 0) &#123; node-&gt;next = head-&gt;next; // 循环 head-&gt;next = node; return head; &#125; Node *current_node = head-&gt;next; // head为标记的尾节点 int count = 0; while (current_node != head &amp;&amp; count &lt; index - 1) &#123; current_node = current_node-&gt;next; count++; &#125; if (count == index - 1) &#123; node-&gt;next = current_node-&gt;next; current_node-&gt;next = node; &#125; // 此时更新尾节点 if (node == head-&gt;next) &#123; head = node; &#125; return head;&#125;// 约瑟夫环void output_josephus(LinkedList head, int m) &#123; Node *current_node = head; head = NULL; while (current_node-&gt;next != current_node) &#123; for (int i = 1; i &lt; m; i++) &#123; current_node = current_node-&gt;next; &#125; printf(\"%d \", current_node-&gt;next-&gt;data); Node *delete_node = current_node-&gt;next; current_node-&gt;next = current_node-&gt;next-&gt;next; free(delete_node); &#125; printf(\"%d\\n\", current_node-&gt;data); free(current_node);&#125;int main() &#123; LinkedList linkedlist = NULL; int n, m; scanf(\"%d %d\", &amp;n, &amp;m); for (int i = 1; i &lt;= n; i++) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;data = i; node-&gt;next = NULL; linkedlist = insert(linkedlist, node, i - 1); &#125; output_josephus(linkedlist, m); return 0;&#125; 循环链表需要记录的是尾结点的位置。那么插入节点就不会循环一圈，才能找到尾结点。 增加 dummy node 的解法 单向 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;typedef struct Node&#123; int val; struct Node *next;&#125; Node;typedef struct List&#123; // Node *head; 改为 dummy node, 而不再是一个指针 Node head; int len;&#125; List;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; return n;&#125;void freeNode(Node *n) &#123; if (n) free(n); return;&#125;List *initList() &#123; List *l = (List *)malloc(sizeof(List)); l-&gt;head.next = NULL; l-&gt;len = 0; return l;&#125;void freeList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *del_ = NULL; while (p) &#123; del_ = p; p = p-&gt;next; free(del_); &#125; free(l); return;&#125;// 插入输入节点int insertNode(List *l, int idx, Node *n) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt; l-&gt;len) return 0; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; n-&gt;next = prev-&gt;next; prev-&gt;next = n; l-&gt;len++; return 1;&#125;// 插入输入值int insertValue(List *l, int idx, int val) &#123; Node *n = initNode(val); if (!insertNode(l, idx, n)) &#123; freeNode(n); return 0; &#125; return 1;&#125;// 删除int erase(List *l, int idx) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt;= l-&gt;len) return 0; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; Node *tmp = prev-&gt;next; prev-&gt;next = tmp-&gt;next; freeNode(tmp); l-&gt;len--; return 1;&#125;int search(List *l, int idx) &#123; if (!l) return -1; if (idx &lt; 0 | idx &gt;= l-&gt;len) return -1; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; return prev-&gt;next-&gt;val;&#125;void showList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *tmp = &amp;(l-&gt;head); printf(\"List+: [\"); while (p) &#123; tmp = p; printf(\"%d-&gt;\", p-&gt;val); p = p-&gt;next; &#125; printf(\"NULL]\\n\"); return;&#125; 增加 dummy node 的解法 双向 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;typedef struct Node&#123; int val; struct Node *next; struct Node *prev;&#125; Node;typedef struct List&#123; // Node *head; 改为 dummy node, 而不再是一个指针 Node head; int len;&#125; List;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; n-&gt;prev = NULL; return n;&#125;void freeNode(Node *n) &#123; if (n) free(n); return;&#125;List *initList() &#123; List *l = (List *)malloc(sizeof(List)); l-&gt;head.next = NULL; l-&gt;head.prev = NULL; // not necessarily l-&gt;len = 0; return l;&#125;void freeList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *del_ = NULL; while (p) &#123; del_ = p; p = p-&gt;next; free(del_); &#125; free(l); return;&#125;// 插入输入节点int insertNode(List *l, int idx, Node *n) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt; l-&gt;len) return 0; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; n-&gt;next = prev-&gt;next; prev-&gt;next = n; n-&gt;prev = prev; if (n-&gt;next) n-&gt;next-&gt;prev = n; l-&gt;len++; return 1;&#125;// 插入输入值int insertValue(List *l, int idx, int val) &#123; Node *n = initNode(val); if (!insertNode(l, idx, n)) &#123; freeNode(n); return 0; &#125; return 1;&#125;// 删除int erase(List *l, int idx) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt;= l-&gt;len) return 0; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; Node *tmp = prev-&gt;next; prev-&gt;next = tmp-&gt;next; if (tmp-&gt;next) tmp-&gt;next-&gt;prev = prev; freeNode(tmp); l-&gt;len--; return 1;&#125;int reverse(List *l) &#123; if (!l || l-&gt;len == 0) return 0; Node *p = l-&gt;head.next; Node *cur = NULL; l-&gt;head.next = NULL; l-&gt;len = 0; while (p) &#123; cur = p; p = p-&gt;next; insertNode(l, 0, cur); &#125; return 1;&#125;void showList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *tmp = &amp;(l-&gt;head); int len = l-&gt;len; // printf(\"List+: [\"); // while (len--) &#123; // tmp = p; // printf(\"%d-&gt;\", p-&gt;val); // p = p-&gt;next; // &#125; // printf(\"NULL]\\n\"); // printf(\"List-: [\"); // len = l-&gt;len; // while (len--) &#123; // printf(\"%d-&gt;\", tmp-&gt;val); // tmp = tmp-&gt;prev; // &#125; // printf(\"HEAD]\\n\"); printf(\"List+: [\"); while (p) &#123; tmp = p; printf(\"%d-&gt;\", p-&gt;val); p = p-&gt;next; &#125; printf(\"NULL]\\n\"); printf(\"List-: [\"); while (tmp != &amp;(l-&gt;head)) &#123; printf(\"%d-&gt;\", tmp-&gt;val); tmp = tmp-&gt;prev; &#125; printf(\"HEAD]\\n\"); return;&#125;int main(int argc, char **argv) &#123; srand(time(NULL)); int cnt = 20; List *l = initList(); while (cnt--) &#123; int val = rand() % 100; int opt = rand() % 5; int idx = rand() % (l-&gt;len + 3) - 1; switch (opt) &#123; case 0: case 1: case 2: printf(\"insert %d at %d, res = %s\\n\", val, idx, insertValue(l, idx, val) ? \"SUCCESS\" : \"FAILED\"); break; case 3: printf(\"erease at %d, res = %s\\n\", idx, erase(l, idx) ? \"SUCCESS\" : \"FAILED\"); break; case 4: printf(\"reverse, res = %s\\n\", reverse(l) ? \"SUCCESS\" : \"FAILED\"); break; &#125; showList(l); printf(\"\\n\"); &#125; return 0;&#125; 练习 LeetCode 剑指offer： 24 35 18 06 25 22 36 52 顺序表循环左移 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define ERROR 0#define OK 1typedef struct Vector &#123; int size, len; int *data;&#125; Vec;void initVec(Vec *v, int size) &#123; if (!v) return; v-&gt;size = size; v-&gt;len = 0; v-&gt;data = (int *)malloc(sizeof(int) * size); return;&#125;int expand(Vec *v) &#123; int asize = v-&gt;size; int *tmp = NULL; while (asize) &#123; tmp = (int *)realloc(v-&gt;data, sizeof(int) * (asize + v-&gt;size)); if (tmp) break; asize &gt;&gt;= 2; &#125; if (tmp == NULL) return ERROR; v-&gt;data = tmp; v-&gt;size += asize; return OK;&#125;int insert(Vec *v, int idx, int val) &#123; if (!v) return ERROR; if (idx &lt; 0 | idx &gt; v-&gt;len) return ERROR; if (v-&gt;size == v-&gt;len) if (!expand(v)) return ERROR; memcpy(v-&gt;data + idx + 1, v-&gt;data + idx, sizeof(int) * (v-&gt;len - idx)); v-&gt;data[idx] = val; v-&gt;len++; return OK;&#125;void freeVec(Vec *v) &#123; if (!v) return; free(v-&gt;data); free(v);&#125;// moveint moveBlock(Vec *v, int num) &#123; if (!v) return ERROR; if (num &lt; 0 | num &gt; v-&gt;len) return ERROR; int *tmp = (int *)malloc(num * sizeof(int)); memcpy(tmp, v-&gt;data, sizeof(int) * num); memcpy(v-&gt;data, v-&gt;data + num, sizeof(int) * (v-&gt;len - num)); memcpy(v-&gt;data + (v-&gt;len - num), tmp, sizeof(int) * num); return OK;&#125;void printVec(Vec *v) &#123; if (!v) return; for (int i = 0; i &lt; v-&gt;len; i++) &#123; (i &gt; 0) &amp;&amp; printf(\" \"); printf(\"%d\", *(v-&gt;data + i)); &#125; return;&#125;int main() &#123; Vec *v = (Vec *)malloc(sizeof(Vec)); (void)initVec(v, 10); int n, k, val; scanf(\"%d%d\", &amp;n, &amp;k); for (int i=0; i&lt;n; i++) &#123; scanf(\"%d\", &amp;val); if (!insert(v, i, val)) return 3; &#125; if (!moveBlock(v, k)) return 4; (void)printVec(v); (void)freeVec(v); return 0;&#125; reverse chars 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;typedef struct Node&#123; int val; struct Node *next;&#125; Node;typedef struct List&#123; // Node *head; 改为 dummy node, 而不再是一个指针 Node head; int len;&#125; List;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; return n;&#125;void freeNode(Node *n) &#123; if (n) free(n); return;&#125;List *initList() &#123; List *l = (List *)malloc(sizeof(List)); l-&gt;head.next = NULL; l-&gt;len = 0; return l;&#125;void freeList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *del_ = NULL; while (p) &#123; del_ = p; p = p-&gt;next; free(del_); &#125; free(l); return;&#125;// 插入输入节点int insertNode(List *l, int idx, Node *n) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt; l-&gt;len) return 0; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; n-&gt;next = prev-&gt;next; prev-&gt;next = n; l-&gt;len++; return 1;&#125;// 插入输入值int insertValue(List *l, int idx, int val) &#123; Node *n = initNode(val); if (!insertNode(l, idx, n)) &#123; freeNode(n); return 0; &#125; return 1;&#125;// 实现reverseint reverse(List *l) &#123; if (!l) return 0; Node *cur = l-&gt;head.next; Node *tmp = NULL; l-&gt;head.next = NULL; l-&gt;len = 0; while (cur) &#123; tmp = cur; cur = cur-&gt;next; tmp-&gt;next = l-&gt;head.next; l-&gt;head.next = tmp; l-&gt;len++; &#125; return 1;&#125;int search(List *l, int idx) &#123; if (!l) return -1; if (idx &lt; 0 | idx &gt;= l-&gt;len) return -1; Node *prev = &amp;(l-&gt;head); while (idx--) prev = prev-&gt;next; return prev-&gt;next-&gt;val;&#125;void showList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; while (p) &#123; printf(\"%c\", p-&gt;val); p = p-&gt;next; (p != NULL) &amp;&amp; printf(\" \"); &#125; return;&#125;int main() &#123; int n; char c; scanf(\"%d\", &amp;n); List *l = initList(); getchar(); for (int i=0; i&lt;n * 2 - 1; i++) &#123; scanf(\"%c\", &amp;c); if (c &gt; 40) &#123; if (!insertValue(l, i / 2, c)) return 1; &#125; &#125; if (!reverse(l)) return 2; (void)showList(l); freeList(l); return 0;&#125; 双向循环 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;typedef struct Node&#123; int val; struct Node *next; struct Node *prev;&#125; Node;typedef struct List&#123; // Node *head; 改为 dummy node, 而不再是一个指针 Node head; int len;&#125; List;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; n-&gt;prev = NULL; return n;&#125;void freeNode(Node *n) &#123; if (n) free(n); return;&#125;List *initList() &#123; List *l = (List *)malloc(sizeof(List)); l-&gt;head.next = NULL; l-&gt;head.prev = NULL; // not necessarily l-&gt;len = 0; return l;&#125;void freeList(List *l) &#123; if (!l) return; Node *p = l-&gt;head.next; Node *del_ = NULL; int len = l-&gt;len; while (len) &#123; // 更改为按长度free del_ = p; p = p-&gt;next; free(del_); len--; &#125; free(l); return;&#125;// 插入输入节点: 更改为循环链表int insertNode(List *l, int idx, Node *n) &#123; if (!l) return 0; if (idx &lt; 0 | idx &gt; l-&gt;len) return 0; if (l-&gt;len == 0) &#123; l-&gt;head.next = n; n-&gt;prev = n; n-&gt;next = n; l-&gt;len++; return 1; &#125; Node *prev = l-&gt;head.next; while (idx--) prev = prev-&gt;next; n-&gt;next = prev-&gt;next; prev-&gt;next = n; n-&gt;prev = prev; if (n-&gt;next) n-&gt;next-&gt;prev = n; if (n == l-&gt;head.next-&gt;next) &#123; l-&gt;head.next = n; &#125; l-&gt;len++; return 1;&#125;// 插入输入值int insertValue(List *l, int idx, int val) &#123; Node *n = initNode(val); if (!insertNode(l, idx, n)) &#123; freeNode(n); return 0; &#125; return 1;&#125;// 更改为循环链表 从 k 位置反向输出void showList(List *l, int k) &#123; if (!l) return; Node *p = l-&gt;head.next; int c = l-&gt;len; while (p-&gt;val != k &amp;&amp; c) &#123; p = p-&gt;next; c--; &#125; if (c == 0 &amp;&amp; p-&gt;val != k) return; Node *tmp = p; int len = l-&gt;len; while (tmp != p-&gt;next) &#123; printf(\"%d \", tmp-&gt;val); tmp = tmp-&gt;prev; &#125; printf(\"%d\", p-&gt;next-&gt;val); return;&#125;int main(int argc, char **argv) &#123; int n, val; scanf(\"%d\", &amp;n); List *l = initList(); for (int i = 0; i &lt; n; i++) &#123; scanf(\"%d\", &amp;val); insertValue(l, i, val); &#125; int k; scanf(\"%d\", &amp;k); showList(l, k); freeList(l); return 0;&#125; 队列 队列有一个很重要的性质，就是 先进先出 ，First In First Out(FIFO)。 利用两个变量head和tail维护队列的进出。 简单实现 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct Queue&#123; int *data; int head, tail, length;&#125;Queue;void init(Queue *q, int length) &#123; q-&gt;data = (int *)malloc(sizeof(int) * length); q-&gt;head = 0; q-&gt;tail = -1; q-&gt;length = length;&#125;int push(Queue *q, int element) &#123; if (q-&gt;tail + 1 &gt;= q-&gt;length) return 0; q-&gt;data[++q-&gt;tail] = element; return 1; &#125;void output(Queue *q) &#123; for (int i = q-&gt;head; i &lt;= q-&gt;tail; ++i) &#123; (i != q-&gt;head) &amp;&amp; printf(\" \"); printf(\"%d\", q-&gt;data[i]); &#125;&#125;int front(Queue *q) &#123; return q-&gt;data[q-&gt;head];&#125;void pop(Queue *q) &#123; q-&gt;head++;&#125;int empty(Queue *q) &#123; return q-&gt;head &gt; q-&gt;tail;&#125;void clear(Queue *q) &#123; free(q-&gt;data); free(q);&#125;int main() &#123; Queue *queue = (Queue *)malloc(sizeof(Queue)); init(queue, 100); int n, val; scanf(\"%d\", &amp;n); for (int i = 0; i &lt; n; i++) &#123; scanf(\"%d\", &amp;val); if (!push(queue, val)) return 1; &#125; if (!empty(queue)) &#123; int k; scanf(\"%d\", &amp;k); while (k--) &#123; pop(queue); &#125; &#125; if (!empty(queue)) &#123; printf(\"%d\\n\", front(queue)); output(queue); &#125; else &#123; printf(\"0\"); &#125; clear(queue); return 0;&#125; 循环队列 在循环队列里，不能单纯通过比较 tail 和 head 标记来判断循环队列是否已满，否则在初始化状态就会被认为循环队列已满。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct Queue &#123; int *data; int head, tail, length, count;&#125;Queue;void init(Queue *q, int length) &#123; q-&gt;data = (int *)malloc(sizeof(int) * length); q-&gt;length = length; q-&gt;head = 0; q-&gt;tail = -1; q-&gt;count = 0;&#125;// 扩容需要考虑循环结构int expand(Queue *q) &#123; if (!q) return ERROR; int expsize = q-&gt;length; int *tmp = NULL; while (expsize &gt; 0) &#123; // 因为要重新组织数据，所以不用realloc了 tmp = (int *)malloc(sizeof(int) * (q-&gt;length + expsize)); if (tmp) break; expsize &gt;&gt;= 1; &#125; if (!tmp) return ERROR; // 复制到新空间中 int i, j; int end = (q-&gt;tail + 1) % q-&gt;length; for (i = q-&gt;head, j = 0; i != end; i = (i + 1) % q-&gt;length, j++) &#123; tmp[j] = q-&gt;data[i]; &#125; free(q-&gt;data); q-&gt;data = tmp; q-&gt;length += expsize; q-&gt;head = 0; q-&gt;tail = j - 1; return OK;&#125;int push(Queue *q, int element) &#123; if (q-&gt;count &gt;= q-&gt;length) &#123; if (!expand(q)) return ERROR; &#125; q-&gt;tail = (q-&gt;tail + 1) % q-&gt;length; // 环 取余 q-&gt;data[q-&gt;tail] = element; q-&gt;count++; return OK;&#125;void output(Queue *q) &#123; int i = q-&gt;head; // 从 head 到 tail do &#123; printf(\"%d \", q-&gt;data[i]); i = (i + 1) % q-&gt;length; &#125; while(i != (q-&gt;tail + 1) % q-&gt;length); printf(\"\\n\");&#125;int front(Queue *q) &#123; return q-&gt;data[q-&gt;head];&#125;void pop(Queue *q) &#123; q-&gt;head = (q-&gt;head + 1) % q-&gt;length; // 环 取余 q-&gt;count--;&#125;int empty(Queue *q) &#123; return q-&gt;count == 0;&#125;void clear(Queue *q) &#123; free(q-&gt;data); free(q);&#125;int main() &#123; Queue *q = (Queue *)malloc(sizeof(Queue)); init(q, 100); for (int i = 1; i &lt;= 10; i++) &#123; push(q, i); &#125; output(q); if (!empty(q)) &#123; printf(\"%d\\n\", front(q)); pop(q); &#125; output(q); clear(q); return 0;&#125; 链表实现队列 链表使用了一个dummy node. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;ctype.h&gt;#include &lt;time.h&gt;#define ERROR 0;#define OK 1;typedef struct Node &#123; int val; struct Node *next;&#125; Node, *Node_p;// 设计为带有dummy head的链表typedef struct Queue &#123; Node *head; Node *tail; int len;&#125; Queue, *Queue_p;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; return n;&#125;Queue *initStack() &#123; Queue_p q = (Queue_p)malloc(sizeof(Queue)); q-&gt;head = initNode(-1); q-&gt;tail = NULL; q-&gt;len = 0; return q;&#125;int push(Queue_p q, int val) &#123; if (!q) return ERROR; Node_p node = initNode(val); // 有tail就在tail之后增加 if (q-&gt;tail) &#123; q-&gt;tail-&gt;next = node; q-&gt;tail = node; &#125; else &#123; node-&gt;next = q-&gt;head-&gt;next; q-&gt;head-&gt;next = node; q-&gt;tail = node; &#125; q-&gt;len++; return OK;&#125;void freeNode(Node_p n) &#123; if (n) free(n);&#125;int pop(Queue_p q) &#123; if (!q || !q-&gt;head || !q-&gt;head-&gt;next) return ERROR; Node_p tmp = q-&gt;head-&gt;next; q-&gt;head-&gt;next = q-&gt;head-&gt;next-&gt;next; freeNode(tmp); // 最后一个节点被弹出 if (q-&gt;head-&gt;next == NULL) q-&gt;tail = NULL; q-&gt;len--; return OK;&#125;int top(Queue_p q) &#123; if (!q || !q-&gt;head || !q-&gt;head-&gt;next) return ERROR; return q-&gt;head-&gt;next-&gt;val;&#125;int empty(Queue_p q) &#123; return (!q || !q-&gt;head || !q-&gt;head-&gt;next);&#125;void freeStack(Queue_p q) &#123; if (!q || !q-&gt;head) return; Node *p = q-&gt;head, *tmp; while (p) &#123; tmp = p; p = p-&gt;next; freeNode(tmp); &#125; free(q);&#125;void showStack(Queue_p q) &#123; if (!q || !q-&gt;head) return; Node_p p = q-&gt;head-&gt;next; while (p) &#123; (p != q-&gt;head-&gt;next) &amp;&amp; printf(\" \"); printf(\"%d\", p-&gt;val); p = p-&gt;next; &#125; printf(\"\\n\");&#125;int main() &#123; srand(time(NULL)); Queue_p q = initStack(); int cnt = 20; while (cnt--) &#123; int val = rand() % 100; int opt = rand() % 4; switch (opt) &#123; case 0: case 1: case 2: printf(\"push %d, %s\\n\", val, push(q, val) ? \"SUC\": \"ERROR\"); showStack(q); break; case 3: empty(q) ? printf(\"Nothing to pop.\\n\") : printf(\"Pop.\\n\"); pop(q); showStack(q); break; &#125; &#125; printf(\"\\nFinal: \\n\"); showStack(q); return 0;&#125; 栈 栈有一个重要的性质，就是 先进后出 ，First In Last Out(FILO)。例如，浏览器页面的多次跳转和多次返会功能，就是栈的应用。 利用一个变量维护栈顶位置。 栈，通过实现一个表达式解析问题，进行实现。 用栈实现表达式求值的算法流程如下： 使用两个栈分别存储数值和运算符。 读取表达式字符，数值存入数值栈，运算符和栈顶运算符比较优先级。 通过运算符优先级不同选择将它压入栈或取出数值栈中两个元素进行计算，计算结果入栈。 返回步骤 2，直至表达式全部读完。 弹出一个运算符和两个数值进行运算，计算结果存储数值栈。 当运算符栈不为空时，返回步骤 5，否则数值栈中剩余的最后一个元素就是表达式求值结果。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;ctype.h&gt;#define ERROR 0#define OK 1typedef struct Stack &#123; int *elements; int max_size, top_index;&#125; Stack;void init(Stack *s, int length) &#123; s-&gt;elements = (int *)malloc(sizeof(int) * length); s-&gt;max_size = length; s-&gt;top_index = -1;&#125;int expand(Stack *s) &#123; if (!s) return ERROR; int expsize = s-&gt;max_size; int *tmp; while (expsize &gt; 0) &#123; tmp = (int *)realloc(s-&gt;elements, sizeof(int) * (s-&gt;max_size + expsize)); if (tmp) break; expsize &gt;&gt;= 1; &#125; if (!tmp) return ERROR; s-&gt;elements = tmp; s-&gt;max_size += expsize; return OK;&#125;int push(Stack *s, int element) &#123; if (s-&gt;top_index &gt;= s-&gt;max_size - 1) &#123; if (!expand(s)) return ERROR; &#125; s-&gt;top_index++; s-&gt;elements[s-&gt;top_index] = element; return OK;&#125;int pop(Stack *s) &#123; if (s-&gt;top_index &lt; 0) &#123; return ERROR; &#125; s-&gt;top_index--; return OK;&#125;int top(Stack *s) &#123; return s-&gt;elements[s-&gt;top_index];&#125;int empty(Stack *s) &#123; if (s-&gt;top_index &lt; 0) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;// 优先级判断int precede(char a, char b) &#123; if ((a == '*'||a=='/') &amp;&amp; (b == '+'||b == '-')) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;// 计算 a theta bint operate(char theta, int a, int b) &#123; if (theta == '+') &#123; return a + b; &#125; else if (theta == '*')&#123; return a * b; &#125; else if (theta == '-')&#123; return a - b; &#125; else if (theta == '/')&#123; return a / b; &#125;&#125;// 计算表达式void calc(Stack *numbers, Stack *operators) &#123; int a = top(numbers); pop(numbers); int b = top(numbers); pop(numbers); push(numbers, operate(top(operators), b, a)); pop(operators);&#125;void clear(Stack *s) &#123; free(s-&gt;elements); free(s);&#125;int main() &#123; int n; scanf(\"%d\", &amp;n); Stack *numbers = (Stack *)malloc(sizeof(Stack)); init(numbers, n); Stack *operators = (Stack *)malloc(sizeof(Stack)); init(operators, n); char *buffer = (char *)malloc(sizeof(char) * (n + 1)); scanf(\"%s\", buffer); int i = 0; while (i &lt; n) &#123; if (isdigit(buffer[i])) &#123; push(numbers, buffer[i] - '0'); i++; &#125; else &#123; if (empty(operators) || precede(buffer[i], top(operators))) &#123; push(operators, buffer[i]); i++; &#125; else &#123; calc(numbers, operators); &#125; &#125; &#125; while (!empty(operators)) &#123; calc(numbers, operators); &#125; printf(\"%d\\n\", top(numbers)); clear(numbers); clear(operators); free(buffer); return 0;&#125; 单调栈 地上从左到右竖立着 n 块木板，从 1 到 n 依次编号，如下图所示。我们知道每块木板的高度，在第 n 块木板右侧竖立着一块高度无限大的木板，现对每块木板依次做如下的操作：对于第 i 块木板，我们从其右侧开始倒水，直到水的高度等于第 i 块木板的高度，倒入的水会淹没 \\(a_i\\) 块木板（如果木板左右两侧水的高度大于等于木板高度即视为木板被淹没）。求 n 次操作后，所有 \\(a_i\\) 的和是多少。 如图所示，在第 4 块木板右侧倒水，可以淹没第 5 块和第 6 块一共 2 块木板，\\(a_4\\) = 2。 建立一个从栈顶到栈底递增的单调栈。假设木板p是栈顶元素，木板q是当前待入栈元素。 将q push 到栈的过程中，如果栈顶元素p出栈则表明我们已经找到了木板p右侧第一块比它高的木板q。 然后只需要记录q与p之间的木板数，求和。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#define ERROR 0#define OK 1typedef struct Node &#123; int id, height;&#125; Node;typedef struct Stack &#123; Node *elements; int max_size, top_index;&#125; Stack;void init(Stack *s, int length) &#123; s-&gt;elements = (Node *)malloc(sizeof(Node) * length); s-&gt;max_size = length; s-&gt;top_index = -1;&#125;int push(Stack *s, Node element) &#123; if (s-&gt;top_index &gt;= s-&gt;max_size - 1) &#123; return ERROR; &#125; s-&gt;top_index++; s-&gt;elements[s-&gt;top_index] = element; return OK;&#125;int pop(Stack *s) &#123; if (s-&gt;top_index &lt; 0) &#123; return ERROR; &#125; s-&gt;top_index--; return OK;&#125;Node top(Stack *s) &#123; return s-&gt;elements[s-&gt;top_index];&#125;int empty(Stack *s) &#123; if (s-&gt;top_index &lt; 0) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;void clear(Stack *s) &#123; free(s-&gt;elements); free(s);&#125;int main() &#123; int n, ans = 0; scanf(\"%d\", &amp;n); Stack *stack = (Stack *)malloc(sizeof(Stack)); init(stack, n); Node temp; for (int i = 1; i &lt;= n; i++) &#123; scanf(\"%d\", &amp;temp.height); temp.id = i; while (!empty(stack) &amp;&amp; top(stack).height &lt;= temp.height) &#123; ans = ans + i - top(stack).id - 1; pop(stack); &#125; push(stack, temp); &#125; while (!empty(stack)) &#123; ans = ans + n + 1 - top(stack).id - 1; pop(stack); &#125; printf(\"%d\\n\", ans); clear(stack); return 0;&#125; 链表实现栈 使用一个带有dummy node的链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;ctype.h&gt;#include &lt;time.h&gt;#define ERROR 0;#define OK 1;typedef struct Node &#123; int val; struct Node *next;&#125; Node, *Node_p;// 设计为带有dummy head的链表typedef struct Stack &#123; Node *head; int len;&#125; Stack, *Stack_p;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;next = NULL; return n;&#125;Stack *initStack() &#123; Stack_p s = (Stack_p)malloc(sizeof(Stack)); s-&gt;head = initNode(-1); s-&gt;len = 0; return s;&#125;int push(Stack_p s, int val) &#123; if (!s) return ERROR; Node_p node = initNode(val); node-&gt;next = s-&gt;head-&gt;next; s-&gt;head-&gt;next = node; s-&gt;len++; return OK;&#125;void freeNode(Node_p n) &#123; if (n) free(n);&#125;int pop(Stack_p s) &#123; if (!s || !s-&gt;head || !s-&gt;head-&gt;next) return ERROR; Node_p tmp = s-&gt;head-&gt;next; s-&gt;head-&gt;next = s-&gt;head-&gt;next-&gt;next; freeNode(tmp); s-&gt;len--; return OK;&#125;int top(Stack_p s) &#123; if (!s || !s-&gt;head || !s-&gt;head-&gt;next) return ERROR; return s-&gt;head-&gt;next-&gt;val;&#125;int empty(Stack_p s) &#123; return (!s || !s-&gt;head || !s-&gt;head-&gt;next);&#125;void freeStack(Stack_p s) &#123; if (!s || !s-&gt;head) return; Node *p = s-&gt;head, *tmp; while (p) &#123; tmp = p; p = p-&gt;next; freeNode(tmp); &#125; free(s);&#125;void showStack(Stack_p s) &#123; if (!s || !s-&gt;head) return; Node_p p = s-&gt;head-&gt;next; while (p) &#123; (p != s-&gt;head-&gt;next) &amp;&amp; printf(\" \"); printf(\"%d\", p-&gt;val); p = p-&gt;next; &#125; printf(\"\\n\");&#125;int main() &#123; srand(time(NULL)); Stack_p s = initStack(); int cnt = 20; while (cnt--) &#123; int val = rand() % 100; int opt = rand() % 4; switch (opt) &#123; case 0: case 1: case 2: printf(\"push %d, %s\\n\", val, push(s, val) ? \"SUC\": \"ERROR\"); showStack(s); break; case 3: empty(s) ? printf(\"Nothing to pop.\\n\") : printf(\"Pop.\\n\"); pop(s); showStack(s); break; &#125; &#125; printf(\"\\nFinal: \\n\"); showStack(s); return 0;&#125; 树 定义 基本概念 分支度：节点拥有的子节点个数。 阶层：从根节点层往下，阶层从 1 往下依次增加。 高度（深度）：树的最大阶层值。⾼度和深度是相反的， ⾼度是从下往上数， 深度是从上往 下。 因此根节点的深度和叶⼦节点的⾼度是 1。 祖先：某节点到根节点的路径上所有节点都是该节点祖先。 树林：多个数的集合。 歪斜树：所有节点只有左孩子，左歪斜树。右歪斜树同理。 满二叉树 一个二叉树，如果每一个层的结点数都达到最大值，则这个二叉树就是满二叉树。 也就是说，如果一个二叉树的层数为 k，且结点总数是\\(2^k -1\\) ，则它就是满二叉树。 完全二叉树 一棵深度为k的有n个结点的二叉树，对树中的结点按从上至下、从左到右的顺序进行编号，如果编号为i（1≤i≤n）的结点与满二叉树中编号为i的结点在二叉树中的位置相同，则这棵二叉树称为完全二叉树。 性质 二叉树第 i 层对多 \\(2^{i-1}\\) 个节点。 层数为 k 的满二叉树节点数为 \\(2^k -1\\)。 二叉树中，终端节点个数，等于度数为 2 的节点个数 + 1。 遍历 先序遍历时，遍历的顺序是从根结点开始，先访问当前结点，如果左子树不为空则继续访问左子树，之后若右子树不为空再访问右子树。在左右子树中依然按照这样的顺序进行遍历。 中序遍历的顺序是从当前结点的左子树开始遍历，再访问当前结点，最后访问右子树。 后序遍历先访问当前结点的左子树，再访问右子树，最后访问当前结点。 简单实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;typedef struct Node &#123; int data; struct Node *lchild, *rchild;&#125; Node;typedef struct Tree &#123; int len; Node *root;&#125; Tree;Node* init(int data) &#123; Node *node =(Node *)malloc(sizeof(Node)); node-&gt;data = data; node-&gt;lchild = NULL; node-&gt;rchild = NULL; return node;&#125;// 保持二叉搜索树结构，插入值Node *insert(Node *root, int val) &#123; if (!root) &#123; Node *n = init(val); return n; &#125; if (root-&gt;data &lt; val) &#123; root-&gt;rchild = insert(root-&gt;rchild, val); &#125; else &#123; root-&gt;lchild = insert(root-&gt;lchild, val); &#125; return root;&#125;// 保持二叉搜索树结构，插入值. 不返回指针，直接在原地址中操作。void insert_no_return(Node **raddr, int val) &#123; if (!(*raddr)) &#123; *raddr = init(val); return; &#125; if ((*raddr)-&gt;data &lt; val) &#123; (*raddr)-&gt;rchild = insert(&amp;((*raddr)-&gt;rchild), val); &#125; else &#123; (*raddr)-&gt;lchild = insert(&amp;((*raddr)-&gt;lchild), val); &#125; return;&#125;void insert_tree(Tree *t, int val) &#123; if (!t) return; t-&gt;root = insert(t-&gt;root, val); t-&gt;len++;&#125;Node* build_demo() &#123; Node *node = init(1); node-&gt;lchild = init(2); node-&gt;rchild = init(3); node-&gt;lchild-&gt;lchild = init(4); node-&gt;lchild-&gt;rchild = init(5); node-&gt;rchild-&gt;rchild = init(6); return node;&#125;void preorder(Node *node) &#123; printf(\"%d \", node-&gt;data); if (node-&gt;lchild != NULL) &#123; preorder(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; preorder(node-&gt;rchild); &#125;&#125;void inorder(Node *node) &#123; if (node-&gt;lchild != NULL) &#123; inorder(node-&gt;lchild); &#125; printf(\"%d \", node-&gt;data); if (node-&gt;rchild != NULL) &#123; inorder(node-&gt;rchild); &#125;&#125;void postorder(Node *node) &#123; if (node-&gt;lchild != NULL) &#123; postorder(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; postorder(node-&gt;rchild); &#125; printf(\"%d \", node-&gt;data);&#125;void clear(Node *node) &#123; if (node-&gt;lchild != NULL) &#123; clear(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; clear(node-&gt;rchild); &#125; free(node);&#125;void clear_tree(Tree *t) &#123; if (t-&gt;root) clear(t-&gt;root); free(t); return;&#125; 已知先序和中序求后序遍历 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;typedef struct Node &#123; int data; struct Node *lchild, *rchild;&#125; Node;Node* init(int data) &#123; Node *node =(Node *)malloc(sizeof(Node)); node-&gt;data = data; node-&gt;lchild = NULL; node-&gt;rchild = NULL; return node;&#125;void postorder(Node *node) &#123; if (node-&gt;lchild != NULL) &#123; postorder(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; postorder(node-&gt;rchild); &#125; printf(\"%d \", node-&gt;data);&#125;// 根据先序和中序建立二叉树的函数 buildNode *build(char pre_str[], char in_str[], int len) &#123; Node *p = init(pre_str[0] - '0'); int pos = strchr(in_str, pre_str[0]) - in_str; if (pos &gt; 0) &#123; p-&gt;lchild = build(pre_str + 1, in_str, pos); &#125; if (len - pos - 1 &gt; 0) &#123; p-&gt;rchild = build(pre_str + pos + 1, in_str + pos + 1, len - pos - 1); &#125; return p;&#125;void clear(Node *node) &#123; if (node-&gt;lchild != NULL) &#123; clear(node-&gt;lchild); &#125; if (node-&gt;rchild != NULL) &#123; clear(node-&gt;rchild); &#125; free(node);&#125;int main() &#123; char pre_str[] = \"136945827\"; char in_str[] = \"963548127\"; Node *root = build(pre_str, in_str, strlen(pre_str)); postorder(root); printf(\"\\n\"); clear(root); return 0;&#125; Huffman编码 1952 年由 David A. Huffman 提出的一种无损数据压缩的编码算法。 哈夫曼编码先统计出每种字母在字符串里出现的频率，根据频率建立一棵路径带权的二叉树，也就是哈夫曼树。 树上每个结点存储字母出现的频率，根结点到结点的路径即是字母的编码，频率高的字母使用较短的编码，频率低的字母使用较长的编码，这样使得编码后的字符串占用空间最小。 实现方法 首先统计每个字母在字符串里出现的频率，把每个字母看成一个结点，结点的权值即是字母出现的频率。 把每个结点看成一棵只有根结点的二叉树，一开始把所有二叉树（结点）都放在一个集合里，接下来开始如下编码： 步骤一：从集合里取出两个根结点权值最小的树a和b，构造出一棵新的二叉树c，二叉树c的根结点的权值为a和b的根结点权值和，二叉树c的左右子树分别是a和b。（合并） 步骤二：将二叉树a和b从集合里删除，把二叉树c加入集合里。（更新候选） 重复以上两个步骤，直到集合里只剩下一棵二叉树，最后剩下的就是哈夫曼树了。 规定每个有孩子的结点，到左孩子的路径为 0，到右孩子的路径为 1。每个字母的编码就是根结点到字母对应结点的路径。 广义表 示例： 广义表为：(5 ( 3 ( 1, 4 ), 6 (, 8 ( 7, ) ) ) ) 实现根据广义表输入，构建树。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576...// 上接树的实现 #include &lt;string.h&gt;// 栈处理广义表typedef struct Stack &#123; Node **elements; int max_size, top_index;&#125; Stack;Stack *init_stack(int length) &#123; Stack *s = (Stack *)malloc(sizeof(Stack)); s-&gt;elements = (Node **)malloc(sizeof(Node *) * length); s-&gt;max_size = length; s-&gt;top_index = -1; return s;&#125;void freeStack(Stack *s) &#123; if (!s) return; free(s-&gt;elements); free(s);&#125;int push(Stack *s, Node *n) &#123; if (!s) return 0; if (s-&gt;top_index == s-&gt;max_size - 1) return 0; s-&gt;elements[++s-&gt;top_index] = n; return 1;&#125;int is_empty(Stack *s) &#123; return !(s &amp;&amp; s-&gt;top_index != -1);&#125;Node *pop(Stack *s) &#123; return s-&gt;elements[s-&gt;top_index--];&#125;Node *build_tree(char *str) &#123; Stack *s = init_stack(100); Node *root, *n; int flag = 0; while(str[0]) &#123; switch (str[0]) &#123; case '(': push(s, n); flag = 0; break; case ',': flag = 1; break; case ')': root = pop(s); break; default: if (str[0] &lt; '0' || str[0] &gt; '9') break; int num = 0; while (str[0] &gt;= '0' &amp;&amp; str[0] &lt;= '9') &#123; num = num * 10 + (str[0] - '0'); &#125; str--; n = init(num); if (!is_empty(s)) flag ? (s-&gt;elements[s-&gt;top_index]-&gt;rchild = n) : \\ (s-&gt;elements[s-&gt;top_index]-&gt;lchild = n); &#125; ++str; &#125; freeStack(s); return root;&#125; 线索二叉树 利用叶子节点的空指针，建立所需要的前驱后继结构。比如下面实现中序遍历的前驱后继。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;time.h&gt;// ltag 和 rtag 表示节点的左右孩子是普通孩子节点还是前驱后继节点。// 前驱后继节点，将叶子节点的空指针孩子指向中序遍历的前驱后继节点。enum &#123; CHILD = 0, THREAD = 1&#125;;typedef struct Node &#123; int val; struct Node *lchild; struct Node *rchild; int rtag, ltag;&#125; Node;Node *initNode(int val) &#123; Node *n = (Node *)malloc(sizeof(Node)); n-&gt;val = val; n-&gt;lchild = NULL; n-&gt;rchild = NULL; n-&gt;ltag = CHILD; n-&gt;rtag = CHILD; return n;&#125;void freeNode(Node *n) &#123; if (!n) return; free(n); return;&#125;Node *insert(Node *root, int val) &#123; if (!root) return initNode(val); if (val &gt; root-&gt;val) root-&gt;rchild = insert(root-&gt;rchild, val); else root-&gt;lchild = insert(root-&gt;lchild, val); return root;&#125;void freeTree(Node *root) &#123; if (!root) return; if (root-&gt;rtag == CHILD) freeTree(root-&gt;rchild); if (root-&gt;ltag == CHILD) freeTree(root-&gt;lchild); freeNode(root); return;&#125;void inorder(Node *root) &#123; if (!root) return; if (root-&gt;ltag == CHILD) inorder(root-&gt;lchild); if (root-&gt;rtag == CHILD) printf(\"%d \", root-&gt;val); inorder(root-&gt;rchild); return;&#125;// 线索二叉树Node *pre = NULL;// 类似inordoer的遍历方法，同时更新叶子节点的前驱和后继void build_thread_tree(Node *root) &#123; if (!root) return; build_thread_tree(root-&gt;lchild); // 更新当前节点（叶子节点）的前驱，及前驱节点的后继。 if (root-&gt;lchild == NULL) &#123; root-&gt;lchild = pre; root-&gt;ltag = THREAD; &#125; if (pre &amp;&amp; !pre-&gt;rchild) &#123; pre-&gt;rchild = root; pre-&gt;rtag = THREAD; &#125; pre = root; build_thread_tree(root-&gt;rchild);&#125;Node *getLeftMost(Node *n) &#123; while (n &amp;&amp; n-&gt;ltag == CHILD &amp;&amp; n-&gt;lchild) n = n-&gt;lchild; return n;&#125;void output(Node *root) &#123; if (!root) return; Node *n = getLeftMost(root); while (n) &#123; printf(\"%d \", n-&gt;val); if (n-&gt;rtag == CHILD) n = getLeftMost(n-&gt;rchild); // 非叶子节点，符合中序遍历 else n = n-&gt;rchild; // 后继 &#125;&#125;int main() &#123; srand(time(NULL)); Node *root = NULL; int cnt = 10; printf(\"Input : \"); while (cnt--) &#123; int val = rand() % 100; printf(\"%d \", val); root = insert(root, val); &#125; printf(\"\\n\"); printf(\"Inorder output: \"); inorder(root); printf(\"\\n\"); printf(\"Thread output : \"); build_thread_tree(root); output(root); printf(\"\\n\"); freeTree(root); return 0;&#125; 图 一个图有很少边（如 \\(e &lt; n\\log (n)\\)，e 指边数，n 指点数）的图称为稀疏图，反之称为稠密图。 顶点的 度 是指依附于某个顶点的边数。 入度 是以该顶点为终点的弧的数目，出度 是以该顶点为起点的弧的数目。 稀疏图，一般用邻接表来存储，这样可以节省空间；如果是稠密图，一般用邻接矩阵来存储。 邻接矩阵 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define MAX_N 500typedef struct Graph &#123; int mat[MAX_N][MAX_N]; int n;&#125; Graph;void init(Graph *g, int n) &#123; g-&gt;n = n; memset(g-&gt;mat, 0, sizeof(g-&gt;mat));&#125;void insert(Graph *g, int a, int x, int y) &#123; if (x &lt; 0 || x &gt;= g-&gt;n || y &lt; 0 || y &gt;= g-&gt;n) &#123; return ; &#125; if (a) &#123; g-&gt;mat[x][y] = 1; g-&gt;mat[y][x] = 1; &#125; else &#123; g-&gt;mat[x][y] = 1; &#125;&#125;void output(Graph *g) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; for (int j = 0; j &lt; g-&gt;n; ++j) &#123; (j) &amp;&amp; printf(\" \"); printf(\"%d\", g-&gt;mat[i][j]); &#125; (i &lt; g-&gt;n - 1) &amp;&amp; printf(\"\\n\"); &#125;&#125;int main() &#123; int n, m, x, y; scanf(\"%d %d\", &amp;n, &amp;m); Graph *graph = (Graph *)malloc(sizeof(Graph)); init(graph, n); for (int i = 0; i &lt; m; ++i) &#123; scanf(\"%d %d\", &amp;x, &amp;y); insert(graph, x, y); &#125; output(graph); free(graph); return 0;&#125; 邻接表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define MAX_N 10000typedef struct Node &#123; int vertex; struct Node *next;&#125; Node, *LinkedList;LinkedList insert_node(LinkedList head, int index) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;vertex = index; node-&gt;next = head; head = node; return head;&#125;typedef struct Graph &#123; int n; LinkedList edges[MAX_N];&#125; Graph;void init(Graph *g, int n) &#123; g-&gt;n = n; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; g-&gt;edges[i] = NULL; &#125;&#125;void insert(Graph *g, int a, int x, int y) &#123; if (x &lt; 0 || x &gt;= g-&gt;n || y &lt; 0 || y &gt;= g-&gt;n) &#123; return ; &#125; if (a) &#123; g-&gt;edges[x] = insert_node(g-&gt;edges[x], y); g-&gt;edges[y] = insert_node(g-&gt;edges[y], x); &#125; else &#123; g-&gt;edges[x] = insert_node(g-&gt;edges[x], y); &#125;&#125;void output(Graph *g) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; printf(\"%d:\", i); for (Node *j = g-&gt;edges[i]; j != NULL; j = j-&gt;next) &#123; printf(\" %d\", j-&gt;vertex); &#125; (i &lt; g-&gt;n - 1) &amp;&amp; printf(\"\\n\"); &#125;&#125;void clear(Graph *g) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; Node *head = g-&gt;edges[i]; while (head != NULL) &#123; Node *delete_node = head; head = head-&gt;next; free(delete_node); &#125; &#125; free(g);&#125;int main() &#123; int n, m, a, x, y; scanf(\"%d %d\", &amp;n, &amp;m); Graph *graph = (Graph *)malloc(sizeof(Graph)); init(graph, n); for (int i = 0; i &lt; m; ++i) &#123; scanf(\"%d %d %d\", &amp;a, &amp;x, &amp;y); insert(graph, a, x, y); &#125; output(graph); clear(graph); return 0;&#125; 图搜索 深度优先 从开始节点，遍历相邻节点，若该节点没有被访问过，递归遍历其相邻节点。遇到没有遍历的，就向下递归。 利用邻接表实现DFS 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define MAX_N 10000typedef struct Node &#123; int vertex; struct Node *next;&#125;Node, *LinkedList;LinkedList insert_node(LinkedList head, int index) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;vertex = index; node-&gt;next = head; head = node; return head;&#125;typedef struct Graph &#123; LinkedList edges[MAX_N]; int n; int visited[MAX_N];&#125;Graph;void init(Graph *g, int n) &#123; g-&gt;n = n; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; g-&gt;edges[i] = NULL; &#125; memset(g-&gt;visited, 0, sizeof(g-&gt;visited));&#125;void insert(Graph *g, int x, int y) &#123; if (x &lt; 0 || x &gt;= g-&gt;n || y &lt; 0 || y &gt;= g-&gt;n) &#123; return ; &#125; g-&gt;edges[x] = insert_node(g-&gt;edges[x], y); g-&gt;edges[y] = insert_node(g-&gt;edges[y], x);&#125;void clear(Graph *g) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; Node *head = g-&gt;edges[i]; while (head != NULL) &#123; Node *delete_node = head; head = head-&gt;next; free(delete_node); &#125; &#125; free(g);&#125;void dfs(Graph *g, int vertex) &#123; printf(\"%d\\n\", vertex); g-&gt;visited[vertex] = 1; for (Node *adj = g-&gt;edges[vertex]; adj != NULL; adj = adj-&gt;next) &#123; if (!g-&gt;visited[adj-&gt;vertex]) &#123; dfs(g, adj-&gt;vertex); &#125; &#125;&#125;int main() &#123; int n, m, k; scanf(\"%d %d\", &amp;n, &amp;m); Graph *graph = (Graph *)malloc(sizeof(Graph)); init(graph, n); for (int i = 0; i &lt; m; ++i) &#123; int x, y; scanf(\"%d %d\", &amp;x, &amp;y); insert(graph, x, y); &#125; scanf(\"%d\", &amp;k); dfs(graph, k); clear(graph); return 0;&#125; 广度优先 先遍历完一个节点的所有相邻节点，再遍历相邻节点的相邻节点。 使用 queue 实现广度优先搜索。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;string.h&gt;#define MAX_N 10000typedef struct Queue &#123; int *data; int head, tail, length;&#125; Queue;void init_queue(Queue *q, int length_input) &#123; q-&gt;data = (int *)malloc(sizeof(int) * length_input); q-&gt;length = length_input; q-&gt;head = 0; q-&gt;tail = -1;&#125;void push(Queue *q, int element) &#123; if (q-&gt;tail + 1 &lt; q-&gt;length) &#123; q-&gt;tail++; q-&gt;data[q-&gt;tail] = element; &#125;&#125;int front(Queue *q) &#123; return q-&gt;data[q-&gt;head];&#125;void pop(Queue *q) &#123; q-&gt;head++;&#125;int empty(Queue *q) &#123; if (q-&gt;head &gt; q-&gt;tail) &#123; return 1; &#125; else &#123; return 0; &#125;&#125;void clear_queue(Queue *q) &#123; free(q-&gt;data); free(q);&#125;typedef struct Node &#123; int vertex; struct Node *next;&#125;Node, *LinkedList;LinkedList insert_node(LinkedList head, int index) &#123; Node *node = (Node *)malloc(sizeof(Node)); node-&gt;vertex = index; node-&gt;next = head; head = node; return head;&#125;typedef struct Graph &#123; LinkedList edges[MAX_N]; int visited[MAX_N]; int n;&#125;Graph;void init_graph(Graph *g, int n) &#123; g-&gt;n = n; memset(g-&gt;visited, 0, sizeof(g-&gt;visited)); for (int i = 0; i &lt; g-&gt;n; ++i) &#123; g-&gt;edges[i] = NULL; &#125;&#125;void insert(Graph *g, int x, int y) &#123; if (x &lt; 0 || x &gt;= g-&gt;n || y &lt; 0 || y &gt;= g-&gt;n) &#123; return ; &#125; g-&gt;edges[x] = insert_node(g-&gt;edges[x], y); g-&gt;edges[y] = insert_node(g-&gt;edges[y], x);&#125;void clear_graph(Graph *g) &#123; for (int i = 0; i &lt; g-&gt;n; ++i) &#123; Node *head = g-&gt;edges[i]; while (head != NULL) &#123; Node *delete_node = head; head = head-&gt;next; free(delete_node); &#125; &#125; free(g);&#125;void bfs(Graph *g, int start_vertex) &#123; Queue *queue = (Queue *)malloc(sizeof(Queue)); init_queue(queue, g-&gt;n); push(queue, start_vertex); g-&gt;visited[start_vertex] = 1; while (!empty(queue)) &#123; int vertex = front(queue); printf(\"%d\\n\", vertex); pop(queue); for (Node *adj = g-&gt;edges[vertex]; adj != NULL; adj = adj-&gt;next) &#123; if (!g-&gt;visited[adj-&gt;vertex]) &#123; g-&gt;visited[adj-&gt;vertex] = 1; push(queue, adj-&gt;vertex); &#125; &#125; &#125; clear_queue(queue);&#125;int main() &#123; int n, m, k; scanf(\"%d %d\", &amp;n, &amp;m); Graph *graph = (Graph *)malloc(sizeof(Graph)); init_graph(graph, n); for (int i = 0; i &lt; m; ++i) &#123; int x, y; scanf(\"%d %d\", &amp;x, &amp;y); insert(graph, x, y); &#125; scanf(\"%d\", &amp;k); bfs(graph, k); clear_graph(graph); return 0;&#125;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Data Structure","slug":"Notes/Data-Structure","permalink":"https://racleray.github.io/categories/Notes/Data-Structure/"}],"tags":[{"name":"data structure","slug":"data-structure","permalink":"https://racleray.github.io/tags/data-structure/"},{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"}],"author":"HeRui"},{"title":"离散数学","slug":"离散数学","date":"2021-10-03T12:54:44.000Z","updated":"2023-08-07T11:54:31.048Z","comments":true,"path":"posts/fbbb19a4.html","link":"","permalink":"https://racleray.github.io/posts/fbbb19a4.html","excerpt":"Brief summary notes about discrete math.","text":"Representation Set: no order set roster notation: \\(\\{1, 2, 3\\}\\) set builder notation: \\(\\{x|P(x)\\}\\), P means Property Empty set: \\(\\phi \\subset \\{1,2,3\\}\\) is vacuously true. （空真） Ordered Pairs (a, b, c): order matters a, b, c : can be different type of elements Cartesian Product: 笛卡尔积 \\(A × B\\) ： is all ordered pairs (a, b) where a \\(\\in\\) A, b \\(\\in\\) B. \\(\\{a,b\\} × \\{0,1\\}\\) = { (a, 0), (a, 1), (b, 0), (b, 1) } A point in a two-dimensional coordinate system can be represented by \\(R \\times R\\). 二维坐标系中的点可表示为 \\(R \\times R\\) Relations Def: ​ A relation R between A and B is a subset of \\(A \\times B\\). Function Def: ​ A function F between A and B is a relation between A and B such that: for every \\(x \\in A\\) there is a \\(y \\in B\\) such that \\((x,y) \\in F\\) if \\((x,y)\\in F \\ and \\ (x,z) \\in F\\) then \\(y=z\\) Ex: \\(x^2 + y^2 = 1\\) is not a function in Discrete Math domain definition. Statement Def: ​ A sentence that is either True or False. Combine Form: \\(\\lnot p\\) \\(p \\land q\\) \\(p\\lor q\\) Ex 1: ​ \"My shirt is gray but my shorts are not.\" ​ p: \"my shirt is gray\", q: \"my shorts are gray\" ​ ~q: \"my shorts are not gray\" ​ the result logic form: \\(p \\land \\lnot q\\) Truth table p q \\(\\lnot p\\) \\(p \\land q\\) \\(p\\lor q\\) T T F T T T F F F T F T T F T F F T F F Logically equivalent Def: ​ Two statements have the same truth table. Ex: \\(p \\equiv \\lnot(\\lnot p)\\) Tautology Def: ​ A tautology is a statement that is always True. Contradiction Def: ​ A contradiction is a statement that is always False. Basic Laws Obvious but useful DeMorgan`s Law: \\(\\lnot(p \\land q) \\equiv (\\lnot p) \\lor (\\lnot) q\\) \\(\\lnot(p \\lor q) \\equiv (\\lnot p) \\land (\\lnot) q\\) Identity Laws: \\(p \\lor contradiction \\equiv p\\) \\(p \\land tautology \\equiv p\\) Universal Bound Laws: \\(p \\land contradiction \\equiv contradiction\\) \\(p \\lor tautology \\equiv tautology\\) Ex: \\[ \\begin{align*} (\\lnot(p \\lor \\lnot q)) \\land tautology(t) &amp; \\\\ (DeMorgan`s\\ Law) &amp; \\equiv (\\lnot p \\land \\lnot(\\lnot q)) \\land t \\\\ (Double\\ Negative) &amp; \\equiv (\\lnot p \\land q) \\land t \\\\ (Identity) &amp; \\equiv \\lnot p \\land q \\end{align*} \\] Conditional statements Def: ​ \\(p \\to q\\) means \"if p is True then q is True\" p q \\(p \\to q\\) \\(\\lnot p \\lor q\\) T T T T T F F F F T T(vacuously true) T F F T(vacuously true) T If p is false, then I didn`t have a assumption indeed. Think it as a vacuously true statement. Vacuously true statement When the hypothesis (\\(p\\)) is false. Ex: if I am a pig, then the world will be better. ​ equal form: Either I am not a pig, or the world will be better. ​ \"I am not a pig.\". It is vacuously true, anyway. So the whole statement is True. Negating a conditional \\(\\lnot (p \\to q) \\equiv \\lnot (\\lnot p \\lor q)\\) \\[ \\begin{align*} \\lnot (\\lnot p \\lor q) &amp; \\equiv (\\lnot \\lnot p \\land \\lnot q) \\\\ &amp; \\equiv p \\land \\lnot q \\end{align*} \\] Contrapositive of a statement Def: ​ \\(p \\to q \\equiv \\lnot q \\to \\lnot p\\) equal form: \\(\\lnot p \\lor q \\equiv q \\lor \\lnot p\\) Converse of a statement Def: ​ Converse of \\(p \\to q\\) is \\(q \\to p\\). Not necessarily logically equivalent. Inverse of a statement Def: ​ Inverse of \\(p \\to q\\) is \\(\\lnot p \\to \\lnot q\\). Cause the contrapositive is \\(\\lnot q \\to \\lnot p\\) which is the converse of \\(\\lnot p \\to \\lnot q\\), so: the inverse of \\(p \\to q\\) is the converse of the contrapositive of \\(p \\to q\\). Biconditional Def: ​ \\(p \\leftrightarrow q\\) means \\(p \\to q\\) and \\(q \\to p\\). Ex: I will pass the exam if and only if I study hard. ​ It means biconditional. Logic Arguments Def: ​ A valid argument is a list of premises from which the conclusion follows. premises: statements. Modus Ponens: 肯定前件推理 if \\(p\\), then \\(q\\) \\(p\\) therefore \\(q\\) p q \\(p \\to q\\)(premises) p(premises) conclusion T T T T T T F F \\(\\boxtimes\\) \\(\\boxtimes\\) F T T \\(\\boxtimes\\) \\(\\boxtimes\\) F F T \\(\\boxtimes\\) \\(\\boxtimes\\) We focus on where premises is true. Modus Tollens: 否定后件推理 if \\(p\\), then \\(q\\) \\(\\lnot q\\) therefore \\(\\lnot p\\) We focus on where premises is true. Generalization \\(p\\) (True) therefore, \\(p \\lor q\\) Specialization \\(p \\land q\\) (True) therefore, \\(p\\) Contradiction \\(\\lnot p \\to contradiction\\) therefore, \\(p\\) Predicate Def: ​ A predicate(谓词) is a statement depending on variables which becomes a statement upon substituting values in the domain. Ex: ​ P(x): x is the variable that is a factor of 12 with domain \\(Z^+\\). P(x) is a predicate. When x = 6, P(6) is True. When x = 7, P(7) is False. True set Def: ​ It is a predicate P(x) satisfies that \\(\\{x \\in D | P(x)\\ is\\ True\\}\\) Universal Quantifier \\(\\forall\\) means \"for all\". \\(\\forall x \\in D, P(x)\\) means: For all x in domain D, \\(P(x)\\) is True. Ex: ​ Every dog is mammal. D is the dog domain, P(x) means x is a mammal. Existential Quantifier \\(\\exists\\) mean \"there exists\" \\(\\exists x \\in D, P(x)\\) means: There exists x in the domain, such that P(x) is True. Ex: ​ Some person is the oldest in the world. D is people in the world. P(x) means that x is the oldest. Summary P: \"The earth is round.\" -&gt; Statement P(x): \"x is round.\" -&gt; Predicate Q: \\(\\forall x \\in D, P(x)\\): \"Every dog is a mammal.\" -&gt; Statement Q: \\(\\exists x \\in D, P(x)\\): \"Some person is the oldest in the world.\" -&gt; Statement Negate Quantifier For example, we have \"\\(\\forall x \\in Z^+, x &gt; 4\\)\". Negate the quantifier: \\[ \\lnot(\\forall x \\in D, P(x)) \\equiv (\\exists x \\in D, \\lnot P(x)) \\] So, for the example, we have \"\\(\\exists x \\in Z^+, x \\leqslant 4\\)\". Ex: ​ Every integer has a larger integer. It is \\(\\forall x \\in Z, P(x): (\\exists y \\in Z, P(y): (y &gt; x))\\). Negate it, we have \\(\\exists x \\in Z, (\\forall y \\in Z, y \\leqslant x)\\), which can`t be True. Ex: ​ Some number in D is the largest. It is \\(\\exists x \\in D, (\\forall y \\in D, x \\geqslant y)\\). Negate it, we have \\(\\forall x \\in D, (\\exists y \\in D, x &lt; y)\\) Conditional Predicate Universal-Conditionals Def: ​ \\(P(x) \\Rightarrow Q(x)\\) means \\(\\forall x \\in D, P(x) \\to Q(x)\\ is\\ True\\). Ex: ​ If x is a dog, then x is a mammal. D maybe all possible animals. Sufficient Condition Def: ​ If \\(A(x)\\) , then \\(B(x)\\). We have \\(A(x)\\) is a sufficient condition for \\(B(x)\\). Necessary Condition Def: ​ If $ A(x)$ , then $ B(x)$. We have \\(A(x)\\) is a necessary condition for \\(B(x)\\). Use contrapositive statement to explain above definition. Proof Precisely define even &amp; odd integers n is even integer if \\(\\exists k \\in Z\\), such that \\(n=2k\\). n is odd integer if \\(\\exists k \\in Z\\), such that \\(n=2k+1\\). Theorem 1 to proof An even integer plus an odd integer is another odd integer. Proof Assumption: ​ Suppose m is even and n is odd. Definitions: ​ \\(\\exists k_1 \\in Z \\ and \\ \\exists k_2 \\in Z\\) , so that \\(m = 2*k_1\\) and \\(n=2*k_2+1\\). Manipulate: Then, \\[ \\begin{align*} m+n &amp; = 2k_1+2k_2+1 \\\\ &amp; = 2(k_1+k_2) + 1 \\end{align*} \\] Definitions: Thus, ​ \\(\\exists k_3 \\in Z\\) so that \\(m+n=2k_3+1\\). Conclusion: ​ Thus \\(m+n\\) is odd. Theorem 2 to proof An even integer times an even integer is another even integer. Proof: Formally stated theorem: ​ \\(\\forall m,n \\in Z\\) if m, n are even, then \\(mn\\) is even. Assumption: ​ Suppose m, n are even. Definition: ​ \\(\\exists k_1, k_2 \\in Z\\) so that \\(m = 2k_1, n=2k_2\\). Manipulation: \\[ \\begin{align*} mn &amp; = 2k_1 *2k_2 \\\\ &amp; = 2(2*k_1*k_2) \\end{align*} \\] Definition: ​ Let t = \\(2k_1k_2\\), \\(t \\in Z\\). Conclusion: ​ Thus, \\(mn\\) is even. Proof by counterexample Aim is prove \\(P(x) \\Rightarrow Q(x)\\) is false. To do that find one \\(a \\in D\\) where \\(P(a) \\land \\lnot Q(a)\\) is true. Proof by division into class Divide the theorem into different cases. Proof by contradiction Suppose \\(\\lnot p\\) is true; Find a contradiction like 0 = 1; Therefore, \\(p\\) is true. \\(\\lnot p \\to contradiction\\) therefore, \\(p\\) Proof by contrapositive \\(p \\to q \\equiv \\lnot q \\to \\lnot p\\) Goal: prove \\(P(x) \\Rightarrow Q(x)\\). Instead, prove: \\(\\lnot Q(x) \\Rightarrow \\lnot P(x)\\). Sequence Def: ​ A sequence is a function \\(f: Z^+ \\to C\\). Ex: ​ \\(f(k) = (-1)^k(3*k)\\) Induction Goal: prove \\(P(n), \\forall n \\geqslant 1\\). Step 1: prove \\(P(a), P(b), ...,P(x)\\) is true. Step 2: Assume \\(P(k)\\) is true, Prove \\(P(k+1)\\) is true, \\(x &lt; k\\). Skipped things Relations of Sets. Permutation and combination. Bayes` theorem. Markov Chain. Graph Def: ​ A graph (V, E) has a set V called \"vertices\" and a set E called \"edges\" that consisting of two-element subsets of V. Ex: ​ \\(V = \\{A,B,C,D\\}\\) ​ \\(E = \\{\\{A,B\\}, \\{B,A\\}, \\{B,D\\}, \\{D,B\\}, \\{B,C\\}, \\{C,D\\}, \\{A,C\\}\\}\\) Complete Graph Def: ​ A simple undirected graph in which every pair of distinct vertices is connected by a unique edge. The notation is \\(K_n\\). The edge number is \\(\\frac {n(n-1)}{2}\\). Connected Graph Def: ​ A graph is connected if you can get from any vertex to any other via edges. Induced Subgraph Def: ​ \\((V_1, E_1)\\) is an induced subgraph of \\((V_2,E_2)\\) if it is a graph where \\(V_1 \\subseteq V_2\\) and \\(E_1\\) contains all possible edges consisting of vertices in \\(V_1\\) and \\(E_1 \\subset E_2\\). Degree Def: ​ The degree of a vertex is the number of edges attached. Fact: ​ Sum of degrees of all vertices is even. Cause every edge adds 2 degrees. Ex: ​ Among 5 people, could everyone be friends with exactly 2 people? The degree is \\(5 * 2=10\\), it`s possible. ​ Among 5 people, could everyone be friends with exactly 3 people? The degree is \\(5 * 3=15\\), it`s not possible. Euler Path Def: ​ An Euler Path walks through a graph using every edge exactly once. ​ An Euler Circuit starts and stops at the same vertex when walking through a Euler Path. Theorem: ​ If a graph has an Euler Circuit, then every vertex has even degree. Contrapositive Theorem: ​ If a graph has a vertex with odd degree, no Euler Circuit is possible.","categories":[{"name":"Math","slug":"Math","permalink":"https://racleray.github.io/categories/Math/"},{"name":"basic","slug":"Math/basic","permalink":"https://racleray.github.io/categories/Math/basic/"}],"tags":[{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"},{"name":"math","slug":"math","permalink":"https://racleray.github.io/tags/math/"}],"author":"HeRui"},{"title":"Embedding & Searching","slug":"Embedding-Searching","date":"2021-09-05T06:59:46.000Z","updated":"2023-08-07T11:54:31.028Z","comments":true,"path":"posts/9e991c30.html","link":"","permalink":"https://racleray.github.io/posts/9e991c30.html","excerpt":"","text":"相似性搜索常见于以图搜图，听歌识曲...这类抽象查找问题中。你没有明确的Key，不能使用SQL之类的方法查找数据库。但是可以通过抽象的 embedding 向量来进行检索。 一般样本向量表示可以通过 Skip-gram with negative sampling 方法、DSSM 这类方法、BERT 这类方法等等。得到向量表达之后，一般还需要高效的召回索引方法，因为暴力匹配在较大数据量场景下，速度通常差强人意。 这类通过抽象 embedding 的具有语义检索能力的方法，如下图所示： 一组相关算法的 benchmarks 对比图如下： Annoy Spotify音乐公司开源的工具库。 这是一种基于树的方法。 算法简单描述： 定义： N -- 树模型的总个数； ​ K -- 叶节点最大样本数； ​ M -- 目标Top M样本数； ​ D -- 联通两个子空间所要求的最大距离； 过程： ==建树== 1234for i in (1, ... , `N`): 1. 随机选取两个样本点，计算两者中点处超平面，划分两侧样本为两个子树； 2. 继续划分样本点，直到叶子节点中样本数不大于 `K`； 3. 保存二叉树（即保存每个划分点的值）； ==搜索== 1234567for j in (1, ... , `N`): 1. 从根节点开始，根据二分节点，向下遍历； 2. if 两个子空间相近（随机选的两个点距离小于 `D`），同时向两个子节点，向下遍历； 3. 保存找到的叶子节点空间（节点集合）；聚合`N`个叶子节点集合，以找到距离目标节点最近的 Top `M `个样本点；返回结果 Annoy利用二叉树的结构对空间进行随机划分，建索引阶段效率有所提升。二叉树结构在检索时效率也很高。 特点： 召回率较优，和暴力搜索法相比较基本一致 查询速度很快 千万量级的item，时间为若干小时，尚可以忍受 千万量级的item，以100棵树为例，索引文件大约几个G，有点大了 多核利用支持的不是很好 ScaNN ScaNN是google提出的高效向量检索算法，文中说这个方法比目前已有的其他方法有更高的精度，检索速度也要快两倍。工具库开源地址 GitHub。 这种算法主要是优化 Inner product 距离度量下的搜索。这类问题被叫做 maximum inner-product search (MIPS)。在大数据量场景下，MIPS的计算复杂度是比较高的，穷举搜索几乎不可能在期望的时间范围内完成。 论文中，阐述了目前使用的向量压缩（比如聚类或者降维）所使用方法，会使得压缩后的向量间平均距离变小。这可能导致了向量差异性的损失，比如与query向量的内积的相对大小关系会出现错误。比如这样： 上图中，x 被分别压缩到 c 点，其余 q 的内积大小关系发生颠倒。本来 \\(&lt;q,x_2&gt; greater\\ than &lt;q,x_1&gt;\\)， 压缩后 \\(&lt;q,x_1&gt; greater\\ than &lt;q,x_2&gt;\\)。 论文中指出，这种方法压缩，只考虑了距离的长度大小，而没有考虑距离的角度方向。 一个简单的例子，计算两个向量的内积，当一个向量在平行于投影方向（两个向量方向都可作为投影方向）变化 k ，内积的变化为 \\(d_1\\)。而如果实在一个向量的垂直方向，变化 k ，内积的变化为 \\(d_2\\)。那么，\\(d_2 ≥ d_1\\)一定成立。画个图就能验证。 所以，现在在压缩时，对于 x 与 c 在平行于 x 向量方向上的变化，给予一个大的惩罚项；而对于 x 与 c 在垂直于 x 向量方向上的变化，给予一个较小的惩罚项。以此来进行压缩。取了名字叫，Anisotropic Vector Quantization。 效果如下： 内积相对大小关系，没有变化。按照论文中的说法，这样做最大化了压缩后各个点之间的平均距离，有利于差异化相似度的值。 算法大致流程： 根据样本数量大小，选择是否将数据进行 Partitioning。若 Partitioning，在检索时，会先选出 Top m 个Partitioning，再进行细化检索。 Scoring：使用快速的粗粒度的距离度量，计算query相对所有样本（或Partition的样本）的距离。选出 Top k 个。 Rescoring: 对 Top k 个Scoring结果，进行更精确的距离度量，重拍后输出 Top k 的结果。 使用 doc 比较简短，可以参考。 HNSW HNSW是一种图算法。其根据搜索场景的特点，设计出了这种算法。 首先，简单想想图的搜索，麻烦的问题可能有哪些。可能有孤立的节点，或者可能相邻节点太多。在近邻搜索场景下，可能有几个距离目标很近的节点，但是没有相互连通，那么就需要遍历更多的路径，从而遍历完全这几个节点。 另外，节点众多时，当两个节点距离相对较远时，遍历数量会指数级增加。 HNSW的解决方法如下： 定义： ​ N -- 总样本数； ​ K -- 每个节点最多有K个相连的近邻节点； ​ P1 -- 以 P1 的概率将节点设置为二级索引； ​ P2 -- 以 P2 的概率将节点设置为一级索引（P1 &gt; P2，两者指数递减）； ​ M -- 目标Top M样本数； 过程： ==建图== 1234567891011121314151617181920for i in (1, ... , N): if (随机概率值 p) &gt; P1: 二级索引图插入节点： 1. 将被插入节点连接指向最近邻的 K 个节点（小于等于 K）； 2. 更新被连接的 K 个节点（小于等于 K）的最近邻的 K 个节点； 3. 保证每个节点都有连接，且最大连接不超过 K； if (随机概率值 p) &gt; P2: 一级索引图插入节点： 1. 将被插入节点连接指向最近邻的 K 个节点（小于等于 K）； 2. 更新被连接的 K 个节点（小于等于 K）的最近邻的 K 个节点； 3. 保证每个节点都有连接，且最大连接不超过 K； 全量样本索引图插入节点： 1. 将被插入节点连接指向最近邻的 K 个节点（小于等于 K）； 2. 更新被连接的 K 个节点（小于等于 K）的最近邻的 K 个节点； 3. 保证每个节点都有连接，且最大连接不超过 K； 返回建立的多级索引图 ==搜索== 1231. 在二级索引图中找到最近邻节点 A2. 在一级索引图中从 A 开始找到最近邻节点 B3. 在全量样本索引图中从 B 开始找到 Top M 的目标节点 HNSW设计的插入机制，保证了图具有良好的连通性和局部搜索便捷性。类似跳表结构的多级索引机制，提高了搜索效率，降低了整体搜索复杂度。 算法细节看文章，这里只是草草写写思想。开源工具库C++版：hnswlib 特点： 召回率优秀，和暴力搜索基本一致 千万量级的item，构图可在分钟级别完成 多核利用优秀 查询速度很快 LSH 在 min hash 之上，更进一步优化了在大量文本相似性聚类的算法。首先需要计算每一个文本的多个不同 min hash 表示，构成 min hash 值向量。然后对 min hash 向量分块进行简单的元素对比，检查是否相似。此处LSH，只关注基于 Jaccard similarity 的文本处理，其他LSH实现不涉及。 这就是一种基于词统计的 hash 分桶算法。其中没有计算欧式距离这种计算量较大的操作，仅仅是元素对比。 算法如下： 定义: K -- 取文本中连续 K 个词为bag of words的计数对象（这里被称为 K-Shingling）； ​ N -- min hash向量的维度，即进行随机 min hash 的次数； ​ M -- 文本总数； ​ b -- LSH中hash分组的组数； ​ r -- b组 min hash 子向量的维度； 过程： ==min hash== 1234561. 对整个文本集构建以 K 个连续词为一个对象的bag of words统计矩阵，文本中存在的对象标记为1，不存在标记为0；2. for i in (1, ... , N): A. 随机指定统计矩阵中一个 index 为起始位置（保存index）； B. 从 index 开始，在每一个文档列中，向下查找第一非 0 位置； C. 取第一非 0 位置与 index 位置的偏移量为 min hash 向量的第 i 行，有 M 个 min hash 向量；3. 得到 (N, M) 的 min hash 矩阵 ==局部敏感hash(LSH)== 1234561. 将 min hash 矩阵均分为 b 组；2. 分桶聚类： for j in (1, ..., b): A. 两两对比第 j 组 min hash 子向量； B. 子向量完全相同时，将该两个向量对应文本，放到一个相似桶中（类似聚类）；3. 保存分桶结果，每个桶中就是相似的文本 关于 min hash，它就是一种 Jaccrad similarity 的另一种表示。 \\(P(hash(S_i)=hash(S_j))\\) 就等价于 \\(Jaccard(S_i, S_j)\\)。证明也挺简单的： 只看两个 min hash 向量中不全为 0 的行，记为 \\(sub\\_hash\\)。 此时 \\(hash(S_i)=hash(S_j)\\) 概率就等价于 \\(sub\\_hash\\) 的第一行都为 1 的概率。 \\(sub\\_hash\\) 的第一行都为 1 的概率，就等于\\(sub\\_hash\\) 中｛两列都为 1 的行数 / 任意一列为 1 的行数｝。 \\(sub\\_hash\\) 中｛两列都为 1 的行数 / 任意一列为 1 的行数｝，就是 \\(Jaccard(S_i, S_j)\\)。 另外，调整参数 b 和 r 可以间接调整分桶的相似性阈值。假设两个 min hash 向量相似概率为 \\(p\\)。分到同一个桶中的概率可表示为（只要有一组 sub hash 相同就分到一个桶）： \\[ 1 - (1 - p^r)^b \\] 开源工具库：mattilyra/LSH 特点： 查找速度快 方便去重处理 处理大量数据速度较快 Product Quantizer Product Quantizer简称为PQ，是一种建立索引的方法。优化了K-Means聚类的计算量。 定义： ​ N -- 样本总量； ​ D -- embedding维度； ​ K -- 子空间数量； ​ C -- K-means聚类类别数； 过程： ==建索引== 123456781. 将 N 个 D 维embedding，切分为 K 组 (N, D/K) 的embedding；2. for i in (1, ..., K): A. 对第 i 个 (N, D/K)的embedding，计算 C 个聚类中心点3. for j in (1, ..., N): for k in (1, ..., K): A. 将第 j 个样本标记到第 k 组聚类的所属中心点编号； B. 循环得到第 j 个样本的 K 维中心点编号向量； 得到 (N, K) 的样本编码矩阵 ==搜索== 123456789101112131415161. 将输入 embedding 划分为 K 组 sub embedding；2. for i in (1, ..., K): for j in (1, ..., C): A. 计算 sub embedding 与聚类中心 j 的距离； B. 循环得到 C 维的距离向量； 循环得到 (K, C) 的距离矩阵3. 计算输入与所有样本的距离： for i in (1, ..., N): 取 (N, K) 的样本编码矩阵中第 i 行 (1, K), 记为 Q; 定义输入与样本 i 的距离为 di； for k in (1, ..., K): A. 取 Q[k] 的值，记为 q； B. 取 (K, C) 的距离矩阵中第 k 行，记为 dist； C. di += dist[q]; 记录输入与样本 i 的距离为 di 的值；4. 从 N 个 di 距离中找到需要的样本 这种方法，将暴力搜索，转化为 K * N 次距离表查找过程。 Inverted File System 这个方法（简称 IVF）相当于使用倒排表来优化索引。比如 IVF + PQ，来优化 K * N 次的查询操作。IVF， PQ 在 Faiss 库中都有实现。 简述一下方法： 在 PQ 建立索引的过程中增加一步： 将 (N, K) 的样本，分别保存到 C_ivf个聚类之下，即，保存到 C_ivf个代表一个类的数组中。得到 C_ivf 个类别字典，key 为聚类中心点（id），value 为该聚类中所有样本点的 PQ 编码数组。（按照 C_ivf个聚类类别进行倒排样本） 在 PQ 查询中增加一步： 在第三步第二层 for 循环中，先求得与输入样本最近的 C_ivf 聚类中心，然后在该聚类的 PQ 编码数组中计算每个样本与输入的距离。 利用倒排，减少了搜索范围。从全部样本搜索，变成先找聚类，再在类中搜索。 这里建立倒排的 K-means 聚类与 PQ 中使用的不同，使用一个更粗粒度的 K-means 聚类，但是没有对 N 个 embedding 进行划分。 显然，IVF虽然建立索引的过程更复杂一些，但是搜索的过程会更快速。 Structure-based Approximations 这类近似搜索，将搜索过程抽象为一个神经网络表示的函数，输入对象，输出搜索结果。当然以下方法需要监督数据支持，或者可以构建无监督的共现关系。 Negative sampling softmax: 简化版的NCE近似计算，将多分类，转为多个正负例二分类。复杂的负例分布抽样时，可以采用importance sampling进行简化。 Class-based softmax: 使用两个softmax，第一个预测 class，第二个基于输入和 class 预测搜索目标。 Hierarchical softmax： 构建 Huffman tree，然后每个节点只进行一个二值分类预测，直到叶子节点。预测直接变成了 tree height 次二分类计算。 Binary code prediction： 将所有对象的index，转化为二进制表示，使用多个sigmoid，预测每个位置是 0 还是 1，得到目标index的二进制表示。 Embedding Prediction： 预测 embedding 表示。这个方法是对 Embedding inference 模型的近似，思路基本上和蒸馏差不多。对于下游搜索匹配，没有影响。这个方法用到一种损失，Von-Mises Fisher distribution loss，约束了输出embedding靠近一个单位球面（超球面）。 End 相似性搜索，和计算 margin loss 的分类问题目标比较接近。 \\[ Loss_{margin}(x,y,\\hat{y})=max(0, 1+s(\\hat{y}|x)-s(y|x)) \\] 另外，基于树的搜索，还有KD Tree算法，这里不涉及。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"searching","slug":"searching","permalink":"https://racleray.github.io/tags/searching/"}]},{"title":"Practical BERT","slug":"Practical-BERT","date":"2021-07-22T08:01:17.000Z","updated":"2023-08-07T11:54:31.033Z","comments":true,"path":"posts/372ecc5c.html","link":"","permalink":"https://racleray.github.io/posts/372ecc5c.html","excerpt":"","text":"BERT类模型，基本使用流程：1. Further pretrain. 2. single-task or multi-task finetuning. 3. inference Further pretraining一般使用任务数据进行，也可以使用与任务数据相似的 in-domain 数据，或者使用数据量更大但是和任务数据不那么相关的数据进行。 一般而言使用任务数据的效果会好一些。但是数据量不足，且能找到与任务数据相似的 in-domain 数据，也可以稳定提高模型效果。（ref） Transformer Representations Transformer 不同的层捕获不同层次的表示。比如、下层是表层（字、词）特征，中层是句法特征，上层是语义特征。 如图为不同的embedding表示，输入BiLSTM进行NER任务的结果对比。 BERT的不同层编码的信息非常不同，因此适当的池化策略，应该根据不同应用而改变，因为不同的层编码不同的信息。 Hugging Face的BERT模型一般输出为： last hidden state (batch size, seq len, hidden size) which is the sequence of hidden states at the output of the last layer. pooler output (batch size, hidden size) - Last layer hidden-state of the first token of the sequence hidden states (n layers, batch size, seq len, hidden size) - Hidden states for all layers and for all ids. Pooler output pooler output，最后一层的[CLS] token的hidden state，接一个Linear layer 和一个 Tanh activation function的结果。预训练时，作为next sentence prediction (classification) objective的计算结果。 在config中可以设置 pooling layer 为 False，不输出这一结果。 1234567891011121314151617181920212223242526...max_seq_length = 256_pretrained_model = 'roberta-base'config = AutoConfig.from_pretrained(_pretrained_model)model = AutoModel.from_pretrained(_pretrained_model, config=config)tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)clear_output()features = tokenizer.batch_encode_plus( train_text, add_special_tokens=True, padding='max_length', max_length=max_seq_length, truncation=True, return_tensors='pt', return_attention_mask=True)outputs = model(features['input_ids'], features['attention_mask'])pooler_output = outputs[1]logits = nn.Linear(config.hidden_size, 1)(pooler_output) # regression head... More than last hidden state 最后一层输出的 hidden state，[batch, maxlen, hidden_state]。其中[batch, 1, hidden_state]对应 [CLS]。 CLS Embeddings 12345with torch.no_grad(): outputs = model(features['input_ids'], features['attention_mask'])last_hidden_state = outputs[0]cls_embeddings = last_hidden_state[:, 0] 可以处理简单的下游任务，将cls_embeddings作为整个序列的一个简单表示。 Mean Pooling 1234567891011121314151617181920212223features = tokenizer.batch_encode_plus( train_text, add_special_tokens=True, padding='max_length', max_length=max_seq_length, truncation=True, return_tensors='pt', return_attention_mask=True)attention_mask = features['attention_mask']...# Step 1: Expand Attention Mask from [batch_size, max_len] to [batch_size, max_len, hidden_size].input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()# Step 2: Sum Embeddings along max_len axis so now we have [batch_size, hidden_size]sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)# Step 3: Sum Mask along max_len axis.sum_mask = input_mask_expanded.sum(1)sum_mask = torch.clamp(sum_mask, min=1e-9)# Step 4: Take Average.mean_embeddings = sum_embeddings / sum_masklogits = nn.Linear(config.hidden_size, 1)(mean_embeddings) # regression head 在max len维度，进行平均。 Max Pooling 在max len维度，进行max pooling 123# input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()# last_hidden_state[input_mask_expanded == 0] = -1e9 # Set padding tokens to large negative valuemax_embeddings = torch.max(last_hidden_state, 1)[0] Mean-Max Pooling (Head) 123456789101112input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)sum_mask = input_mask_expanded.sum(1)sum_mask = torch.clamp(sum_mask, min=1e-9)mean_embeddings = sum_embeddings / sum_maskmax_pooling_embeddings, _ = torch.max(last_hidden_state, 1)cls_embeddings = last_hidden_state[:, 0]mean_max_embeddings = torch.cat((mean_pooling_embeddings, max_pooling_embeddings, cls_embeddings), 1)logits = nn.Linear(config.hidden_size * 3, 1)(mean_max_embeddings) # 3 hidden size Conv-1D Pooling 12345678# first define layerscnn1 = nn.Conv1d(768, 256, kernel_size=2, padding=1)cnn2 = nn.Conv1d(256, 1, kernel_size=2, padding=1)last_hidden_state = last_hidden_state.permute(0, 2, 1) # (batch size, hidden size, seq len)cnn_embeddings = F.relu(cnn1(last_hidden_state))cnn_embeddings = cnn2(cnn_embeddings)logits, _ = torch.max(cnn_embeddings, 2) # max pooling in Length dim More than Hidden States Output embeddings 与 每一层的输出集合，(n_layers, batch_size, sequence_length, hidden_size)。 123456789101112131415161718192021222324...max_seq_length = 256_pretrained_model = 'roberta-base'config = AutoConfig.from_pretrained(_pretrained_model)# 设置输出选项config.update(&#123;'output_hidden_states':True&#125;)model = AutoModel.from_pretrained(_pretrained_model, config=config)tokenizer = AutoTokenizer.from_pretrained(_pretrained_model)clear_output()features = tokenizer.batch_encode_plus( train_text, add_special_tokens=True, padding='max_length', max_length=max_seq_length, truncation=True, return_tensors='pt', return_attention_mask=True)with torch.no_grad(): outputs = model(features['input_ids'], features['attention_mask'])all_hidden_states = torch.stack(outputs[2]) CLS Layer Embeddings 倒数第二层示例 1234layer_index = 11 # second to last hidden layercls_embeddings = all_hidden_states[layer_index, :, 0] # 13 layers (embedding + num of blocks)logits = nn.Linear(config.hidden_size, 1)(cls_embeddings) # regression head GitHub的bert-as-service 项目，就是默认取得倒数第二层的输出。更好地表示语义，而不被MLM任务和NSP任务影响太多。 Concatenate Pooling 最后四层 CLS concat. 12345678all_hidden_states = torch.stack(outputs[2])concatenate_pooling = torch.cat( (all_hidden_states[-1], all_hidden_states[-2], all_hidden_states[-3], all_hidden_states[-4]), -1)concatenate_pooling = concatenate_pooling[:, 0] # first tokenlogits = nn.Linear(config.hidden_size*4, 1)(concatenate_pooling) Weighted Layer Pooling 基于一个intuition，fine-tuning时，最容易被训练的应该是middle layer的表达，因为顶层是专门用于 language modeling pre-train 任务的。所以只使用顶层的输出进行下游任务，会限制模型的效果。（没有实证，一个假设） 12345678910111213141516171819202122232425class WeightedLayerPooling(nn.Module): def __init__(self, num_hidden_layers, layer_start: int = 4, layer_weights = None): super(WeightedLayerPooling, self).__init__() self.layer_start = layer_start self.num_hidden_layers = num_hidden_layers self.layer_weights = layer_weights if layer_weights is not None \\ else nn.Parameter( torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float) ) def forward(self, all_hidden_states): all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :] weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size()) weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) / self.layer_weights.sum() return weighted_average # 使用：最后四层的 CLS 表示，计算加权和layer_start = 9pooler = WeightedLayerPooling( config.num_hidden_layers, layer_start=layer_start, layer_weights=None)weighted_pooling_embeddings = pooler(all_hidden_states)weighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]logits = nn.Linear(config.hidden_size, 1)(weighted_pooling_embeddings) LSTM/GRU Pooling \\[ o = h^L_{LSTM} =LSTM(h^i_{CLS}), i ∈ [1, L] \\] CLS token输入LSTM，得到最终表示 12345678910111213141516171819202122class LSTMPooling(nn.Module): def __init__(self, num_layers, hidden_size, hiddendim_lstm): super(LSTMPooling, self).__init__() self.num_hidden_layers = num_layers self.hidden_size = hidden_size self.hiddendim_lstm = hiddendim_lstm self.lstm = nn.LSTM(self.hidden_size, self.hiddendim_lstm, batch_first=True) self.dropout = nn.Dropout(0.1) def forward(self, all_hidden_states): ## forward hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze() for layer_i in range(1, self.num_hidden_layers+1)], dim=-1) hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size) out, _ = self.lstm(hidden_states, None) out = self.dropout(out[:, -1, :]) return outhiddendim_lstm = 256pooler = LSTMPooling(config.num_hidden_layers, config.hidden_size, hiddendim_lstm)lstm_pooling_embeddings = pooler(all_hidden_states)logits = nn.Linear(hiddendim_lstm, 1)(lstm_pooling_embeddings) # regression head Attention Pooling dot-product attention： \\[ o = W^T_{h} softmax(qh^T_{CLS})h_{CLS} \\] \\(W^T_{h}\\) 和 \\(q\\)为可学习参数。 1234567891011121314151617181920212223242526272829303132class AttentionPooling(nn.Module): def __init__(self, num_layers, hidden_size, hiddendim_fc): super(AttentionPooling, self).__init__() self.num_hidden_layers = num_layers self.hidden_size = hidden_size self.hiddendim_fc = hiddendim_fc self.dropout = nn.Dropout(0.1) q_t = np.random.normal(loc=0.0, scale=0.1, size=(1, self.hidden_size)) self.q = nn.Parameter(torch.from_numpy(q_t)).float() w_ht = np.random.normal(loc=0.0, scale=0.1, size=(self.hidden_size, self.hiddendim_fc)) self.w_h = nn.Parameter(torch.from_numpy(w_ht)).float() def forward(self, all_hidden_states): hidden_states = torch.stack([all_hidden_states[layer_i][:, 0].squeeze() for layer_i in range(1, self.num_hidden_layers+1)], dim=-1) hidden_states = hidden_states.view(-1, self.num_hidden_layers, self.hidden_size) out = self.attention(hidden_states) out = self.dropout(out) return out def attention(self, h): v = torch.matmul(self.q, h.transpose(-2, -1)).squeeze(1) v = F.softmax(v, -1) v_temp = torch.matmul(v.unsqueeze(1), h).transpose(-2, -1) v = torch.matmul(self.w_h.transpose(1, 0), v_temp).squeeze(2) return vhiddendim_fc = 128pooler = AttentionPooling(config.num_hidden_layers, config.hidden_size, hiddendim_fc)attention_pooling_embeddings = pooler(all_hidden_states)logits = nn.Linear(hiddendim_fc, 1)(attention_pooling_embeddings) # regression head WKPooling 来自论文: SBERT-WK: A Sentence Embedding Method By Dissecting BERT-based Word Models 通过计算 每一层每个token的 alignment and novelty properties，得到每个token的 unified word representation。然后根据计算得到每个token 的 word importance ，加权求和得到一个 Sentence Embedding 表示。 计算中用到 QR分解，然而pytorch在GPU上计算QR很慢，所以转到CPU上计算，但是这依然很慢（相比于其他在GPU上的操作）。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192class WKPooling(nn.Module): def __init__(self, layer_start: int = 4, context_window_size: int = 2): super(WKPooling, self).__init__() self.layer_start = layer_start self.context_window_size = context_window_size def forward(self, all_hidden_states): ft_all_layers = all_hidden_states org_device = ft_all_layers.device all_layer_embedding = ft_all_layers.transpose(1,0) all_layer_embedding = all_layer_embedding[:, self.layer_start:, :, :] # Start from 4th layers output # torch.qr is slow on GPU (see https://github.com/pytorch/pytorch/issues/22573). So compute it on CPU until issue is fixed all_layer_embedding = all_layer_embedding.cpu() attention_mask = features['attention_mask'].cpu().numpy() unmask_num = np.array([sum(mask) for mask in attention_mask]) - 1 # Not considering the last item embedding = [] # One sentence at a time for sent_index in range(len(unmask_num)): sentence_feature = all_layer_embedding[sent_index, :, :unmask_num[sent_index], :] one_sentence_embedding = [] # Process each token for token_index in range(sentence_feature.shape[1]): token_feature = sentence_feature[:, token_index, :] # 'Unified Word Representation' token_embedding = self.unify_token(token_feature) one_sentence_embedding.append(token_embedding) ##features.update(&#123;'sentence_embedding': features['cls_token_embeddings']&#125;) one_sentence_embedding = torch.stack(one_sentence_embedding) sentence_embedding = self.unify_sentence(sentence_feature, one_sentence_embedding) embedding.append(sentence_embedding) output_vector = torch.stack(embedding).to(org_device) return output_vector def unify_token(self, token_feature): ## Unify Token Representation window_size = self.context_window_size alpha_alignment = torch.zeros(token_feature.size()[0], device=token_feature.device) alpha_novelty = torch.zeros(token_feature.size()[0], device=token_feature.device) for k in range(token_feature.size()[0]): left_window = token_feature[k - window_size:k, :] right_window = token_feature[k + 1:k + window_size + 1, :] window_matrix = torch.cat([left_window, right_window, token_feature[k, :][None, :]]) Q, R = torch.qr(window_matrix.T) r = R[:, -1] alpha_alignment[k] = torch.mean(self.norm_vector(R[:-1, :-1], dim=0), dim=1).matmul(R[:-1, -1]) / torch.norm(r[:-1]) alpha_alignment[k] = 1 / (alpha_alignment[k] * window_matrix.size()[0] * 2) alpha_novelty[k] = torch.abs(r[-1]) / torch.norm(r) # Sum Norm alpha_alignment = alpha_alignment / torch.sum(alpha_alignment) # Normalization Choice alpha_novelty = alpha_novelty / torch.sum(alpha_novelty) alpha = alpha_novelty + alpha_alignment alpha = alpha / torch.sum(alpha) # Normalize out_embedding = torch.mv(token_feature.t(), alpha) return out_embedding def norm_vector(self, vec, p=2, dim=0): ## Implements the normalize() function from sklearn vec_norm = torch.norm(vec, p=p, dim=dim) return vec.div(vec_norm.expand_as(vec)) def unify_sentence(self, sentence_feature, one_sentence_embedding): ## Unify Sentence By Token Importance sent_len = one_sentence_embedding.size()[0] var_token = torch.zeros(sent_len, device=one_sentence_embedding.device) for token_index in range(sent_len): token_feature = sentence_feature[:, token_index, :] sim_map = self.cosine_similarity_torch(token_feature) var_token[token_index] = torch.var(sim_map.diagonal(-1)) var_token = var_token / torch.sum(var_token) sentence_embedding = torch.mv(one_sentence_embedding.t(), var_token) return sentence_embedding def cosine_similarity_torch(self, x1, x2=None, eps=1e-8): x2 = x1 if x2 is None else x2 w1 = x1.norm(p=2, dim=1, keepdim=True) w2 = w1 if x2 is x1 else x2.norm(p=2, dim=1, keepdim=True) return torch.mm(x1, x2.t()) / (w1 * w2.t()).clamp(min=eps) 123pooler = WKPooling(layer_start=9)wkpooling_embeddings = pooler(all_hidden_states)logits = nn.Linear(config.hidden_size, 1)(wkpooling_embeddings) # regression head Other methods Dense Pooling Word Weight (TF-IDF) Pooling Async Pooling Parallel / Heirarchical Aggregation 每个任务上，不同的方法表现有差异，应该根据情况选择使用不同的方法。 Few-Shot Stability Fine-tuning Transformer models通常不稳定，收到超参数的影响大，不同的 random seed 也会导致不同的结果。 比如，在训练时，每个epoch中多进行evaluating，而不是在每个epoch之后evaluating，能够增加stability。 其他方法有： Debiasing Omission In BertADAM Re-Initializing Transformer Layers Utilizing Intermediate Layers Layer-wise Learning Rate Decay (LLRD) Mixout Regularization Pre-trained Weight Decay Stochastic Weight Averaging 通常不会一起用，可能会互向影响。方法各自提出的环境也不同。所以一般一两种方法的使用，能够提高模型性能。 Debiasing Omission In BERTAdam 1234rescaled_grad = clip(grad * rescale_grad, clip_gradient)m = beta1 * m + (1 - beta1) * rescaled_gradv = beta2 * v + (1 - beta2) * (rescaled_grad**2)w = w - learning_rate * (m / (sqrt(v) + epsilon) + wd * w) 和标准Adam，差别在于 wd * w，增加了 weight decay。论文 Fixing Weight Decay Regularization in Adam 中提出的 AdamW，保留了 bias-correction terms （上面伪代码第2，3行），并且将 wd * w 加入 learning_rate 的影响。将下图中的紫色 weight decay 方法，改为绿色的部分。这样更新 x 时，weight decay 不会耦合参数 \\(\\beta\\) 和 \\(w_t\\)（第7、8行），而是直接作用于 x（第12行）。 Debiasing Omission就是要保留 bias-correction terms 。在HuggingFace Transformers AdamW中设置 correct_bias parameter 为 True （default）。 1234567891011121314151617181920lr = 2e-5epsilon = 1e-6weight_decay = 0.01no_decay = [\"bias\", \"LayerNorm.weight\"]optimizer_grouped_parameters = [&#123; \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay, \"lr\": lr&#125;, &#123;\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0, \"lr\": lr&#125;]optimizer = AdamW( optimizer_grouped_parameters, lr=lr, eps=epsilon, correct_bias=True) Reinitializing Transformer Layers 来自计算机视觉的直觉，高层与预训练任务相关的层，可以重新训练。 比如，Reinitialize Pooler layer的参数 12345678910111213141516171819202122232425add_pooler = Truereinit_pooler = Trueclass Net(nn.Module): def __init__(self, config, _pretrained_model, add_pooler): super(Net, self).__init__() self.roberta = RobertaModel.from_pretrained(_pretrained_model, add_pooling_layer=add_pooler) self.classifier = RobertaClassificationHead(config) def forward(self, input_ids, attention_mask): outputs = self.roberta(input_ids, attention_mask=attention_mask,) sequence_output = outputs[0] logits = self.classifier(sequence_output) return logits model = Net(config, _pretrained_model, add_pooler)if reinit_pooler: print('Reinitializing Pooler Layer ...') encoder_temp = getattr(model, _model_type) encoder_temp.pooler.dense.weight.data.normal_(mean=0.0, std=encoder_temp.config.initializer_range) encoder_temp.pooler.dense.bias.data.zero_() for p in encoder_temp.pooler.parameters(): p.requires_grad = True print('Done.!') 对 RoBERTa 的transformer layer进行 Reinitialize 。 RoBERTa 不同于 BERT 在于： 没有 next-sentence pretraining objective ，更改了超参，更大的 batch size 和learning rate 使用 byte level BPE 的 tokenizer 没有 token_type_ids，只需要 用 sep_token 分开不同 sentence。 检查是否被初始化： 1234for layer in model.roberta.encoder.layer[-reinit_layers:]: for module in layer.modules(): if isinstance(module, nn.Linear): print(module.weight.data) 重新初始化 12345678910111213141516171819reinit_layers = 2# TF version uses truncated_normal for initialization. This is Pytorchif reinit_layers &gt; 0: print(f'Reinitializing Last &#123;reinit_layers&#125; Layers ...') encoder_temp = getattr(model, _model_type) for layer in encoder_temp.encoder.layer[-reinit_layers:]: for module in layer.modules(): if isinstance(module, nn.Linear): module.weight.data.normal_(mean=0.0, std=config.initializer_range) if module.bias is not None: module.bias.data.zero_() elif isinstance(module, nn.Embedding): module.weight.data.normal_(mean=0.0, std=config.initializer_range) if module.padding_idx is not None: module.weight.data[module.padding_idx].zero_() elif isinstance(module, nn.LayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) print('Done.!') 另外 XLNet 实现方式有些不同 （Transformer-XL）： 123456789101112131415161718192021222324252627reinit_layers = 2if reinit_layers &gt; 0: print(f'Reinitializing Last &#123;reinit_layers&#125; Layers ...') for layer in model.transformer.layer[-reinit_layers :]: for module in layer.modules(): if isinstance(module, (nn.Linear, nn.Embedding)): module.weight.data.normal_(mean=0.0, std=model.transformer.config.initializer_range) if isinstance(module, nn.Linear) and module.bias is not None: module.bias.data.zero_() elif isinstance(module, nn.LayerNorm): module.bias.data.zero_() module.weight.data.fill_(1.0) elif isinstance(module, XLNetRelativeAttention): for param in [ module.q, module.k, module.v, module.o, module.r, module.r_r_bias, module.r_s_bias, module.r_w_bias, module.seg_embed, ]: param.data.normal_(mean=0.0, std=model.transformer.config.initializer_range) print('Done.!') BART，是 seq2seq的结构，\"BERT\"作为encoder，\"GPT\"作为decoder（left-to-right）。采用了多样的噪声预训练方式。 随机token masking 随机token deletion 随机连续tokens替换为一个mask，或者直接插入一个mask 随机打乱文本sentence顺序 将文本序列连成环，随机选择文本开始位置 BART文本理解任务效果可以持平RoBERTa，且适合文本生成任务，而模型大小仅仅比BERT大10%。 12345678910111213reinit_layers = 2_model_type = 'bart'_pretrained_model = 'facebook/bart-base'config = AutoConfig.from_pretrained(_pretrained_model)config.update(&#123;'num_labels':1&#125;)model = AutoModelForSequenceClassification.from_pretrained(_pretrained_model)if reinit_layers &gt; 0: print(f'Reinitializing Last &#123;reinit_layers&#125; Layers ...') for layer in model.model.decoder.layers[-reinit_layers :]: for module in layer.modules(): model.model._init_weights(module) print('Done.!') 实验表明， Re-initialization 对 random seed 更 robust。不建议初始化超过6层的layer，不同任务需要实验找到最好的参数。 Utilizing Intermediate Layers 此部分就是本文第一节 Transformer Representations 的内容。 LLRD - Layerwise Learning Rate Decay 就是 low layer 通用信息，低学习率，top layer 任务相关信息，相对较高学习率。 一种方法是，每层有一个 decay rate 12345678910111213141516171819202122232425262728293031323334353637def get_optimizer_grouped_parameters( model, model_type, learning_rate, weight_decay, layerwise_learning_rate_decay): no_decay = [\"bias\", \"LayerNorm.weight\"] # initialize lr for task specific layer optimizer_grouped_parameters = [ &#123; \"params\": [p for n, p in model.named_parameters() if \"classifier\" in n or \"pooler\" in n], \"weight_decay\": 0.0, \"lr\": learning_rate, &#125;, ] # initialize lrs for every layer num_layers = model.config.num_hidden_layers layers = [getattr(model, model_type).embeddings] + list(getattr(model, model_type).encoder.layer) layers.reverse() lr = learning_rate for layer in layers: lr *= layerwise_learning_rate_decay optimizer_grouped_parameters += [ &#123; \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay, \"lr\": lr, &#125;, &#123; \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0, \"lr\": lr, &#125;, ] return optimizer_grouped_parameters 其他方法 见 第三节 Training Strategies 的 Differential / Discriminative Learning Rate 部分。 Mixout Regularization 不同于 Dropout 将神经元以概率p丢弃，Mixout 是将神经元参数以概率 p 替换为预训练模型的参数。意思就是有两组参数，一组来自预训练好的模型，另一组为当前训练的参数。 如图，替换为红色模型的参数。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# https://github.com/bloodwass/mixoutimport torchimport torch.nn as nnimport torch.nn.init as initimport torch.nn.functional as Ffrom torch.nn import Parameterfrom torch.autograd.function import InplaceFunctionclass Mixout(InplaceFunction): @staticmethod def _make_noise(input): return input.new().resize_as_(input) @classmethod def forward(cls, ctx, input, target=None, p=0.0, training=False, inplace=False): if p &lt; 0 or p &gt; 1: raise ValueError(\"A mix probability of mixout has to be between 0 and 1,\" \" but got &#123;&#125;\".format(p)) if target is not None and input.size() != target.size(): raise ValueError( \"A target tensor size must match with a input tensor size &#123;&#125;,\" \" but got &#123;&#125;\".format(input.size(), target.size()) ) ctx.p = p ctx.training = training if ctx.p == 0 or not ctx.training: return input if target is None: target = cls._make_noise(input) target.fill_(0) target = target.to(input.device) if inplace: ctx.mark_dirty(input) output = input else: output = input.clone() ctx.noise = cls._make_noise(input) if len(ctx.noise.size()) == 1: ctx.noise.bernoulli_(1 - ctx.p) else: ctx.noise[0].bernoulli_(1 - ctx.p) ctx.noise = ctx.noise[0].repeat(input.size()[0], 1) ctx.noise.expand_as(input) if ctx.p == 1: output = target else: output = ((1 - ctx.noise) * target + ctx.noise * output - ctx.p * target) / (1 - ctx.p) return output @staticmethod def backward(ctx, grad_output): if ctx.p &gt; 0 and ctx.training: return grad_output * ctx.noise, None, None, None, None else: return grad_output, None, None, None, Nonedef mixout(input, target=None, p=0.0, training=False, inplace=False): return Mixout.apply(input, target, p, training, inplace)class MixLinear(torch.nn.Module): __constants__ = [\"bias\", \"in_features\", \"out_features\"] # for jit optimization def __init__(self, in_features, out_features, bias=True, target=None, p=0.0): super(MixLinear, self).__init__() self.in_features = in_features self.out_features = out_features self.weight = Parameter(torch.Tensor(out_features, in_features)) if bias: self.bias = Parameter(torch.Tensor(out_features)) else: self.register_parameter(\"bias\", None) self.reset_parameters() self.target = target self.p = p def reset_parameters(self): init.kaiming_uniform_(self.weight, a=math.sqrt(5)) if self.bias is not None: fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight) bound = 1 / math.sqrt(fan_in) init.uniform_(self.bias, -bound, bound) def forward(self, input): return F.linear(input, mixout(self.weight, self.target, self.p, self.training), self.bias) def extra_repr(self): type = \"drop\" if self.target is None else \"mix\" return \"&#123;&#125;=&#123;&#125;, in_features=&#123;&#125;, out_features=&#123;&#125;, bias=&#123;&#125;\".format( type + \"out\", self.p, self.in_features, self.out_features, self.bias is not None ) 使用： 123456789101112131415if mixout &gt; 0: print('Initializing Mixout Regularization') for sup_module in model.modules(): for name, module in sup_module.named_children(): if isinstance(module, nn.Dropout): module.p = 0.0 if isinstance(module, nn.Linear): target_state_dict = module.state_dict() bias = True if module.bias is not None else False new_module = MixLinear( module.in_features, module.out_features, bias, target_state_dict[\"weight\"], mixout ) new_module.load_state_dict(target_state_dict) setattr(sup_module, name, new_module) print('Done.!') Mixout相当于一种自适应的 L2-regularizer ，使得参数变化不会很剧烈。能够提高finetuning稳定性。 Pre-trained Weight Decay 将 weight decay 中，gradient减去的 \\(\\lambda w\\) 变为 \\(\\lambda (w - w^{pretrained})\\)。在Mixout文章中，实验表明这样比普通的 weight decay ，finetuning更稳定。 123456789101112131415161718192021222324252627282930313233343536class PriorWD(Optimizer): def __init__(self, optim, use_prior_wd=False, exclude_last_group=True): super(PriorWD, self).__init__(optim.param_groups, optim.defaults) self.param_groups = optim.param_groups self.optim = optim self.use_prior_wd = use_prior_wd self.exclude_last_group = exclude_last_group self.weight_decay_by_group = [] for i, group in enumerate(self.param_groups): self.weight_decay_by_group.append(group[\"weight_decay\"]) group[\"weight_decay\"] = 0 # w pretrained self.prior_params = &#123;&#125; for i, group in enumerate(self.param_groups): for p in group[\"params\"]: self.prior_params[id(p)] = p.detach().clone() def step(self, closure=None): if self.use_prior_wd: for i, group in enumerate(self.param_groups): for p in group[\"params\"]: if self.exclude_last_group and i == len(self.param_groups): p.data.add_(-group[\"lr\"] * self.weight_decay_by_group[i], p.data) else: # w - w pretrained p.data.add_( -group[\"lr\"] * self.weight_decay_by_group[i], p.data - self.prior_params[id(p)], ) loss = self.optim.step(closure) return loss def compute_distance_to_prior(self, param): assert id(param) in self.prior_params, \"parameter not in PriorWD optimizer\" return (param.data - self.prior_params[id(param)]).pow(2).sum().sqrt() 使用: 12345678910optimizer_grouped_parameters = get_optimizer_grouped_parameters(model, learning_rate, weight_decay)optimizer = AdamW( optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon, correct_bias=not use_bertadam)# 修改 optimizeroptimizer = PriorWD(optimizer, use_prior_wd=use_prior_wd) Training Strategies 提升模型速度或准确性的方法： Stochastic Weight Averaging MADGRAD Optimizer Differential / Discriminative Learning Rate Dynamic Padding and Uniform Length Batching Gradient Accumulation Freeze Embedding Numeric Precision Reduction Gradient Checkpointing Stochastic Weight Averaging learning rate schedule经过设计，使得模型在“最优解”附近徘徊，而不是收敛到一点（理论上）。比如75%的时间使用standard decaying learning rate strategy ；而剩下的训练在一个相对较高的constant learning rate上训练。 计算训练最后阶段的滑动平均作为SWA的权重 SWA权重在训练时，不参与计算。计算Batch Normalization的activation statistics时，在训练结束后，单独进行一次 forward pass 得到activation statistics。 所以，有两组权重，一组训练BP，一组计算保存SWA权重。 示例 123456789101112131415161718192021222324from torch.optim.swa_utils import AveragedModel, SWALRfrom torch.optim.lr_scheduler import CosineAnnealingLRloader, optimizer, model, loss_fn = ...swa_start = 5swa_model = AveragedModel(model)swa_scheduler = SWALR(optimizer, swa_lr=0.05)scheduler = CosineAnnealingLR(optimizer, T_max=100)for epoch in range(100): for input, target in loader: optimizer.zero_grad() loss_fn(model(input), target).backward() optimizer.step() if epoch &gt; swa_start: swa_model.update_parameters(model) swa_scheduler.step() else: scheduler.step()# Update bn statistics for the swa_model at the endtorch.optim.swa_utils.update_bn(loader, swa_model)# Use swa_model to make predictions on test data preds = swa_model(test_input) refernce MADGRAD Optimizer AdaGrad派生出的新优化器，在以下任务中表现较好，包括视觉中的分类和图像到图像的任务，以及自然语言处理中的循环和双向掩蔽模型。 但是，weight decay不同于其他，常常设置为0。 learning rate 的设置也和SGD与Adam不同，必要时，先进行一次learning rate查找。 paper 1pip -q install madgrad 另外小数据集上的训练，可以尝试使用RAdam + Lookahead而不是AdamW，效果可能会更好，因为AdamW的warm-up阶段受到数据集大小size的影响。 Differential / Discriminative Learning Rate 模型底层为普遍的字词信息，越往上得到与任务相关的抽象信息。所以 fine-tune 时，对通用层设置较小学习率，越往上学习率相对更大。自定义层学习率单独设置，一般较大。 12345678910111213141516171819202122232425262728293031323334def get_optimizer_params(model, type='unified'): # differential learning rate and weight decay param_optimizer = list(model.named_parameters()) learning_rate = 5e-5 no_decay = ['bias', 'gamma', 'beta'] if type == 'unified': optimizer_parameters = filter(lambda x: x.requires_grad, model.parameters()) elif type == 'module_wise': optimizer_parameters = [ &#123;'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.01&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay_rate': 0.0&#125;, &#123;'params': [p for n, p in model.named_parameters() if \"roberta\" not in n], 'lr': 1e-3, 'weight_decay_rate':0.01&#125; ] elif type == 'layer_wise': group1=['layer.0.','layer.1.','layer.2.','layer.3.'] group2=['layer.4.','layer.5.','layer.6.','layer.7.'] group3=['layer.8.','layer.9.','layer.10.','layer.11.'] group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.'] optimizer_parameters = [ &#123;'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay_rate': 0.01&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.01, 'lr': learning_rate/2.6&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.01, 'lr': learning_rate&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.01, 'lr': learning_rate*2.6&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay_rate': 0.0&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.0, 'lr': learning_rate/2.6&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.0, 'lr': learning_rate&#125;, &#123;'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.0, 'lr': learning_rate*2.6&#125;, &#123;'params': [p for n, p in model.named_parameters() if \"roberta\" not in n], 'lr':1e-3, \"momentum\" : 0.99&#125;, ] return optimizer_parameters Strategies: 对于小数据集，复杂的learning rate scheduling strategies（linear with warmup or cosine with warmup etc.）在预训练和finetuning阶段都没什么效果。小数据集，使用简单的scheduling strategies就行。 12345678from transformers import ( get_constant_schedule, get_constant_schedule_with_warmup, get_cosine_schedule_with_warmup, get_cosine_with_hard_restarts_schedule_with_warmup, get_linear_schedule_with_warmup, get_polynomial_decay_schedule_with_warmup) Interpreting Transformers with LIT transfomer可视化工具 Paper: The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models Blog: The Language Interpretability Tool (LIT): Interactive Exploration and Analysis of NLP Models Official Page: Language Interpretability Tool Examples: GitHub Dynamic Padding and Uniform Length Batching 常规padding策略如上图所示，pad到最大长度。 Dynamic Padding就是每个batch，分别pad到该batch中最长的序列长度。 而Uniform Length Batching，则是将长度相近的序列组合成一个batch。 示例程序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126import randomimport numpy as npimport multiprocessingimport more_itertoolsimport torchimport torch.nn as nnfrom torch.utils.data import Sampler, Dataset, DataLoaderclass SmartBatchingDataset(Dataset): “tokenize并得到dataloader” def __init__(self, df, tokenizer): super(SmartBatchingDataset, self).__init__() # 这里 df.excerpt 表示dataframe中的文本所在列，使用时需要替换 self._data = ( f\"&#123;tokenizer.bos_token&#125; \" + df.excerpt + f\" &#123;tokenizer.eos_token&#125;\" ).apply(tokenizer.tokenize).apply(tokenizer.convert_tokens_to_ids).to_list() self._targets = None if 'target' in df.columns: self._targets = df.target.tolist() self.sampler = None def __len__(self): return len(self._data) def __getitem__(self, item): if self._targets is not None: return self._data[item], self._targets[item] else: return self._data[item] def get_dataloader(self, batch_size, max_len, pad_id): self.sampler = SmartBatchingSampler( data_source=self._data, batch_size=batch_size ) collate_fn = SmartBatchingCollate( targets=self._targets, max_length=max_len, pad_token_id=pad_id ) dataloader = DataLoader( dataset=self, batch_size=batch_size, sampler=self.sampler, collate_fn=collate_fn, num_workers=(multiprocessing.cpu_count()-1), pin_memory=True ) return dataloader class SmartBatchingSampler(Sampler): “按序列长度排序，得到一组shuffle之后的batch data” def __init__(self, data_source, batch_size): super(SmartBatchingSampler, self).__init__(data_source) self.len = len(data_source) sample_lengths = [len(seq) for seq in data_source] argsort_inds = np.argsort(sample_lengths) self.batches = list(more_itertools.chunked(argsort_inds, n=batch_size)) self._backsort_inds = None def __iter__(self): if self.batches: last_batch = self.batches.pop(-1) np.random.shuffle(self.batches) self.batches.append(last_batch) self._inds = list(more_itertools.flatten(self.batches)) yield from self._inds def __len__(self): return self.len @property def backsort_inds(self): “未shuffle时，batch序列按照长度排序的结果” if self._backsort_inds is None: self._backsort_inds = np.argsort(self._inds) return self._backsort_inds class SmartBatchingCollate: “每个batch分别pad到最大长度，得到attention mask，处理target” def __init__(self, targets, max_length, pad_token_id): self._targets = targets self._max_length = max_length self._pad_token_id = pad_token_id def __call__(self, batch): if self._targets is not None: sequences, targets = list(zip(*batch)) else: sequences = list(batch) input_ids, attention_mask = self.pad_sequence( sequences, max_sequence_length=self._max_length, pad_token_id=self._pad_token_id ) if self._targets is not None: output = input_ids, attention_mask, torch.tensor(targets) else: output = input_ids, attention_mask return output def pad_sequence(self, sequence_batch, max_sequence_length, pad_token_id): max_batch_len = max(len(sequence) for sequence in sequence_batch) max_len = min(max_batch_len, max_sequence_length) padded_sequences, attention_masks = [[] for i in range(2)] attend, no_attend = 1, 0 for sequence in sequence_batch: # 限制model所允许的最大长度 new_sequence = list(sequence[:max_len]) attention_mask = [attend] * len(new_sequence) pad_length = max_len - len(new_sequence) new_sequence.extend([pad_token_id] * pad_length) attention_mask.extend([no_attend] * pad_length) padded_sequences.append(new_sequence) attention_masks.append(attention_mask) padded_sequences = torch.tensor(padded_sequences) attention_masks = torch.tensor(attention_masks) return padded_sequences, attention_masks 使用： 12dataset = SmartBatchingDataset(train, tokenizer)dataloader = dataset.get_dataloader(batch_size=24, max_len=max_len, pad_id=tokenizer.pad_token_id) 已经证明，这种技术不仅显著的减少了训练时间，而且不会减少准确性（在某些情况下甚至提高）。 Freeze Embedding Freezing Embedding Layer of transformers加速训练并节省显存。 一种解释是（看看就好，没有严格证明）：finetuning用的小数据集中，新出现的token会导致原language model中学习好的局部同义词间结构关系被破坏。 12345678910import transformersfrom transformers import AutoConfig, AutoModelForSequenceClassificationfreeze_embedding = Trueconfig = AutoConfig.from_pretrained('roberta-base')model = AutoModelForSequenceClassification.from_pretrained( _pretrained_model, config=config)model.base_model.embeddings.requires_grad_(not freeze_embedding) 这种方法，可以一试，可以用更大的batch size。 Numeric Precision Reduction 混合精度。常见的推理加速方法。 在过去的几年，GPU硬件对float16操作的糟糕支持，意味着降低权重和激活值的精度通常会适得其反，但是NVIDIA Volta和Turing架构与张量核心的引入，意味着现代GPU可以更高效的支持float16运算。 大多数transformer网络都可以简单地转换为float16权值和激活值计算，而没有精度损失。 之所以要保留float32，是因为像softmax这类，计算时有较长的连加运算，这时使用float16可能存在精度损失。 半精度的加速来源于，半精度计算指令本身的速度更快，另外此时可以使用更大的batch size。 开源工具：NVIDIA-apex；torch.cuda.amp--相比AMP，使用更灵活一些，但是用起来都差不多。 注意：小batch size时，混合精度会由于频繁IO导致的时间损失大于小batch的半精度训练所节省的时间。 示例 Gradient Accumulation 就是累计梯度几个轮次，然后进行一次参数更新。 示例程序 1234567891011optimizer.zero_grad() # Reset gradients tensorsfor i, (inputs, labels) in enumerate(training_set): predictions = model(inputs) # Forward pass loss = loss_function(predictions, labels) # Compute loss function loss = loss / accumulation_steps # Normalize our loss (if averaged) loss.backward() # Backward pass if (i+1) % accumulation_steps == 0: # Wait for several backward steps optimizer.step() # Now we can do an optimizer step optimizer.zero_grad() # Reset gradients tensors if (i+1) % evaluation_steps == 0: # Evaluate the model when we... evaluate_model() # ...have no gradients accumulated 如果我们的损失是在训练样本上平均的，我们还需要除以积累步骤的数量。 Gradient Checkpointing 以时间为代价，节省GPU内存。 通过将模型分为不同的段，每个段计算时，分别进行计算，将当前段计算结果传给下一个段后，当前段的中间状态都不会保存。 示例 PyTorch Checkpoint多GPU优化 其他开源工具 DeepSpeed FairScale Accelerate Some Exp 讲实话小样集的比赛，是个调参游戏，对于回归任务，更是如此。结构上玩出花，效果反而不好。优化方法上，常见的FGM和SWA对于一些回归任务，基本没啥效果提升。没有尝试过对比学习，可能这是一个好方法。 比赛中对于这类强学习模型，还是最好bagging，降降variance。 只把BERT这些庞然大物当做特征抽取器，然后用传统ML算法来学习，会是更方便快捷的方法，而且实践上也确实可行，只是很容易过拟合，BERT特征还是太强了（当然这取决于如何训练和训练的程度）。这可能更快，但是得到好结果需要一些“运气”，还有一点直觉。 对于比赛而言，large模型得到好结果还是容易一些，就像好多论文，其实方法就那样，主要BERT 类模型用large调得好，也容易出好结果。可解释的余地还是太差了。各部分又几乎是关联的，耦合在一起，所以算了。 好的算法，不应该是亿级参数对数据的变相记忆，那是“数据库”的加权求和与激活函数筛选。让人感觉失望。这里的一面之词是，大模型，总是“不太好”。可是，效果有了，很多时候，还是会真香。 NLP Tutorial The Super Duper NLP Repo Huggingface Community Hugging Face’s notebooks reference： kaggle rhtsingh REVISITING FEW-SAMPLE BERT FINE-TUNING ON THE STABILITY OF FINE-TUNING BERT SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models Fine-Tuning Pretrained Language Models:Weight Initializations, Data Orders, and Early Stopping MIXOUT: EFFECTIVE REGULARIZATION TO FINETUNE LARGE-SCALE PRETRAINED LANGUAGE MODELS How to Fine-Tune BERT for Text Classification? Sentence Encoders on STILTs: Supplementary Training on Intermediate Labeled-data Tasks","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"https://racleray.github.io/tags/BERT/"}]},{"title":"BERT_topic_analysis","slug":"BERT-topic-analysis","date":"2021-07-19T14:26:37.000Z","updated":"2023-08-07T11:54:31.026Z","comments":true,"path":"posts/c228cec2.html","link":"","permalink":"https://racleray.github.io/posts/c228cec2.html","excerpt":"简单学习下BERT Topic Analysis的相关内容，看看Kaggle上的代码实验。","text":"运行 Colab Connect to kaggle 12345678!pip install --user kaggle!mkdir /root/.kaggle!mv /content/kaggle.json /root/.kaggle/kaggle.json!kaggle competitions download -c commonlitreadabilityprize!ls 123456import osfor filename in os.listdir('.'): if filename.endswith('.zip'): os.system(\"unzip &#123;&#125;\".format(filename)) os.system(\"rm &#123;&#125;\".format(filename)) Visualization with Bokeh 123456789!pip install --upgrade pip!pip install --upgrade numpy!pip install --upgrade sentence_transformers!conda install -c conda-forge hdbscan --y!pip install bokeh!pip install --upgrade bertopic[visualization]# !pip uninstall numpy# !pip install numpy 1234567891011121314151617181920212223from bertopic import BERTopicimport pandas as pdfrom sentence_transformers import SentenceTransformerimport sklearn.manifoldimport umapimport numpy as npimport pandas as pdimport randomfrom nltk.corpus import stopwordsrandom.seed(42)from bokeh.io import output_file, showfrom bokeh.models import ColumnDataSource, HoverTool, LinearColorMapperfrom bokeh.palettes import plasma, d3, Turbo256from bokeh.plotting import figurefrom bokeh.transform import transformimport bokeh.iobokeh.io.output_notebook()import bokeh.plotting as bplimport bokeh.models as bmobpl.output_notebook() 读取数据 123456789101112test = pd.read_csv('test.csv')train = pd.read_csv('train.csv')train['set'] = 'train'test['set'] = 'test'combined = pd.concat([train, test], ignore_index=True)combined.target.fillna(3, inplace=True)texts = combined.excerpt.values.tolist()targets = combined.target.values.tolist()sets = combined.set.values.tolist() 文本处理（可选） 123456789101112131415161718192021222324252627282930313233343536373839# def preprocess_tweet_data(data,name):# # Lowering the case of the words in the sentences# data[name]=data[name].str.lower()# # Code to remove the Hashtags from the text# data[name]=data[name].apply(lambda x:re.sub(r'\\B#\\S+','',x))# # Code to remove the links from the text# data[name]=data[name].apply(lambda x:re.sub(r\"http\\S+\", \"\", x))# # Code to remove the Special characters from the text # data[name]=data[name].apply(lambda x:' '.join(re.findall(r'\\w+', x)))# # Code to substitute the multiple spaces with single spaces# data[name]=data[name].apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))# # Code to remove all the single characters in the text# data[name]=data[name].apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))# # Remove the twitter handlers# data[name]=data[name].apply(lambda x:re.sub('@[^\\s]+','',x))def preprocess(data): excerpt_processed=[] for e in data['excerpt']: # find alphabets e = re.sub(\"[^a-zA-Z]\", \" \", e) e = re.sub(r'\\s+', ' ', e, flags=re.I) # # convert to lower case # e = e.lower() # tokenize words e = nltk.word_tokenize(e) # remove stopwords e = [word for word in e if not word.lower() in set(stopwords.words(\"english\"))] # lemmatization lemma = nltk.WordNetLemmatizer() e = [lemma.lemmatize(word) for word in e] e=\" \".join(e) excerpt_processed.append(e) return excerpt_processed 使用sentence bert计算文档向量 1model = SentenceTransformer('stsb-distilbert-base') id1embeddings = model.encode(texts) 降维 t-SNE 与 umap t-SNE保留数据中局部结构。 UMAP保留数据中的本地和大部分全局结构。 UMAP比tSNE要快得多，当面对更多数据、更高维数据时 12color_mapper = LinearColorMapper(palette='Plasma256', low=min(targets), high=max(targets))out = sklearn.manifold.TSNE(n_components=2).fit_transform(embeddings) 绘制Boken。test set标签设置为3。 123456789101112131415161718192021SETS = ['train', 'test']MARKERS = ['circle', 'triangle']list_x = out[:,0]list_y = out[:,1]desc = textssource = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc, targets=targets, dset=sets))hover = HoverTool(tooltips=[ (\"index\", \"$index\"), (\"(x,y)\", \"(@x, @y)\"), ('desc', '@desc'), ('targets', '@targets'), ('dset', '@dset')])p = figure(plot_width=800, plot_height=800, tools=[hover], title=\"First Look at the Data\")p.scatter('x', 'y', size=10, source=source, legend='dset', color=&#123;'field': 'targets', 'transform': color_mapper&#125;, marker=bokeh.transform.factor_mark('dset', MARKERS, SETS),)bpl.show(p) 12umap_model = umap.UMAP(n_neighbors=15, n_components=2, metric='cosine')out_umap = umap_model.fit_transform(embeddings) 123456789101112131415161718192021SETS = ['train', 'test']MARKERS = ['circle', 'triangle']list_x = out_umap[:,0]list_y = out_umap[:,1]desc = textssource = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc, targets=targets, dset=sets))hover = HoverTool(tooltips=[ (\"index\", \"$index\"), (\"(x,y)\", \"(@x, @y)\"), ('desc', '@desc'), ('targets', '@targets'), ('dset', '@dset')])p = figure(plot_width=800, plot_height=800, tools=[hover], title=\"First Look at the Data\")p.scatter('x', 'y', size=10, source=source, legend='dset', color=&#123;'field': 'targets', 'transform': color_mapper&#125;, marker=bokeh.transform.factor_mark('dset', MARKERS, SETS),)bpl.show(p) Kmeans 观察数据 12345from sklearn.decomposition import PCAfrom sklearn.cluster import MiniBatchKMeansfrom sklearn.metrics import silhouette_samples, silhouette_scoreimport matplotlib.pyplot as pltimport matplotlib.cm as cm 1234567891011121314151617def find_optimal_clusters(data, max_k): iters = range(2, max_k+1, 1) sse = [] # 轮廓系数 for k in iters: cluster = MiniBatchKMeans(n_clusters=k, init_size=256, batch_size=512, random_state=20).fit(data) silhouette_avg = silhouette_score(data, cluster.labels_) sse.append(silhouette_avg) print('Fit &#123;&#125; clusters'.format(k)) f, ax = plt.subplots(1, 1) ax.plot(iters, sse, marker='o') ax.set_xlabel('Cluster Centers') ax.set_xticks(iters) ax.set_xticklabels(iters) ax.set_ylabel('SSE') ax.set_title('SSE by Cluster Center Plot') colab1find_optimal_clusters(embeddings, 20) 1clusters_2 = MiniBatchKMeans(n_clusters=2, init_size=256, batch_size=512, random_state=20).fit_predict(embeddings) 1234567891011121314151617181920212223242526def plot_tsne_pca_umap(data, labels): max_label = max(labels)+1 max_items = np.random.choice(range(data.shape[0]), size=2700, replace=False) # reducer = umap.UMAP(n_components=2) pca = PCA(n_components=2).fit_transform(data[max_items,:]) tsne = sklearn.manifold.TSNE(n_components=2).fit_transform(embeddings) uma = umap.UMAP(n_components=2).fit_transform(embeddings) idx = np.random.choice(range(pca.shape[0]), size=320, replace=False) label_subset = labels[max_items] label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]] f, ax = plt.subplots(1, 3, figsize=(14, 6)) ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset) ax[0].set_title('PCA Cluster Plot') ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset) ax[1].set_title('TSNE Cluster Plot') ax[2].scatter(uma[idx,0],uma[idx,1],c=label_subset) ax[2].set_title('UMAP Cluster Plot') plot_tsne_pca_umap(embeddings, clusters_2) Topic BERT topic BERTopic依赖于句子嵌入和聚类算法，以及降维，生成文档主题簇。 12model = BERTopic(language=\"english\", min_topic_size=20)topics, probs = model.fit_transform(texts) 12345678topic_words = ['-1: outlier']for i in range(len(set(topics))-1): tpc = model.get_topic(i)[:8] words = [x[0] for x in tpc] tw = ' '.join([str(i) + ':'] + words) topic_words.append(tw)exp_topics = [topic_words[x+1] for x in topics] 1len(set(topics)) 1234567891011121314151617181920212223242526clrs = random.sample(Turbo256, len(set(topics)))color_map = bmo.CategoricalColorMapper(factors=topic_words, palette=clrs)list_x = out[:,0]list_y = out[:,1]desc = textssource = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc, topic=exp_topics, target=targets, dset=sets,))hover = HoverTool(tooltips=[ (\"index\", \"$index\"), ('desc', '@desc'), ('topic', '@topic'), ('target', '@target'), ('dset', '@dset'),])p = figure(plot_width=800, plot_height=800, tools=[hover], title=\"Topics from BERTopic model\")p.scatter('x', 'y', size=10, source=source, fill_color=transform('topic', color_map), marker=bokeh.transform.factor_mark('dset', MARKERS, SETS), legend='dset')# p.legend.location = \"top_left\"# p.legend.click_policy=\"hide\"bokeh.plotting.show(p) 123456789101112topic_df = model.get_topic_freq()def get_keywords(i): if i == -1: return 'outlier' tpc = model.get_topic(i)[:8] words = [x[0] for x in tpc] tw = ' '.join(words) return twtopic_df['keywords'] = topic_df['Topic'].apply(get_keywords)topic_df 1model.get_topic(0) 1model.visualize_topics() 1# model.visualize_distribution(probs) Classic LDA id1import os 1!pip install -Uqq gensim==3.8.3 123456# import os #importing os to set environment variable# def install_java():# !apt-get install -y openjdk-8-jdk-headless -qq &gt; /dev/null #install openjdk# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" #set environment variable# !java -version #check java version# install_java() 12!wget -q http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip!unzip -qq mallet-2.0.8.zip 12os.environ['MALLET_HOME'] = '/content/mallet-2.0.8'mallet_path = '/content/mallet-2.0.8/bin/mallet' # you should NOT need to change this 12345678910111213141516import gensimimport gensim.corpora as corporafrom gensim.utils import simple_preprocessfrom gensim.models.wrappers import LdaMalletfrom gensim.models.coherencemodel import CoherenceModelfrom gensim import similaritiesimport os.pathimport reimport globimport nltknltk.download('stopwords')from nltk.tokenize import RegexpTokenizerfrom nltk.corpus import stopwords 1234567891011121314151617181920212223242526272829303132333435def preprocess_data(doc_set,extra_stopwords = &#123;&#125;): # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python # replace all newlines or multiple sequences of spaces with a standard space doc_set = [re.sub('\\s+', ' ', doc) for doc in doc_set] # initialize regex tokenizer tokenizer = RegexpTokenizer(r'\\w+') # create English stop words list en_stop = set(stopwords.words('english')) # add any extra stopwords if (len(extra_stopwords) &gt; 0): en_stop = en_stop.union(extra_stopwords) # list for tokenized documents in loop texts = [] # loop through document list for i in doc_set: # clean and tokenize document string raw = i.lower() tokens = tokenizer.tokenize(raw) # remove stop words from tokens stopped_tokens = [i for i in tokens if not i in en_stop] # add tokens to list texts.append(stopped_tokens) return textsdef prepare_corpus(doc_clean): # adapted from https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python # Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean) dictionary = corpora.Dictionary(doc_clean) dictionary.filter_extremes(no_below=5, no_above=0.5) # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean] # generate LDA model return dictionary,doc_term_matrix 12doc_clean = preprocess_data(texts,&#123;&#125;)dictionary, doc_term_matrix = prepare_corpus(doc_clean) 12number_of_topics=30 # adjust this to alter the number of topicswords=10 #adjust this to alter the number of words output for the topic below 1ldamallet = LdaMallet(mallet_path, corpus=doc_term_matrix, num_topics=number_of_topics, id2word=dictionary, alpha=10) 123456789# topic_words = ldamallet.show_topics(num_topics=number_of_topics,num_words=5)# topic_words = [x[1] for x in topic_words]topic_words = []for i in range(number_of_topics): tpc = ldamallet.show_topic(i, topn=7, num_words=None) words = [x[0] for x in tpc] tw = ' '.join([str(i) + ':'] + words) topic_words.append(tw) 1topic_words 123456789101112# show resulttopics_docs = list()for m in ldamallet[doc_term_matrix[:1000]]: topics_docs.append(m)x = np.array(topics_docs[:1000])y = np.delete(x,0,axis=2)y = y.squeeze()best_topics = np.argmax(y, axis=1) # 结果是一个分布topics = list(best_topics)topics = [topic_words[x] for x in topics] 1234567891011121314151617181920212223clrs = random.sample(Turbo256, number_of_topics)color_map = bmo.CategoricalColorMapper(factors=topic_words, palette=clrs)list_x = out[:,0]list_y = out[:,1]desc = textssource = ColumnDataSource(data=dict(x=list_x, y=list_y, desc=desc, topic=topics))hover = HoverTool(tooltips=[ (\"index\", \"$index\"), ('desc', '@desc'), ('topic', '@topic')])p = figure(plot_width=1200, plot_height=600, tools=[hover], title=\"Test\")p.circle('x', 'y', size=10, source=source, fill_color=transform('topic', color_map), # legend='topic')# p.legend.location = \"top_left\"# p.legend.click_policy=\"hide\"bpl.show(p) 看上面的图表，由LDA重新识别的主题内文档不一定相互接近。 与BERTopic是互补的，可以得到不同的主题表示。 Bertopic在短文本这类可能只有一个主题的文本中表现较好，而LDA可以更好地处理主题组合较多的文本。 两者可以互补，因此尝试两者都有意义。 这和两者的原理是相关的，Bertopic是空间距离的聚类，LDA是统计层面的共现规律分析。 12# pyLDAvis可视化!pip install -Uqq pyLDAvis==2.1.2 1gensimmodel = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet) 123456import pyLDAvisimport pyLDAvis.gensimpyLDAvis.enable_notebook()p = pyLDAvis.gensim.prepare(gensimmodel, doc_term_matrix, dictionary)p ref: https://skok.ai/2021/05/27/Topic-Models-Introduction.html BERTopic 详解 BERTopic，利用BERT嵌入和c-TF-IDF来创建密集的集群，使话题易于解释，同时在话题描述中保留重要词汇。其核心步骤主要是做三件事： 用基于BERT的Sentence Transformers提取语句嵌入 通过UMAP和HDBSCAN，将文档嵌入进行聚类，语义相近的语句将聚集成簇群 用c-TF-IDF提取主题词 c-TF-IDF就是将一个主题下的所有文档连接在一起成为一个文档，在主题间计算TF-IDF的方法。 12345678910111213import numpy as npimport pandas as pdimport jiebaimport umapimport hdbscanfrom sentence_transformers import SentenceTransformerfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.metrics.pairwise import cosine_similarityfrom tqdm import tqdmimport matplotlib.pyplot as plt# import sys# sys.setrecursionlimit(1000000) 1234567891011121314151617181920212223242526272829303132333435363738394041424344# model = SentenceTransformer(r'my_pretrained_chinese_embeddings')# embeddings = model.encode(data['review'].tolist(), show_progress_bar=True)#### 降维umap_embeddings = umap.UMAP( n_neighbors=25, n_components=10, min_dist=0.00, metric='cosine', random_state=2020).fit_transform(embeddings)#### 聚类# 使用HDBSCAN来寻找高密簇cluster = hdbscan.HDBSCAN( min_cluster_size=30, metric='euclidean', cluster_selection_method='eom', prediction_data=True).fit(umap_embeddings)#### c-TF-IDFdef c_tf_idf(documents, m, ngram_range=(1, 1)): my_stopwords = [i.strip() for i in open('stop_words_zh.txt',encoding='utf-8').readlines()] count = CountVectorizer( ngram_range=ngram_range, stop_words= my_stopwords).fit(documents) t = count.transform(documents).toarray() w = t.sum(axis=1) tf = np.divide(t.T, w) sum_t = t.sum(axis=0) idf = np.log(np.divide(m, sum_t)).reshape(-1, 1) tf_idf = np.multiply(tf, idf) return tf_idf, count#### 主题归并# 通过比较主题之间的c-TF-IDF向量，合并最相似的向量，最后重新计算c-TF-IDF向量来更新主题的表示","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"https://racleray.github.io/tags/BERT/"},{"name":"topic model","slug":"topic-model","permalink":"https://racleray.github.io/tags/topic-model/"}]},{"title":"Jacobi BP整理","slug":"Jacobi-BP整理","date":"2021-06-18T13:54:30.000Z","updated":"2023-08-07T11:54:31.030Z","comments":true,"path":"posts/ff0063e5.html","link":"","permalink":"https://racleray.github.io/posts/ff0063e5.html","excerpt":"从Jabobi Matrix角度，梳理神经网络反向传播过程。","text":"1 方向导数 在自变量空间的某个位置\\(w\\)处，选择一个方向\\(d\\)（单位向量），在该方向的倒数为： \\[ \\nabla_df(\\mathbf{w})=\\lim_{\\alpha \\to 0}\\frac{f(\\mathbf{w}+\\alpha \\mathbf{d}) - f(\\mathbf{w})}{\\alpha} \\] \\(\\alpha\\)趋于0时，\\(\\alpha \\mathbf{d}\\)也趋于0，平均变化率变为瞬时变化率，即导数数。 \\[ \\nabla_df(\\mathbf{w})=\\lim_{\\alpha \\mathbf{d} \\to 0}\\frac{f(\\mathbf{w}+\\alpha \\mathbf{d}) - f(\\mathbf{w})}{\\alpha \\mathbf{d}} \\] 方向导数就是多元函数在某位置，沿着某方向的瞬时变化率。 优化目标函数（最小化为例），就是要找到方向导数为负数且最小的方向，函数值在此方向下降最快。 下面就是使用梯度，快速找到方向导数为负数且最小的方向。 多元函数，分别优化每一个元，每个变量单独分析其偏导数。因为运动方向可以分解为多个分量的和。偏导数形式下的多元函数的函数值变化写成（二元为例）： \\[ \\Delta f=\\frac{\\partial f}{\\partial w_1}\\Delta w_1 + \\frac{\\partial f}{\\partial w_2}\\Delta w_2 \\] 运动的距离（二维）变为： \\[ \\sqrt {(\\Delta w_1)^2 + (\\Delta w_2)^2} \\] 那么，函数值在运动方向上的偏导数就是： \\[ \\frac {\\frac{\\partial f}{\\partial w_1}\\Delta w_1 + \\frac{\\partial f}{\\partial w_2}\\Delta w_2}{\\sqrt {(\\Delta w_1)^2 + (\\Delta w_2)^2}} = \\begin{pmatrix} \\frac{\\partial f}{\\partial w_1}, \\frac{\\partial f}{\\partial w_2} \\end{pmatrix} \\begin{pmatrix} \\frac{\\Delta w_1}{\\sqrt {(\\Delta w_1)^2 + (\\Delta w_2)^2}} \\\\ \\frac{\\Delta w_2}{\\sqrt {(\\Delta w_1)^2 + (\\Delta w_2)^2}} \\end{pmatrix} \\] 其中第一个向量就是函数在\\(\\mathbf{w}\\)处的梯度，第二个向量就是\\(\\Delta \\mathbf{w}\\)的变化方向上的单位向量。 所以，多元函数的方向导数，就是该位置梯度与自变量变化方向向量的内积。也即，梯度向量在自变量变化方向向量上的投影。 那么，最优的最小化方向为，\\(cos(\\theta)\\)为-1的自变量变化方向。即自变量变化为负梯度方向。 垂直于梯度方向时，函数值不变化。 当然梯度下降，需要设置一个较小的学习率，因为，梯度反映的是函数瞬时的变化率，只保证在当前位置的较小领域内的变化规律。 2 Jacobi 有n维向量 \\(\\mathbf{w}\\)，经过一层网络，得到 m 维 \\(f(\\mathbf w)\\)。 \\[ \\mathbf f(\\mathbf w)=\\begin{pmatrix} f_1(\\mathbf w) \\\\ f_2(\\mathbf w) \\\\ ... \\\\ f_m(\\mathbf w) \\end{pmatrix} \\] 每一维为一个标量函数，可以对 n维向量 \\(\\mathbf{w}\\)进行求偏导操作，求得梯度。得到 \\[ \\begin{bmatrix} \\frac{\\partial f_1}{\\partial w_1} &amp; ... &amp; \\frac{\\partial f_1}{\\partial w_n} \\\\ ... &amp; ... &amp; ... \\\\ \\frac{\\partial f_m}{\\partial w_1} &amp; ... &amp; \\frac{\\partial f_m}{\\partial w_n} \\end{bmatrix} \\] 这就是函数在 \\(\\mathbf{w}\\) 处的Jacobi matrix。每一行是 \\(\\mathbf f(\\mathbf w)\\)的分量在\\(\\mathbf{w}\\) 处的梯度向量。 线性近似（理解为函数在某一位置处的一阶泰勒展开并忽略高阶无穷小）的误差，是自变量变化值趋于0时，相对自变量变化值的高阶无穷小。也就是说，线性近似是用一个高阶的函数来近似低阶的原函数，而其误差可以趋于无穷小。 在多元函数中，近似关系可写为： \\[ \\mathbf f(\\mathbf w) \\approx \\mathbf f(\\mathbf w^*) + \\mathbf J_f(\\mathbf w) \\cdot (\\mathbf w - \\mathbf w^*) \\] Jacobi matrix每一行是原函数一个分量的梯度，每一维分量的近似组合成了对原函数整体的近似。 3 Back propagation with Jacobi 计算图中，一对父子节点就是一个多对多的映射，都可以求一个Jacobi matrix。 在chain rule之下，一个复合映射的Jacobi是容易求的。 \\[ f(g(h(\\mathbf w))) \\] 其Jacobi表示为 \\(\\mathbf J_f(g(h(\\mathbf w))) \\cdot \\mathbf J_g(h(\\mathbf w)) \\cdot \\mathbf J_h(\\mathbf w)\\)。其中输入输出维度是相对应的。 计算图中，每个节点将结果通过，对自己的Jacobi（单位矩阵）乘上对父节点的Jacobi，将信息传递给上一层。 那么最终结果对一个节点的Jacobi，可以计算： \\[ \\mathbf J_f = \\sum_s \\mathbf J_{rs} \\mathbf J_{sf} \\] \\(\\mathbf J_{rs}\\)是最终结果对s节点的Jacobi（表示累计误差BP到s的梯度），\\(\\mathbf J_{sf}\\)是节点s对f节点的Jacobi。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859class Node(object): \"\"\" 计算图节点基类 \"\"\" def __init__(self, *parents, **kargs): self.kargs = kargs self.graph = kargs.get('graph', default_graph) self.need_save = kargs.get('need_save', True) self.parents = list(parents) # 父节点列表 self.children = [] # 子节点列表 self.value = None # 本节点的值 self.jacobi = None # 结果节点对本节点的雅可比矩阵 # 将本节点添加到父节点的子节点列表中 for parent in self.parents: parent.children.append(self) # 将本节点添加到计算图中 self.graph.add_node(self) ... def forward(self): \"\"\" 前向传播计算本节点的值，若父节点的值未被计算，则递归调用父节点的forward方法 \"\"\" for node in self.parents: if node.value is None: node.forward() self.compute() def backward(self, result): \"\"\" 反向传播，计算结果节点对本节点的雅可比矩阵 \"\"\" if self.jacobi is None: if self is result: # 对自己的Jacobi # self.dimension()表示节点值向量的维度 self.jacobi = np.mat(np.eye(self.dimension())) else: self.jacobi = np.mat( np.zeros((result.dimension(), self.dimension()))) for child in self.get_children(): if child.value is not None: # 为None时，表示不在当前计算路径上 # 即为上文中的计算公式 10 self.jacobi += child.backward(result) * child.get_jacobi(self) return self.jacobi def dimension(self): \"\"\" 返回本节点的值展平成向量后的维数 \"\"\" return self.value.shape[0] * self.value.shape[1] ... 调用结果节点forworad，得到所有value属性，缓存在每个节点。调用某个节点backworad，将计算该节点的梯度保存在jacobi属性（梯度的转置，若规定梯度为列向量）中。 同时，实现Jacobi的乘法关系下的计算，可以进行推导。过程比较繁琐但是不难，直接写结果了： 矩阵 \\(\\mathbf C=\\mathbf A \\mathbf B\\)，A为(m,n)维，B为(n,k)维。将三个矩阵全部展开，拼接为一个列向量。 那么C对A的Jacobi为（mk, mn） \\[ \\begin{bmatrix} B^T &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; B^T &amp; ... &amp; 0 \\\\ ... &amp; ... &amp; ... &amp; ... \\\\ 0 &amp; 0 &amp; ... &amp; B^T \\end{bmatrix} \\] C对B的Jacobi为（mk, nk）的 \\[ \\begin{bmatrix} diagonal(a_{1,1}) &amp; diagonal(a_{1,2}) &amp; ... &amp; diagonal(a_{1,n}) \\\\ diagonal(a_{2,1}) &amp; diagonal(a_{2,2}) &amp; ... &amp; diagonal(a_{2,n}) \\\\ ... &amp; ... &amp; ... &amp; ... \\\\ diagonal(a_{m,1}) &amp; diagonal(a_{m,2}) &amp; ... &amp; diagonal(a_{m,n}) \\end{bmatrix} \\] 到此，基于Jacobi的BP计算方式，就基本没有问题了。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"},{"name":"BP","slug":"BP","permalink":"https://racleray.github.io/tags/BP/"}]},{"title":"SimCSE-文本对比学习","slug":"SimCSE-文本对比学习","date":"2021-06-15T13:27:06.000Z","updated":"2023-08-07T11:54:31.034Z","comments":true,"path":"posts/9e098b96.html","link":"","permalink":"https://racleray.github.io/posts/9e098b96.html","excerpt":"","text":"文本对比学习不同于图像的一点，就是增广方式。文本随机删除、乱序、替换，好像都可以，但是有没有道理，效果能有多大提升，都不那么清楚。这方面也没有比较公认处理方法流程。 论文 SimCSE (Git)，提出一种简单的对比学习方法，直接在BERT类模型之上，使用设计的对比学习损失进行fine tune，取得了比较好的效果。 方法 首先在图像领域使用的对比学习损失公式是 本文提出的方法，不使用文本增广生成对比样本，而是通过随机Dropout模型的intermediate representations，得到一组不同mask下的对比学习样例，作为正样本。而不是来自同一个原文本的 intermediate representations 组成多组负样本。 两个绿圈正是不同dropout下的一组正样本。思路确实挺简单的，但是别人做出来了，还整理得有条理。唉，我又搞得了什么鬼贡献呢。 论文结果，在取0.1 dropout rate时，无监督句子向量的效果最好。STS-B任务的Spearman`s correlation 达到 79.1。 论文还在有监督条件下，进程了实验。在NLI数据集上，加入两个源文本 同义(entailment)、中立(neutral)、反义(contradiction) 三种情况的监督信息。 正例来自同义的句子对，负例来自不同含义的句子。同时使用严格反义的句子对作为负例时，效果会有提升。 这里没有使用不同dropout，毕竟已经有正负样本标签了。作者也做了实验，使用dropout增广并没有带来提升。 对比学习效果评价 在看Bert Flow时，了解到向量表示的各向异性很重要，尤其在语义相似性任务中，对相似性指标影响很大。另外，直觉来讲，对比学习目的就是将同类相似的聚在一起，同时将向量分布尽量保持均匀，以保留更可能多的信息。 好的学习效果，应该保证 结果的对齐性和均匀性(Alignment and uniformity)。原论文推导较多，结论就是，对比学习的损失，可以转换为对齐损失和均匀损失之和。过程挺复杂的，这里并不关心(挺麻烦的)。 论文给出了代码： 注意，以上计算中的x和y都是经过 L2 normalize 的向量。 原论文做了很多实验，发现要同时将对齐损失和均匀损失达到最优，很难，至少在作者的实验中，是达不到的。 SimCSE中，将这两个损失，作为metrics使用，评价sentence embedding的对比学习效果。两个指标，都是尽量小更好，但是很难保证同时最优。 经验 无监督句向量训练，只使用随机Dropout，得到两个representations 作为正例，比在源文本上进行随机删除替换等操作的效果更好。 有监督条件下，不需要随机Dropout生成样本表达，利用监督信息就能得到很好的学习效果。 训练BERT base时，使用256或者512 batch size效果相对较好。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"sentence embedding","slug":"sentence-embedding","permalink":"https://racleray.github.io/tags/sentence-embedding/"},{"name":"SimCSE","slug":"SimCSE","permalink":"https://racleray.github.io/tags/SimCSE/"}]},{"title":"回顾神经网络初始化方法","slug":"回顾神经网络初始化方法","date":"2021-06-14T15:51:41.000Z","updated":"2023-08-07T11:54:31.040Z","comments":true,"path":"posts/1498e8ac.html","link":"","permalink":"https://racleray.github.io/posts/1498e8ac.html","excerpt":"","text":"那么首先我们已经知道，全0或者常数、过大、过小的权重初始化都有梯度消失或者梯度爆炸的问题。而我们所期望的初始化状态是：期望为0，方差在一定范围内，同时尽量保证不同层的权重方差的一致性。这样出现internal covariance shift的可能性会大幅降低。闲来无事，适巧康康。 为了更简洁的期望与方差 首先需要知道期望\\(E\\)和方差\\(Var\\)的计算方法，基本公式： \\[ Var(X)=E(X^2)-(E(X))^2 \\] \\[ Covariance(X,Y)=E((X-E(X))(Y-E(Y)))\\\\ =E(XY)-E(X)E(Y) \\] 当X，Y为独立的随机变量，Corvariance(即Cov)为0。 可以表示出两个独立随机变量的和的方差： \\[ Var(X+Y)=E((X+Y)^2)-(E(X+Y))^2\\\\ =E(X^2+Y^2+2XY)-(E(X)+E(Y))^2\\\\ =E(X^2)-(E(X))^2+E(Y^2)-(E(Y))^2\\\\ =Var(X)+Var(Y) \\] 两个独立随机变量的积的方差： \\[ Var(XY)=E((XY)^2)-(E(XY))^2\\\\ =E(X^2)E(Y^2)-(E(X)E(Y))^2\\\\ =(Var(X)+E(X)^2)(Var(Y)+E(Y)^2)-(E(X)E(Y))^2\\\\ =Var(X)Var(Y)+E(X)^2Var(Y)+Var(X)E(Y)^2 \\] 神经网络计算过程一般性表达 基本的计算方式： \\[ Z_l=WA_{l-1}+B\\\\ A_l=f(Z_l) \\] W与A是相互独立的。每一层不同神经元的权重\\(w_i\\)是独立同分布的。 假如W与A的分布已知，那么Z的方差可以计算： \\[ Var(z)=Var(\\sum_{i}^{fan\\_in} w_ia_i)\\\\ =fan\\_in \\times (Var(w)Var(a)+E(w)^2Var(a)+Var(w)E(a)^2) \\] fan_in表示输入单元个数，每个输入单元的激活值与对应的权重相乘求和，得到当前层的激活值。独立同分布，所以和的方差可以简化为方差之和。 不同激活函数的影响 初始化的目的始终是避免梯度爆炸或者消失，最好可以加快收敛速度。那么在分析整个网络的参数逐层变化时，需要分析一般性的变化规律。 激活值的分布，受到激活函数的形式影响，这是一个关键的因素。 对称型激活函数 tanh，sigmoid等关于x轴对称的函数，可以保证激活值的期望也为0。而此时，方差的计算得到简化： \\[ Var(Z_l)=fan\\_in^{l} \\times \\prod_{l=1}^{L}Var(w_l) \\times Var(Z_{l-1}) \\] 同时考虑反向传播的过程，其差异在于fan_in 变为 fan_out，详细过程见论文。同时约束前向和反向的系数都为1，那么，可以假设权重的分布为 \\[ N(0, \\frac{2}{fan\\_in+fan\\_out}) \\] 若为均匀分布，可以假设 \\[ U(-\\frac{\\sqrt{6}}{fan\\_in+fan\\_out}, \\frac{\\sqrt{6}}{fan\\_in+fan\\_out}) \\] 非对称分段激活函数 ReLU这类激活函数，激活值的期望不再为0，公式(6)可以进行另一种变换。注意w的期望依然是我们所假设的0，这和激活值的分布是独立的。 \\[ Var(z) =fan\\_in \\times (Var(w)Var(a)+E(w)^2Var(a)+Var(w)E(a)^2)\\\\ =fan\\_in \\times (Var(w)Var(a)+Var(w)E(a)^2)\\\\ =fan\\_in \\times (Var(w)[E(a^2)-E(a)^2]+Var(w)E(a)^2)\\\\ =fan\\_in \\times Var(w) \\times E(a^2) \\] 何凯明推出： \\[ Var(Z_l)=\\frac{1}{2} \\times fan\\_in \\times Var(w_l) \\times Var(Z_{l-1}) \\] 假设两层之间系数为1，权重可假设为分布： \\[ N(0, \\frac{2}{fan\\_in}) \\] 如果按照反向传播计算： \\[ N(0, \\frac{2}{fan\\_out}) \\] 在caffe的实现中，可以选择使用 \\[ N(0, \\frac{4}{fan\\_in + fan\\_out}) \\] 当然也可以不从激活函数入手，使用Normalization方法，强制变换分布。 来自神经网络训练动力学研究的一点点总结 神经网络学习过程，可以从信号频域角度分析。神经网络擅长并且优先学习低频信号信息，而不擅长学习高频振荡信号。也因此，通常模型的参数学习结果是一个比较平滑的曲面（该研究主要是在浅层网络上实验）。论文中说，如无必要，勿增频率。 报告中还展示了一种设计思路，以提高网络处理高频信号的能力，就是scale到较低的值域，以获得相对较小的参数空间。 而参数的初始化，不仅对参数的频率信息有影响，还影响着模型在不同条件下的收敛速度和收敛性（报告展示了浅层网络在MNIST实验的结果）。 实验也指出了多种常用初始化在该理论中，都具有较好的性能。这里的变量和公式，比较复杂，这里也只关心了结论。 凝聚机制 大模型训练后期，参数会偏向简化参数空间，向更少的方向聚集参数梯度向量，出现趋向相同方向的权重。 所以论文的结论为，神经网络存在这小网络偏好，如无必要，勿增神经元。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"},{"name":"initialization","slug":"initialization","permalink":"https://racleray.github.io/tags/initialization/"}]},{"title":"Knowledge Distillation Note","slug":"Knowledge-Distillation-Note","date":"2021-04-18T15:39:09.000Z","updated":"2023-08-07T11:54:31.031Z","comments":true,"path":"posts/62f6fe86.html","link":"","permalink":"https://racleray.github.io/posts/62f6fe86.html","excerpt":"","text":"知识蒸馏模型采用类似迁移学习的方法，通过采用预先训练好的老师模型(Teacher model）的输出作为监督信号去训练另外一个简单的学生模型(Student model)。 所谓的知识就是从输入向量引至输出向量的节点图。 大概分为三类：知识蒸馏（模型压缩），跨域迁移无标签转换，集成蒸馏。 此处关注知识蒸馏（模型压缩）这一类。 First Step 原文， 综述 1、训练复杂的教师模型（teacher model）：先用硬目标（hard target），也就是正常的标签（label）训练大模型。 2、计算软目标（soft target）：利用训练好的大模型来计算软目标（soft target），也就是大模型预测后再经过softmax层的输出。 3、训练小模型，在小模型的基础上再加一个额外的软目标（soft target）的损失函数，通过权重参数来调节两个损失函数的比重。 4、预测时，将训练好的小模型来进行实验。 软目标（soft target），尽量提高复杂模型里的信息量，也就是熵。 离散的数据在等概的情况下熵值是最大的，在分类的过程中，要尽量贴近与等概率的情况，这样就可以使得软目标（soft target）在每次训练的过程中获得更多的信息和更小的梯度方差，小模型可以用更少的数据和更小的学习率来进行训练，进一步压缩。 方法分类： 模型传递训练集成算法：训练学生模型，使其参数和教师模型一样，而不是压缩模型。如图，从教师训练学生1，以此由学生i训练学生i+1，最后集成所有的学生模型。 交替式训练模型算法：采用多个网络同时进行训练，每个网络在训练过程中不仅接受来自真值标记的监督，还参考同伴网络的学习经验来进一步提升泛化能力。在整个过程中，两个网络之间不断分享学习经验，实现互相学习共同进步。两个网络的优化是迭代进行的，直到收敛。 特征表示训练：使用回归模块来配准部分学生网络和部分教师网络的输出特征，并且对输出特征进行处理，可以将网络处理的重点放在得到相似的特征层。 自注意力蒸馏算法：Self Attention Distillation，称为SAD。对于多通道的主力意图有三种方法：1.绝对值求和；2.绝对值指数求和，指数大于1；3.绝对值指数求最大值。让浅层特征来学习高层特征的表达，从而加强网络的整体特征表达能力。这种底层特征模仿高层特征的学习过程，属于自注意力蒸馏(Self Attention Distillation)。 此处更关注特征表示训练这一类。NLP中用的最多的一些方法也来自这一类。比如：DistilBERT学习最后一层的表示。PKDBERT（Patient Knowledge Distillation）同时学习中间层的表示。TinyBERT将embedding层也纳入学习的范畴。同时关注新的基于对比学习的方法。 Contrastive Representation Distillation 论文， Git 基本假设，知识蒸馏应该要迁移的是表征representation，而不是概率分布（不管是使用KL散度还是L2距离）。同时之前的不是基于对比学习的方法，会丢失Teacher模型输出表征representation的结构信息，即忽略了维度间有很复杂的依赖关系。 因为KL散度或者L2距离计算将每个维度认为是独立的，在表征学习中，很难保证这里的独立假设是完成成立的。 符号定义： 这里 \\(Z^T=W_T(T), Z^S=W_S(S)\\), \\(\\sigma\\) 为 softmax 函数。 损失函数： 两个H表示不同的函数，第一个表示交叉熵（标签），第二个表示KL散度（表征）。 对比学习引入 S和T的输入是相同的时，表征应该相似。S和T的输入是不同的时，表征应该不同。 同时衡量表征差异的方法，变为NCE，从而引入的负例，即 对比学习的目标函数为： 如果只是使用该方法，那到此已经可以用了。 目标函数推导 假设，S和T的输入是相同时，C=1(T, S同分布)，否则C=0(T, S不同分布)。有1个相关输入对，N个无关输入对，M为数据集的大小。 计算\\(q(C=1|T,S)\\): 取对数，同时乘上-1： 交换 log(N) 项，两边按p(T,S)求积分： 定义： 引入NCE，不等式右边写成： 设 \\(h^*(T, S)=q(C=1|T,S), h^*=argmax\\ L_{critic}(h)\\) 由于 \\(h^*\\) 是极大值点，所以，一般性的 \\(h\\) 都有下式成立: 其中h为： 所以方法就是先找到 T和S 表征的互信息的上界，然后，优化student模型，使得互信息关于S的下界最大，得到最优的学生模型。 Softmax Regression Representation Distillation 论文，Git 方法基本如图所示，就是设计了三种损失相加。 works slightly better than the cross-entropy loss. 相对而言，没有设计对比学习，但是论文实验结果还是不错的，相比上一节CRD方法，简单不少，但是效果也不错。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Distillation","slug":"Notes/Distillation","permalink":"https://racleray.github.io/categories/Notes/Distillation/"}],"tags":[{"name":"knowledge distillation","slug":"knowledge-distillation","permalink":"https://racleray.github.io/tags/knowledge-distillation/"},{"name":"CRD","slug":"CRD","permalink":"https://racleray.github.io/tags/CRD/"},{"name":"SRRD","slug":"SRRD","permalink":"https://racleray.github.io/tags/SRRD/"}]},{"title":"再看NCE","slug":"再看NCE","date":"2021-04-08T15:41:42.000Z","updated":"2023-08-07T11:54:31.039Z","comments":true,"path":"posts/68340e67.html","link":"","permalink":"https://racleray.github.io/posts/68340e67.html","excerpt":"","text":"概率语言模型，如最大熵模型和概率神经模型，在参数估计时，都有计算量大的问题，词汇表实在是太大了。这让配分函数的计算量大得就想优化它。 只看NCE和Negative Sampling（以下简写为NS），就不说其他的方法了。 NCE和NS刚接触时，看着好像一样一样的。再看，还真是大意了，不够严谨。（废话真多） 标准开头 假设以下是一个模型，根据上下文 \\(c\\) 预测词表 \\(V\\) 中的词 \\(w\\)。 \\[ p_{\\theta}(w \\mid c)=\\frac{u_{\\theta}(w, c)}{\\sum_{w^{\\prime} \\in V} u_{\\theta}\\left(w^{\\prime}, c\\right)}=\\frac{u_{\\theta}(w, c)}{Z_{\\theta}(c)} \\] 假设 \\(u_\\theta = \\exp(s_\\theta(w, c))\\)，\\(Z_\\theta(c)\\)自然是配分函数。\\(s_\\theta\\) 假设是一个可微的函数。 如果了解概率图模型，应该见过使用 Importance Sampling + MC 的方式近似配分函数的期望的法子。NCE走得就是这条路，只是稍稍改进了些。 其他定义： 经验分布表示为 \\(\\hat{p}(w|c)\\) 和 \\(\\hat{p}(c)\\) 模型表示的分布\\(p_\\theta(w|c)\\) NCE的目的就是通过一个 噪声分布 \\(q(w)\\) （w的均匀分布，也可以是对每个概率取幂0&lt; α &lt;1和再normalize得到），得到配分函数的渐进无偏估计。 NCE 方法：通过建立二分类模型，区分来自经验分布（训练数据）和噪声分布的数据，得到最优的模型参数，就是需要的参数求解结果。 数据生成 Label d = ｛0，1｝，表示数据属于噪声或者真实数据。分布表示为： \\[ p(d, w \\mid c)=\\left\\{\\begin{array}{ll} \\frac{k}{1+k} \\times q(w) &amp; \\text { if } d=0 \\\\ \\frac{1}{1+k} \\times \\tilde{p}(w \\mid c) &amp; \\text { if } d=1 \\end{array}\\right. \\] 转换成 d 关于 c 和 w 的条件分布（定义直接推导）： \\[ \\begin{aligned} p(d=0 \\mid c, w) &amp;=\\frac{\\frac{k}{1+k} \\times q(w)}{\\frac{1}{1+k} \\times \\tilde{p}(w \\mid c)+\\frac{k}{1+k} \\times q(w)} \\\\ &amp;=\\frac{k \\times q(w)}{\\tilde{p}(w \\mid c)+k \\times q(w)} \\\\ p(d=1 \\mid c, w) &amp;=\\frac{\\tilde{p}(w \\mid c)}{\\tilde{p}(w \\mid c)+k \\times q(w)} \\end{aligned} \\] 以上是构建的\"proxy corpus\"代理的训练数据分布，那么把模型拟合的分布\\(p_\\theta(w|c)\\)带入，替换掉经验分布 \\(\\hat{p}(w|c)\\)，就可以得到一个可以操作的目标。 方法 但是，\\(\\hat{p}(w|c)\\)还是有 \\(Z_\\theta(c)\\) 啊，这不相当于构造这么多都白干了？ 所以NCE继续改进，将 \\(Z_\\theta(c)\\) 整体但做一个关于 c 的参数，参数化。然后一通操作，发现在神经网络模型这种参数巨多的情况下，将 \\(Z_\\theta(c)\\) 直接设为 1 ，反而是一种有效且高效的做法，文章里叫做 self-normalized。 所以，得到的关于 参数 \\(\\theta\\) 的分布表示为： \\[ \\begin{aligned} p(d=0 \\mid c, w) &amp;=\\frac{k \\times q(w)}{u_{\\theta}(w, c)+k \\times q(w)} \\\\ p(d=1 \\mid c, w) &amp;=\\frac{u_{\\theta}(w, c)}{u_{\\theta}(w, c)+k \\times q(w)} \\end{aligned} \\] 直接操作掉了求和配分项。目标也变成了一个二分类： \\[ \\mathcal{L}_{\\mathrm{NCE}_{k}}=\\sum_{(w, c) \\in \\mathcal{D}}\\left(\\log p(d=1 \\mid c, w)+k \\mathbb{E}_{\\bar{w} \\sim q} \\log p(d=0 \\mid c, \\bar{w})\\right) \\] 只是，其中求期望的部分需要所有单词在噪声分布下求值，得到期望，这显然是低效的。简单的办法，直接MC，期望转Sampling： \\[ \\begin{aligned} \\mathcal{L}_{\\mathrm{NCE}_{k}}^{\\mathrm{MC}} &amp;=\\sum_{(w, c) \\in \\mathcal{D}}\\left(\\log p(d=1 \\mid c, w)+k \\times \\sum_{i=1, \\bar{w} \\sim q}^{k} \\frac{1}{k} \\times \\log p(d=0 \\mid c, \\bar{w})\\right) \\\\ &amp;=\\sum_{(w, c) \\in \\mathcal{D}}\\left(\\log p(d=1 \\mid c, w)+\\sum_{i=1, \\bar{w} \\sim q}^{k} \\log p(d=0 \\mid c, \\bar{w})\\right) \\end{aligned} \\] 完 可以证明（求极值点嘛，再分析一波k趋于无穷的极限），这个目标函数的最优时，就是模型所表示的分布和经验分布匹配的时候。 NS(Negative Sampling) word2vec使用过的方法，直接可以写出目标条件分布： \\[ \\begin{array}{l} p(d=0 \\mid c, w)=\\frac{1}{u_{\\theta}(w, c)+1} \\\\ p(d=1 \\mid c, w)=\\frac{u_{\\theta}(w, c)}{u_{\\theta}(w, c)+1} \\end{array} \\] 可以看成 k = |V| 且 q 为均匀分布的NCE特殊情况。在分析一次像 \\(\\mathcal{L}_{\\mathrm{NCE}_{k}}\\) 的损失函数，或者是hinge loss形式的损失函数，求导分析极值，可以发现它最优时，模型所表示的分布和经验分布并不匹配，可以不一致。 也就是说，NS虽然在word2vec训练时，可以学习到word的representation，但是它并不适用于语言模型等更general的情景。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"ML","slug":"Notes/ML","permalink":"https://racleray.github.io/categories/Notes/ML/"}],"tags":[{"name":"NCE","slug":"NCE","permalink":"https://racleray.github.io/tags/NCE/"},{"name":"language model","slug":"language-model","permalink":"https://racleray.github.io/tags/language-model/"}]},{"title":"UDA & MixMatch","slug":"UDA-MixMatch","date":"2021-03-25T14:51:11.000Z","updated":"2023-08-07T11:54:31.035Z","comments":true,"path":"posts/d4b0eecc.html","link":"","permalink":"https://racleray.github.io/posts/d4b0eecc.html","excerpt":"简要记录两种比较新且有实际效果的数据增强方法思路。","text":"UDA 使用无监督方法，基本架构类似SimCLR等对比学习模型。 图中M为自定义的模型。 模型损失，UDA部分： 加上监督学习部分： 数据增广 图片数据：AutoAugment (RandAugment)、Cutout 文本数据：BackTranslation；基于TF-IDF的词替换，保留重要关键词，替换不重要词 训练方法 Training Signal Annealing (TSA)，在训练时逐步释放训练信号： 模型在有标注数据上，先拟合预测概率小于 \\(\\eta_t\\) 的数据，逐渐调整 \\(\\eta_t\\)，训练先难后易。 三种策略设置三种 \\(\\alpha_t\\)： log： linear: exp: 当模型容易过度拟合时，例如，当问题相对容易或标记数据非常有限时，exp-schedule是最合适的。相反，当模型不太可能过度拟合时(例如，当有丰富的标记数据或当模型使用有效的正则化时)，log-schedule更合适。 其他处理 在使用模型M预测 无标注数据时，使用Confidence-based masking，将概率小于 \\(\\beta\\) 的过滤掉。 使用sharpening prediction，大的更大，小的更小。 可以使用entropy minimization： ​ 上式中还可以使用MixMatch sharpenig： Domain-relevance Data Filtering：对于外部搜集的数据，对于每个类别，都基于对所有示例进行排序属于该类别的概率，并选择概率最高的示例，构成外部数据。 官方实现 MixMatch paper，tf2.0 方法 另一种混合有标签和无标签数据的方法。整体方法如下： 对于有标签数据，增广batch个，联合其标签 \\(p_b\\) 构成 \\(\\hat{X}\\) 对无标签数据，每个样本增广K个，通过Label Guessing，取模型对K个样本预测的均值，作为这K个样本的伪标签。构成样本集 \\(\\hat{U}\\). Shuffle样本集，将其进行MixUp，得到MixMatch样本集合 然后通过下式计算模型损失，模型为自定义模型： H就表示普通交叉熵损失，若是分类问题时。p、q就是MixMatch中得到的标签。q来自Label Guessing。 细节 使用的方法： 数据增广：crop, flip等常见方法 Label Guessing： sharpening： MixUp： 模型训练时可采用滑动平均、weight decay等方法 12345678910111213141516171819@tf.functiondef sharpen(p, T): return tf.pow(p, 1/T) / tf.reduce_sum(tf.pow(p, 1/T), axis=1, keepdims=True)@tf.functiondef mixup(x1, x2, y1, y2, beta): beta = tf.maximum(beta, 1-beta) x = beta * x1 + (1 - beta) * x2 y = beta * y1 + (1 - beta) * y2 return x, y...def ema(model, ema_model, ema_decay): for var, ema_var in zip(model.variables, ema_model.variables): if var.trainable: ema_var.assign((1 - ema_decay) * var + ema_decay * ema_var) else: ema_var.assign(tf.identity(var)) 消融实验结果： 使用论文中提到的所有方法，才能取得最好的结果。其中MixUp操作的影响最大。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Data Augmentation","slug":"Notes/Data-Augmentation","permalink":"https://racleray.github.io/categories/Notes/Data-Augmentation/"}],"tags":[{"name":"data augmentation","slug":"data-augmentation","permalink":"https://racleray.github.io/tags/data-augmentation/"}]},{"title":"Self-supervised methods note","slug":"self-supervised-methods-note","date":"2021-03-25T10:56:12.000Z","updated":"2023-08-07T11:54:31.037Z","comments":true,"path":"posts/aa316e6c.html","link":"","permalink":"https://racleray.github.io/posts/aa316e6c.html","excerpt":"简要记录从JEM、SupContrast、Momentum Contrast到Simple Siamese的一系列对比学习方法。","text":"JEM paper, link Google 建模联合概率，factorize为监督部分和非监督部分。同时论文指出从y开始factorize的话，效果会下降。 energy部分使用 Stochastic Gradient Langevin Dynamics （SGLD）对生成数据采样，最小化其energy framework： 达到效果：提高了 calibration（输出与真实分布具有一致性）, robustness, 和out-of-distribution detection方面的性能。 问题：训练自然会变慢（CIFAR10一个epoch为半个小时），另外就是不稳定，SGLD采样使用随机数据开始。 JEM采样方式来自 Implicit Generation and Modeling with EBM Buffer Sample igebm-pytorch， link，Google 先比于VAE和GAN，是一种根据energy采样的隐式采样，使用一个主网络，可以将生成的限制和目标构建成损失函数。但是生成样本分布需要更多的计算迭代次数。 energy based采样主要是高维数据采样困难。真实图片分布在high energy区域，噪声图片分布在low energy区域。 方法： 随机输入开始 使用模型输出 + SGLD迭代采样出sample i 将更新过的sample i重新写入buffer 2、3步迭代 K 次。K不能太小。论文中60，JEM 20 SGLD： SupContrast paper，git，Google contrastive learning一般的方式是，先数据增广，然后通过模型识别出来自同一张图的输入，将不是来自同一张图的输入的相似度变小。 Supervised Contrastive Learning将识别对象变为同一类图片，而不是同一张图片。 损失函数定义为： 论文中还有另一种形式的L，但是效果不好。 通过引入标签数据，计算同一类图片的相似度。在计算损失时，处理同一类图片方法如下 12345678910111213141516...# 将相同label的mask出来mask = torch.eq(labels, labels.T).float()# tile mask：增广数据shape匹配mask = mask.repeat(anchor_count, contrast_count)# mask-out 自己对自己位置进行masklogits_mask = torch.scatter( torch.ones_like(mask), 1, torch.arange(batch_size * anchor_count).view(-1, 1).to(device), 0)mask = mask * logits_mask... 模型训练分为两个阶段。如果只是训练一个encoder，只需要第一阶段。若是要进行分类任务，需要固定encoder训练第二阶段分类器。 实验结果：模型对超参数的敏感性降低，使用Supervised Contrastive Loss能够提升分类准确率（论文中在Imagenet等多个数据集上进行了实验）。 Hybrid Discriminative-Generative paper，git, UCB 通过对比学习contrastive learning，混合监督与非监督一起训练。和Supervised Contrastive Learning中的encoder框架不同，没有两个编码的 contrastive 计算。从 变为： 只有一个f(x)编码，和 label y。 这个和 cross entropy 有点像。但是数据来源不同，这里数据是来自K大小的 normalization samples，也是来自SGLD方法中设计的buffer。 这个方法也是针对loss进行变化： 计算两个部分的cross entropy loss。 论文结果，相比于JEM，在CIFAR10上的效果，有一定提升 提高了 calibration, robustness, 和out-of-distribution detection方面的性能。 同时K越大，效果越好。“有钱人的游戏”。 Momentum Contrast paper，git，FB PyContrast：pytorch implementation of a set of (improved) SoTA methods using the same training and evaluation pipeline. 首先contrastive loss 计算的一般框架： 就是 k+1个softmax分类器。 目前的训练方式： end to end：输入一个batch，进行数据增广，然后优化来自同一张图片的相似度。如果有多个类，那么一个batch的数据，覆盖的类是有限的。 memory bank: 使用一个encoder，构建memory bank（样本的vector表示集合），随机抽取batch size个数据，与query正例计算loss。然后再更新memory bank中的样本表达数据。如此循环。 MoCo：使用一个大小为k的queue，出队batch size个数据经过 momentum encoder，与query正例计算loss。然后使用 encoder的参数，以一定 momentum 更新 momentum encoder参数。重新入队batch size个样本。 momentum 更新，论文实验发现，m应该设置为一个很接近1的数，比如0.9999： MoCo伪代码： 经过MoCo得到一个预训练模型 encoder，使用就类似 BERT 之类的模型。 接一个简单 Linear classifier 就可以达到接近监督训练模型的效果。 SimCLR paper，git，Google 在MoCo之上的改进。两个点： 数据增广加一倍，一个图片会有两个 aug(x) 。 计算NCE时，使用Cosine Similarity，而不是MoCo中的 inner product。 增广方法使用了： framework： 模型结构： 使用encoder时，如果使用预训练的Resnet之类在ImageNet训练的模型，输出部分重新设计，不需要转到1000类，直接输出模型表征经过avgpool + fc 之后的结果，即 图中 h。 h 之后还增加 g(x) DNN层变换。 其他变化： 放弃 MoCo的 queue，也不用 memory bank，直接用很大的batch size.... 使用LARS优化器 32到128个GPU训练 论文给出的self-supervised模型+linear classifier在ImageNet上的Top-1 accuracy. Bootstrap your own latent paper, git， Google 结合SimCLR和MoCo： 结构还是SimCLR的结构，只是一个分支在这里称为online，一个称为target。 使用MoCo的 momentum 更新 momentum encoder参数的方法，更新target网路参数。 target部分不计算bp。 重新定义损失为mean squared error形式，不需要构造负样本对： 结构： 当然，又是一次提升: 同时，BYOL对batch的敏感性比SimCLR低一些。 Simple Siamese: simpler &amp; better paper，git， FB 结合了BOYL和SimCLR，将Siamese结构加入模型，得到一个更neat的设计。 删掉BOYL的 momentum encoder。 不用构造SimCLR中的negative pairs。 encoder对两个aug(x)计算hidden representation： 计算损失函数： 伪代码： stopgrad部分视为一个constant输入。 其他设计： 优化器SGD + 0.9 momentum动量 + 0.0001 的 weight decay 不再需要很大的batch size，batch size 512即可 消融实验结果： lr不要要decay，效果最好 batch size最大512即可 loss设计为 cosine 形式更好，在hidden层和predict层之前使用BN效果最好 Loss的对称计算形式提升了效果。 SimSiam不再需要negative sample pairs ，large batches ，和momentum encoders。终于找到一个简洁而有效的model。当然，后浪依然会有。 Why stop-gradient? 论文假设只是一个EM模型： 那么问题转化为： 假设从 \\(\\eta\\) 网络输出就假设为当前期望估计。更新\\(\\theta\\)就是在计算最优化损失。所以，计算期望时，自然是不更新\\(\\theta\\)的。 这里predictor的作用也是假设loss是在两侧 expectation 上进行计算的，这只是假设，严格证明没有。 这因为EM显示的合理性，所以在 stop-gradient 时，模型能够收敛到一个好的结果。而如果不进行 stop-gradient，训练反而会变得不稳定。 Contrastive Learning with Adversarial Examples 理论偏研究，没有开源实现。 基于SimCLR框架，选择能最大化差异的样本，然后再使之相似性损失最小化。 将SimCLR损失写成： 然后固定一个增广样本输入，然后找差异最大的另一个： 计算扰动： 得到对抗样本： 计算损失（16）： 算法设计： 计算 \\(W^*\\)（15）: More Adversarial Examples Improve Image Recognition paper，在EfficientNet中使用的效果评测 提升的基本思想，加入对抗样本，联合原数据进行训练 max部分为求得最大化差异的对抗样本损失。 但是，实际效果并不好，因为原数据与对抗样本的分布不匹配。所以提出，两个BN分布处理原数据和对抗样本。 然后，在预测时，只使用主BN层。 当模型越大，效果提升越明显。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Contrastive Learning","slug":"Notes/Contrastive-Learning","permalink":"https://racleray.github.io/categories/Notes/Contrastive-Learning/"}],"tags":[{"name":"self-supervise","slug":"self-supervise","permalink":"https://racleray.github.io/tags/self-supervise/"},{"name":"contrastive learning","slug":"contrastive-learning","permalink":"https://racleray.github.io/tags/contrastive-learning/"}]},{"title":"Joint Extraction of Entities and Relations 2020","slug":"Joint-Extraction-of-Entities-and-Relations-2020","date":"2021-03-24T15:37:55.000Z","updated":"2023-08-07T11:54:31.030Z","comments":true,"path":"posts/aa5b647c.html","link":"","permalink":"https://racleray.github.io/posts/aa5b647c.html","excerpt":"记录2020几篇关系抽取论文，包括CasRel、TPlinker和Two-are-better-than-one。其思路很机智。","text":"CasRel git 实体识别和关系分类一体化，使用一个model解决两个问题。优势在于，通过两个任务的相关性，设计model，减少两阶段预测中的级联误差。虽然，也有两阶段模型实际效果可以很好。 除了解决级联误差，还需要解决什么问题？ Overlapping relations：一个实体可以出现则同一文本中的多个关系中。 Nested relations：不同的三元组可能包含或共享嵌套实体。 Texts Triplets Normal [The United States] President [Trump] will meet [Xi Jinping], the president of [China]. (The United States, president, Trump) (China, president, Xi Jinping) Single Entity Overlapping Two of them, [Jeff Francoeur] and [Brian McCann], are from [Atlanta]. (Jeff Francoeur, live in, Atlanta) (Brian McCann, live in, Atlanta) Entity Pair Overlapping The new mayor of [New York City] [De Blasio] is native-born. (New York City, mayor, De Blasio) (De Blasio, born in, New York City) Nested The new mayor of [[New York] City] [De Blasio] is native-born. (De Blasio, live in, New York City) (De Blasio, live in, New York) (New York, contains, New York City) 同时，还存在的问题有：预测标签的不平衡，相同实体出现不同关系时model难以拟合。 framework CasRel是一种framework，将关系预测从标注分类转化为一个隐式变换，参与模型训练： \\[ f(s, o) \\rightarrow r \\] 本来是subject + object推断relation，变化为： \\[ f_r(s) \\rightarrow o \\] 将relation建模成一种function。 将likelihood变成以下形式： 模型整体架构 模型在多个关系抽取任务上，效果提升明显。 TPlinker git TPlinker是不同于现有模型的一种一体式关系抽取模型。解码方式独特。 通过实体边界词，区分嵌套实体：New York City -&gt; (New, City), New York -&gt; (New, York) 通过实体边界，分解三元组：(De Blasio, live in, New York City) -&gt; (De, live in, New) and (Blasio, live in, City) 两种常见的处理 relation overlapping 的模式： 都同时存在两个问题： 暴漏偏差（exposure bias） ：指在训练阶段是gold实体输入进行关系预测，而在推断阶段是上一步的预测实体输入进行关系判断；导致训练和推断存在不一致 嵌套实体（nested entities）：并没有有效处理嵌套实体关系。 TPlinker的标注方式 首先对不同的关系，分别进行标注。每种关系的标注方式相同。 三类标记： 紫色标记：单个实体的头尾对应关系。和关系类型无关。shape: len(text) * len(text) 红色标记：对应 subject和object 的 start对应标记。每种关系一个单独标记矩阵。shape: R * len(text) * len(text) 蓝色标记：对应 subject和object 的 end对应标记。每种关系一个单独标记矩阵。shape: R * len(text) * len(text) 同时，红色标记和蓝色标记，在len(text) * len(text)的矩阵中，存在两个对称的标记，比如（New, De）与 (De, New)。为了提高效率，将下三角部分映射到上三角部分，同时值变成2。 模型为： 图中 Handshaking Kernel，就是将 标记矩阵展开得到的一维编码。token pair 遍历了所有可能的 对应关系。图中S -- subject；O -- object; H -- head; T -- tail； E -- entity 通过这些标记，训练模型计算损失，token pair的encoder output拼接在一起，输入softmax，每个关系类型都有一个softmax。 解码过程 预测模型计算结果 结果EH-to-ET可以得到句子中所有的实体，EH的 token idx作为key，EH-to-ET的entity作为value，存入 D 中，得到可选实体； 开始遍历 不同关系； 结果SH-to-OH可以得到某种关系可选的 head 对应关系，取这些head index，从 D 中，取出对应实体对 token pair 存于 D2； 结果ST-to-OT可以得到某种关系可选的 tail 对应关系，将这个对应关系的 token pair 存入 E； 遍历D2，并检查 每一个 D2中的 token pair 的 tail 是否存在于 E 中，若存在，那么输出 该关系下的该三元组信息。 More 实体对输出的embedding表示是直接 concatenate，那么如果两个实体的context相似，那么理论上会影响预测效果。 由于要预测 N 个词中选 2 的排列个 pair，所以对于长文本，代价会很大。 Two-are-better-than-one git 统一NER和RE任务到一个表格标记预测任务。 标记形式很直观。只是这在14年就已经有这种方法的尝试了。这篇论文作者是对其进行了改进。 除了一些关系抽取任务常见的问题，这种标记方式还有一个问题： 现有的基于Table-Filling方法，会将表结构转化成一个序列结构，表结构的标记方式直接退化。 结构设计 首先是 Text Embedding：Glove词向量、LSTM字符向量和BERT词向量的共同构成。 Table Encoder：学习表格中每个位置的向量表达，shape: len(text) * len(text)。表格第 i 行第 j 列的向量表示，与句子中的第 i 个和第 j 个词相对应。 使用 MD-RNN 融合表格中上下左右的信息。接收 Sequence Encoder 编码的 当前 第 i 、第 j 个词的向量表示。 但是，论文实验发现，不必计算四组，只需要两组，就能达到几乎无损的效果。只计算 a 和 c。 Sequence Encoder：Sequence Encoder的结构与Transformer类似，不同之处在于将Transformer中的scaled dot-product attention 替换为 table-guided attention。 原 transformer 的 attention ： 变为： 直接使用 \\(T_{l,i,j}\\)节省了计算量，同时交互两个部分信息。 Pre-trained Attention Weights：利用预训练的 BERT 中每一层的 attention 信息，得到 \\(T^l\\) ，联合 S 构成MD-RNN的初始输入 。 预测结果表示为： 表格中对称位置，在预测时直接求和，得到一个关系的得分。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"relation extraction","slug":"relation-extraction","permalink":"https://racleray.github.io/tags/relation-extraction/"}]},{"title":"jupytext and nbdev","slug":"jupytext-and-nbdev","date":"2021-03-24T13:54:30.000Z","updated":"2023-08-07T12:20:13.022Z","comments":true,"path":"posts/c1cb8a12.html","link":"","permalink":"https://racleray.github.io/posts/c1cb8a12.html","excerpt":"记录fast.AI团队开源的 jupytext 与 nbdev 工具的配置。","text":"Jupytext 界面点击操作 当你 Pair 一种文件，保存时，会自动生成你 pair 关联的格式的文件。在其中一个文件修改的任何文本都会同步到两个文件中。 首先你有文件 然后，在jupyter lab中 ctrl + shift + c，选择pair的文件类型（jupyter notebook在功能菜单中找到） 然后，保存 关联成功。py文件可以在自己喜欢IDE环境编辑，ipynb文件可以同步展示。删除任意一个，都可以再次恢复。 取消pair，则不会同步。（unpair 取消关联） py文件中包含的 metadata 如下格式 命令行操作 Notebook to text 12jupytext --to markdown notebook.ipynbjupytext --to script notebook.ipynb text to notebook 12jupytext --to notebook notebook.pyjupytext --to notebook notebook.md text to notebook and preserve outputs 12jupytext --to notebook --update notebook.pyjupytext --to notebook --update notebook.md 命令行pair操作 1jupytext --set-formats ipynb,py:percent notebook.ipynb 同步paired notebooks，当在其他文件中修改后，同步到ipynb 1jupytext --sync notebook.ipynb 应用code style(flake8, black, isort) 1jupytext --pipe flake8 notebook.ipynb 支持文件格式 NBDEV开发 step 1 新建git 使用 the template 创建一个github repo 1pip install nbdev 可选，使用git自带的服务器：在setting中 在edit中添加生成的网址 step 2 编辑settings.ini 编辑settings.ini （注意这里的lib_name就是生成的包名，所以有空格很不规范）。前面的个人相关信息基本都要取消注释，后面在git上创建网页展示环境要check。 1git clone https://github.com/RacleRay/Hello_nbdev.git step 3 安装git_hooks 1nbdev_install_git_hooks 出现conflict错误时 1nbdev_fix_merge filename.ipynb step 4 编辑代码 开始编辑ipynb文件 标记类别： #default_exp 对于新创建的 .ipynb，需要加入 #default_exp target_module_name 这会导出生成以下py文件 1lib_name/target_module_name.py lib_name与settings时，保持一致 #export：效果如下 导出后显示如下 如果是类里面的方法，显示doc需要使用函数 show_doc 1from nbdev,showdoc import show_doc 不加标记会显示代码和输出 测试代码也可以写在这里，不代标记 1assert say_hello(\"Jeremy\")==\"Hello Jeremy!\" step 5 nbdev_build_lib 1nbdev_build_lib 注意，发生keyError时，多半是settings.ini配置不完整 生成新的lib包文件夹及py文件 core.py step 6 编写index文件 编辑index.ipnb step 7 生成docs文档 1nbdev_build_docs 生成HTML文档 step 8 上传git 1234git add -Agit statusgit commit -m \"test\"git push step 9 bug 检查 commits中的问题 没有设置keywords，在settings.ini中进行设置。 optional step 10 发布pypi 上传到pypi 注册pypi 在用户家目录下新建~/.pypirc 123[pypi]username = your_pypi_usernamepassword = your_pypi_password pip install twine make release 附 其他事项 安装包，同时同步所有在源码上的编辑 1pip install -e . 可以将自己开发的包链接到python包路径下 1ln -s lib_path lib_name autoreload 12%load_ext autoreload%autoreload 2 在ipynb结尾添加以下代码，用以代替命令行nbdev_build_lib。 1from nbdev.export import notebook2script; notebook2script() 检查可读的notebook 1nbdev_read_nbs 检查可能造成merge conficts的文件， 1nbdev_clean_nbs 可能导致cleaned项不通过，此时用，然后再push 检查notebook和已经导出的lib files之间的是否有差异 1nbdev_diff_nbs 运行notebooks中的测试 1nbdev_test_nbs fastai文档 nbdev git nbdev_template","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"python","slug":"Tools/python","permalink":"https://racleray.github.io/categories/Tools/python/"}],"tags":[{"name":"tools","slug":"tools","permalink":"https://racleray.github.io/tags/tools/"},{"name":"jupyter","slug":"jupyter","permalink":"https://racleray.github.io/tags/jupyter/"}]},{"title":"Pointer review","slug":"Pointer-review","date":"2021-03-18T14:37:39.000Z","updated":"2023-08-07T11:54:31.032Z","comments":true,"path":"posts/6875388b.html","link":"","permalink":"https://racleray.github.io/posts/6875388b.html","excerpt":"回顾一下Pointer network。在学习 transformer + pointer 的摘要生成模型时，得空稍稍记录一下。","text":"回顾一下Pointer network Pointer ? What ? 从一堆点中，找出 凸包 边界点。 convex hull：一条圈住所有点的橡皮筋 Seq generation bad seq2seq，在处理生成任务时，无法处理 OOV 问题。可以理解为，在decode时映射的词表vocabulary是变化的。 How to solve？ [^]: Get To The Point: Summarization with Pointer-Generator Networks 假如，将输入text中每个word当做一个点，seq2seq任务转换为在输入的文本中找到一个 “convex hull” 可以summarize整个输入的语义内容，是否可行？ 输出全都从 输入 中 copy。 根据输入的语义表示，用一个language model输出生成文本。 假如要保持 decoder 的language model的泛化生成能力，同时copy一些输入中的重要信息。模型被修改为下面这样 输出为：联合 source text 中的attention distribution 和 decoder在vocabulary上的预测分布，以 Pgen 加权的结果。 Final distribution的分布中是包含了 source text 和 vocabulary 中的所有词的。 同时，\\(P_{gen}\\)设计为可学习的参数。 Reduce repeats？ 将 attention distribution 进行历史累计，在下一步计算attention时输入。即，Coverage Mechanism。 同时，添加新的损失项。让模型不要过分关注某些词。： 最终损失为： More 注意事项: （1）在模型训练到一定程度后，再使用Coverage Mechanism。 （2）在模型的训练环节，刚开始的时候，大约有70%的输出序列是由Pointer Network产生的，随着模型逐渐收敛，这个概率下降到47%。然而，在测试环节中，有83%的输出序列是由Pointer Network产生的。作者猜测这个差异的原因在于：训练环节的decoder使用了真实的目标序列。 （3）作者曾尝试使用一个15万长度的大词表，但是并不能显著改善模型效果。 Code 其他同类模型： 将LM部分也变为 attention ，进行多个 source text 的输入的生成任务。Multi-Source Pointer Network 不计算 \\(P_{gen}\\) ，而直接分成多种 情况，进行不同的 生成过程。CopyNet","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"pointer net","slug":"pointer-net","permalink":"https://racleray.github.io/tags/pointer-net/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"}]},{"title":"实体识别模型与策略2020","slug":"实体识别模型与策略2020","date":"2021-03-10T15:21:03.000Z","updated":"2023-08-07T11:54:31.041Z","comments":true,"path":"posts/548512d6.html","link":"","permalink":"https://racleray.github.io/posts/548512d6.html","excerpt":"记录NER比赛中用到的新东西。FLAT模型结构，MRC等方法的设计思路，FGM和SWA等训练技巧，伪标签设计思路等。","text":"模型 FLAT FLAT部分Blog原文：https://mp.weixin.qq.com/s/6aU6ZDYPWPHc3KssuzArKw 论文：FLAT: Chinese NER Using Flat-Lattice Transformer 将Lattice图结构无损转换为扁平的Flat结构的方法，并将LSTM替换为了更先进的Transformer Encoder，更好地建模了序列的长期依赖关系； 提出了一种针对Flat结构的相对位置编码机制，使得字符与词汇信息交互更直接，在基于词典的中文NER模型中取得了SOTA。 由于中文词汇的稀疏性和模糊性，基于字符的序列标注模型往往比基于词汇的序列标注模型表现更好，但在基于字符的模型中引入分词信息往往能够带来性能的提升，尤其是对于NER任务来说，词汇能够提供丰富的实体边界信息。 Lattice LSTM首次提出使用Lattice结构在NER任务中融入词汇信息，如图所示，一个句子的Lattice结构是一个有向无环图，每个节点是一个字或者一个词。 设计适应Lattice结构的模型 Lattice LSTM (ACL 2018): 将词汇信息引入中文NER的开篇之作，作者将词节点编码为向量，并在字节点以注意力的方式融合词向量。 Lexicon Rethink CNN(IJCAI 2019): 作者提出了含有rethink机制的CNN网络解决Lattice LSTM的词汇冲突问题。 RNN和CNN难以建模长距离的依赖关系，且在Lattice LSTM中的字符只能获取前向信息，没有和词汇进行足够充分的全局交互 FLAT Git Repo 从Transformer的position representation得到启发，作者给每一个token/span(字、词)增加了两个位置编码，分别表示该span在sentence中开始(head)和结束(tail)的位置 扁平的结构允许我们使用Transformer Encoder，其中的self-attention机制允许任何字符和词汇进行直接的交互 Relative Position Encoding of Spans span是字符和词汇的总称，span之间存在三种关系：交叉、包含、分离，然而作者没有直接编码这些位置关系，而是将其表示为一个稠密向量。作者用 和 表示span的头尾位置坐标，并从四个不同的角度来计算 和 的距离： 使用\\(A^{*}_{i,j}\\)代替 tranformer 的self attention 中的 \\(A_{i,j}\\): 通过FLAT模型后，取出token的编码表示，将其送入CRF层进行解码得到预测的标签序列。 论文中给出的结果显示，FLAT相较于一众NER模型，取得了SOTA的效果。同时，使用较大规模数据时，效果更好。在对比实验中发现，字符与包含它的词汇之间的充分交互是很重要的。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091class MultiHeadAttention(nn.Module): def __init__(self, hidden_size, num_heads, scaled=True, attn_dropout=None): super(MultiHeadAttentionRel, self).__init__() self.hidden_size = hidden_size self.num_heads = num_heads self.per_head_size = self.hidden_size // self.num_heads self.scaled = scaled assert (self.per_head_size * self.num_heads == self.hidden_size) # 正常 attention 的 q,k,v 变换矩阵 self.w_k = nn.Linear(self.hidden_size, self.hidden_size) self.w_q = nn.Linear(self.hidden_size, self.hidden_size) self.w_v = nn.Linear(self.hidden_size, self.hidden_size) # 计算 Rij 的权重 self.w_r = nn.Linear(self.hidden_size, self.hidden_size) # 计算 A* 的权重 self.u = nn.Parameter(torch.randn(self.num_heads, self.per_head_size), requires_grad=True) self.v = nn.Parameter(torch.randn(self.num_heads, self.per_head_size), requires_grad=True) self.dropout = nn.Dropout(attn_dropout) def forward(self, key, query, value, pos, flat_mask): \"pos 为 自定义的 postion embedding，对应公式的 Rij\" batch = key.size(0) # 输入线性变换 key = self.w_k(key) query = self.w_q(query) value = self.w_v(value) rel_pos_embedding = self.w_r(pos) ####### 计算 A* 矩阵的方法 和 论文不是完全一致 # batch, seq_len, n_head, d_head key = torch.reshape(key, [batch, -1, self.num_heads, self.per_head_size]) query = torch.reshape(query, [batch, -1, self.num_heads, self.per_head_size]) value = torch.reshape(value, [batch, -1, self.num_heads, self.per_head_size]) # batch, seq_len, seq_len, n_head, d_head rel_pos_embedding = torch.reshape(rel_pos_embedding, list(rel_pos_embedding.size()[:3]) + [self.num_heads, self.per_head_size]) # batch, n_head, seq_len, d_head key = key.transpose(1, 2) query = query.transpose(1, 2) value = value.transpose(1, 2) # batch, n_head, d_head, seq_len key = key.transpose(-1, -2) # 1, num_heads, 1, d_head u_for_c = self.u.unsqueeze(0).unsqueeze(-2) # batch, n_head, seq_len, d_head query_and_u_for_c = query + u_for_c # batch, n_head, seq_len, seq_len A_C = torch.matmul(query_and_u_for_c, key) # batch, n_head, seq_len, d_head, seq_len rel_pos_embedding_for_b = rel_pos_embedding.permute(0, 3, 1, 4, 2) # batch, n_head, seq_len, seq_len, 1, d_head query_for_b = query.view([batch, self.num_heads, query.size(2), 1, self.per_head_size]) # batch, n_head, seq_len, seq_len, 1, d_head query_for_b_and_v_for_d = query_for_b + self.v.view(1, self.num_heads, 1, 1, self.per_head_size) # batch, n_head, seq_len, seq_len B_D = torch.matmul(query_for_b_and_v_for_d, rel_pos_embedding_for_b).squeeze(-2) # batch, n_head, seq_len, seq_len attn_score_raw = A_C + B_D # 计算 score if self.scaled: attn_score_raw = attn_score_raw / math.sqrt(self.per_head_size) mask = 1 - flat_mask.long().unsqueeze(1).unsqueeze(1) attn_score_raw_masked = attn_score_raw.masked_fill(mask.bool(), -1e15) # batch, n_head, seq_len, seq_len attn_score = F.softmax(attn_score_raw_masked, dim=-1) attn_score = self.dropout(attn_score) # batch, n_head, seq_len, d_head value_weighted_sum = torch.matmul(attn_score, value) # batch, seq_len, n_head, d_head -&gt; batch, seq_len, hidden_size result = value_weighted_sum.transpose(1, 2).contiguous().reshape(batch, -1, self.hidden_size) return result BERT 教程博客很多，比如 http://jalammar.github.io/illustrated-bert/ CRF 参考 note1 note2 MRC 论文：A Unified MRC Framework for Named Entity Recognition Git Repo 转换为阅读理解（MRC）任务，来解决NER问题。似乎有很多搞研究的，都在尝试将NLP问题转换到MRC框架下，解决问题。 目的，解决NER中的实体重叠、嵌套关系问题。这是序列建模方式，比较难处理的问题。 数据，处理为三元组形式：(问题，答案，上下文) 其中，问题：一段对 实体类型 的描述文字，多种实体，就有多个问题；答案：为 实体的起始 index；上下文就是待识别的整个文本。 模型，使用BERT： 每个token预测输出有两个，是否为实体开始字，是否为实体结束字。 输出为 2 维，是和不是的预测概率。分别对每个位置判断，是否为开始字或者结束字。 但是这个两个集合，在有监督数据条件下，即训练时，并没有必要，只在预测推断时使用（推断需要通过下式计算所有组合的概率 P）。因为下式： 直接根据标注数据的 i, j 对标注部分计算 P。而不用对所有 i, j 组合算一次 P。 损失，多个预测损失之和： 权重为超参数。 Simple-Lexicon 论文：Simple-Lexicon：Simplify the Usage of Lexicon in Chinese NER Git Repo 在Embedding信息的输入上进行改进，尝试了多种方式。 Softword：使用分词工具，标记词的 BMESO，结合字向量和标记向量输入。存在误差传播问题，无法引入一整个词汇对应word embedding ExtendSoftword：组合所有字的所有BME，得到可能的词，但是无法复原原始的词汇信息是怎样 Soft-lexicon：对当前字符，依次获取BMES对应所有词汇集合。 根据词频加权词向量，与字向量求和。 该模型比Lattice LSTM, WC-LSTM等，在输入embedding上进行改进的模型，效果更好，更容易使用和迁移。 策略 Positive-unlabeled learning -- PU Learning 在只有正类和无标记数据的情况下，训练二分类器 Method 1 Directly 将正样本和部分筛选出的未标记样本分别看作是positive samples和negative samples 训练一个分类器，输出样本属于正、负类的概率 使用训练好的分类器。分类未标注数据，若正类的概率 大于 负类的概率，则该未标注样本的更可能为正类 Method 2 PU bagging 将所有正样本和未标记样本进行随机组合 bootstrap 来创建训练集； 将正样本和未标记样本视为positive和negative，训练一个分类器； 将分类器应用于不在训练集中的未标记样本 OOB（“out of bag”），并记录其分数； 重复上述三个步骤，最后每个未标记样本的分数为每一轮 OOB分数 的平均值。 Method 3 人工标注一部分确认为负类的数据，训练分类器识别这些 确认为 负类的数据。 示例 示例 论文：Distantly Supervised Named Entity Recognition using Positive-Unlabeled Learning，将PU Learning应用在NER任务上 Git Repo： 首先有 未标记数据 Du，实体字典 Dict； 使用最大匹配方法，标记一部分 Du，是NE则为正类，不是NE则为负类； 对每一种NE类型（比如，Loc，Nane）训练一个PU 分类器（自定义的神经网络模型）； 使用多个PU 分类器，对剩余的 Du，进行预测，每一个词，取预测概率最大的那一类标记； 若某些 词 多次被预测为 实体，且每次出现都被预测为同一类实体，那么，将这个词，加入Dict； 重复以上步骤，直到Dict不再改变。 FGM 引用Blog原文 对抗可以作为一种防御机制，并且经过简单的修改，便能用在NLP任务上，提高模型的泛化能力。对抗训练可以写成一个插件的形式，用几行代码就可以在训练中自由地调用。 在原始输入样本 上加一个扰动 ，得到对抗样本后，用其进行训练。将输入样本向着损失上升的方向再进一步，得到的对抗样本就能造成更大的损失，提高模型的错误率。问题可以被抽象成这么一个模型： 其中， 为gold label， 为模型参数。Goodfellow认为，神经网络由于其线性的特点，很容易受到线性扰动的攻击。于是，他提出了 Fast Gradient Sign Method (FGSM) ： 其中， 为符号函数， 为损失函数。Goodfellow发现，令 ，用这个扰动能给一个单层分类器造成99.9%的错误率。 Goodfellow还总结了对抗训练的两个作用： 提高模型应对恶意对抗样本时的鲁棒性； 作为一种regularization，减少overfitting，提高泛化能力。 从优化的视角，问题重新定义成了一个找鞍点的问题，Min-Max：内部损失函数的最大化，外部经验风险的最小化： 内部max是为了找到worst-case的扰动，也就是攻击，其中， 为损失函数， 为扰动的范围空间。 外部min是为了基于该攻击方式，找到最鲁棒的模型参数，也就是防御，其中 是输入样本的分布。 CV任务的输入是连续的RGB的值，而NLP问题中，输入是离散的单词序列，一般以one-hot vector的形式呈现，如果直接在raw text上进行扰动，那么扰动的大小和方向可能都没什么意义。Goodfellow在17年的ICLR中提出了可以在连续的embedding上做扰动。在CV任务，根据经验性的结论，对抗训练往往会使得模型在非对抗样本上的表现变差，然而神奇的是，在NLP任务中，模型的泛化能力反而变强了。 因此，在NLP任务中，对抗训练的角色不再是为了防御基于梯度的恶意攻击，反而更多的是作为一种regularization，提高模型的泛化能力。 对抗训练，FSGM的修改版本，取消了符号函数，对梯度计算进行scale，而不是只使用 +1 或者 -1 代替。 原网络进行一次，前向反向传播，得到梯度g 计算embedding矩阵的修正梯度 r: \\(r=\\frac{\\epsilon g}{\\|g\\|_{2}}\\) 输入 embedding + r ，计算对抗梯度 ga 将 ga 累加到 g 中，得到 gf 恢复原网络的embedding数值，使用 gf 对参数进行更新 Projected Gradient Descent（PGD）：“小步走，多走几步”，如果走出了扰动半径为 的空间，就映射回“球面”上，以保证扰动不要过大。 其中 为扰动的约束空间， 为小步的步长。 PGD模型能够得到一个非常低且集中的loss分布。 另外在半监督条件下，也可以使用对抗训练方法Virtual Adversarial Training进行半监督训练。 示例代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import torchgrad_backup = &#123;&#125;def save_grad(tensorName): def backward_hook(grad: torch.Tensor): grad_backup[tensorName] = grad return backward_hookclass PGD: def __init__(self, model): self.model = model self.emb_backup = &#123;&#125; def attack(self, epsilon=1., alpha=0.3, emb_name='emb.', is_first_attack=False): for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: if is_first_attack: self.emb_backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm != 0 and not torch.isnan(norm): r_at = alpha * param.grad / norm param.data.add_(r_at) param.data = self.project(name, param.data, epsilon) def restore(self, emb_name='emb.'): # emb_name这个参数要换成你模型中embedding的参数名 for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: assert name in self.emb_backup param.data = self.emb_backup[name] self.emb_backup = &#123;&#125; def project(self, param_name, param_data, epsilon): r = param_data - self.emb_backup[param_name] if torch.norm(r) &gt; epsilon: r = epsilon * r / torch.norm(r) return self.emb_backup[param_name] + r def backup_grad(self): # 此处也可以直接用一个成员变量储存 grad，而不用 register_hook 存储在全局变量中 for name, param in self.model.named_parameters(): if param.requires_grad: param.register_hook(save_grad(name)) def restore_grad(self): for name, param in self.model.named_parameters(): if param.requires_grad: param.grad = grad_backup[name]if __name__ == '__main__': # 示例过程 pgd = PGD(model) K = 3 # 小步走的步数 for batch_input, batch_label in data: # 正常训练 loss = model(batch_input, batch_label) loss.backward() # 反向传播，得到正常的grad pgd.backup_grad() # 对抗训练 for t in range(K): pgd.attack(is_first_attack=(t==0)) # 在embedding上添加对抗扰动, first attack时备份param.data if t != K-1: model.zero_grad() else: pgd.restore_grad() loss_adv = model(batch_input, batch_label) loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度 pgd.restore() # 恢复embedding参数 # 梯度下降，更新参数 optimizer.step() model.zero_grad() 12345678910111213141516171819202122232425262728293031323334353637383940import torchclass FGM: def __init__(self, model): self.model = model self.backup = &#123;&#125; def attack(self, epsilon=1, emb_name='emb.'): for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: self.backup[name] = param.data.clone() norm = torch.norm(param.grad) if norm != 0 and not torch.isnan(norm): r_adv = epsilon * param.grad / norm param.data.add_(r_adv) def restore(self, emb_name='emb.'): for name, param in self.model.named_parameters(): if param.requires_grad and emb_name in name: assert name in self.backup param.data = self.backup[name] self.backup = &#123;&#125;if __name__ == \"__main__\": # 示例过程 fgm = FGM(model) for batch_input, batch_label in data: # 正常训练 loss = model(batch_input, batch_label) loss.backward() # 反向传播，得到正常的grad # 对抗训练 fgm.attack() # 在embedding上添加对抗扰动 loss_adv = model(batch_input, batch_label) loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度 fgm.restore() # 恢复embedding参数 # 梯度下降，更新参数 optimizer.step() model.zero_grad() SWA Stochastic Weight Averaging，方法的提出者认为，训练期间得到的局部最小值 倾向于 在损失值较低的区域的边界，而不是集中在损失更低的区域中心部分。所以，Stochastic Weight Averaging可以通过对边界的平均，得到更好性能和更好泛化性能的模型。Git Repo 保存两套权重w, wswa； 使用循环学习率，训练w； 达到指定轮次，更新ws，\\(n_{models}\\)指更新\\(w_{swa}\\)时，中间间隔的轮次: \\(w_{swa} = \\frac{w_{swa}n_{models}+w}{n_{models}+1}\\) 循环以上步骤，最后使用wswa，作为最终模型 有可以直接使用的工具，比较方便。~from torchcontrib.optim import SWA~ 12345678910111213141516optimizer = torch.optim.Adam(params_lr)# Stochastic Weight Averagingoptimizer = SWA(optimizer)if ...: optimizer.update_swa() ...# 训练结束时使用收集到的swa moving averageoptimizer.swap_swa_sgd()# optimizer.bn_update(# train_dataloader,# model) # 更新BatchNorm的 running mean# save 参考链接： 2020CCF-NER Flat-Lattice-Transformer","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"NER","slug":"NER","permalink":"https://racleray.github.io/tags/NER/"},{"name":"FLAT","slug":"FLAT","permalink":"https://racleray.github.io/tags/FLAT/"},{"name":"methodology","slug":"methodology","permalink":"https://racleray.github.io/tags/methodology/"}]},{"title":"字符串匹配从KMP到AC自动机","slug":"字符串匹配从KMP到AC自动机","date":"2021-02-27T15:21:03.000Z","updated":"2023-08-07T11:54:31.040Z","comments":true,"path":"posts/51024bbf.html","link":"","permalink":"https://racleray.github.io/posts/51024bbf.html","excerpt":"记录从多输入单pattern文本匹配算法KMP，到多输入多pattern文本匹配算法AC自动机。","text":"Knuth-Morris-Pratt 在一个字符串S内查找一个词W的出现位置。KMP目的是什么？比如是暴力匹配（两个for循环）最坏的情况，O(m*n)： 12S: aaaaaaaaaaaaaaW: aaab 想法，不匹配就只是个结果，没有其它信息产生吗？显然，如果有一部分匹配，后续搜索应该可以利用。 怎么利用？ 首先，有一部分匹配，才可以操作，才有多的信息嘛。 那么，目的就是保证已经匹配的部分，在S中，不再重复匹配。 12S: aaaaaababaaaaaW: ababc 比如，上例，abab匹配，c不匹配，重新回退4个位置匹配吗？W的信息我们是知道的，这末端abc不匹配，里面还有aba啊，并且从S当前匹配的信息，发现aba，已经再次匹配，S的指针不需要回退！ 现在的问题，是W的信息怎么表示？需要什么信息？部分子串前缀和后缀相同的信息。 只需要一个数组，告诉我们，当出现不匹配字符时，可以跳过那些重复的一定会再次匹配的部分。 现在，问题就是：找到 W 对应的回退数组N，顺序匹配 S 与 W，按照 N 的信息回退。 时间复杂度：O(|S|+|W|) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;string&gt;using namespace std;void get_backoff(string &amp;pattern, vector&lt;int&gt; &amp;book)&#123; for (int i = 1, j = 0; i &lt; (int)pattern.size(); i++) &#123; while (j &amp;&amp; pattern[i] != pattern[j]) &#123; // 不匹配，回退到上一个相同前缀 j = book[j - 1]; &#125; // 匹配，更新相同的前缀的最后一个index if (pattern[i] == pattern[j]) j++; // 更新book，记录相同的前缀的最后一个index book[i] = j; &#125;&#125;int kmp(string &amp;s, string &amp;p, int begin)&#123; int res = -1; vector&lt;int&gt; book(p.size()); get_backoff(p, book); for (int i = begin, j = 0; i &lt; (int)s.size(); ++i) &#123; while (j &amp;&amp; s[i] != p[j]) // 回退p j = book[j - 1]; if (s[i] == p[j]) j++; if (j == (int)p.size()) &#123; res = i - p.size() + 1; &#125; &#125; return res;&#125;int main(int argc, char *argv[])&#123; string s = \"abskajakajkafkkakaj\"; string p = \"kkakaj\"; string pre = \"abskajakajkaf\"; cout &lt;&lt; kmp(s, p, 0) &lt;&lt; endl; cout &lt;&lt; pre.size() &lt;&lt; endl; return 0;&#125; Trie前缀树或字典树 从头到尾，使用输入串中的一个字符来确定要进入的下一个状态。选择标记有相同字符的边缘以行走。每一步都消耗一个字符。 一个节点的所有子孙都有相同的前缀，也就是这个节点对应的字符。 有什么强的？ 从根到叶遍历所需的时间不取决于数据库的大小，而是与键的长度成比例。因此，在一般情况下，它通常比B树或任何基于比较的索引方法快得多。它的时间复杂度与哈希技术相当。 除了效率外，在拼写错误的情况下，trie还提供了搜索最近路径的灵活性。 能做什么？ 多patttern匹配 比如，你有很多个pattern，要同时在一个string s中查找，是否出现。使用Trie保存pattern，作为索引，在string中搜索，可以加快效率。 。。。 brute force需要 O(|Text| *|Patterns|) Trie需要 O(|Text| * |Len of Longest Pattern|) 加上构造 Trie 的 O(|Patterns|)。 需要额外空间复杂度，O(|Patterns|)。 怎么实现？ 最基础的实现方式如上图。实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;bits/stdc++.h&gt;using namespace std;int trie[STACSIZE][N]; //手动开辟栈空间// 或者// vector&lt;map&lt;char,int&gt;&gt; trie[STACSIZE][N]; //手动开辟栈空间// 或者// vector&lt;unordered_map&lt;char,int&gt;&gt; trie[STACSIZE][N]; //手动开辟栈空间int counts[STACSIZE]; //计数int idx = 0;void insert(string &amp;s)&#123; int p = 0; for (int i = 0; i &lt; s.size(); i++) &#123; int c = s[i] - 'a'; if (trie[p][c] == 0) trie[p][c] = ++idx; //没有节点，创造节点，指定在栈空间的位置 p = trie[p][c]; //更新当前位置，进入c节点 &#125; counts[p]++; //更新计数&#125;int query(string &amp;s)&#123; int p = 0; for (int i = 0; i &lt; s.size(); i++) &#123; int c = s[i] - 'a'; if (!trie[p][c]) return 0; p = trie[p][c]; &#125; return counts[p];&#125;int main(int argc, char **argv)&#123; for (int i = 0; i &lt; 5; i++) &#123; string s; cin &gt;&gt; s; insert(s); &#125; for (int i = 0; i &lt; 5; i++) &#123; string s; cin &gt;&gt; s; cout &lt;&lt; query(s) &lt;&lt; endl; &#125; return 0;&#125; 以上方法，就空间使用而言，这是相当奢侈的，因为在trie中，大多数节点只有几个分支，而大多数表单元格为空白。更紧凑的方案是使用链表存储每个状态的转换。但是由于是线性搜索，导致访问速度变慢。 因此，大佬设计出了快速访问的表压缩技术来解决该问题。 Double-Array Trie：包含base和check两个数组。base数组的每个元素表示一个Trie节点，即一个状态 ；check数组表示某个状态的前驱状态。 示意图所示，构造两个数组，一个base对应一个前驱check。 一步状态转移逻辑为： t := base [ s ] + c ; 如果 check[ t ] = s， 则 next s := t 如果 check[ t ] = -s， 则 next s := t 且 t 是一个终止状态（比如表示一个词的结尾） 否则 转移失败 实现，就暂时不研究了。 More about 多个pattern匹配 除了Trie，还有针对 string s 建树的 Suffix Trie. 对 string：p a n a m a b a n an a s。构建后缀Trie 将 pattern 逐个输入，进行匹配。此时，不需要匹配到叶子节点。 额外空间复杂度，和 string s 有关。压缩无分支后缀之后，就成了Suffix Tree。 若再压缩空间，可以将suffix 变成 idx + length。 额外空间复杂度为，O(|Text|)。使用 Suffix Trie，多pattern匹配的时间复杂度为： O(|Text| + |Patterns|)，Trie 为 O(|Text| * |Len of Longest Pattern|)。 额外空间复杂度为 O(|Text|)，Trie 为 O(|Patterns|)。只是 O(|Text|) 的系数约为20，当文本很长时，还是算了吧。 AC自动机 还是，多 pattern 匹配问题，在只是使用Trie，在string s上暴力遍历，只是寻找成功的那一刻，失败的匹配那都是没有意义的吗？ 不不不，得让失败的存在，这东西越混沌，信息熵越高，有用。 AC自动机，Trie + KMP，加速多pattern匹配过程。匹配过程时间复杂度O（|Text| * Trie树高）。 构建 patterns 的 Trie 构建 fail 指针 开始匹配 构建 fail 指针 在Trie中，BFS遍历，第一层，fail指针都指向 root 第一层之后，每个节点的 fail 指针，指向 【该节点的父节点】 的 【fail指针指向的节点】 的 【儿子节点】中 【和该节点（自己）同字符的节点】。如果没有找到，【fail指针指向的节点】继续向上找 fail 节点，直到 root。 啥是 fail ？ 当前单词的最长后缀。 匹配过程 输入string s，trie从 root开始，进行匹配 当匹配失败，跳转到fail指针指向的节点，如果fail到 root，输入此位置之后的string s的部分，继续查找。 当匹配成功（Trie标记的节点），也跳转到fail指针指向的节点，如果此时跳转到 root，进行回溯到前一个最长的trie路径节点。 跳转，就是在匹配后缀。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#include &lt;bits/stdc++.h&gt;using namespace std;static int idx = 0;struct trieNode&#123; int son[26] = &#123;0&#125;; int counts = 0; int fail = -1;&#125;;void insert(const string &amp;s, vector&lt;trieNode&gt; &amp;trie)&#123; int p = 0; for (int i = 0; i &lt; s.size(); i++) &#123; int cha = s[i] - 'a'; if (!trie[p].son[cha]) trie[p].son[cha] = ++idx; p = trie[p].son[cha]; &#125; trie[p].counts++;&#125;void build_fail(vector&lt;trieNode&gt; &amp;trie)&#123; queue&lt;int&gt; q; for (int i = 0; i &lt; 26; i++) &#123; if (trie[0].son[i] != 0) &#123; //儿子节点 int son = trie[0].son[i]; trie[son].fail = 0; //第一层 fail q.push(son); &#125; &#125; while (q.size()) &#123; int father = q.front(); q.pop(); for (int i = 0; i &lt; 26; i++) &#123; if (trie[father].son[i] != 0) &#123; int cur = trie[father].son[i]; // 要找fail的儿子 int failOfFather = trie[father].fail; // 父节点的fail // ~(0): -1, ~(-1): 0 //不是根节点且没有找到目标同字符 while (~failOfFather &amp;&amp; !trie[father].son[i]) failOfFather = trie[failOfFather].fail; if (~failOfFather) //找到目标同字符 trie[cur].fail = trie[failOfFather].son[i]; else // 根节点 trie[cur].fail = 0; q.push(cur); &#125; &#125; &#125;&#125;int query(const string &amp;s, vector&lt;trieNode&gt; &amp;trie)&#123; int ans = 0; int ptr = 0; for (int i = 0; i &lt; s.size(); i++) &#123; int word = s[i] - 'a'; // 儿子不存在且fail不是根节点，跳转 fail while (!trie[ptr].son[word] &amp;&amp; ~trie[ptr].fail) ptr = trie[ptr].fail; if (trie[ptr].son[word]) // 儿子存在，匹配，进入节点，继续查找 ptr = trie[ptr].son[word]; else // 是根节点，下一个 s 字符 continue; int ptr_temp = ptr; // copt ptr，ptr为回溯位置 while (~trie[ptr_temp].fail) //到根节点退出，下一个外层for回溯 &#123; ans += trie[ptr_temp].counts; // counts在不是目标处为0 ptr_temp = trie[ptr_temp].fail; &#125; &#125; return ans;&#125;int main(int argc, char **argv)&#123; vector&lt;trieNode&gt; trie; trie.resize(50); insert(\"she\", trie); string s = \"he\"; insert(s, trie); insert(\"her\", trie); insert(\"is\", trie); insert(\"this\", trie); insert(\"his\", trie); build_fail(trie); cout &lt;&lt; query(\"sherthis\", trie) &lt;&lt; endl;&#125;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"Algorithm","slug":"Notes/Algorithm","permalink":"https://racleray.github.io/categories/Notes/Algorithm/"}],"tags":[{"name":"algorithm","slug":"algorithm","permalink":"https://racleray.github.io/tags/algorithm/"},{"name":"kmp","slug":"kmp","permalink":"https://racleray.github.io/tags/kmp/"},{"name":"trie","slug":"trie","permalink":"https://racleray.github.io/tags/trie/"},{"name":"Aho-Corasick","slug":"Aho-Corasick","permalink":"https://racleray.github.io/tags/Aho-Corasick/"}]},{"title":"浅涉python与分布式","slug":"浅涉python与分布式","date":"2021-02-24T12:23:25.000Z","updated":"2023-08-07T11:54:31.046Z","comments":true,"path":"posts/cdee5c8d.html","link":"","permalink":"https://racleray.github.io/posts/cdee5c8d.html","excerpt":"记录关于并行计算的一点东西。包括并行与分布式的概念、使用python工具包的简单示例。","text":"并行和分布式计算介绍 现代计算的特点，主板上安装多块处理器（每个处理器含有多个核心），这使得计算机能真正地实现并发。 一个处理器同一时间只能处理同一事务；后面章节我们会看到，当处理器快到一定程度，就可以给出同一时间进行多项任务的假象。若要真正实现同一时间多任务，就需要多个处理器。 另一个是高速计算机网络。它让无穷多的电脑实现了相互通讯。 并行计算 并行计算是同时使用多个处理器处理事务。 分布式计算 分布式计算是指同一时间使用多台计算机处理一个任务。只有当计算机之间互相连接时，才可以使用分布式计算。要记住，真正的瓶颈往往是数据而不是CPU。 并行和分布式计算的最明显的差异就是底层的内存架构和访问方式不同。 并行和四个处理器可以访问同一内存地址。对于分布式应用，不同的并发任务不能正常访问同一内存。原因是，一些任务是在这一台计算机运行，一些任务是在另一台计算机运行，它们是物理分隔的，通过网络进行数据传输。 现实中，计算机网络通讯就像一个纯粹的分布式内存架构。然而，每台计算机有多个处理器，运行着共享式内存架构。 分布式内存系统扩展性强、组建成本低：需要更高性能，扩展即可。另一优点是，处理器可以访问各自的内存，不必担心发生Race condition。 缺点是，开发者需要手动写数据传输的策略，需要考虑数据存储的位置。另外，不是所有算法都能容易移植到这种架构。 阿姆达尔定律 考虑一个部分并行的算法，称P为并行分量，S为序列分量（即非并行分量），P+S=100%。T(n)为运行时间，处理器数量为n。 对比T(n)和T(1)可以得到，分布式处理的加速比。 随着n的提高，加速的效果不让人满意。使用10个处理器，是9.2倍速。使用100个处理器，则是50倍速。使用1000个处理器，仅仅是91倍速。 阿姆达尔定律告诉我们两点：我们最快可以将倍速提高到多少；收益减少时，何时应该减少硬件资源的投入。 异步编程（非阻塞编程） 与传统的同步编程相比，异步编程或非阻塞编程，可以使性能获得极大提高。 理想的状态应该是安排一下任务，当一个任务等待I/O时，它处于悬停状态，就让另一个任务接管CPU。这就是异步（也称为事件驱动）编程。 使用多线程在不同的线程并行运行，也可以达到同样的效果。但是，有一个显著的不同：使用多线程时，是由操作系统决定哪个线程处于运行或悬停。然而，在异步编程中，每个任务可以自己决定是否放弃CPU。 另外，单单使用异步编程，我们不能做出真正的并发：同一时间仅仅有一个任务在运行。 另一点要注意的是，异步编程更善于处理I/O密集型任务，而不是CPU密集型任务（暂停任务不会使性能提高）。 任何异步代码都要精心选择非阻塞的库，以防使用阻塞代码。 协程 在Python中，让一个功能中途暂停的关键是使用协程。 协程就是一类函数，它可以通过yield，在指定位置暂停或继续任务。需要注意，尽管协程是强化的生成器，在概念意义上并不等于生成器。原因是，协程与迭代无关。另一不同点，生成器产生值，而协程消除值。 生成器就是一个callable，它生成一个结果序列，而不是返回结果。这是通过产生（通过yield关键字）值而不是返回值 1234def mygenerator(n): while n: n -= 1 yield n next()从生成的序列产生一个值，本质上，生成器是简化的迭代器，免去了定义类中__iter__和__next__的方法。外，生成器是一次性操作，不能重复生成的序列。 __iter__和__next__方法，运行了迭代协议：前者返回了迭代的对象，后者逐个返回了序列中的元素。 12345678910class MyIterator(object): def __init__(self, xs): self.xs = xs def __iter__(self): return self def __next__(self): if self.xs: return self.xs.pop(0) else: raise StopIteration 协程有三种主要的结构: yield()： 用来暂停协程的执行 send()： 用来向协程传递数据（以让协程继续执行） close()：用来关闭协程 示例 1234567891011def complain_about(substring): print('Please talk to me!') try: while True: # 执行到此处，控制点返回shell，直到外部send数据到yield处，传递给text text = (yield) if substring in text: print('Oh no: I found a %s again!' % (substring)) except GeneratorExit: print('Ok, ok: I am quitting.') 1234567891011&gt;&gt;&gt; c = complain_about('Ruby')&gt;&gt;&gt; next(c)Please talk to me!&gt;&gt;&gt; c.send('Test data')&gt;&gt;&gt; c.send('Some more random text')&gt;&gt;&gt; c.send('Test data with Ruby somewhere in it')Oh no: I found a Ruby again!&gt;&gt;&gt; c.send('Stop complaining about Ruby or else!')Oh no: I found a Ruby again!&gt;&gt;&gt; c.close()Ok, ok: I am quitting. 复制ErrorOK! 通过 next 启动协程，close关闭。 词汇计数示例，文本来自http://www.gutenberg.org/cache/epub/2600/pg2600.txt。 逐行读取文件（通过cat函数）；统计每行中substring的出现次数（grep协程）；求和并打印数据（count协程）。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from functools import wrapsdef coroutine(fn): @wraps(fn) def wrapper(*args, **kwargs): c = fn(*args, **kwargs) next(c) return c return wrapperdef cat(f, case_insensitive, child): if case_insensitive: line_processor = lambda l: l.lower() else: line_processor = lambda l: l for line in f: child.send(line_processor(line))@coroutinedef grep(sub_str, case_insensitive, child): if case_insensitive: sub_str = sub_str.lower() while True: text = (yield) # 等待send发送的数据 child.send(text.count(sub_str))@coroutinedef count(sub_str): n = 0 try: while True: n += (yield) except GeneratorExit: print(sub_str, n)@coroutinedef fanout(children): # 多个目标同时输入，计数 while True: data = (yield) for child in children: child.send(data)if __name__ == '__main__': import argparse parser = argparse.ArgumentParser() parser.add_argument('-i', action='store_true', dest='case_insensitive') parser.add_argument('pattern', type=str) parser.add_argument('infile', type=argparse.FileType('r')) args = parser.parse_args() cat(args.infile, args.case_insensitive, grep(args.pattern, args.case_insensitive, count(args.pattern))) cat( args.infile, args.case_insensitive, fanout( [grep(p, args.case_insensitive, count(p)) for p in args.pattern])) 1python grep.py -i love pg2600.txt 并行计算 如何使用多个CPU进行并行编程的。具体目标是加速CPU密集型任务。 多线程 在单CPU系统中，使用多线程并不是真正的并行，在给定时间只有一个线程在运行。只有在多CPU计算机上，线程才是并行的。 尽管Python的线程是OS原生的，全局锁却使特定时间只有一个是运行的。 当一个协程或进程等待I/O时，让另一个运行CPU，也可以达到并发的效果。当一个任务需要占用CPU大量时间时，CPU Bound，就不会有多大提高。 123456789101112131415161718192021222324252627from threading import Threadfrom queue import Queueimport urllib.requestURL = 'http://finance.yahoo.com/d/quotes.csv?s=&#123;&#125;=X&amp;f=p'def get_rate(pair, outq, url_tmplt=URL): with urllib.request.urlopen(url_tmplt.format(pair)) as res: body = res.read() outq.put((pair, float(body.strip())))if __name__ == '__main__': import argparse parser = argparse.ArgumentParser() parser.add_argument('pairs', type=str, nargs='+') args = parser.parse_args() outputq = Queue() for pair in args.pairs: t = Thread(target=get_rate, kwargs=&#123;'pair': pair, 'outq': outputq&#125;) t.daemon = True # 结束时，会自行回收线程资源 t.start() for _ in args.pairs: pair, rate = outputq.get() print(pair, rate) outputq.task_done() outputq.join() 多进程 为避免全局锁对CPU制约型线程的影响，使用多进程。多进程有一些缺点，它必须启动Python的多个实例，启动时间长，耗费内存多。 多进程有它们各自的内存空间，使用的是无共享架构，数据访问十分清晰。 实现并行进程，python提供两个module：multiprocessing和concurrent.futures 1234567891011121314151617181920212223import concurrent.futures as cfdef fib(n): if n &lt;= 2: return 1 elif n == 0: return 0 elif n &lt; 0: raise Exception('fib(n) is undefined for n &lt; 0') return fib(n - 1) + fib(n - 2)if __name__ == '__main__': import argparse parser = argparse.ArgumentParser() parser.add_argument('-n', type=int, default=1) parser.add_argument('number', type=int, nargs='?', default=34) args = parser.parse_args() assert args.n &gt;= 1, 'The number of threads has to be &gt; 1' with cf.ProcessPoolExecutor(max_workers=args.n) as pool: results = pool.map(fib, [args.number] * args.n) 在四核处理器的计算机上运行时，可以实现真正的并行，运行一次到四次，时间差不多。 进程数比处理器数目多时，性能会急剧下降。 在工作进程之间交换数据，在学习C时，会用到 管道 (使用最简单) 信号 (开销最小) 共享映射区 (无父子关系) 本地套接字 (最稳定) multiprocessing模块提供的方法是队列和管道。 1234567891011121314151617181920212223242526272829303132333435363738394041import multiprocessing as mpdef fib(n): if n &lt;= 2: return 1 elif n == 0: return 0 elif n &lt; 0: raise Exception('fib(n) is undefined for n &lt; 0') return fib(n - 1) + fib(n - 2)def worker(inq, outq): \"inq 任务队列， outq 输出队列\" while True: data = inq.get() if data is None: # 检测 哨兵值 return fn, arg = data outq.put(fn(arg))if __name__ == '__main__': import argparse parser = argparse.ArgumentParser() parser.add_argument('-n', type=int, default=1) parser.add_argument('number', type=int, nargs='?', default=34) args = parser.parse_args() assert args.n &gt;= 1, 'The number of threads has to be &gt; 1' tasks = mp.Queue() results = mp.Queue() for i in range(args.n): tasks.put((fib, args.number)) for i in range(args.n): mp.Process(target=worker, args=(tasks, results)).start() for i in range(args.n): print(results.get()) for i in range(args.n): # 输入哨兵值，停止worker tasks.put(None) 开发并行应用的主要难点就是控制数据访问，避免竞争条件或篡改共享数据。要明确何时停止。阿姆达尔定律指出，并行是收益递减的。并行化可能耗时巨大。一定要知道，哪段代码是需要并行化的，理论加速上限又是多少。 另外，避免收益递减的方法是增加任务量，提升并行任务的占比，这是古斯塔夫森定律的核心。 Celery Celery（http://www.celeryproject.org）是用到的第一个第三方库。Celery是一个分布任务队列，就是一个以队列为基础的系统。 轻量化的队列任务调度包：https://python-rq.org/ 分别在主机安装RabbitMQ （windows: exe erlang），ubuntu机器开启 redis-server ( sudo apt-get install redis-server )，python环境安装 celery，redis。（pip install celery[Redis]） 就学习示例而言，自己搭建整个环境耗时太大，尤其在windows环境下！最好能云服务器环境，只管写代码，环境一般不会出问题。 直接在几个虚拟机上测试比较方便。 在机器1上： 123456789import celery app = celery.Celery('test', broker='redis://192.168.56.104', backend='redis://192.168.56.103') @app.taskdef echo(message): return message 建立机器1，worker池 1celery -A test worker --loglevel=info celery命令会默认启动CPU数目相同的worker进程。worker会使用test模块中的应用app（我们可以使用实例的名字celery -A test.app worker），并使用INFO等级在控制台显示日志。 在机器2上（此处即为 master节点），保存一份test.py相同代码 进入 python 交互环境： 1&gt;&gt;&gt; from test import echo 12&gt;&gt;&gt; res = echo.delay('Python rocks!')&gt;&gt;&gt; res.result 可以在worker机器上执行程序，并返回结果 分布式任务队列 master-worker架构，有一个中间件层，中间件层使用多个任务请求队列（即任务队列），和一个用于存储结果的队列（即结果后台）。 主进程（也叫作client或producer）将任务请求安插到某个任务队列，从结果后台获取数据。worker进程订阅任务队列以明确任务是什么，并把结果放到结果后台。 只管定制好 任务队列 和 结果后台，其他worker、producer如何变化、什么程序都无所谓。 也称作 Master Worker 模式： Master Worker 示例 123456789101112131415161718192021222324252627282930313233343536373839404142# master.pyimport random, time, queuefrom multiprocessing.managers import BaseManager# 发送任务的队列:task_queue = queue.Queue()# 接收结果的队列:result_queue = queue.Queue()# 从BaseManager继承的QueueManager:class QueueManager(BaseManager): pass# 把两个Queue都注册到网络上, callable参数关联了Queue对象:QueueManager.register('get_task_queue', callable=lambda: task_queue)QueueManager.register('get_result_queue', callable=lambda: result_queue)# 绑定端口5000, 设置验证码'abc':manager = QueueManager(address=('', 5000), authkey=b'abc')# 启动Queue:manager.start()# 获得通过网络访问的Queue对象:task = manager.get_task_queue()result = manager.get_result_queue()# 注册任务for i in range(10): n = random.randint(0, 10000) print('Put task %d...' % n) task.put(n)# 读取结果:print('Try get results...')for i in range(10): r = result.get(timeout=10) print('Result: %s' % r)# 关闭:manager.shutdown()print('master exit.') 1234567891011121314151617181920212223242526272829303132333435363738394041# worker.pyimport time, sys, queuefrom multiprocessing.managers import BaseManager# 可以在多个机器上同时开启多个worker# 创建类似的QueueManager:class QueueManager(BaseManager): pass# 由于这个QueueManager只从网络上获取Queue，所以注册时只提供名字:QueueManager.register('get_task_queue')QueueManager.register('get_result_queue')# 连接到服务器，也就是运行task_master.py的机器:server_addr = '127.0.0.1'print('Connect to server %s...' % server_addr)# 端口和验证码注意保持与task_master.py设置的完全一致:m = QueueManager(address=(server_addr, 5000), authkey=b'abc')# 从网络连接:m.connect()# 获取Queue的对象:task = m.get_task_queue()result = m.get_result_queue()# 从task队列取任务,并把结果写入result队列:for i in range(10): try: n = task.get(timeout=1) print('run task %d * %d...' % (n, n)) r = '%d * %d = %d' % (n, n, n * n) time.sleep(1) result.put(r) except queue.Queue.Empty: print('task queue is empty.')# 处理结束:print('worker exit.') 使用中间件传递消息（基于网络），类似Go语言的channel（基于内存）。 另一种消息传递方式是，直接传递，Actor模型，一般有一个Global Schedule，设计的目的是计算。 Actor消息传递示例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# actor.pyfrom queue import Queuefrom threading import Thread, Eventclass ActorExit(Exception): passclass Actor(object): def __init__(self): self._mailbox = Queue() def send(self, msg): \"向_mailbox提交任务\" self._mailbox.put(msg) def recv(self): \"从_mailbox获取任务\" msg = self._mailbox.get() if msg is ActorExit: raise ActorExit() return msg def close(self): self.send(ActorExit) def start(self): \"启动线程执行任务\" self._terminated = Event() t = Thread(target=self._bootstrap) t.daemon = True t.start() def _bootstrap(self): try: self.run() except ActorExit: pass finally: self._terminated.set() def join(self): self._terminated.wait() def run(self): ''' Run method to be implemented by the user ''' while True: msg = self.recv() 12345678910111213141516171819202122232425262728293031323334353637383940# test_actor.pyfrom .actor import Actorfrom threading import Eventclass Result(object): def __init__(self): self._evt = Event() self._result = None def set_result(self, value): self._result = value self._evt.set() # 当执行完成计算任务时，解除block @property def result(self): self._evt.wait() # 等待计算结果程序的执行完成，thread block return self._resultclass Worker(Actor): def submit(self, func, *args, **kwargs): \"注册任务\" r = Result() self.send((func, args, kwargs, r)) return r def run(self): \"重写actor的run逻辑，执行用户程序\" while True: func, args, kwargs, r = self.recv() r.set_result(func(*args, **kwargs))if __name__ == '__main__': worker = Worker() worker.start() r = worker.submit(pow, 2, 4) print('it will not block') print(r.result) Ray 基于Master Slaves，Actor的分布式框架。DOC tutorials。分布计算、深度学习调参等等，可是请问 在下去哪里能领到更多的物理机器呢。。。 Global Scheduler：Master上启动了一个全局调度器，用于接收本地调度器提交的任务，并将任务分发给合适的本地任务调度器执行 Redis Server：Master上启动了一到多个Redis Server用于保存分布式任务的状态信息（ControlState），包括对象机器的映射、任务描述、任务debug信息等。 Local Scheduler：每个Slave上启动了一个本地调度器，用于提交任务到全局调度器，以及分配任务给当前机器的Worker进程 Worker：每个Slave上可以启动多个Worker进程执行分布式任务，并将计算结果存储到Object Store，每一个有全局唯一的 Object ID Object Store：每个Slave上启动了一个Object Store存储只读数据对象，Worker可以通过共享内存的方式访问这些对象数据（通过Object ID），这样可以有效地减少内存拷贝和对象序列化成本。Object Store底层由Apache Arrow实现 Plasma：每个Slave上的Object Store都由一个名为Plasma的对象管理器进行管理，它可以在Worker访问本地Object Store上不存在的远程数据对象时，主动拉取其它Slave上的对象数据到当前机器 Ray简易环境搭建 配置Conda 123$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh$ sh Miniconda3-latest-Linux-x86_64.sh 安装Ray，推荐python3.7，其他版本存在已知的Bug（官方） 12345$ conda create --name ray python=3.7$ conda activate ray$ pip install ray Ray 集群搭建: 部署Redis服务(下面假设部署在localhost:6379) 选择任意一台主机作为Master启动 ray start --head --ip localhost --redis-port=6379 在集群其他机器上启动 ray start --address=[head_node_address]:6379 运行一个Map Reduce示例。数据准备，下载Wiki数据，使用WikiExtractor解析： 12$ wget https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream9.xml-p1791081p2336422.bz2$ python WikiExtractor.py -o /data enwiki-latest-pages-articles-multistream9.xml-p1791081p2336422.bz2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144import argparsefrom collections import Counter, defaultdictimport heapqimport numpy as npimport osimport rayparser = argparse.ArgumentParser()parser.add_argument(\"--num-mappers\", help=\"number of mapper actors used\", default=3, type=int)parser.add_argument(\"--num-reducers\", help=\"number of reducer actors used\", default=4, type=int)@ray.remoteclass Mapper(object): def __init__(self, content_stream): self.content_stream = content_stream self.num_articles_processed = 0 self.articles = [] self.word_counts = [] def get_new_article(self): article = self.content_stream.next() # Count the words and store the result. self.word_counts.append(Counter(article.split(\" \"))) self.num_articles_processed += 1 def get_range(self, article_index, keys): \"\"\"keys: list of 2 chars article_index：当前mapper处理的文章index，需要与GlobalScheduler中任务参数进行通信， 保证当前article在整个处理序列中处于article_index的位置\"\"\" # Process more articles if this Mapper hasn't processed enough yet. while self.num_articles_processed &lt; article_index + 1: self.get_new_article() # Return the word counts from within a given character range. return [(k, v) for k, v in self.word_counts[article_index].items() if len(k) &gt;= 1 and k[0] &gt;= keys[0] and k[0] &lt;= keys[1]]@ray.remoteclass Reducer(object): def __init__(self, keys, *mappers): \"针对不同范围的开头字母区间，进行reduce\" self.mappers = mappers self.keys = keys def next_reduce_result(self, article_index): word_count_sum = defaultdict(lambda: 0) # Get the word counts for this Reducer's keys from all of the Mappers # and aggregate the results. count_ids = [ mapper.get_range.remote(article_index, self.keys) for mapper in self.mappers ] # From many Mappers for count_id in count_ids: for k, v in ray.get(count_id): word_count_sum[k] += v return word_count_sumdef get_content(file, floder='/data/'): file = floder + file f = open(file, 'r') return f.read()class Stream(object): \"数据流生成\" def __init__(self, max, folder): \"\"\"max: 最大提取文件数量 folder: 文件夹名称 \"\"\" self.index = 0 self.max = max self.folder = folder self.g = None def init(self): self.g = self.content() def file(self): return f\"wiki_&#123;0&#125;&#123;self.index&#125;\" if self.index &lt; 10 else f\"wiki_&#123;self.index&#125;\" def content(self): while self.index &lt; self.max: yield get_content(self.file(), self.folder) self.index += 1 def next(self): \"生成器\" if not self.g: self.init() return next(self.g)if __name__ == \"__main__\": MAX = 10 args = parser.parse_args() ray.init() # Create one streaming source of articles per mapper. directory = os.path.dirname(os.path.realpath(__file__)) streams = [] folders = ['/data/AA/', '/data/AB/', '/data/AC/'] for i in range(args.num_mappers): streams.append(Stream(MAX, folders[i % len(folders)])) # Partition the keys among the reducers. chunks = np.array_split([chr(i) for i in range(ord(\"a\"), ord(\"z\") + 1)], args.num_reducers) keys = [[chunk[0], chunk[-1]] for chunk in chunks] # Create a number of mappers. mappers = [Mapper.remote(stream) for stream in streams] # Create a number of reduces, each responsible for a different range of # keys. This gives each Reducer actor a handle to each Mapper actor. reducers = [Reducer.remote(key, *mappers) for key in keys] # Most frequent 10 words. article_index = 0 while True: print(\"article index = &#123;&#125;\".format(article_index)) wordcounts = &#123;&#125; counts = ray.get([ reducer.next_reduce_result.remote(article_index) for reducer in reducers ]) for count in counts: wordcounts.update(count) most_frequent_words = heapq.nlargest(10, wordcounts, key=wordcounts.get) for word in most_frequent_words: print(\" \", word, wordcounts[word]) article_index += 1 然后，在每个节点保存一份代码，启动worker和master节点 1234ray start --head --ip localhost --redis-port=6379 # 修改head_node_address为 master 节点的ip地址ray start --address=[head_node_address]:6379 在master运行程序得到结果。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"CS","slug":"Notes/CS","permalink":"https://racleray.github.io/categories/Notes/CS/"}],"tags":[{"name":"ray","slug":"ray","permalink":"https://racleray.github.io/tags/ray/"},{"name":"distributed","slug":"distributed","permalink":"https://racleray.github.io/tags/distributed/"},{"name":"asyc","slug":"asyc","permalink":"https://racleray.github.io/tags/asyc/"}]},{"title":"有用的python package","slug":"有用的python-package","date":"2021-02-24T12:19:59.000Z","updated":"2023-08-07T11:54:31.046Z","comments":true,"path":"posts/4e471b0e.html","link":"","permalink":"https://racleray.github.io/posts/4e471b0e.html","excerpt":"记录几个有用的python包。argh处理命令行参数、schedule定时运行脚本等。","text":"argh--懒人版argparse 123456789101112131415161718192021import arghdef do_the_thing(required_arg, optional_arg=1, other_optional_arg=False): \"\"\" I am a docstring \"\"\" print((required_arg, type(required_arg))) print((optional_arg, type(optional_arg))) print((other_optional_arg, type(other_optional_arg)))@argh.arg('--bool-arg-for-flag', '-b', help=\"Flip this flag for things\")@argh.arg('arg_with_choices', choices=['one', 'two', 'three'])def do_the_other_thing(arg_with_choices, bool_arg_for_flag=False): print(arg_with_choices) print(bool_arg_for_flag)if __name__ == '__main__': # argh.dispatch_command(do_the_thing) argh.dispatch_commands([do_the_thing, do_the_other_thing]) msgpack--二进制版json 123456789101112131415161718192021222324252627282930313233343536373839404142import msgpackimport jsonimport randomdef msgpack_example_1(): example_dict = &#123;i: random.random() for i in range(10000)&#125; with open('json_file.json', 'w') as f: json.dump(example_dict, f) with open('json_file.json') as f: back_from_json = json.load(f) # Saving and loading with open('msgpack_file.msgpack', 'wb') as f: # f.write(msgpack.packb(example_dict)) # f.write(msgpack.packb(example_dict, use_single_float=True)) f.write(msgpack.packb(example_dict)) with open('msgpack_file.msgpack', 'rb') as f: back_from_msgpack = msgpack.unpackb(f.read()) # Data integrity print(type(next(iter(back_from_json.keys())))) print(type(next(iter(back_from_msgpack.keys()))))def msgpack_example_2(): list_of_dicts = [&#123;0: random.random()&#125; for i in range(100)] with open('streamed.msgpack', 'wb') as f: for d in list_of_dicts: f.write(msgpack.packb(d)) # 迭代读取 with open('streamed.msgpack', 'rb') as f: loaded_list_of_dicts = [item for item in msgpack.Unpacker(f)] print(list_of_dicts[3][0], loaded_list_of_dicts[3][0])if __name__ == '__main__': # msgpack_example_1() msgpack_example_2() redis_cache--使用redis缓存函数 12345678910111213141516171819# sudo apt install redis-server# sudo systemctl enable redis-server.service# sudo systemctl start redis-server.service# pip/pip3 install git+https://github.com/YashSinha1996/redis-simple-cache.gitimport timefrom redis_cache import cache_it, cache_it_json@cache_it(limit=1000, expire=5)def function_that_takes_a_long_time(i): print(f\"function was called with input &#123;i&#125;\") return i**2if __name__ == '__main__': for i in range(10): print(i, function_that_takes_a_long_time(2)) schedule--定时运行函数 1234567891011121314151617import timeimport scheduledef test_function(): print(f'test called at &#123;time.time()&#125;')def test_function_2(): print(f'test 2 called at &#123;time.time()&#125;')if __name__ == '__main__': schedule.every(1).seconds.do(test_function) schedule.every(3).seconds.do(test_function_2) # schedule.every(1).days.do(daily_task) # schedule.every().thursday.at(\"10:00\").do(day_time_task) while True: schedule.run_pending() tqdm--进度条显示 123456789101112131415161718192021222324252627282930313233343536373839404142434445from tqdm import tqdm, trangeimport randomimport timedef tqdm_example_1(): for i in tqdm(range(10)): time.sleep(0.2)def tqdm_example_2(): for i in trange(10, desc=\"outer_loop\"): for j in trange(10, desc=\"inner_loop\"): time.sleep(0.01)def tqdm_example_3(add_tot=False): max_iter = 100 tot = 0 if add_tot: bar = tqdm(desc=\"update example\", total=max_iter) else: bar = tqdm() while tot &lt; max_iter: update_iter = random.randint(1, 5) bar.update(update_iter) tot += update_iter time.sleep(0.03)def tqdm_example_4(): t = trange(100) for i in t: t.set_description(f\"on iter &#123;i&#125;\") time.sleep(0.02)if __name__ == \"__main__\": # tqdm_example_1() # tqdm_example_2() # tqdm_example_3() # tqdm_example_3(True) tqdm_example_4() Numba--矩阵运算加速 12345678910111213141516171819202122232425262728293031323334353637383940414243import numpy as npfrom numba import njitfrom concurrent.futures import ThreadPoolExecutordef test_func(x): out=0 for i in range(100000000): out += i return outdef test_heavy_func(times): arr = np.random.rand(10000, 10000) return arr * arrif __name__ == \"__main__\": jitted_func = njit(test_func) jitted_func_2 = njit(test_heavy_func) # 计算使用 jit 并 关闭 gil，提升矩阵运算速度 jitted_func_3 = njit(test_heavy_func, nogil=True) import time # start = time.time() # with ThreadPoolExecutor(4) as ex: # ex.map(jitted_func, range(1000)) # end = time.time() # print(\"[Python origin test] Used time: \", end - start) start = time.time() with ThreadPoolExecutor(4) as ex: ex.map(jitted_func_2, range(100)) end = time.time() print(\"[Numpy origin test] Used time: \", end - start) start = time.time() with ThreadPoolExecutor(4) as ex: ex.map(jitted_func_3, range(100)) end = time.time() print(\"[Numpy no gil test] Used time: \", end - start) 注意：在numba中使用一个普通的python列表不是一个好主意，因为它将花费很长时间来验证类型。 使用ndarray，才是正确的方法！才能带来速度的提升。 另外，@vectorize可以将处理一个元素的函数，转换成可以接受 array输入的优化函数，只是第一次使用需要对内存分配进行优化，会慢一些。 12345678@vectorize(nopython=True)def non_list_function(item): if item % 2 == 0: return 2 else: return 1 non_list_function(test_list) 弹簧阻尼系统计算实例 123456789101112131415161718192021222324def friction_fn(v, vt): if v &gt; vt: return - v * 3 else: return - vt * 3 * np.sign(v)def simulate_spring_mass_funky_damper(x0, T=10, dt=0.0001, vt=1.0): times = np.arange(0, T, dt) positions = np.zeros_like(times) v = 0 a = 0 x = x0 positions[0] = x0/x0 for ii in range(len(times)): if ii == 0: continue t = times[ii] a = friction_fn(v, vt) - 100*x v = v + a*dt x = x + v*dt positions[ii] = x/x0 return times, positions 1%time _ = simulate_spring_mass_funky_damper(0.1) 运行280ms，当输入x0为从0到10000，每次增加0.1，需要7个小时 12345678910111213141516171819202122232425262728@njitdef friction_fn(v, vt): if v &gt; vt: return - v * 3 else: return - vt * 3 * np.sign(v)@njitdef simulate_spring_mass_funky_damper(x0, T=10, dt=0.0001, vt=1.0): times = np.arange(0, T, dt) positions = np.zeros_like(times) v = 0 a = 0 x = x0 positions[0] = x0/x0 for ii in range(len(times)): if ii == 0: continue t = times[ii] a = friction_fn(v, vt) - 100*x v = v + a*dt x = x + v*dt positions[ii] = x/x0 return times, positions_ = simulate_spring_mass_funky_damper(0.1) 运行时间 1.99 ms，加速 200x。 再加速： 12345# 使用多线程from concurrent.futures import ThreadPoolExecutorwith ThreadPoolExecutor(8) as ex: ex.map(simulate_spring_mass_funky_damper, np.arange(0, 1000, 0.1)) 当输入x0为从0到1000，每次增加0.1，需要19.3s。 再加速，利用多核，在矩阵运算时，关闭 GIL 锁： 1234567891011121314151617181920212223242526272829@njit(nogil=True)def friction_fn(v, vt): if v &gt; vt: return - v * 3 else: return - vt * 3 * np.sign(v)@njit(nogil=True)def simulate_spring_mass_funky_damper(x0, T=10, dt=0.0001, vt=1.0): times = np.arange(0, T, dt) positions = np.zeros_like(times) v = 0 a = 0 x = x0 positions[0] = x0/x0 for ii in range(len(times)): if ii == 0: continue t = times[ii] a = friction_fn(v, vt) - 100*x v = v + a*dt x = x + v*dt positions[ii] = x/x0 return times, positions# compile：先编译，那么使用时，省去了这段时间_ = simulate_spring_mass_funky_damper(0.1) 1234from concurrent.futures import ThreadPoolExecutorwith ThreadPoolExecutor(8) as ex: ex.map(simulate_spring_mass_funky_damper, np.arange(0, 1000, 0.1)) 当输入x0为从0到1000，每次增加0.1，需要1.83s。 不使用多线程，使用numba自带的多进程并行，也是可以的 12345678910from numba import prange@njit(nogil=True, parallel=True)def run_sims(end=1000): for x0 in prange(int(end/0.1)): if x0 == 0: continue simulate_spring_mass_funky_damper(x0*0.1) run_sims()","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"python","slug":"Tools/python","permalink":"https://racleray.github.io/categories/Tools/python/"}],"tags":[{"name":"python","slug":"python","permalink":"https://racleray.github.io/tags/python/"},{"name":"tool","slug":"tool","permalink":"https://racleray.github.io/tags/tool/"}]},{"title":"浅涉知识图谱","slug":"浅涉知识图谱","date":"2021-02-22T13:16:40.000Z","updated":"2023-08-07T11:54:31.047Z","comments":true,"path":"posts/6a1e61f5.html","link":"","permalink":"https://racleray.github.io/posts/6a1e61f5.html","excerpt":"简要记录了知识图谱基本概念，NER模型方法（HMM、MEMM、CRF），关系分类方法，知识表示（Trans系列）等。","text":"基本概念 “A knowledge graph consists of a set of interconnected typed entities and their attributes.” 知识图谱由一些相互连接的实体和他们的属性构成的。 是由一条条知识组成，每条知识表示为一个 SPO 三元组 技术体系简图： SPO相关 SPO三元组：（实体，关系，实体），（实体，属性，字面量） 构建的难点之一就是，Schema设计。 设计知识图谱的结构，要构建哪些类别的实体，实体有什么属性，实体间有什么关系，关系有什么属性 SPO的背后 RDF(Resource Description Framework)，即资源描述框架，其本质是一个数据模型（Data Model）。RDF形式上表示为SPO三元组。 RDF由节点和边组成，节点表示实体/资源、属性，边则表示了实体和实体之间的关系以及实体和属性的关系。 RDF的表达能力有限，无法区分类和对象，也无法定义和描述类的关系/属性。就是不能反映一个 类 的特征信息。 RDFS/OWL 用来描述RDF数据。RDFS/OWL序列化方式和RDF没什么不同，其实在表现形式上，它们就是RDF。 RDFS，即“Resource Description Framework Schema”，是最基础的模式语言。定义了类，将类进行抽象。 RDFS的表达能力还是相当有限，因此提出了OWL，Web Ontology Language。我们也可以把OWL当做是RDFS的一个扩展，其添加了额外的预定义词汇。 owl区分数据属性和对象属性（对象属性表示实体和实体之间的关系）。 OWL 使用场景：本体结构中有大量相互链接的类和属性，设计者想用自动推理机得到里面复杂的关系。需要结合基于规则的推理引擎（rule-based reasoning engine）的场合。 命名实体识别 概率图方法 有向图 ⽆向图 概率⽆无向图模型，⼜称为⻢马尔可夫随机场 它假设随机场中任意⼀一个结点的赋值，仅仅和它的邻结点的取值有关，和不不相邻的结点的取值无关。⽆向图G中任何两个结点均有边连接的结点⼦集称为团。 若C是⽆向图的⼀个团，且不能再加进任何一个G的结点使其成为更大的⼀个团，则此C为最⼤团。 联合概率可以表示为其最大团C 随机变量的函数的乘积。 成对⻢马尔可夫性：没有直连边的任意两个节点是独立的。 局部⻢马尔可夫性：给定直连节点时，中心节点和其他节点条件独立。 全局⻢马尔可夫性：给定一个节点集合将全集划分为两个独立集合时，两个点集的任意子集，是相互独立的。 HMM HMM是⽤用于描述由隐藏的状态序列列和显性 的观测序列列组合⽽而成的双重随机过程。 通过可观测到的数据，预测不不可观测到的状态数据。 HMM的假设⼀：⻢马尔可夫性假设。当前时刻的状态值，仅依赖于前 ⼀时刻的状态值，⽽不依赖于更早时刻的状态值。 HMM的假设⼆：⻬次性假设。状态转移概率矩阵与时间⽆关。即所 有时刻共享同⼀个状态转移矩阵。 HMM的假设三：观测独⽴立性假设。当前时刻的观察值，仅依赖于当 前时刻的状态值。 此处的问题是，预测隐状态序列（假设模型参数已经学习得到）。实例演示： 已知： 状态值集合：{晴天，阴天，⾬雨天}；观测值集合：{宅，打球}； 过去状态值序列：{晴晴晴阴⾬雨晴}；对应观测值序列：{球宅宅球宅宅}； 从历史数据学习，已得到模型参数为： 求当观测序列是{宅球宅}，最有可能的天⽓状况序列？（动态规划求解概率最⼤大路路径） 求解过程： 定义: 定义在时刻t状态为i的所有单个路径（i1，i2，... it )中的概率最大值为 t+1时刻的最大概率: 再定义⼀个变量，⽤来回溯最⼤路径：在时刻t状态i的所有单个路径（i1，i2，，， it-1,it)中，概率最⼤的路路径第t-1个节点为（在t时刻选出上一个时刻的最优路径）： 计算第一天 \\(\\delta_1\\)（雨天）= \\(\\pi\\)(雨天) * B(雨天，宅) = 0.28 \\(\\delta_1\\)（阴天）= \\(\\pi\\)(阴天) * B(阴天，宅) = 0.16 \\(\\delta_1\\)（晴天）= \\(\\pi\\)(晴天) * B(晴天，宅) = 0.1 第二天 \\(\\delta_2\\)（雨天）= \\(\\max\\)( [\\(\\delta_1\\)(雨天) * A(雨天，雨天)， \\(\\delta_1\\)(阴天) * A(阴天，雨天)， \\(\\delta_1\\)(晴天) * A(晴天，雨天)]) * B(雨天，打球) = 0.042 ​ 前一时刻选择，雨天 \\(\\delta_2\\)（阴天）= \\(\\max\\)( [\\(\\delta_1\\)(雨天) * A(雨天，阴天)， \\(\\delta_1\\)(阴天) * A(阴天，阴天)， \\(\\delta_1\\)(晴天) * A(晴天，阴天)]) * B(阴天，打球) = 0.0504 ​ 前一时刻选择，雨天 \\(\\delta_2\\)（晴天）= \\(\\max\\)( [\\(\\delta_1\\)(雨天) * A(雨天，晴天)， \\(\\delta_1\\)(阴天) * A(阴天，晴天)， \\(\\delta_1\\)(晴天) * A(晴天，晴天)]) * B(晴天，打球) = 0.028 ​ 前一时刻选择，雨天 第三天 \\(\\delta_3\\)（雨天）= \\(\\max\\)( [\\(\\delta_2\\)(雨天) * A(雨天，雨天)， \\(\\delta_2\\)(阴天) * A(阴天，雨天)， \\(\\delta_2\\)(晴天) * A(晴天，雨天)]) * B(雨天，打球) = 0.0147 ​ 前一时刻选择，雨天 \\(\\delta_3\\)（阴天）= \\(\\max\\)( [\\(\\delta_2\\)(雨天) * A(雨天，阴天)， \\(\\delta_2\\)(阴天) * A(阴天，阴天)， \\(\\delta_2\\)(晴天) * A(晴天，阴天)]) * B(阴天，打球) = 0.01008 ​ 前一时刻选择，阴天 \\(\\delta_3\\)（晴天）= \\(\\max\\)( [\\(\\delta_2\\)(雨天) * A(雨天，晴天)， \\(\\delta_2\\)(阴天) * A(阴天，晴天)， \\(\\delta_2\\)(晴天) * A(晴天，晴天)]) * B(晴天，打球) = 0.00756 ​ 前一时刻选择，阴天 最后，选择t3时刻最大路径，雨天。回溯结果为，{雨天，雨天，雨天} HMM的缺陷 ⻢尔可夫性（有限历史性）：实际上在NLP领域的文本数据，很多词语都是有⻓依赖的。 齐次性：序列列不同位置的状态转移矩阵可能会有所变化，即位置信息会影响预测结果。 观测独立性：观测值和观测值（字与字）之间是有相关性的。 单向图：只与前序状态有关，和后续状态无关。在NLP任务中，上下文的信息都是必须的。 标记偏置Label Bias：若状态A能够向N种状态转移，状态B能够向M种状态转移。若N&lt;&lt;M，则预测序列更有可能选择状态A，因为A的局部转移概率较大 MEMM最大熵马尔可夫模型 解决了观测独立问题，但是依然存在标记偏置。 最大熵（熵：分布的不确定性）： “无知比错误更可取，一个什么都不相信的人比一个相信错误的人离真理更近” 找到最优分布中，最偏向 uniform 的结果，即为最大熵的目标。 H（x）=–∑x log x是凸函数 最大熵模型的likeliihood形式为： 求导之后可以发现： MEMM： 根据历史状态序列，预测当前状态，每一时间步，预测一个状态。 CRF 在给定随机变量序列X的情况下，随机变量Y的条件概率分布P(Y|X)构成条件随机场，即满⾜足马尔可夫性： P(Yi| X,Y1,Y2,...Yn) = P(Yi| X,Yi−1,Yi+1) 则称P(Y|X)为线性链条件随机场。 传统的CRF定义如下： 特征函数分为两类 只和当前节点有关 只和当前节点和上⼀个节点有关，局部特征函数 linear-CRF由 tk, λk, sl, µl 共同决定 i -- 表示从0到T的序列位置；k, l -- 表示自定义的特征函数编号。 在深度学习中使用时，用深度模型代替特征函数： CRF相对于HMM的优点 规避了马尔可夫性，能够获取长文本的远距离依赖的信息 规避了齐次性，并且序列的 位置信息会影响预测出的状态序列 规避了观测独立性，观测值之间的相关性信息能够被提取 不是单向图，而是无向图，能够充分提取上下文信息作为特 征 改善了标记偏置Label Bias问题 CRF的思路是利用多个特征，对状态序列进行预测。HMM 的表现形式使他无法使⽤多个复杂特征 CRF的缺点 CRF训练代价大、复杂度高 需要人为构造特征函数，特征工程对CRF模型的影响很大 实体连接 候选实体生成： 根据输入文本中检测出的实体mention集合M，从给定知识图谱中找到可能属于M的候选实体集合m 候选实体排序： 负责对候选实体集合m中多个候选实体打分的排序， 并输出得分最高的候选实体，作为实体链接结果 关系分类简介 关系抽取: 从一个句子中判断两个entity是否有关系，一般是一个二分类问题，指定某种关系 关系分类: 一般是判断一个句子中两个entity是哪种关系，属于多分类问题。 标注工具: BRAT BRAT是一个基于web的文本标注工具，主要用于对文本的结构化标注，用BRAT生成的标注结果能够把无结构化的原始文本结构化，供计算机处理。利用该工具可以方便的获得各项NLP任务需要的标 注语料。 方法： 基于规则的方法——人工模板 基于规则的方法——基于统计的方法 基于监督学习的方法——CNN/RNN 基于监督学习的方法——PCNN 半监督学习的方法——自举 半监督学习的方法——远程监督 基于统计的方法 输入关系集合中的一个 搜索一组实体对，满足关系 输入实体对，搜索包含实体对的句子，保存 将保存句子中的实体对，使用同一类关系模版替换 计算成功替换，模版匹配正确的比例，即概率，作为模版的得分（置信度）。留下得分高的模版 神经网络方法 输入embedding，可加上相对位置embedding，训练分类器。 PCNN的Piecewise Convolutional，只是使用Piecewise max pooling，从实体所在位置处，分段进行pooling。 半监督 Bootstrapping 创建空的列表； 使用精心选择的种子初始化列表； 利用列表中的内容从训练语料库中查找更多内容； 给那些新发现的内容打分；把得分最高的内容加到列表中。 重复步骤3和4，直到达到最大迭代次数或者其它停止条件为止。 Snowball优化了部分的细节 定义pattern 成为&lt;left, tag1, middle, tag2, right&gt;; tuples 为 &lt;tag1 , tag2&gt; 生成新pattern时，评估其与已有pattern的相似性，取一个阈值之上的，加入pattern集合。 tuples要经过可信度计算，选择可信度高的留下，计算方法看论文或者blog吧。 远程监督 将已有的知识对应到丰富的非结构化语料中从而生成大量的训练数据。知识来源：人工标注、现有的知识库、特定的语句结构。 Distant supervised 会产生有大量噪音或者被错误标注的数据，直接使用supervised的方法进行关系分 类，效果很差。 知识表示Embedding TransE TransE，⼀种将实体与关系嵌⼊到低维向量空间中的简单模型 。该模型已经成为了知识 图谱向量化表示的 baseline，并衍⽣出不同的变体。原理简述： h，t为实体向量，r为关系向量。 以L2 距离为例，梯度的计算相对⽐较简单，⽬标函数变为 求解导数： 完整算法： 评测方法： 在测试时，以⼀个三元组为例，⽤语料中所有实体替换当前三元组的头实体计算距离d(h ′+l,t)，将结果按升序排序，⽤正确三元组 的排名情况来评估学习效果（同理对尾实体这样做）。(若替换到在训练集中的三元组，可以选择 删掉) 度量标准选择hits@10和mean rank，前者代表 命中前10的次数/总查询次数，后者代表 正确结果排名之和/总查询次数 训练速度快、易于实现。另外，可以将word2vec和TransE一起融合训练，此处不作展开。 TransH 虽然 TransE 模型具有训练速度快、易于实现等优点，但是它不能够解决多对⼀和⼀对多关系的问题。以 多对⼀关系为例，固定 r 和 t，TransE 模型为了满⾜三⻆闭包关系，训练出来的头节点的向量会很相似。⽽TransH是⼀种将头尾节点映射到关系平⾯的模型，能够很好地解决这⼀问题。 对于多对⼀关系，TransH 不在严格要求 h+r-l=0，⽽是只需要保证头结点和尾节点在关系平 ⾯上的投影在⼀条直线上即可。 TransR TransE 和 TransH 都假设实体和关系嵌⼊在相同的空间中。然⽽，⼀个实体是多种属性的综合体，不同关 系对应实体的不同属性，即头尾节点和关系可能不在⼀个向量空间中。 TransD TransR同样有它的问题，首先对于一种关系，它的头实体和尾实体使用同样的变换矩阵映射到关系空间，而头实体和尾实体往往是完全不同类的实体，也应该使用不同的方法进行映射。 TransD模型对每个实体或关系使用两个向量进行表示，一个向量表示语义，另一个（用下表p表示）用来构建映射矩阵。 问答应用 基于知识图谱的问答KBQA 基本流程： ⾃然语⾔查询--&gt;意图识别(Intention Recognition)--&gt;实体链指(Entity Linking)+关系识别(Relation Detection) --&gt; 查询语句拼装(Query Construction)--&gt;返回结果选择(Answering Selection) 意图识别(Intention Recognition)：预先准备好意图模板，可以通过相似度来匹配，也可以通过机器学习⾥的 分类问题来解决，这个是所有问答系统都要⾯临的问题。 实体链指(Entity Linking)+关系识别(Relation Detection)：将查询语句中出现的实体和关系映射到知识图谱⾥，本质是⼀个NER问题，只是需要将NER结果进⼀步链接到图谱。 查询语句拼装(Query Construction)：需要根据底层知识图谱的查询语⾔，拼装成对应的query来查询(sparq 等)，最简单的⽅法就是预先定义好查询模板，根据之前解析出来的(意图，实体，关系)填进模板查询即可。 返回结果选择(Answering Selection)：图谱查询之后的结果可能存在多个，需要选择⼀个最合适的答案，可 以预先指定排序规则去选择答案。 参考： 实例：NL2SQL比赛第三名方案，待学习 基于知识表示的问答KEQA KEQA KEQA的目标不是直接推断头部实体和谓词，而是联合恢复知识图嵌入空间中问题的头部实体、谓词和尾部实体表示(eh, p, et)。分别训练两个模型，一个学习 谓词p 和 实体e 的表示，一个识别问题中到的Head实体。如下面两个图所示。 Entity Linking: Finding Extracted Entities in a Knowledge Base ↩︎ Improved Neural Relation Detection for Knowledge Base Question Answering ↩︎","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"relation extraction","slug":"relation-extraction","permalink":"https://racleray.github.io/tags/relation-extraction/"},{"name":"NER","slug":"NER","permalink":"https://racleray.github.io/tags/NER/"},{"name":"TransE","slug":"TransE","permalink":"https://racleray.github.io/tags/TransE/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"https://racleray.github.io/tags/Knowledge-Graph/"}]},{"title":"BERT-flow and more","slug":"BERT-flow-and-more","date":"2021-02-12T16:10:19.000Z","updated":"2023-08-07T11:54:31.025Z","comments":true,"path":"posts/a954bb00.html","link":"","permalink":"https://racleray.github.io/posts/a954bb00.html","excerpt":"记录对BERT-flow模型论文的一点思考，以及记录苏剑林提出的更简单的矩阵空间变换方法解决句子向量表达能力不理想的问题。","text":"论文: On the Sentence Embeddings from Pre-trained Language Models Author: Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, Lei Li Organization: ByteDance AI Lab; Language Technologies Institute, Carnegie Mellon University Info: 类别: BERT语义表示应用优化 研究目标：将原本BERT训练模式下生成的语义表示的各向异性空间，通过设计的无监督方法，转化为各向同性的语义表示空间。 研究成果：结合Flow based model，使用无监督方式提升语义表示的效果。 存在的问题：按文章的思路，就是将BERT sentence embedding做一个转化，从非正交（orthogonal space）转化到正交的空间，以满足cos similarity适用的条件，以提高效果。但是一定需要Flow based 方法来实现吗？有没有更好的方法？ 关键词：BERT；Flow based model；semantic similarity Brief Summary: 首先解释了直接基于BERT生成的sentence embedding为什么其语义表达能力较差，然后提出一种在不引入更多监督数据条件下，提升其语义表达能力的方法，flow based model。 Outline: 搞清楚BERT-induced sentence embedding的空间有什么特性 BERT-flow怎么设计的 [来自苏剑林的质疑] BERT-flow？没必要那么复杂，BERT-whitening更优雅 Main Thought: 首先是flow based model在干什么，这首先是一个生成网络（以下图片内容来自李宏毅老师课程）： 这是一种直接在object function基础之上优化计算的方法，粗暴但是实现起来并不简单。直接从\\(\\pi(z)\\)，由设计的网络\\(x=f(z)\\)，直接逼近data分布\\(p(x)\\)。原理如下： 相应变量的微小变化，导致的面积变化是一致的。扩展到二维变量，其变化相乘变成了面积的\\(\\Delta s\\)。而矩阵的行列式就表示二维空间中图像代表的面积，所以有以下推导： 式中矩阵就是Jacobian matrix，自然得到： \\(p(x&#39;)=\\pi(z&#39;)|\\frac{1}{det(J_f)}|\\) 那么由\\(z\\)的分布就可以求出\\(x\\)的分布。只是这个网络\\(G\\)的设计有一点麻烦，需要保证参数矩阵需要是逆变换不复杂且Jacobian matrix容易计算。因为： \\(p(x&#39;)=\\pi(z&#39;)|det(J_{G^{-1}})|\\) 以及 \\(z&#39;=G^{-1}(x&#39;)\\) 都需要转化成与输入相关的函数（网络）。 同时flow的网络设计，真的是有点低效，参数空间内将参数分组进行更新。 如图，上下两部分不是同时更新，而是分步更新计算。 Key sentences: 用BERT将一个句子编码成一个固定长度的向量，方法是计算BERT最后几层中context embeddings的平均值，或者在[CLS]标记的位置提取。一般前一种方法效果更好。但是在语义表示性能上还不及averaged GloVe embeddings方法。 1. 语义相似度与BERT预训练的关系 ​ 首先，BERT将传统的auto regressive LM的目标，修改为masked predict： \\[ log(p(x_{1:T}))=\\sum_{t=1}^{T}logp(x_t|context_t) \\] 变为： \\[ p(x_{masked}|context_{of\\ masked})=\\sum_{t=1}^{T}mask_t \\times p(x_t|context_t) \\] 两者建立model的共同形式如下： \\[ p(x|context)=\\frac{\\exp(VectorFromNet_{context}^TEmbedding_x)}{\\sum_{x&#39;}\\exp(VectorFromNet_{context}^TEmbedding_{x&#39;})} \\] 这种表达形式的关键在\\(VectorFromNet_{context}^TEmbedding_x\\)，以下简单写作\\(h_c^Tw_x\\)。这篇论文证明了： \\[ h_c^Tw_x\\approx\\log p^*(x|c) + \\lambda_c=PMI(x,c)+\\log p(x)+\\lambda_c \\] ​ PMI指point wise mutual information。这种共现特征通常可以捕捉到语义信息，只是在“word”层面而言。 ​ 同时，\\(h_c\\)随着网路参数的更新而更新，不同的context在训练过程中相互影响，可视为一种高阶的high-order共现语义信息捕捉。 2. 问题所在 用BERT将一个句子编码成一个固定长度的向量，方法是计算BERT最后几层中context embeddings的平均值，或者在[CLS]标记的位置提取。一般前一种方法效果更好。但是在语义表示性能上还不及averaged GloVe embeddings方法。 为什么效果不及averaged GloVe embeddings？ 文章 How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings 比较了预训练LM的word embedding，有如下结论： Contextualized representations(模型每一层输出)在每一个非输入层都是各向异性的，即非标准正交空间； 高层的各向异性更显著； 高层的Contextualized representations更与context相关，不同context下的数值表示相差更大。同时不同词与context相关的相关程度不同，比如，stop words与context的相关性会很大； 不同模型的每一层输出间的相关性也不同：EMLo的低层和高层的输出更相似一些；BERT的低层和高层的输出变化较大，同时句子内词的表示会更接近，和其他句子中词的表示相对更差异化一些；GPT-2的输出不同，某一个输出与当前句子内的词的相似性和当前句子之外的词的相似性，是接近的。 BERT-flow文中给出的理由： 词频差异使得低频词和高频词的学习程度不同，公式7可以作为一个依据； 低频词的表示更偏稀疏，而高频词的表示更稠密。 3. 可逆变换到standard Gaussian latent space 借鉴Glow模型的实现方式，实现flow-based generative model。将BERT参数固定，只学习可逆变换的网络参数。只使用了add coupling layer，将1x1卷积变成直接permutation。 求z为： 以上将D维向量，分为两部分计算更新。同时z的先验为标准正态分布。那么 \\(f^{-1}\\)和\\(J_{f^{-1}}\\)都是可以计算的，目标函数就可以表示。 4. Lexical Similarity在不同模型中的规律实验 论文通过一组简单的对比实验，得出以下结论： BERT-Induced Similarity的与Lexical Similarity存在着过度的相关性。 Flow-Induced Similarity 与Lexical Similarity的相关性较低。 方法是通过Similarity与Edit distance的对应关系，实验结果如图： Confusion: 正交标准化？标准化协方差矩阵？ 求一个线性变换\\(W\\)使得BERT输出的向量矩阵成一个标准化的矩阵。直接达到正交化的目的，是否也是可行？ 首先求原始协方差矩阵： \\[ \\mu=\\frac{1}{N}\\sum_{k=1}^Nxi \\] 那么 \\[ \\Sigma=\\frac{1}{N}\\sum_{k=1}^N(x_i-\\mu)^T(x_i-\\mu) \\] 变换\\(W\\)满足： \\[ W^T\\Sigma W=I \\] 其中\\(\\Sigma\\)为一个正交对称矩阵。对其SVD，有如下关系： \\[ \\Sigma=U\\Lambda U^T=(W^{-1})^TW^{-1} \\] 得到变换\\(W\\)为： \\[ W=U\\sqrt{\\Lambda^{-1}} \\] 作者实验的结果显示，该方法简洁，且效果与flow模型相差无几。 作者称该方法为BERT-whitening。 核心代码： 1234567def compute_w_bias(vecs, n_components): vecs = np.concatenate(vecs, axis=0) mu = vecs.mean(axis=0, keepdims=True) cov = np.cov(vecs.T) u, s, vh = np.linalg.svd(cov) W = np.np.dot(u, np.diag(1/np.sqrt(s))) return W[:, :n_components], -mu 标准化 1234def transform_and_normalize(vecs, kernel=None, bias=None): if not (kernel is None or bias is None): vecs = (vecs + bias).dot(kernel) return vecs / (vecs**2).sum(axis=1, keepdims=True)**0.5 实现细节 大语料计算内存问题？ 句子向量均值递归计算 \\[ \\mu_{n+1}=\\frac{n}{n+1}\\mu_n+\\frac{1}{n+1}x_{n+1} \\] 协方差递归计算 \\[ \\Sigma_{n+1}=\\frac{n}{n+1}\\Sigma_n+\\frac{1}{n+1}(x_{n+1}-\\mu)^T(x_{n+1}-\\mu) \\] BERT模型在任务数据上，先微调 论文中先在任务数据上微调，比如先进行情感分类任务，再用来计算句子向量。 flow做不到的简单的句子向量降维 SVD中直接取\\(W\\)前n个维度，即PCA，得到降维的结果。该结果将更具任务语境特征的维度提取出来。不仅提升了句子向量间相似度的速度，还可能提升预测的效果。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"sentence embedding","slug":"sentence-embedding","permalink":"https://racleray.github.io/tags/sentence-embedding/"},{"name":"BERT","slug":"BERT","permalink":"https://racleray.github.io/tags/BERT/"},{"name":"nlp","slug":"nlp","permalink":"https://racleray.github.io/tags/nlp/"}]},{"title":"语言模型Sampling方法","slug":"语言模型Sampling方法","date":"2021-01-20T06:59:46.000Z","updated":"2023-08-07T11:54:31.051Z","comments":true,"path":"posts/e798f591.html","link":"","permalink":"https://racleray.github.io/posts/e798f591.html","excerpt":"","text":"在text generation模型预测时，如果我们总是抽取最有可能的单词，标准语言模型训练目标会容易陷入“I don’t know. I don’t know. I don’t know.” 这种循环中。所以有了sample based generation方法。但是，它有一个潜在问题： 假如依照logit softmax生成的分布进行sample，假设有60%的词的概率极低以至于基本不会被选择，但是这60%的词的总的CDF占了30%，这意味着模型预测方向可能有30%的概率偏离了“正确”的方向。 而如果是在预测前期发生偏离，那么由于错误向后预测的累积，直接导致了预测的效果变差。 已有论文研究发现，经常被使用的Beam search方法，其生成效果和人类的表达有着一定的gap。 [^]: Humans often choose words that surprise language models (Holtzman et al 2019) https://arxiv.org/abs/1904.09751 解决方法：temperature sampling和top k sampling. Temperature sampling 借鉴热力学中现象，温度越高，则低energy的状态出现的概率会增加。 以logits作为“energy”，在进行softmax之前，除以temperature。 1234567891011&gt;&gt;&gt; import torch&gt;&gt;&gt; import torch.nn.functional as F&gt;&gt;&gt; a = torch.tensor([1,2,3,4.])&gt;&gt;&gt; F.softmax(a, dim=0)tensor([0.0321, 0.0871, 0.2369, 0.6439])&gt;&gt;&gt; F.softmax(a/.5, dim=0)tensor([0.0021, 0.0158, 0.1171, 0.8650])&gt;&gt;&gt; F.softmax(a/1.5, dim=0)tensor([0.0708, 0.1378, 0.2685, 0.5229])&gt;&gt;&gt; F.softmax(a/1e-6, dim=0)tensor([0., 0., 0., 1.]) NOTE：temperature越大，分布越趋向均匀 Top k sampling Top k sampling是指根据概率进行排序将第k个token以下的概率都归零。 但是存在一个问题：某些时候，分布较均匀，可以选择的token大于k；某些时候，分布较集中，可以选择的token小于k。 直接导致了预测错误的概率增大。 Top p sampling（nucleus sampling） 1，按概率sort预测分布； 2，计算CDF； 3，将CDF大于某个设定p值之后的logit值，设为一个很大的负值； 4，softmax，之后进行采样。 这样就能动态的改变可选择的token数量，且错误概率相对降低。 1234567891011121314151617181920212223242526272829303132import torchimport torch.nn.functional as Fimport numpy as npdef top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')): \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering Args: logits: logits distribution shape (..., vocabulary size) top_k &gt;0: keep only top k tokens with highest probability (top-k filtering). top_p &gt;0.0: keep the top tokens with cumulative probability &gt;= top_p (nucleus filtering). \"\"\" top_k = min(top_k, logits.size(-1)) # Safety check if top_k &gt; 0: # Remove all tokens with a probability less than the last token of the top-k indices_to_remove = logits &lt; torch.topk(logits, top_k)[0][..., -1, None] logits[indices_to_remove] = filter_value if top_p &gt; 0.0: sorted_logits, sorted_indices = torch.sort(logits, descending=True) cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1) # Remove tokens with cumulative probability above the threshold sorted_indices_to_remove = cumulative_probs &gt;= top_p # Shift the indices to the right to keep also the first token above the threshold sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone() sorted_indices_to_remove[..., 0] = 0 indices_to_remove = torch.zeros_like(logits, dtype=torch.uint8).scatter_( dim=-1, index=sorted_indices, src=sorted_indices_to_remove ) logits[indices_to_remove] = filter_value return logits End 虽然有这些方法来改进模型生成的效果，但是这些仅仅是模型的“补丁”。如何提高模型本身的性能，如何让模型能够直接生成多样性的、更“人类”的语句？emmm...","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"language model","slug":"language-model","permalink":"https://racleray.github.io/tags/language-model/"},{"name":"sampling","slug":"sampling","permalink":"https://racleray.github.io/tags/sampling/"}]},{"title":"贝叶斯超参数搜索","slug":"贝叶斯超参数搜索","date":"2020-11-30T13:48:56.000Z","updated":"2023-08-07T11:54:31.051Z","comments":true,"path":"posts/d941eeb.html","link":"","permalink":"https://racleray.github.io/posts/d941eeb.html","excerpt":"贝叶斯方法跟踪过去的评估结果，建立形成一个概率模型，将超参数映射到目标函数上得分的概率。这个模型被称为目标函数的代理。评估目标函数后不断地更新概率模型。","text":"paper by Bergstra et al 一句话概括Bayesian hyperparameter optimization： build a probability model of the objective function and use it to select the most promising hyperparameters to evaluate in the true objective function. 常用算法：Sequential Model-Based Optimization (SMBO) with the Tree Parzen Estimator (TPE) 原理简述 贝叶斯方法相较于随机搜索和网格搜索，是更高效的。随机搜索和网格搜索根本不关注过去的结果，而是继续在整个范围内搜索，即使最优答案(可能)很明显在一个小区域内。 与随机或网格搜索相反，贝叶斯方法跟踪过去的评估结果，建立形成一个概率模型，将超参数映射到目标函数上得分的概率。这个模型被称为目标函数的代理。代理模型（也叫做响应面）又相对更容易优化。 运行过程为： 建立目标函数的替代概率模型，代理模型 查找在代理上性能最佳的超参数 将这些超参数应用于真正的目标函数 更新包含新结果的代理模型 重复步骤2-4，直到达到最大迭代次数或时间为止 贝叶斯推理的目的是通过在每次评估目标函数后不断地更新概率模型，从而获得更多的数据，减少错误。 贝叶斯优化方法是有效的，因为它们有根据的选择了下一个超参数。 基本思想是：花更多的时间选择下一个超参数，以减少对目标函数的调用。 实际上，与在目标函数中花费的时间相比，选择下一个超参数所花费的时间是很少的。 通过评估从过去的结果看似更有希望的超参数，贝叶斯方法可以在更少的迭代中找到比随机搜索更好的模型设置。 一个简单的解释如下图： 代理模型是粗黑线和器上线界细黑线组成的区域。红色虚线表示真实的目标函数。 经过贝叶斯优化几轮迭代之后，得到： 代理模型逐渐趋近目标函数。 Sequential Model-Based Optimization Sequential是指一个接一个地进行试验，每次都通过应用贝叶斯推理更新概率模型(代理)来获取更好的超参数。组成部分有： 要搜索的超参数域 以超参数为输入并输出得分的目标函数 目标函数的代理模型 一个criteria（Selection Function），称为选择函数，用于评估从替代模型中下一步要选择的超参数 该算法由（得分，超参数）对组成的历史记录，该历史对由算法用于更新代理模型 代理模型的选择有：Gaussian Processes, Random Forest Regressions, , Tree Parzen Estimators (TPE). criteria常用Expected Improvement 代理模型，也称为响应面，是利用以前的评估结果建立的目标函数的概率表示。 Expected Improvement选择函数 y* -- 目标函数的阈值 x -- 超参数组合的集合 y -- 输入x超参数得到的目标函数真实返回值 p(y | x) -- 代理模型输出的概率 其目的是最大化关于x的Expected Improvement。 如果p (y | x)在y &lt; y*处，都为零，则超参数x不会产生任何改进。 如果积分为正，则意味着超参数x预期会产生比阈值更好的结果。 Tree-structured Parzen Estimator (TPE) 使用贝叶斯公式，计算p(y | x) p (x | y)，是给定目标函数得分的超参数的概率。 对超参数做了两种不同的分布:一种是目标函数的值小于阈值，l(x)，另一种是目标函数的值大于阈值，g(x)。 结合SMBO以及一点直观的印象，我们希望从l(x)而不是从g(x)中得出x的值，因为这种分布只基于产生低于阈值得分的x的值。 最终得到的Expected Improvement： 可以发现Expected Improvement和l(x) / g(x)的比值成反比。提高EI，正是需要从l(x)中多得到p (x | y)的值。 该算法利用历史得分建立l(x)和g(x)，提出目标函数的概率模型，代理模型随着每次迭代而改进。 工具 Spearmint， MOE ： Gaussian Process (surrogate) Hyperopt ：Tree-structured Parzen Estimator Notebook示例，示例，示例 SMAC ：Random Forest regression. 都使用Expected Improvement选择函数。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"ML","slug":"Notes/ML","permalink":"https://racleray.github.io/categories/Notes/ML/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://racleray.github.io/tags/machine-learning/"}]},{"title":"CRF--SimpleNote","slug":"CRF-SimpleNote","date":"2020-11-18T08:29:17.000Z","updated":"2023-09-23T13:45:02.173Z","comments":true,"path":"posts/798499bc.html","link":"","permalink":"https://racleray.github.io/posts/798499bc.html","excerpt":"NER标注输出不仅仅是简单的分类，而是具有一定关联规律的标注输出。CRF正是输出这种结构化结果的一种算法。结合HMM（状态转移和状态释放）和最大熵模型（log linear model建模特征函数，寻找最优的条件概率），在MEMM的基础上，建立隐变量X的概率无向图解决了MEMM的局部归一化问题。","text":"Motivation NER标注输出不仅仅是简单的分类，而是具有一定关联规律的标注输出。CRF正是输出这种结构化结果的一种算法。 结合HMM（状态转移和状态释放）和最大熵模型（log linear model建模特征函数，寻找最优的条件概率），在MEMM的基础上，建立隐变量X的概率无向图解决了MEMM的局部归一化问题。 CRF loss in Deep Learning loss的优化目标是使得模型输出的最优路径和groud truth路径尽量接近。即，最大化最优路径概率。 最优路径概率定义为， 在CRF loss中，Pi通过两组变量求得，Emission Score和Transition Score（传统CRF中的特征函数）。 Emission Score：对应神经网络输出的hidden state。可视为每一步输出标签的分类概率分布预测，每一步输出维度为[#tags, 1]。 Transition Score：对应CRF层中定义的状态转移权重矩阵。保存在CRF layer中，[#tags, #tags] 两者在深度模型中都是作为权重变量来优化学习的。 深度模型中的优化目标实际实现为： 分子计算可以简单计算发现，就是当前路径的Emission Score和Transition Score沿路径之和。计算难点在于分母，可以通过递归实现其多分支结构计算。 递归过程： 维护两个记录列表：obs和 previous。previous存储了之前步骤的结果，obs代表当前状态可能的输出选择。 以step 0到step 1为例： 扩展到#tags维度，沿着1轴，和transition matrix同维度 对score的每一列取指数，求和，在取对数，得到新的previous 递归计算，直到达到最后一个时间步。 求得分母 最佳路径导出Viterbi 动态规划算法，过程类似loss中求分母的部分，但是状态转移方程（动态规划）不同。 同样以step 0到step 1为例： 扩展到#tags维度，沿着1轴，和transition matrix同维度 previous保存的累计最优得分状态变化为： 然后保存这一轮状态转移中，不同step 0状态下对应的最优得分，以及最优得分对应的step 1最优状态。即，保存两个列表，在下一次转移时，将最优得分和对应的最优状态append到结果中。 根据最后一步的最优得分，回溯重构出最优路径。 对比MEMM 虽然CRF在MEMM的基础上，建立隐变量X的概率无向图解决了MEMM的局部归一化问题。但是，并不是说MEMM的结果就一定崩塌。 MEMM的一个明显的特点是实现简单、速度快，因为它只需要每一步单独执行softmax，所以MEMM是完全可以并行的，速度跟直接逐步Softmax基本一样。 它的每一步计算，不需要整个序列的信息计算分母，而是依赖于上一步的状态。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"nlp","slug":"nlp","permalink":"https://racleray.github.io/tags/nlp/"},{"name":"CRF","slug":"CRF","permalink":"https://racleray.github.io/tags/CRF/"}]},{"title":"hadoop+spark环境配置","slug":"hadoop-spark环境配置","date":"2020-10-30T08:56:36.000Z","updated":"2023-08-07T11:54:31.036Z","comments":true,"path":"posts/858c7a63.html","link":"","permalink":"https://racleray.github.io/posts/858c7a63.html","excerpt":"记录 hadoop + spark 的Linux环境配置。","text":"单节点Hadoop 1.安装JDK 12345sudo apt-get updatesudo apt-get install default-jdkjava -version 查看安装路径 1update-alternatives --display java 2.设定 SSH无密码登入 123456sudo apt-get install sshsudo apt-get install rsyncssh-keygen -t dsa -P '' -f ~/.ssh/id_dsall ~/.ssh 为了无密码登录本机，加入公匙到许可证文件 1cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys 1systemctl restart sshd.service 3.下载安装Hadoop 1234567wget https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.9.2/hadoop-2.9.2.tar.gzsudo tar -zxvf hadoop-2.9.2.tar.gzsudo mv hadoop-2.9.2 /usr/local/hadoopll /usr/local/hadoop 4.设定Hadoop环境变数 修改~/.bashrc 1sudo gedit ~/.bashrc 输入下列内容 1234567891011121314export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64export HADOOP_HOME=/usr/local/hadoopexport PATH=$PATH:$HADOOP_HOME/binexport PATH=$PATH:$HADOOP_HOME/sbinexport HADOOP_MAPRED_HOME=$HADOOP_HOMEexport HADOOP_COMMON_HOME=$HADOOP_HOMEexport HADOOP_HDFS_HOME=$HADOOP_HOMEexport YARN_HOME=$HADOOP_HOMEexport HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/nativeexport HADOOP_OPTS=\"-Djava.library.path=$HADOOP_HOME/lib\"export JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native:$JAVA_LIBRARY_PATH 让~/.bashrc修改生效 1source ~/.bashrc 5.修改Hadoop组态设定档 Step1 修改hadoop-env.sh配置文件 1sudo gedit /usr/local/hadoop/etc/hadoop/hadoop-env.sh 输入下列内容: 1export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 Step2 修改core-site.xml，设置HDFS名称 1sudo gedit /usr/local/hadoop/etc/hadoop/core-site.xml 在之间，输入下列内容: 123456&lt;configuration&gt;&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; Step3 修改yarn-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/yarn-site.xml 在之间，输入下列内容: 12345678&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;&lt;/property&gt; Step4 修改mapred-site.xml，监控Map和reduce程序的JobTracker 1sudo cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/mapred-site.xml 在之间，输入下列内容: 1234&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; Step5 修改hdfs-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/hdfs-site.xml 在之间，输入下列内容: dfs.replication设置blocks在其他节点的备份数量 12345678910111213141516&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoop_data/hdfs/namenode&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoop_data/hdfs/datanode&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.http.address&lt;/name&gt; &lt;value&gt;0.0.0.0:50070&lt;/value&gt;&lt;/property&gt; 6.建立与格式化HDFS 目录 1sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode 1sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode 1sudo chown ray:ray -R /usr/local/hadoop chown要根据当前用户名进行修改 格式化 1hadoop namenode -format 7.启动Hadoop 启动start-dfs.sh，再启动 start-yarn.sh 123start-dfs.shstart-yarn.sh 或 启动全部 1start-all.sh 查看目前所执行的行程 1jps stop-dfs.sh stop-yarn.sh stop-all.sh 8.开启Hadoop Resource­ManagerWeb接口 Hadoop Resource­Manager Web接口网址 http://localhost:8088/ image 9.NameNode HDFS Web接口 开启HDFS Web UI网址 http://localhost:50070/ image 多节点Hadoop 由多台电脑组成:有一台主要的电脑master，在HDFS担任NameNode角色，在MapReduce2(YARN)担任ResourceManager角色 有多台的电脑data1、data2、data3，在HDFS担任DataNode角色，在MapReduce2(YARN)担任NodeManager角色 Hadoop Multi NodeCluster规划，整理如下表格: 伺服器名称 IP HDFS YARN master 192.168.0.100 NameNode ResourceManager data1 192.168.0.101 DataNode NodeManager data2 192.168.0.102 DataNode NodeManager data3 192.168.0.103 DataNode NodeManager 1复制Single Node Cluster到data1 将之前所建立的Single Node Cluster VirtualBox hadoop虚拟机器复制到data1 2设定data1伺服器 编辑网路设定档设定固定IP 1sudo gedit /etc/network/interfaces 输入下列内容 : 123456789101112# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackauto eth0iface eth0 inet staticaddress 192.168.0.101netmask 255.255.255.0network 192.168.0.0gateway 192.168.0.1dns-nameservers 192.168.0.1 1sudo vim /etc/NetworkManager/NetworkManager.conf 将managed=false修改成managed=true 1sudo service network-manager restart 重启 设定hostname 1sudo gedit /etc/hostname 输入下列内容: 1data1 设定hosts档案 1sudo gedit /etc/hosts 12345678910111213127.0.0.1 localhost127.0.1.1 hadoop192.168.0.100 master192.168.0.101 data1192.168.0.102 data2192.168.0.103 data3# The following lines are desirable for IPv6 capable hosts::1 ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters 修改core-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/core-site.xml 在之间，输入下列内容: 1234&lt;property&gt; &lt;name&gt;fs.default.name&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt; 修改yarn-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/yarn-site.xml 在之间，输入下列内容: 123456789101112&lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8025&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8050&lt;/value&gt; &lt;/property&gt; 修改mapred-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop /mapred-site.xml 在之间，输入下列内容: 1234&lt;property&gt; &lt;name&gt;mapred.job.tracker&lt;/name&gt; &lt;value&gt;master:54311&lt;/value&gt; &lt;/property&gt; 修改hdfs-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/hdfs-site.xml 在之间，输入下列内容: 12345678&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoop_data/hdfs/datanode&lt;/value&gt; &lt;/property&gt; 3复制data1伺服器至data2、data3、master 4设定data2、data3伺服器 设定data2固定IP 1sudo gedit /etc/network/interfaces 输入下列内容 1234567891011# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackauto eth0iface eth0 inet staticaddress 192.168.0.102netmask 255.255.255.0network 192.168.0.0gateway 192.168.0.1dns-nameservers 192.168.0.1 设定hostname 1sudo gedit /etc/hostname 输入下列内容: 1data2 设定data3固定IP 1sudo gedit /etc/network/interfaces 输入下列内容 1234567891011# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackauto eth0iface eth0 inet staticaddress 192.168.0.103netmask 255.255.255.0network 192.168.0.0gateway 192.168.0.1dns-nameservers 192.168.0.1 设定hostname 1sudo gedit /etc/hostname 输入下列内容: 1data3 5设定master伺服器 设定master固定IP 1sudo gedit /etc/network/interfaces 输入下列内容 1234567891011# interfaces(5) file used by ifup(8) and ifdown(8)auto loiface lo inet loopbackauto eth0iface eth0 inet staticaddress 192.168.0.100netmask 255.255.255.0network 192.168.0.0gateway 192.168.0.1dns-nameservers 192.168.0.1 设定hostname 1sudo gedit /etc/hostname 输入下列内容: 1master 修改hdfs-site.xml 1sudo gedit /usr/local/hadoop/etc/hadoop/hdfs-site.xml 在之间，输入下列内容: 12345678&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.namenode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/hadoop_data/hdfs/namenode&lt;/value&gt; &lt;/property&gt; 设定master档案 1sudo gedit /usr/local/hadoop/etc/hadoop/master 输入下列内容: 1master 设定slaves档案 1sudo gedit /usr/local/hadoop/etc/hadoop/slaves 输入下列内容: 123data1data2data3 6 master连线至data1、data2、data3建立HDFS目录 master SSH连线至data1并建立HDFS目录 12345678910ssh data1# 删除hdfs目录sudo rm -rf /usr/local/hadoop/hadoop_data/hdfs# 创建datanode目录sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode# 更改所有者为当前用户sudo chown ray:ray -R /usr/local/hadoop 回到master端 1exit master SSH连线至data2并建立HDFS目录 12345678910ssh data2# 删除hdfs目录sudo rm -rf /usr/local/hadoop/hadoop_data/hdfs# 创建datanode目录sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode# 更改所有者为当前用户sudo chown ray:ray -R /usr/local/hadoop 回到master端 1exit master SSH连线至data3并建立HDFS目录 12345678910ssh data3# 删除hdfs目录sudo rm -rf /usr/local/hadoop/hadoop_data/hdfs# 创建datanode目录sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/datanode# 更改所有者为当前用户sudo chown ray:ray -R /usr/local/hadoop 回到master端 1exit 7建立与格式化NameNode HDFS目录 12345678# 删除hdfs目录sudo rm -rf /usr/local/hadoop/hadoop_data/hdfs# 创建datanode目录sudo mkdir -p /usr/local/hadoop/hadoop_data/hdfs/namenode# 更改所有者为当前用户sudo chown ray:ray -R /usr/local/hadoop 格式化NameNode HDFS目录 1hadoop namenode -format 8启动Hadoop Multi Node cluster 启动start-dfs.sh，再启动 start-yarn.sh 123start-dfs.shstart-yarn.sh 或 启动全部 1start-all.sh 查看目前所执行的行程 1jps 停止 stop-dfs.sh stop-yarn.sh stop-all.sh 9开启Hadoop Resource-Manager Web介面 http://master:8088/ image 10开启NameNodeWeb介面 HDFS Web UI网址 http://master:50070/ image image 常用命令 123456789hadoop fs -mkdirhadoop fs -lshadoop fs -copyFromLocal # 复制到hdfs，提醒有重名hadoop fs -put # 复制到hdfs，但是直接覆盖重名hadoop fs -cathadoop fs -copyToLocal # 复制到本地，提醒有重名hadoop fs -get # 复制到本地，但是直接覆盖重名hadoop fs -cphadoop fs -rm pyspark scale 12345tar xvf scala-2.11.12.tgzsudo mv scala-2.11.12 /usr/local/scalasudo gedit .bashrc 123# 写入export SCALA_HOME=/usr/local/scalaexport PATH=$PATH:$SCALA_HOME/bin spark 123tar xvf spark-2.4.7-bin-without-hadoop.tgz sudo mv spark-2.4.7-bin-without-hadoop /usr/local/sparksudo gedit .bashrc 写入 12export SPARK_HOME=/usr/local/sparkexport PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin 1source .bashrc 1234cd /usr/local/spark/cp ./conf/spark-env.sh.template ./conf/spark-env.shsudo gedit ./conf/spark-env.sh 写入 1234567export SPARK_DIST_CLASSPATH=$(/usr/local/hadoop/bin/hadoop classpath)export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoopexport PYSPARK_PYTHON=/usr/bin/python3export PYSPARK_DRIVER_PYTHON=/usr/bin/ipython3# 在notebook中运行pyspark# export PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\" 使用HDFS中文件时，先要启动Hadoop 测试 master机器上 123cd ~mkdir wordcount/input -pcp /usr/local/hadoop/LICENSE.txt ~/wordcount/input 1start-all.sh 1234hadoop fs -mkdir -p /user/ray/wordcount/inputcd ~/wordcount/inputhadoop fs -copyFromLocal LICENSE.txt /user/ray/wordcount/inputhadoop fs -ls /user/ray/wordcount/input","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"spark","slug":"Tools/spark","permalink":"https://racleray.github.io/categories/Tools/spark/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://racleray.github.io/tags/spark/"},{"name":"tools","slug":"tools","permalink":"https://racleray.github.io/tags/tools/"}]},{"title":"神经网络normalization","slug":"神经网络normalization","date":"2020-10-19T11:32:28.000Z","updated":"2023-08-07T11:54:31.047Z","comments":true,"path":"posts/5de6e8e6.html","link":"","permalink":"https://racleray.github.io/posts/5de6e8e6.html","excerpt":"记录Normalization相关方法，以及一点点思考。","text":"深度学习中的Normalization，BN/LN/WN 为什么需要 Normalization independent and identically distributed，简称为 i.i.d 并非所有机器学习模型的必然要求（比如 Naive Bayes 模型就建立在特征彼此独立的基础之上，而Logistic Regression 和 神经网络 则在非独立的特征数据上依然可以训练出很好的模型），但独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。 白化（whitening） 数据预处理步骤。 （1）去除特征之间的相关性 —&gt; 独立； （2）使得所有特征具有相同的均值和方差 —&gt; 同分布。 深度学习中的 Internal Covariate Shift 参数更新使每一层的数据分布发生变化，向前叠加，高层的受到数据变化的影响，需要不断重新适应底层的数据变化。 Internal Covariate Shift，简称 ICS. ML经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的” covariate shift是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同 ​ 1. 给定输入，拟合label，条件概率一致的 层间计算导致，各层分布发生改变，边缘概率是不同的 ICS的问题 上层参数需要不断适应新的输入数据分布，降低学习速度 下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止 (想想sigmoid) 每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎 Normalization 的通用框架与基本思想 标准的白化操作代价高昂，特别是我们还希望白化操作是可微的（每一点上必存在非垂直切线），保证白化操作可以通过反向传播来更新梯度。 Normalization 方法退而求其次，进行了简化的白化操作。 Normalization 先对其做平移和伸缩变换， 将 的分布规范化成在固定区间范围的标准分布。 是平移参数（shift parameter）， 是缩放参数（scale parameter） 是再平移参数（re-shift parameter）， 是再缩放参数（re-scale parameter） 最终得到的数据符合均值为 、方差为 的分布 变换为均值为 、方差为 的分布，也并不是严格的同分布，只是映射到了一个确定的区间范围而已 再平移调整的意义 不会过分改变每一层计算结果 第一步的规范化会将几乎所有数据映射到激活函数的非饱和区（线性区），仅利用到了线性变化能力，从而降低了神经网络的表达能力。而进行再变换，则可以将数据从线性区变换到非线性区，恢复模型的表达能力（想想激活函数） 主流 Normalization 方法梳理 Batch Normalization —— 纵向规范化：整个batch的不同维度（channel） image 其中 是 mini-batch 的大小。由于 BN 是针对单个维度定义的，因此标准公式中的计算均为 element-wise。 然后，用一个 mini-batch 的一阶统计量和二阶统计量，规范每一个输入维度 KEYPOINT：mini-batch数据决定，x每个维度的分布，上图可理解为RGB三个通道。 要求：每个 mini-batch 比较大，数据分布比较接近，充分的 shuffle 不适用：动态的网络结构 和 RNN 网络 （最后才知道mini-batch的\\(\\mu\\)）。Batch Normalization基于一个mini batch的数据计算均值和方差，而不是基于整个Training set来做，相当于进行梯度计算式引入噪声。因此，Batch Normalization不适用于对噪声敏感的强化学习、生成模型（Generative model：GAN，VAE）使用。 12345678910111213141516171819def batch_normalization_layer(inputs, out_size, isTrain=True): # in_size, out_size = inputs.get_shape() pop_mean = tf.Variable(tf.zeros([out_size]),trainable=False) pop_var = tf.Variable(tf.ones([out_size]),trainable=False) scale = tf.Variable(tf.ones([out_size])) shift = tf.Variable(tf.zeros([out_size])) eps = 0.001 decay = 0.999 if isTrain: # batch的mean和var。 注原始维度为[batch_size, height, width, channel] batch_mean, batch_var = tf.nn.moments(inputs,[0,1,2]) print(batch_mean.get_shape()) # 记录训练的mean和var train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1-decay)) train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1-decay)) with tf.control_dependencies([train_mean,train_var]): return tf.nn.batch_normalization(inputs,batch_mean,batch_var,shift,scale,eps) else: return tf.nn.batch_normalization(inputs,pop_mean,pop_var,shift,scale,eps) Layer Normalization —— 横向规范化：单个输入 https://arxiv.org/abs/1607.06450 image 考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入 枚举了该层所有的输入神经元。对应到标准公式中，四大参数 , , , 均为标量（BN中是向量），所有输入共享一个规范化变换 KEYPOINT：LN 针对单个训练样本进行，用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间 NOTE：如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。 1234567891011121314151617class layer_norm(Function): @staticmethod def forward(input, gain=None, bias=None): # 这里的输入是unroll的，[batch_size, h x w x c]，按实例norm mean = input.mean(-1, keepdim=True) var = input.var(-1, unbiased=False, keepdim=True) input_normalized = (input - mean) / torch.sqrt(var + 1e-9) if gain is not None and bias is not None: output = input_normalized * gain + bias elif not (gain is None and bias is None): raise RuntimeError(\"gain and bias of LayerNorm should be both None or not None!\") else: output = input_normalized return output ... Weight Normalization —— 参数规范化 https://arxiv.org/abs/1602.07868 将以下方程 理解为： . BN 和 LN 均将规范化应用于输入的特征数据 WN将规范化应用于线性变换的权重 用神经元的权重的欧氏范数对输入数据进行 scale。 是神经元的权重的欧氏范数，因此 是单位向量，决定了 的方向； 是标量，决定了 的长度。 KEYPOINT：WN 的规范化不直接使用输入数据的统计量，因此避免了 BN 过于依赖 mini-batch 的不足，以及 LN 每层唯一转换器的限制，同时也可以用于动态网络结构 Weight Normalization对通过标量g和向量v对权重W进行重写，重写向量v是固定的，因此，基于Weight Normalization的Normalization比Batch Normalization引入更少的噪声。 123456789101112131415161718192021222324def _weight_norm(v, g): 'v就是weights' norm = torch.norm(v, 2) return v * (g * norm)def norm_except_dim(v, pow, dim): '计算g： norm_except_dim(weight, 2, dim).data' if dim is None: return v.norm() if dim != 0: v = v.transpose(0, dim) output_size = (v.size(0),) + (1,) * (v.dim() - 1) v = v.contiguous().view(v.size(0), -1).norm(dim=1).view(*output_size) if dim != 0: v = v.transpose(0, dim) return vclass WeightNorm: ... def compute_weight(self, module): g = getattr(module, self.name + '_g') v = getattr(module, self.name + '_v') return _weight_norm(v, g) ... Cosine Normalization —— 余弦规范化 其中 是 和 的夹角。所有的数据就都是 [-1, 1] 区间。 超简单的变化，直接在wx的上scale，并且不需要再次缩放。 将 点积》》》变为余弦相似度 Instance Norm image InstanceNorm等价于当Group Norm的num_groups等于num_channel. Group Norm https://arxiv.org/abs/1803.08494 image 当Group Norm中group的数量是1的时候, 是与LayerNorm是等价的 12345678910111213def GroupNorm(x, gamma, beta, G, eps=1e−5): # x: input features with shape [N,C,H,W] # gamma, beta: scale and offset, with shape [1,C,1,1] # G: number of groups for GN N, C, H, W = x.shape # group划分 x = tf.reshape(x, [N, G, C // G, H, W]) # 按group求mean var mean, var = tf.nn.moments(x, [2, 3, 4], keepdims=True) x = (x−mean) / tf.sqrt(var + eps) x = tf.reshape(x, [N, C, H, W]) return x∗gamma + beta Normalization 为什么会有效？ 权重伸缩不变性（weight scale invariance） 其中 。 由于 因此，权重的伸缩变化不会影响反向梯度的 Jacobian 矩阵，因此也就对反向传播没有影响，避免了反向传播时因为权重过大或过小导致的梯度消失或梯度爆炸问题，从而加速了神经网络的训练 参数正则 由于 因此，下层的权重值越大，\\(\\lambda\\)越大，那么其梯度就越小。这样，参数的变化就越稳定，相当于实现了参数正则化的效果，避免参数的大幅震荡，提高网络的泛化性能。 数据伸缩不变性（data scale invariance） 当数据 按照常量 进行伸缩时，得到的规范化后的值保持不变，即： 其中 。 数据伸缩不变性仅对 BN、LN 和 CN 成立。WN 不具有这一性质。很明显。 数据伸缩不变性可以有效地减少梯度弥散，简化对学习率的选择 某一层神经元 而言，展开可得（以下式子为示意，没写入激活函数） 每一层神经元的输出依赖于底下各层的计算结果。再次回忆activition function的图像 如果没有正则化，当下层输入发生伸缩变化时，经过层层传递，可能会导致数据发生剧烈的膨胀或者弥散，从而也导致了反向计算时的梯度爆炸或梯度弥散。 而言，其输入 永远保持标准的分布，这就使得高层的训练更加简单。从梯度的计算公式来看： 数据的伸缩变化也不会影响到对该层的权重参数更新，使得训练过程更加鲁棒，简化了对学习率的选择。 参考链接：https://zhuanlan.zhihu.com/p/33173246","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"},{"name":"normalization","slug":"normalization","permalink":"https://racleray.github.io/tags/normalization/"}]},{"title":"Large scale GAN training for high fidelity natural image synthesis","slug":"Large-scale-GAN-training-for-high-fidelity-natural-image-synthesis","date":"2020-08-04T08:08:56.000Z","updated":"2023-08-07T11:54:31.031Z","comments":true,"path":"posts/204efe7c.html","link":"","permalink":"https://racleray.github.io/posts/204efe7c.html","excerpt":"这是得空听百度paddle平台公开课的一个小作业，选了一篇很简单的论文读读。结论有一定参考价值，虽然很少用到GAN。","text":"问题 GAN生成图像的过程是一个敏感的过程。虽然相比于VAE，其优化目标从Elmo最优损失函数的一个下界，变为直接优化生成结果和目标之间差异损失本身，直觉上是一种更好的方法，但是由于动态地交叉训练生成器与判别器，导致对网络设计、训练方法、参数设置等非常敏感。 尽管有研究表明在经验和理论上，获得了在多种设置中可以实现稳定训练的结论。但是GAN生成网络的效果始终有点差强人意。 当前在Image Net建模上的最佳结果仅达到了52.5的IS，而真实数据有233的IS。 Is( inception score)：用来衡量GAN网络的两个指标: 生成图片的质量和多样性 entropy = -sum(p_i * log(p_i)) The conditional probability captures our interest in image quality. KL (C || M) : KL divergence = p(y|x) * (log(p(y|x)) – log(p(y))) The average of the KL divergence for all generated images. C for conditional and M for marginal distributions. 1234567891011121314151617181920# calculate inception score in numpyfrom numpy import asarrayfrom numpy import expand_dimsfrom numpy import logfrom numpy import meanfrom numpy import exp # calculate the inception score for p(y|x)def calculate_inception_score(p_yx， eps=1E-16): # calculate p(y) p_y = expand_dims(p_yx.mean(axis=0)， 0) # kl divergence for each image kl_d = p_yx * (log(p_yx + eps) - log(p_y + eps)) # sum over classes sum_kl_d = kl_d.sum(axis=1) # average over images avg_kl_d = mean(sum_kl_d) # undo the logs is_score = exp(avg_kl_d) return is_score 但是有一个缺陷，概率计算是建立在Inception数据集限制的1000种类别中的，不在其中的类别则无法评估，同时要达到较好的效果，计算score时，不同类别中的数据分布最好是比较均匀的。 《Large scale GAN training for high fidelity natural image synthesis》的研究正是探索生成效果的一项成果，作者成功地将GAN生成图像和真实图像之间的保真度和多样性gap大幅降低。 方法 高分辨率能够带来更为真实的生成图像，在这样的思想的指导下，本论文结合了GAN的各种新技术，并且分析了训练难的原因，最后提出自己的模型。 本文展示了GAN可以从训练规模中显著获益，并且能在参数数量很大和八倍Batch size于之前最佳结果，的条件下，仍然能以2倍到4倍的速度进行训练。 作者引入了两种简单的生成架构变化，提高了可扩展性，并修改了正则化方案以提升conditioning，通过实验说明了这样可以提升性能。 image 这篇论文没有提出新的模型，只是将原有的GAN的模型： 用8倍原有的batch size大小 将隐层的变量数量扩充到原有模型的4倍 训练获得了很好的图片生成的效果。与此同时，在扩充了变量数量和 batch size大小后，模型出现了不稳定的现象。 文章中对出现的不稳定现象，采用现有的比较有效的稳定训练GAN的方法，但是文中发现这样确实会稳定GAN的训练，但是同时会栖牲生成图片的质量。 实验结果 研究表明按8的倍数增加批大小可以将当前最佳的IS提高46%。 研究者假设这是由于每个批量覆盖了更多的模式，为生成器和鉴别器都提供了更好的梯度信息。 这种扩扆带来的值得注意的副作用是，模型以更少的迭代次数达到了更好的性能，但变得不稳定并且遭遇了完全的训练崩溃。 因此在实验中，研究者在崩溃刚好发生之后立刻停止训练，并从之前保存的检查点进行结果报告。 增加了每个层50%的宽度(通道数量)，进一步的21%的IS提升。 生成器和鉴别器中的参数数量几乎翻倍。研究者假设这是由于模型相对于数据集复杂度的容量的增加。将深度翻倍，在ImageNet Based模型上，反而会降低性能。 其他技巧 截断技巧 生成器的随机噪声输入一般使用正态分布或者均匀分布的随机数。 本文采用了截断技术，对正态分布的随机数进行截断处理，实验发现这种方法的结果最好。 对此的直观解释是，如果网络的随机噪声输入的随机数变动范围越大，生成的样本在标准模板上的变动就越大，因此样本的多样性就越强，但真实性可能会降低。 首先用截断态分布N（0，1）随机数产生噪声向量Z，具体做法是如果随机数超出一定范围，则重新采样，使得其落在这个区间里。 这种做法称为截断技巧：向量Z的模超过某一指定阈值的随机数进行重釆样，这样可以提高单个样本的质量，但代价是降低了样本的多样性。 实验后分析 生成器的不稳定性 本文着重对小规模时稳定，大规模时不稳定的问题进行分析。 实验中发现，权重矩阵的前3个奇异值σ0，σ1，σ2蘊含的信息最丰富。 在训练中，G的大部分层的谱范数都是正常的，但有一些是病态的，这些谱范数随着训练的进行不断的增长，最后爆炸，导致训练坍塌。 结论 本文证明了将GAN用于多类自然图像生成任务时，加大模型的规模可以显著的提高生成的图像的质量，对生成的样本的真实性和多样性都是如此。 通过使用一些技巧，本文提出的方法的性能较之前的方法有了大度的提高。 另外，还分析了大规模GAN在训练时的机制，用它们的权重矩阵的奇异值来刻画它们的稳定性。 讨论了稳定性和性能即生成的图像的质量之间的相互作用和影响 参考链接： 百度论文复现课程：https://aistudio.baidu.com/aistudio/education/group/info/1340","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"gan","slug":"gan","permalink":"https://racleray.github.io/tags/gan/"}]},{"title":"C++ Project使用外部库","slug":"C-Project使用外部库","date":"2020-07-08T04:59:37.000Z","updated":"2023-08-07T11:54:31.026Z","comments":true,"path":"posts/b7fe0c52.html","link":"","permalink":"https://racleray.github.io/posts/b7fe0c52.html","excerpt":"记录尝试GLFW demo程序时踩得一点坑，毕竟刚开始接触C++。","text":"Static link 更快速，编译时会优化。在visual studio中需要添加header file和lib file路径。以GLFW库的使用为例 在project文件夹下新建Dependencies文件夹，将下载的 include 和 lib-vc2017 复制到文件夹下。 添加 include 下的header file 添加lib file 添加目标static lib file到project。文件位于lib-vc2017文件夹下。 Dynamic link lib-vc2017文件夹下，还有glfw3dll.lib，这是动态链接需要导入的lib file。即替换上图中的glfw3.lib为glfw3dll.lib。 但是需要注意，dll文件需要与可执行文件 xxx.exe 位于同一个文件夹下，在程序运行时动态链接。否则会报错。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"CS","slug":"Notes/CS","permalink":"https://racleray.github.io/categories/Notes/CS/"}],"tags":[{"name":"c++","slug":"c","permalink":"https://racleray.github.io/tags/c/"},{"name":"library","slug":"library","permalink":"https://racleray.github.io/tags/library/"}]},{"title":"Capsule Net","slug":"CapsuleNet","date":"2020-07-07T11:55:17.000Z","updated":"2023-08-07T11:54:31.028Z","comments":true,"path":"posts/cd141949.html","link":"","permalink":"https://racleray.github.io/posts/cd141949.html","excerpt":"记录 Capsule Net 相关概率思想，虽然很少用到，但是它的网络设计挺有意思的。","text":"一个 capsule 是一组神经元，capsule activity vector 表示特定类型的实体（例如对象或对象部分）的实例化参数。activity vector的长度来表示实体存在的概率，方向表示实例化参数。前一层capsule通过转换矩阵对下一层capsule的实例化参数进行预测。该网络相比于CNN，对于重叠的目标，识别效果更好且能分离出重叠目标。算法使用 iterative routing-by-agreement mechanism，高层级的capsule的activity vector的计算过程将接收低层capsule的计算结果。 相比于CNN，Caps Net用vector-output capsules代替CNN的scalar-output feature detectors，并使用routing-by-agreement代替max-pooling Motivation ​ 人的视觉系统通常之关注图像中很小部分的重要信息，这部分信息视觉系统通过高分辨率处理，而其他不重要的信息常常就忽略掉。 ​ 处理这部分重要信息的模型，文章叫做single fixation。假设single fixation可以提取的信息不仅仅是某一个事物的特征，而是某一个类型的特征信息，并且在建模过程中忽略每一个fixation之间相互左右的规律。只要求模型解析出每一个single fixation。 ​ 一个capsule代表图像中一个部分的目标信息，并且文章中提到“感知拥挤“的研究，来支撑 一个capsule只代表一个目标信息的合理性。 image ​ capsule可以代表目标的很多信息，文章将capsule输出vector的长度约束在1以内，代表存在概率；vector的方向代表目标的特征。 ​ capsule的学习的信息具有一种全局的相关性。这样可以解决以下的问题。CNN倾向于局部特征的检测，整体上的空间关系对其预测结果的影响较小。实际上不是人脸的照片，在此处都检测为正确。 Vector inputs and outputs of a capsule ​ 根据长度代表出现概率的思路，文章提出了以下“激活函数”： image ​ 输出将确保short vectors长度趋于0，long vectors长度趋于1。角度代表的信息由矩阵变换和低层不同特征的向量加权求和得到。 ​ u来自低层级的capsule的输出。\\(c_{ij}\\)由iterative dynamic routing process计算。\\(b_{ij}\\)首先作为log先验概率初始化，在每一步迭代中更新概率。 ​ \\(c_{ij}\\)相当于CNN中max pooling干的事。感觉有点像attention，iterative attention。 image image ​ 这种“routing-by-agreement”应该比通过max-pooling实现的非常原始的路由形式更加有效。后者可以使一层中的神经元忽略该层中除了最活跃的特征之外的所有特征。前者层层迭代，层层parsing，信息损失自然更少。 ​ 之后，Hinton又提出了EM routing[Matrix capsules with EM routing. ICLR (2018)]。 通过计算Existence probability和概率分布完成不同层间的计算，简单示意图如下。code reference image max-pooling problem ​ max-pooling还存在以下问题： 甚至不能区分左和右 对rotation不敏感，不能学习到方向信息 一次只能‘看到’一个object max-pooling只做到spatial invariance image 而不能做到spatial equivariance image capsule net通过： image 计算下一层capsule经过W变换后的u，进行组合，得到不同的结果。错误的预测route被裁剪(pruned)，模型具有一定的spatial equivariance。 image Margin loss for digit existence ​ 分别对每个类别计算损失，同时注意只有capsule对应的部分出现某类目标，才计算损失。 image ​ \\(T_k\\)只有在class k出现的时候等于1。 image ​ \\(\\lambda\\)是为了防止，限制vector长度后在训练开始阶段由于没有识别到目标导致的学习停滞问题，取0.5。总体存在性检测loss是所有类别capsule的损失之和。 ​ 整体损失，还要加上capsule重构图像特征的损失。 image CapsNet architecture image image ​ 32个capsule，每个8维。 image ​ 转换后，每个class capsule16维，不同而维度影响重构图像的不同特征。 ​ routing-by-agreement的迭代过程就是不同的route squash结果进行裁剪的过程，计算结果相差大的route逐渐被移除。 image ​ EM route则是计算primry结果和route squash结果分布的差异。 image ​ capsule输出vector的长度约束在1以内，代表存在概率；vector的方向代表目标的特征。 ​ 其中每个capsule的转化矩阵都是独立的不同的，每个class对应一个capsule。 ​ A simple CapsNet with 3 layers. PrimaryCaps计算第一层capsules的输入， DigitCaps计算部分使用iterative dynamic routing，计算输出capsules。 ​ 重构图像网络，采用训练好的DigitCaps，在此基础上训练重构网络，计算图像特征重构损失。只需要将true label的capsule提取出来进行重构计算即可。因此，多个数字重合的重构也能实现。 Code ​ 重要的网络构建代码如下，完整代码 ​ Caps Net训练相对于CNN慢很多，并且只使用一层dynamic routing，参数量也更大，batch size相比CNN也要取得小一些，在相同条件下。 ​ 网络收敛比较快，而且计算过程损失的变化相对稳定。下图为第一轮训练结果。 ​ 第二轮计算结果 ​ 相关layer定义，以下代码参考了 https://github.com/XifengGuo/CapsNet-Pytorch ，https://github.com/naturomics/CapsNet-Tensorflow.git。 在其上修改了写网络计算过程。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.autograd import Variabledef squash(inputs, axis=-1): \"\"\"capsule输出的激活函数\"\"\" norm = torch.norm(inputs, dim=axis, keepdim=True) scale = norm ** 2 / (1 + norm ** 2) / (norm + 1e-8) return scale * inputsclass PrimaryCaps(nn.Module): \"\"\"计算第一层capsules的输入，转换成32*6*6个8维的capsule vector in_channels：原文中256 out_channels：卷积后的通道数，原文中256 dim_caps: PrimaryCaps输出的每个capsule的维度 kernel_size：原文中9 * 9 stride：2 \"\"\" def __init__(self, in_channels, out_channels, dim_caps, kernel_size, stride=1, padding=0): super(PrimaryCaps, self).__init__() self.dim_caps = dim_caps self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding) def forward(self, input): \"\"\"转换成32*6*6个8维的capsule vector, output size=[batch_size, num_caps, dim_caps]\"\"\" output = self.conv2d(input) output = output.view(input.size(0), -1, self.dim_caps) return squash(output)class DenseCaps(nn.Module): \"\"\"iterative dynamic routing计算capsule目标识别结果vector。 input size = [None, in_num_caps, in_dim_caps]， output size = [None, out_num_caps, out_dim_caps]。 in_num_caps: 第一层的输入capsule数量，32*6*6 in_dim_caps：第一层的输入capsule维度，8 out_num_caps：iterative dynamic routing时及输出的capsule数量，10 out_dim_caps：iterative dynamic routing时及输出的capsule维度，16 iterations：dynamic routing轮次 weight：由32*6*6个8维的capsule vector计算10个16维的capsule vector的transform matrix，在每个[6 * 6] 单元内的capsule是共享权重的。 \"\"\" def __init__(self, in_num_caps, in_dim_caps, out_num_caps, out_dim_caps, iterations=3): super(DenseCaps, self).__init__() self.in_num_caps = in_num_caps self.in_dim_caps = in_dim_caps self.out_num_caps = out_num_caps self.out_dim_caps = out_dim_caps self.iterations = iterations self.weight = nn.Parameter(0.01 * torch.randn(1, in_num_caps, out_num_caps * out_dim_caps, in_dim_caps, 1)) def forward(self, u): \"\"\"u_hat在不同layer的capsules之间传递，每层capsules只能是才c，b在更新。文中结构只接上了一层 dynamic routing capsules layer。\"\"\" # self.weight * u # [1 , in_num_caps, out_num_caps * out_dim_caps, in_dim_caps, 1] # [batch, in_num_caps, out_num_caps * out_dim_caps, in_dim_caps, 1] # =&gt;&gt; [batch, in_num_caps, out_num_caps * out_dim_caps, in_dim_caps, 1] # 按元素相乘，然后在reduce sum u_hat = u[:, :, None, :, None] u_hat = self.weight * u_hat.repeat(1, 1, self.out_num_caps * self.out_dim_caps, 1, 1) u_hat = torch.sum(u_hat, dim=3) # [batch, in_num_caps, out_num_caps, out_dim_caps] u_hat = torch.squeeze(u_hat.view(-1, self.in_num_caps, self.out_num_caps, self.out_dim_caps, 1)) u_hat_for_route = u_hat.detach() # coupling coefficient initialize # [batch, in_num_caps, out_num_caps] b = Variable(torch.zeros(u.size(0), self.in_num_caps, self.out_num_caps)).cuda() for i in range(self.iterations): c = F.softmax(b, dim=2) # [batch, in_num_caps, out_num_caps] if i &lt; self.iterations - 1: # u [batch, in_num_caps, out_num_caps, out_dim_caps] # c [batch, in_num_caps, out_num_caps, 1] # =&gt;&gt; [batch, 1, out_num_caps, out_dim_caps] outputs = squash(torch.sum(torch.unsqueeze(c, 3) * u_hat_for_route, dim=1, keepdims=True)) b = b + torch.sum(outputs * u_hat_for_route, dim=-1) else: # 此时进入bp计算 outputs = squash(torch.sum(torch.unsqueeze(c, 3) * u_hat, dim=1, keepdims=True)) # [batch, out_num_caps, out_dim_caps] return torch.squeeze(outputs, dim=1)def caps_loss(y_true, y_pred, x, x_reconstruct, lamada): \"\"\"Capsule loss = Margin loss + lamada * reconstruction loss. y shape [batch, classes], x shape [batch, channels, height, width]\"\"\" L = y_true * torch.clamp(0.9 - y_pred, min=0) ** 2 + \\ 0.5 * (1 - y_true) * torch.clamp(y_pred - 0.1, min=0) ** 2 L_margin = L.sum(dim=1).mean() L_recon = nn.MSELoss()(x_reconstruct, x) return L_margin + lamada * L_recon ​ 网络定义 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import torchimport torch.nn.functional as Ffrom torch.autograd import Variablefrom torch import nnfrom layers import DenseCaps, PrimaryCapsclass CapsuleNet(nn.Module): \"\"\" Input: (batch, channels, width, height) Output:((batch, classes), (batch, channels, width, height)) input_size: [channels, width, height] classes: number of classes iterations：dynamic routing iterations \"\"\" def __init__(self, input_size, classes, iterations): super(CapsuleNet, self).__init__() self.input_size = input_size self.classes = classes self.iterations = iterations # Layer 1: Just a conventional Conv2D layer self.conv1 = nn.Conv2d(input_size[0], 256, kernel_size=9, stride=1, padding=0) # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_caps, dim_caps] self.primarycaps = PrimaryCaps(256, 256, 8, kernel_size=9, stride=2, padding=0) # Layer 3: Capsule layer. iterative dynamic routing. self.digitcaps = DenseCaps(in_num_caps=32*6*6, in_dim_caps=8, out_num_caps=classes, out_dim_caps=16, iterations=iterations) # reconstruction net self.reconstructor = nn.Sequential( nn.Linear(16*classes, 512), nn.ReLU(inplace=True), nn.Linear(512, 1024), nn.ReLU(inplace=True), nn.Linear(1024, input_size[0] * input_size[1] * input_size[2]), nn.Sigmoid() ) self.relu = nn.ReLU() def forward(self, x, y=None): x = self.relu(self.conv1(x)) x = self.primarycaps(x) x = self.digitcaps(x) # [batch, out_num_caps, out_dim_caps] length = x.norm(dim=-1) # vector lenght代表存在概率 [batch, out_num_caps, 1] if y is None: # during testing, no label given. create one-hot coding using `length` index = length.max(dim=1)[1] # 将index处，更改为1 y = Variable(torch.zeros(length.size()).scatter_(1, index.view(-1, 1).cpu().data, 1.).cuda()) # y[:, :, None]: mask reconstruction = self.reconstructor((x * y[:, :, None]).view(x.size(0), -1)) # 存在概率预测，重构图像像素 return length, reconstruction.view(-1, *self.input_size) Other capsules Text ​ 使用capsule nets处理文本的简单架构[Investigating capsule networks with dynamic routing for text classification. EMNLP (2018)]，如下图所示。 image Graph ​ 结合GNN的简单网络示意[Capsule Graph Neural Network. ICLR (2018)]。 image 3D point cloud ​ 3D重构[3D Point-Capsule Networks. CVPR, (2019)] image Applications ​ Relation extraction，Adversary detection，Brain tumor classification，Classification of Breast Cancer. ​ 比如，Relation extraction方面的研究[Multi-labeled Relation Extraction with Attentive Capsule Network. AAAI (2018)] image ​ [Attention-Based Capsule Networks with Dynamic Routing for Relation Extraction. EMNLP 2018] image Problems Optimizing routing 当存在较多class时，参数量很大 不能驾驭大规模数据集 Resources CVPR2019 Tutorial","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"}],"tags":[{"name":"capsule","slug":"capsule","permalink":"https://racleray.github.io/tags/capsule/"}]},{"title":"A Sentence Embedding Baseline","slug":"A-SENTENCE-EMBEDDINGS-Baseline","date":"2020-07-07T11:45:39.000Z","updated":"2023-08-07T11:54:31.024Z","comments":true,"path":"posts/8be3b59a.html","link":"","permalink":"https://racleray.github.io/posts/8be3b59a.html","excerpt":"记录普林斯顿大学对 SENTENCE EMBEDDING 进行优化的论文，使用SVD来区分出“无区分度的共有信息”和“有区分度的信息”，一种优化 SENTENCE EMBEDDING 的简单方法。","text":"论文《A SIMPLE BUT TOUGH-TO-BEAT BASELINE FOR SENTENCE EMBEDDINGS》 Info: 类别: [engineering ; pragmatic] 研究目标 提升Towards universal paraphrastic sentence embeddings（John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. ）论文提出的监督学习方法，转而进行无监督学习。 提升基于word embedding的句子向量的表现力。 研究成果： completely unsupervised sentence embedding improves performance by about 10% to 30% in textual similarity tasks, and beats sophisticated supervised methods including RNN’s and LSTM’s. new “smoothing” terms that allow for words occurring out of context, as well as high probabilities for words like and, not in all contexts. 存在的问题： 在sentiment相关任务上，效果一般，与LSTM相差较大。 在下游任务为监督学习任务时，效果一般，没有LSTM、Skip-thought等方法有效。 关键词：sentence embedding，无监督学习 Brief Summary: 这项工作提供了一种简单的句子嵌入方法，基于随机游走模型生成句子文本(Arora et al.， 2016)。它简单且无监督，但在各种文本相似性任务上，它的性能明显优于基线，甚至可以击败一些复杂的监督方法，如RNN和LSTM模型。获得的embedding可以作为下游监督任务的特征，与复杂方法相比也能获得不错的结果。 Main Thought: 简单且无监督的Sentence embeddings计算方法。 在 textual similarity tasks上，当选取了合适参数，效果相比于词向量的简单平均、LSTM、Skip-thought等方法有一定提升。 image 上图b中，在不同的领域都是有用的。这对于无监督方法尤其重要，因为未标记的可用数据可以从目标应用程序的不同域中收集。 在下游任务为监督学习任务时，效果下降的原因 这可能是因为similarity任务直接依靠余弦相似性, 这使得该方法倾向于removing the common components(可视为一种去噪denoising)。 而在监督任务, 由于有一些标签的信息用于训练, 分类器可以挑出有用少见的components和忽视常见的components。 无视词序 然而基于LSTM、RNN的结果表明，词序在similarity任务上是有作用的。本文方法忽视了次序，可以考虑两者结合。 忽略词序的方法，更能找到sentiment层面的信息。 image 与Word2vec的联系 Word2vec使用子采样（sub-sampling）技术对单词w进行下采样，概率与1 /√p（w）成正比，其中p（w）是单词w的边缘概率。这种启发式方法不仅可以加快训练速度，而且学习了更常见的单词表示形式。 该文章模型中，对词向量进行隐式加权，因此在一些场景下可以更好的利用文档的统计信息。 Method latent variable generative model for text image \\(c_t\\)为来自latent random walk的句子向量。使用MAP方法预测下一个（time t）生成的词的概率。 Improved Random Walk model. image 添加两个smooth项。\\(p(w)\\): 即使单词的向量与\\(c_s\\)的内积非常低，单词也可能出现。\\(c_0\\): 与语法相关的sentence向量校正项，保证最重要的向量维度处于主导地位。它提高了与\\(c_0\\)方向相近的单词在模型中的共现概率。 Computing the sentence embedding 假设\\(Z\\)是一个常量 image 越常见的word \\(w\\)，权重\\(a/(p(w) +a)\\)就越小，可以使embedding专注于具有代表性的词。 为了删去没有代表性的信息，计算\\(c_0\\)的方向为矩阵的第一个主成分（不进行 centralizing）。 image Notes: PMI: Pointwise Mutual Information \\[ PMI = log \\frac{p(u, v)}{p(u)p(v)} \\] pPMI = max(0,PMI): positive Pointwise Mutual Information","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"sentence embedding","slug":"sentence-embedding","permalink":"https://racleray.github.io/tags/sentence-embedding/"}]},{"title":"Git sheet","slug":"Git-sheet","date":"2020-07-07T11:13:39.000Z","updated":"2023-08-07T11:54:31.029Z","comments":true,"path":"posts/cdcb5601.html","link":"","permalink":"https://racleray.github.io/posts/cdcb5601.html","excerpt":"Git cheat sheet","text":"basic 12345678$ git status # 检查文件当前的状态$ git add [文件名] # 追踪新的文件$ git diff --cached # 若要看已经暂存起来的文件和上次提交时的快照之间的差异$ git commit -m \"Story 182: Fix benchmark\" # 用一行命令提交更新$ git commit -a -m 'added new benchmarks' # 跳过add命令直接提交$ git rm --cached log.log # 从git仓库中删除不小心追踪的文件（用于gitignore之前追踪的文件） $ git mv file_from file_to # 移动文件/重命名文件$ git log ​ # 查看历史操作 关联远程仓库 1234567891011121314$ git init # 初始化这个本地的文件夹为一个Git可以管理的仓库$ git remote add origin https://[地址] # 将本地的仓库和远程的仓库进行关联$ git pull &lt;远程主机名&gt; &lt;远程分支名&gt;:&lt;本地分支名&gt; # git pull origin master:master$ git add$ git commit -m$ git push -u origin master(首次关联加u，后续不用)$ git push origin master$ git clone https://github.com/。。。$ git rm --cached \"文件路径\"，不删除物理文件，仅将该文件从缓存中删除$ git rm --f \"文件路径\"，不仅将该文件从缓存中删除，还会将物理文件删除（不会回收到垃圾桶）$ git commit -m \"delete file\"$ git push branch 123456789101112131415 git branch # 查看分支$ git branch new-branch-name # 创建新分支$ git branch -v # 查看各分支最后一个提交对象$ git branch --merged # 查看已经merge过的分支$ git branch --no-merged # 尚未merge的分支$ git branch -d testing # 删除掉分支(如果还没有merge,会出现错误,-D可以强制删除)$ git branch -a # 查看所有分支（包括远程服务器）$ git push [远程仓库名] [本地分支名]:[远程分支名] # 推送本地分支到远程分支 # 如果本地分支名为空，则会直接删除远程分支名$ git checkout -b iss53 # 新建分支并切换到新分支 =$ git branch iss53; git checkout iss53$ git reset 版本号$ git cherry-pick [id] # 合并某一个单独的commit# 创建并在branch上修改之后，在代码仓库界面，可以在pull request选项中，选择是否merge pull request，合并该分支的修改 organization 1234567# 远程创建organization：New organization# organization中选择Team，创建管理小组# 新建代码仓库，归属于organization，在setting中设置Team权限，合作者权限# pull request中加入@TeamName可以通知所有人 opensource 123456789101112131415# 添加LICENSE，可使用template模板创建# 贡献开源项目，可以首先查看issue中是否有已经出现的相同问题# fork仓库，然后git clone到本地$ git checkout -b fix-bug # 新建分支并切换到新分支# 修改$ git add .$ git commit -m \"message\"$ git push origin fix-bug# 在fork项目中 点击New pull request，向原项目提交更改 log 12$ git log --pretty=format:\"%h - %an, %ar : %s\" # 用特性的format查看log$ git log --graph # 用图表的形式显示git的合并历史 config 1234$ git config --global user.name \"John Doe\" # 配置用户名 ！仅第一次必须$ git config --global user.email je@example.com # 配置电邮 ！仅第一次必须$ git config --list # 查看配置信息$ git config --global alias.stash-unapply '!git stash show -p | git apply -R' # 设置别名 stash 123456$ git stash # 储藏当前工作内容$ git stash list # 查看所有已经储藏的内容$ git stash apply [stash@&#123;0&#125;] # 在当前工作区应用储藏的内容，默认最新$ git stash apply --index # 在当前工作区应用储藏的内容，并保持之前暂存区的状态$ git stash drop # 删除一个储藏$ git stash pop # 弹出一个储藏 rejected问题 12345# 通常是因为在远程仓库中更改了，而本地没有修改，造成冲突。一般先在本地先pull再push，不会出现这样的问题。$ git fetch origin$ git rebase origin/master$ git push ssh密匙问题，没有权限访问仓库 123456789# 重新设置ssh keygit config --global user.name 'racleray'git config --global user.email 'racleme@outlook.com'ssh-keygen -t rsa -C 'racleme@outlook.com'# 在home目录找 .ssh/id_rsa.pub# 在github网站的SSH keys添加new key","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"git","slug":"Tools/git","permalink":"https://racleray.github.io/categories/Tools/git/"}],"tags":[{"name":"git","slug":"git","permalink":"https://racleray.github.io/tags/git/"}]},{"title":"文本相似性深度学习方法","slug":"文本相似性深度学习方法","date":"2020-07-07T10:49:20.000Z","updated":"2023-08-07T11:54:31.045Z","comments":true,"path":"posts/bad75bd3.html","link":"","permalink":"https://racleray.github.io/posts/bad75bd3.html","excerpt":"记录文本语义匹配模型。包括 InferSent、DSSM 等常见模型。","text":"基于深度学习的语义匹配方法一般有两种类型： Representation-based Match：简单，速度快。 Interaction-based Match：计算相对复杂，参数空间也更大。 Representation-based Representation-based Match句子相似度计算的一般训练流程如下： 准备同义句数据集（比如，The Paraphrase Database，ParaNMT-50M）； 选择模型结构（比如，Word Averaging，BiLSTM Averaging等）； Word Averaging模型：平均句子中的所有词向量作为句子语义的表达 BiLSTM Averaging模型：合并前向和反向LSTM编码得到的隐向量作为句子语义的表达 相对进阶一点的模型有InferSent，DSSM，CDSSM 选择负样本： 从当前batch中寻找与当前句子意义（根据当前模型判断）最不相近 的句子。 或者，Mega-batching：从更大的样本（合并多个mini batches）中寻找 意义较远的句子。 优化目标：hinge loss \\[ \\begin{array}{l} \\min _{W_{c}, W_{w}} \\frac{1}{|S|}\\left(\\sum_{\\left\\langle s_{1}, s_{2}\\right\\rangle \\in S} \\max \\left(0, \\delta-\\cos \\left(g\\left(s_{1}\\right), g\\left(s_{2}\\right)\\right)\\right.\\right. \\\\ \\left.+\\cos \\left(g\\left(s_{1}\\right), g\\left(t_{1}\\right)\\right)\\right)+\\max \\left(0, \\delta-\\cos \\left(g\\left(s_{1}\\right), g\\left(s_{2}\\right)\\right)\\right. \\\\ \\left.\\left.+\\cos \\left(g\\left(s_{2}\\right), g\\left(t_{2}\\right)\\right)\\right)\\right)+\\lambda_{c}\\left\\|W_{c}\\right\\|^{2}+\\lambda_{w}\\left\\|W_{w_{\\text {initial}}}-W_{w}\\right\\|^{2} \\end{array} \\] s之间正样本相似度要尽量接近\\(\\delta\\)，与负样本t之间相似度要尽量大。同时在正则化中加入词向量变化约束，计算之后的词向量不能和初始化使用的Glove（或其他）词向量相差过大。 InferSent ​ InferSent Git ​ 给定两个句子，预测两个句子之间的关系 (entailment隐含, contradiction互斥, neutral无关)，即预测三种概率。 image ​ encoder作为语句特征的提取器。 ​ 训练时，若只为学习sentence representation，线性分类器也许会得到比较好的效果。为了达到更好的分类效果，可以采用更复杂的非线性分类器。 DSSM(Deep Structured Semantic Model) ​ 微软研究院使用，用户搜索的关键词和最终点开的网页标题组成的数据，训练相似度计算模型。DSSM将语句映射到语义空间的连续表示，计算相似性。 Word Hashing • 用于解决单词表和out of vocabulary问题 • 把单词(e.g. good)前后加上# (#good#) • 然后取所有的trigram (#go, goo, ood, od#)，表示成bag of trigram 向量 ​ 原词表转换为了Compact representation，大小会变小，节省了空间。 ​ 对拼写错误有一定鲁棒性。 ​ 在大型NLP任务中可以轻松使用。 模型 image image 训练目标： image CDSSM image ​ convolutional layer捕捉了局部上下文的含义。那么相同单词在不同上下文中的多义性，就可能通过模型捕捉。 ​ global pooling捕捉语句整体的意图。实验中，一般情况下max pooling能够较准确地提取出整体语义。 Recurrent DSSM image ​ 对比Seq2Seq，DSSM倾向于在语句语义空间内优化，而Seq2Seq更倾向于在word-level进行学习优化。 评价指标 ​ NDCG a measure of ranking quality. ​ 两个基本假设： 相关度越高，排名越高。 高度相关的排名高于部分相关，部分相关的排名高于无关。 Cumulative Gain image ​ \\(rel_i\\) -- 是相关度分数，比如，第i个结果高度相关为5分 Discounted Cumulative Gain image ​ 对高度相关的结果出现在ranking靠后位置时，进行惩罚。 ​ 另一个形式为： image ​ 这个形式相对比较常用。 NDCG：normalized discounted cumulative gain image image Example ​ 令0 -- 不相关，1，2 -- 不同程度部分相关，3 -- 高度相关。相关性算法排序了前6个结果，降序： image ​ 而用户数据中的相关性分数Ground Truth为，每个位置index代表一个语句： image ​ Cumulative Gain，简单相加： image ​ Discounted Cumulative Gain，DCG结果为： image image ​ NDCG的IDCG，为期望的相关性排列顺序，即期望的最优输出： image image image DSSM的其他应用 训练word embedding：上下文与中心词的语义相似性 Knowledge Base Embedding学习 QA Information Retrieval Contextual Entity Ranking Interaction-based Matching DRMM：Deep Relevance Matching Model image ​ 输入网络的特征是处理过的，把matching分数转化为histogram统计特征： image ​ 即q的term与d计算cosine similarity，不是乘法，然后统计多个不同区间的相似度的统计分布。 Term Gating Network ​ Term Gating Network用于计query中每个term的weight。 image Hinge loss image","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"text similarity","slug":"text-similarity","permalink":"https://racleray.github.io/tags/text-similarity/"},{"name":"DSSM","slug":"DSSM","permalink":"https://racleray.github.io/tags/DSSM/"},{"name":"CDSSM","slug":"CDSSM","permalink":"https://racleray.github.io/tags/CDSSM/"}]},{"title":"文本相似性","slug":"文本相似性","date":"2020-07-07T10:19:49.000Z","updated":"2023-08-07T11:54:31.044Z","comments":true,"path":"posts/1074fc0d.html","link":"","permalink":"https://racleray.github.io/posts/1074fc0d.html","excerpt":"记录传统的文本相似性匹配方法（编辑距离、SimHash等），与word2vec等方法。","text":"基于字符串匹配的文本相似度 Hamming distance ​ 两个相同长度的字符串，有多少个位置是不同的token. e.g., d(cap, cat) = 1 编辑距离 ​ 给定两个句子，最少需要经过多少步操作（删除，添加，替换）能够从一个句子转化成另一个句子 1234567891011121314151617181920212223def editDistDP(s1, s2): \"\"\"编辑距离计算 params：文本1，string 文本2，string \"\"\" m = len(s1.strip()) n = len(s2.strip()) # 创建一张表格记录所有子问题的答案 dp = [[0 for x in range(n+1)] for x in range(m+1)] # 从上往下填充DP表格 for i in range(m+1): for j in range(n+1): if i == 0 or j == 0: dp[i][j] = max(i, j) # 如果两个字符串结尾字母相同，距离不变 elif s1[i-1] == s2[j-1]: dp[i][j] = dp[i-1][j-1] # 如果结尾字母不同，那我们就需要考虑三种情况，取最小的编辑距离 # 替换，添加，删除 else: dp[i][j] = 1 + min(dp[i-1][j-1], dp[i][j-1], dp[i-1][j]) return dp[m][n] Jaccard Similarity ​ 给定两句话，把两句话中出现的单词取交集和并集，交集和并集的大小之商即为Jaccard Similarity。 ​ Jaccard Similarity只考虑单词出现与否，忽略每个单词的含义，忽略单词的顺序，没有考虑单词出现的次数。 123456def jaccard_sim(s1, s2): \"\"\"交并比\"\"\" a = set(s1.strip().split()) b = set(s2.strip().split()) c = a.intersection(b) return float(len(c) / (len(a) + len(b) - len(c))) 基于文本特征的相似度计算方法 SimHash 选择一个hashsize，例如32 V = [0] * 32 把一段text变成features (shingles) 把每个feature都hash成32位 对于每个hash的每个位置，如果位置上是1就把V[i]加1，如果不是就把V[i]减1 最后，如果V[i]&gt;0就设为1，否则设为0，得到的V就是我们想要的simhash值 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123import reimport sysimport hashlibimport loggingimport numbersimport collectionsfrom itertools import groupbydef _hashfunc(x): # 使用的hash函数 return int(hashlib.md5(x).hexdigest(), 16)class Simhash(object): def __init__(self, value, f=64, reg=r'[\\w\\u4e00-\\u9fcc]+', hashfunc=None, log=None): \"\"\" `f` is the dimensions of fingerprints `reg` is meaningful only when `value` is str and describes what is considered to be a letter inside parsed string. Regexp object can also be specified (some attempt to handle any letters is to specify reg=re.compile(r'\\w', re.UNICODE)) `hashfunc` accepts a utf-8 encoded string and returns a unsigned integer in at least `f` bits. \"\"\" self.f = f self.reg = reg self.value = None if hashfunc is None: self.hashfunc = _hashfunc else: self.hashfunc = hashfunc if log is None: self.log = logging.getLogger(\"simhash\") else: self.log = log if isinstance(value, Simhash): self.value = value.value elif isinstance(value, str): # print(\"build by text\") self.build_by_text(str(value)) elif isinstance(value, collections.Iterable): self.build_by_features(value) elif isinstance(value, numbers.Integral): self.value = value else: raise Exception('Bad parameter with type &#123;&#125;'.format(type(value))) def __eq__(self, other): \"\"\" Compare two simhashes by their value. :param Simhash other: The Simhash object to compare to \"\"\" return self.value == other.value def _slide(self, content, width=4): return [ content[i:i + width] for i in range(max(len(content) - width + 1, 1)) ] def _tokenize(self, content): content = content.lower() content = ''.join(re.findall(self.reg, content)) ans = self._slide(content) return ans def build_by_text(self, content): features = self._tokenize(content) features = &#123;k: sum(1 for _ in g) for k, g in groupby(sorted(features))&#125; return self.build_by_features(features) def build_by_features(self, features): \"\"\" 核心方法 `features` might be a list of unweighted tokens (a weight of 1 will be assumed), a list of (token, weight) tuples or a token -&gt; weight dict. \"\"\" v = [0] * self.f # 初始化 [0,0,0,...] masks = [1 &lt;&lt; i for i in range(self.f)] # 二进制下[1000, 0100, 0010, ...] if isinstance(features, dict): features = features.items() for f in features: if isinstance(f, str): h = self.hashfunc(f.encode('utf-8')) # hash成32位 w = 1 else: assert isinstance(f, collections.Iterable) h = self.hashfunc(f[0].encode('utf-8')) w = f[1] for i in range(self.f): # mask位置为1，则vi加上w，否则减去w v[i] += w if h &amp; masks[i] else -w ans = 0 for i in range(self.f): if v[i] &gt; 0: # 如果大于0，就把那一位变成1 ans |= masks[i] self.value = ans def distance(self, another): \"\"\"计算两个vector有多少个位置不一样\"\"\" assert self.f == another.f x = (self.value ^ another.value) &amp; ((1 &lt;&lt; self.f) - 1) # (1 &lt;&lt; self.f) - 1: self.f个位的1 ans = 0 while x: # bin(x)不全为0，即x非0 ans += 1 x &amp;= x - 1 # bin计算，每算一次，低位的第一个1变为0 return ans 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293class SimhashIndex(object): def __init__(self, objs, f=64, k=2, log=None): \"\"\" 使用Simhash进行相似字符串检索 `objs` is a list of (obj_id, simhash) obj_id is a string, simhash is an instance of Simhash `f` is the same with the one for Simhash `k` is the tolerance \"\"\" self.k = k self.f = f count = len(objs) if log is None: self.log = logging.getLogger(\"simhash\") else: self.log = log self.log.info('Initializing %s data.', count) self.bucket = collections.defaultdict(set) for i, q in enumerate(objs): if i % 10000 == 0 or i == count - 1: self.log.info('%s/%s', i + 1, count) self.add(*q) def get_near_dups(self, simhash): \"\"\" `simhash` is an instance of Simhash return a list of obj_id, which is in type of str \"\"\" assert simhash.f == self.f ans = set() for key in self.get_keys(simhash): dups = self.bucket[key] self.log.debug('key:%s', key) if len(dups) &gt; 200: self.log.warning('Big bucket found. key:%s, len:%s', key, len(dups)) for dup in dups: sim2, obj_id = dup.split(',', 1) sim2 = Simhash(int(sim2, 16), self.f) d = simhash.distance(sim2) if d &lt;= self.k: ans.add(obj_id) return list(ans) def add(self, obj_id, simhash): \"\"\" `obj_id` is a string `simhash` is an instance of Simhash \"\"\" assert simhash.f == self.f for key in self.get_keys(simhash): v = '%x,%s' % (simhash.value, obj_id) self.bucket[key].add(v) def delete(self, obj_id, simhash): \"\"\" `obj_id` is a string `simhash` is an instance of Simhash \"\"\" assert simhash.f == self.f for key in self.get_keys(simhash): v = '%x,%s' % (simhash.value, obj_id) if v in self.bucket[key]: self.bucket[key].remove(v) @property def offsets(self): \"\"\" You may optimize this method according to &lt;http://www.wwwconference.org/www2007/papers/paper215.pdf&gt; \"\"\" return [self.f // (self.k + 1) * i for i in range(self.k + 1)] def get_keys(self, simhash): for i, offset in enumerate(self.offsets): if i == (len(self.offsets) - 1): m = 2**(self.f - offset) - 1 else: m = 2**(self.offsets[i + 1] - offset) - 1 c = simhash.value &gt;&gt; offset &amp; m yield '%x:%x' % (c, i) def bucket_size(self): return len(self.bucket) 123456789101112131415161718192021222324data = &#123; 1: u'How are you? I am fine. blar blar blar blar blar Thanks.', 2: u'How are you i am fine. blar blar blar blar blar Thanks.', 3: u'This is a simhash test',&#125;# 序号和hash值保存objs = [(str(k), Simhash(v)) for k, v in data.items()]# 建立SimhashIndex对象，`k` is the toleranceindex = SimhashIndex(objs, k=10)print(\"相似文本Bucket数量：\", index.bucket_size())# 输入需要查找的文本的hash值，get_near_dups获取相似文本s1 = Simhash(u'How are you i am fine. blar blar blar blar blar thanks')print(\"相似文本id：\", index.get_near_dups(s1))# 加入data文本index.add('4', s1)print(\"相似文本id：\", index.get_near_dups(s1))s2 = Simhash(u'How are you i am fine. blar blar blar thanks')index.add('5', s2)print(\"相似文本id：\", index.get_near_dups(s1)) Cosine Similarity 将文本转化为feature vectors。（bag of words或者TF-IDF） 利用feature vectors计算文本相似度 1234567891011from sklearn.feature_extraction.text import CountVectorizerfrom sklearn.metrics.pairwise import cosine_similarity# bowdef bow_cosine(s1, s2): \"\"\"返回值为ndarray类型\"\"\" vectorizer = CountVectorizer() vectorizer.fit([s1, s2]) # 词频统计词表 X = vectorizer.transform([s1, s2]) print(X.toarray()) return cosine_similarity(X[0], X[1]) 12345678from sklearn.feature_extraction.text import TfidfVectorizerdef tfidf_cosine(s1, s2): vectorizer = TfidfVectorizer() vectorizer.fit([s1, s2]) X = vectorizer.transform([s1, s2]) print(X.toarray()) return cosine_similarity(X[0], X[1]) Word2Vec ​ 利用句子中的单词做Word Averaging计算句子相似度。 12345678910111213141516import gensimimport gensim.downloader as apiimport numpy as npfrom sklearn.metrics.pairwise import cosine_similaritymodel = api.load(\"glove-twitter-25\")def wordavg(model, words): \"\"\"计算句子每个词的平均词向量\"\"\" res = np.mean([model.get_vector(w) for w in words if w in model.vocab], 0) return ress1_vec = wordavg(model, s1.lower().split())s2_vec = wordavg(model, s2.lower().split())cosine_similarity(s1_vec.reshape((1, -1)), s2_vec.reshape((1, -1))) Doc2Vec ​ 类似word2vec，只是在输入时加入一个全局的doc vec和word vec一起输入，doc vec由doc id和doc matrix相乘生成。计算方法有两种，DM(Distributed Memory)算法类似CBOW，DBOW(Distributed Bag of Words)类似Skip gram(只输入doc vec预测随机抽取词的概率分布)。 gensim官方文档：https://radimrehurek.com/gensim/models/doc2vec.html 1234567891011121314151617181920212223import gensim.models as gfrom gensim.corpora import WikiCorpusimport loggingfrom hanziconv import HanziConvdocvec_size=192class TaggedWikiDocument(object): def __init__(self, wiki): self.wiki = wiki self.wiki.metadata = True def __iter__(self): import jieba # tags信息是word2vec没有的，辅助训练 for content, (page_id, title) in self.wiki.get_texts(): yield g.doc2vec.LabeledSentence(words=[w for c in content for w in jieba.cut(HanziConv.toSimplified(c))], tags=[title])def my_function(): zhwiki_name = './data/zhwiki-latest-pages-articles.xml.bz2' wiki = WikiCorpus(zhwiki_name, lemmatize=False, dictionary=&#123;&#125;) documents = TaggedWikiDocument(wiki) model = g.Doc2Vec(documents, dm=0, dbow_words=1, size=docvec_size, window=8, min_count=19, iter=5, workers=8) model.save('data/zhiwiki_news.doc2vec') 模型的推断。根据输入文档，在doc matrix的中infer出最后结果，要指定infer_epoch。 123456789101112131415161718192021222324252627282930313233343536import gensim.models as gimport codecsimport numpy as npdef SimlarityCalu(Vector1,Vector2): Vector1Mod=np.sqrt(Vector1.dot(Vector1)) Vector2Mod=np.sqrt(Vector2.dot(Vector2)) if Vector2Mod!=0 and Vector1Mod!=0: simlarity=(Vector1.dot(Vector2))/(Vector1Mod*Vector2Mod) else: simlarity=0 return simlarity#parametersmodel='toy_data/model.bin'test_docs='toy_data/t_docs.txt'output_file='toy_data/test_vectors.txt'#inference hyper-parametersstart_alpha=0.01infer_epoch=1000#load modelm = g.Doc2Vec.load(model)test_docs = [x.strip().split() for x in codecs.open(test_docs, 'r', 'utf-8').readlines()]#infer test vectorsoutput = open(output_file, 'w')a=[]for d in test_docs: output.write(' '.join([str(x) for x in m.infer_vector(d, alpha=start_alpha, steps=infer_epoch)]) + '\\n' ) a.append(m.infer_vector(d, alpha=start_alpha, steps=infer_epoch))output.flush()output.close()print(SimlarityCalu(a[0],a[1]))","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"text similarity","slug":"text-similarity","permalink":"https://racleray.github.io/tags/text-similarity/"},{"name":"word2vec","slug":"word2vec","permalink":"https://racleray.github.io/tags/word2vec/"},{"name":"doc2vec","slug":"doc2vec","permalink":"https://racleray.github.io/tags/doc2vec/"},{"name":"SimHash","slug":"SimHash","permalink":"https://racleray.github.io/tags/SimHash/"}]},{"title":"Vision to text","slug":"Vision-to-text","date":"2020-07-07T09:55:15.000Z","updated":"2023-08-07T11:54:31.035Z","comments":true,"path":"posts/88e9ab16.html","link":"","permalink":"https://racleray.github.io/posts/88e9ab16.html","excerpt":"","text":"视觉问答任务的定义是对于一张图片和一个跟这幅图片相关的问题，机器需要根据图片信息对问题进行回答。 输入：一张图片和一个关于图片信息的问题，常见的问题形式有选择题，判断题 输出：挑选出正确答案 ​ 视觉问答任务本质上是一个多模态的研究问题。这个任务需要我们结合自然语言处理（NLP）和计算机视觉（CV)的技术来进行回答。 自然语言处理（NLP） 举一个在NLP领域常见的基于文本的Q&amp;A问题：how many bridges are there in Paris? 一个NLP Q&amp;A 系统需要首先识别出这是一个什么类型的问题，比如这里是一个“how many” 关于计数的问题，所以答案应该是一个数字。之后系统需要提取出哪个物体（object）需要机器去计数，比如这里是 “bridges“。最后需要我们提取出问题中的背景（context），比如这个问题计数的限定范围是在巴黎这个城市。 当一个Q&amp;A系统分析完问题，系统需要根据知识库（knowledge base）去得到答案。 机器视觉（CV) VQA区别于传统的text QA在于搜索答案和推理部分都是基于图片的内容。所以系统需要进行目标检测（object detection），再进行分类（classification），之后系统需要对图片中物体之间的关系进行推理。 VQA Notes 基于图像信息和文本信息匹配的VQA 通常，一个VQA系统包含了以下三个步骤： 抽取问题特征 抽取图片特征 结合图片和问题特征去生成答案 image image 基于注意力（attention）的VQA ​ VQA方案是使用位置注意力（spatial attention）去生成关于区域（region）的位置特征，并训练一个CNN网络。一般有两种方法去获得一张图片关于方位的区域。 划分成网格状（grid），并根据问题与图片特征去预测每一个网格的attention weight，将图片的CNN的feature通过加权求和的方式得到attention weighted feature，再通过attention weighted feature发现相对比较重要的区域 目标识别的方式生成很多bounding box。 根据生成的区域（region），使用问题去找到最相关的区域，并利用这些区域去生成答案。 Stacked Attention Stacked Attention，多次重复question-image attention。 image 图片使用CNN 编码 \\[\\phi = CNN(I)\\] 问题使用LSTM编码 \\[s= LSTM(E_q)\\] Stacked Attention \\[\\alpha_{c,l} \\propto \\exp F_c(s, \\phi_l) ,~~ \\sum_{l=1}^L \\alpha_{c,l}=1, ~~ x_c = \\sum_l \\alpha_{c,l}\\phi_l\\] classifier, 其中G=[G_1, G_2, ..., G_M]是两层的全连接层 \\[P(a_i|I,q) \\propto \\exp G_i(x,s),~~ x=[x_1, x2,...,x_C]\\] Image captioning 基本模型 ​ 常见的image captioning 系统是由一个CNN+RNN的编码解码模型完成。类比一下machine translation系统，通常由一个RNN encoder + RNN decoder组成。 图像编码 ​ Vinyals et al. (2014) Show and Tell: A Neural Image Caption Generator 这篇文章将seq2seq模型中的LSTM encoder换成CNN encoder，用于提取图片的信息，得到一个固定长度的内容向量（context vector），之后通过一个RNN decoder，将信息使用文字的方式解码出来。 ​ 结合注意力机制 - Kelvin et al. (2014) Show, Attend and Tell: Neural Image Caption Generation with Visual Attention 类比人看图说话：当人在解说一幅图片的时候，每预测一个字，会关注到图片上的不同位置。 在解码器预测文字的时候，会关注到跟当前文字内容和图片最相关的位置。 注意力机制 一张图片的卷积层中的向量有14x14=196个feature maps \\(a_i, i=1...196\\)，每个feature map对应于每个图片中不同的高亮位置。 注意力机制通过计算每个feature map与当前的hidden state计算两者之间的相关度，这里的hidden state \\(h_{t-1}\\) 总结了已经生成的1到t-1个单词的内容。 \\[e_{ti}=f_{att}(a_i, h_{t-1}) \\\\ \\alpha_{ti} = {\\exp(e_{ti}) \\over \\sum_{k=1}^L \\exp(e_{tk}) }\\] 之后通过加权求和得到注意力内容向量 \\(\\hat{z}_t\\)。 \\[\\hat{z}_t=\\phi(\\{a_i\\}, \\{\\alpha_i\\})\\] 通过将196个feature maps求平均值去初始化LSTM中的 memory cell \\(c_0, h_0\\) 根据图片及已经生成的部分单词，去预测下一个单词 \\[c_0 = f_{init,c}({1\\over L} \\sum_i^L a_i) \\\\ h_0 = f_{init,h}({1\\over L} \\sum_i^L a_i) \\\\ p(y_t|a, y_1^{t-1}) \\propto \\exp(L_o (E y_{t-1} + L_h h_t + L_z \\hat{z}_t))\\] show attention and tell beam search 优化 ​ 每次预测下一个单词的时候，计算当前所有路径的log-likelihood并进行排序, 只保留log-likelihood 最大值的K个beams。 目标识别 ​ Fang et al 2014， From Captions to Visual Concepts and Back, 提供了另一个image caption系统的思路。 预测文字： 使用一个CNN去做目标识别，并且根据bounding box生成可能出现的文字 生成句子：通过一个统计语言模型，生成很多个可能的句子集合 重新排序已经生成的句子： 通过学习一个Deep Multimodal Similarity Model （DMSM）去重新排序所有可能的句子集合，取最高分数的句子作为系统输出。 评估指标 常见的image captioning 系统的评估指标使用的是 BLEU， 是常见的机器翻译系统的评估指标，计算的是一句预测的文字与人类标注的参考文字之间的n-gram 重合度（overlap）。 METEOR， 也是常见的机器翻译系统的评估指标，其通过建立一个短语词表（phrase table），考虑了输出文本是否使用了相似短语。 CIDEr， 考虑了句子中的文字与图片的相关性 ROUGE-L，是text summarization的评估指标 SPICE 是专门设计出来用于 image caption 问题的。全称是 Semantic Propositional Image Caption Evaluation。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"visual text","slug":"visual-text","permalink":"https://racleray.github.io/tags/visual-text/"}]},{"title":"Rasa Notes","slug":"Rasa-Notes","date":"2020-07-07T09:47:25.000Z","updated":"2023-08-07T11:54:31.034Z","comments":true,"path":"posts/1b390165.html","link":"","permalink":"https://racleray.github.io/posts/1b390165.html","excerpt":"RASA工具的简短新手tutorial。","text":"rasa_nlu训练数据的生成 对话系统的冷启动都会遇到这样的问题，没有数据。 使用chatito来生成rasa_nlu意图识别需要的数据。这个数据需要反反复复的修改和完善。 Chatito doc 在产生训练数据的时候需要确定的nlu的意图和实体类别，需要在domain.yml文件中配置intents和entities。 官方DOC https://rasa.com/docs/rasa/core/policies/ mini rasa tutorial 创建一个新的项目 查看NLU培训数据 定义模型配置，写下第一个故事Story 定义这个故事story的作用域domain 训练模型 测试你写好的助手 创建新项目 路径指向一个新的空文件夹 cd path/to/a/blank/folder 在这个文件夹里面创建新的rasa项目 rasa init --no-prompt 文件夹中将会生成以下的文件： init.py 空文件用于定位 actions.py 用于定义动作（自定义脚本代码） config.yml 配置NLU和core模型 credentials.yml 连接到其他服务器的细节（不常用） data/nlu.md 自定义NLU训练数据 data/stories.md 自定义stories domain.yml 助手的定义域domian endpoints.yml 连接到fb message等的轨道（不常用） models/.tar.gz 模型及其参数文件 自定义的NLU模型 自定义NLU训练数据 1cat data/nlu.md 自定义stories 查看写好的stories 1cat data/stories.md 自定义动作acitons actions有两种类型： 直接回复，在domain.yml中定义templete 自定义操作，在aciton.py文件中添加 自定义domain 1cat domain.yml intents：用户意图 entities：实体 slots：槽 actions：助手说和做的事情 templates：助手根据actions具体要做的事情 定义模型配置 配置文件config.yml police: core，决定对话状态跟踪策略 pipeline: NLU，Natural Language Understanding and Intent Classification，理解当前用户输入，提取意图。 12345678910language: \"zh\"pipeline: - name: \"JiebaTokenizer\"policies: - name: FallbackPolic fallback_action_name: 'action_default_fallback' nlu_threshold: 0.5 core_threshold: 0.3 训练模型 使用data下面的训练数据 -- core/ -- stories.md -- nlu/ -- nlu.json 自动对模型进行训练，训练好的模型将会被打上时间戳time stamp作为新的模型，保存在models目录下面 rasa train SHELL启动 rasa shell 可视化界面 rasa x","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"rasa","slug":"rasa","permalink":"https://racleray.github.io/tags/rasa/"}]},{"title":"ChatBot Simple Notes","slug":"ChatBot-Notes","date":"2020-07-07T09:42:09.000Z","updated":"2023-08-07T11:54:31.028Z","comments":true,"path":"posts/fc0185e8.html","link":"","permalink":"https://racleray.github.io/posts/fc0185e8.html","excerpt":"粗浅的Chat Bot相关信息记录。","text":"基于检索技术的模型 VS 生成式模型 ​ 基于检索技术的模型较为简单，主要是根据用户的输入和上下文内容，使用了知识库（存储了事先定义好的回复内容）和一些启发式方法来得到一个合适的回复。启发式方法简单的有基于规则的表达式匹配，复杂的有一些机器学习里的分类器。这些系统不能够生成任何新的内容，只是从一个固定的数据集中找到合适的内容作为回复。 ​ 对于基于检索技术的模型，回复的内容语法上较为通顺，较少出现语法错误， 不能结合上下文给出回复。 ​ 生成式模型典型的有基于机器翻译模型的，与传统机器翻译模型不同的是，生成式模型的任务不是将一句话翻译成其他语言的一句话，而是输出一个回答(response)。 ​ 短对话 VS 长对话 ​ 处理长对话内容将更加困难，这是因为你需要在当前对话的情境下知道之前的对话说过什么。 开放域 VS 特定领域 ​ 面向开放域的聊天机器人技术面临更多困难，这是因为会话可能涉及的面太广，没有一个清晰的目标和意图。 ​ 面向特定领域的相关技术则相对简单一些，这是因为特定领域给会话的主题进行了限制，目标和意图也更加清晰，典型的例子有客服系统助手和购物助手。 面临的挑战 如何结合上下文信息 ​ 聊天机器人系统通常需要利用一些上下文信息(Context)，这里的上下文信息包括了对话过程中的语言上下文信息和用户的身份信息等。在长对话中人们关注的是之前说了什么内容以及产生了什么内容的交换，这是语言上下文信息的典型。 语义一致性 ​ 机器人面对相同语义而不同形式的问题应该给予一致的回复，例如这两个问题[How old are you?]和[What’s your age?]，很有可能不是一个个体。最大的原因在于训练模型的数据来源于大量不同的用户，这导致机器人失去了固定统一的人格。 对话模型的评测 ​ 在开放域中的对话系统没有一个清晰的优化目标。用于机器翻译的评测指标BLEU不能适用于此，是因为它的计算基础是语言表面上的匹配程度，而对话中的回答可以是完全不同词型但语义通顺的语句。 意图和回复多样性 ​ 生成式模型中的一个普遍问题是，它们可能生成一些通用的回答，例如[That’s great!]和[I don’t know]这样的可以应付许多的用户询问。 ​ 另外，人们在对话过程中的回复与询问有一定特定关系，是有一定意图的，而许多面向开放域的机器人不具备特定的意图。 ​ 目前深度学习的价值主要体现在能够获取大量数据的特定领域。目前一个无法做的事情是产生一个有意义的对话。 公开语料 dgk_shooter_min.conv.zip 中文电影对白语料，噪音比较大，许多对白问答关系没有对应好 The NUS SMS Corpus 包含中文和英文短信息语料，据说是世界最大公开的短消息语料 ChatterBot中文基本聊天语料 ChatterBot聊天引擎提供的一点基本中文聊天语料，量很少，但质量比较高 Datasets for Natural Language Processing 这是他人收集的自然语言处理相关数据集，主要包含Question Answering，Dialogue Systems， Goal-Oriented Dialogue Systems三部分，都是英文文本。可以使用机器翻译为中文，供中文对话使用 小黄鸡 据传这就是小黄鸡的语料：xiaohuangji50w_fenciA.conv.zip （已分词） 和 xiaohuangji50w_nofenci.conv.zip （未分词） 白鹭时代中文问答语料 由白鹭时代官方论坛问答板块10,000+ 问题中，选择被标注了“最佳答案”的纪录汇总而成。人工review raw data，给每一个问题，一个可以接受的答案。目前，语料库只包含2907个问答。(备份) Chat corpus repository chat corpus collection from various open sources 包括：开放字幕、英文电影字幕、中文歌词、英文推文 保险行业QA语料库 通过翻译 insuranceQA产生的数据集。train_data含有问题12,889条，数据 141779条，正例：负例 = 1:10； test_data含有问题2,000条，数据 22000条，正例：负例 = 1:10；valid_data含有问题2,000条，数据 22000条，正例：负例 = 1:10 基于内容检索式的聊天机器人 image ​ 检索式模型的输入是一段上下文内容 C (会话到目前未知的内容信息) 和一个可能作为回复的候选答案；模型的输出是对这个候选答案的打分。寻找最合适的回复内容的过程是：先对一堆候选答案进行打分及排序，最后选出分值最高的那个最为回复。 ​ Retrieval-Based Conversational Model in Tensorflow：https://github.com/dennybritz/chatbot-retrieval/ ​ 数据可以在Google Drive文件夹中找到：https://drive.google.com/open?id=1RIIbsS-vxR7Dlo2_v6FWHDFE7q1XPPgj ​ 数据文件需要: 123456- glove.6B.50d.txt (Subfolder GloVe)- training_10000.csv (Subfolder MAIN FILES)- validation_1000.csv (Subfolder MAIN FILES)- testing_same_structure_1000.csv (Subfolder MAIN FILES)- testing_different_structure_100.csv (Subfolder MAIN FILES)- saved_model_10000_gpu.pt (Subfolder SAVED MODELS) ​ 数据集为Ubuntu对话数据集。chatbot-retrieval/notebooks/Data Exploration.ipynb文件为数据分析。 Ubuntu对话数据集 训练集 ​ 训练数据有1,000,000条实例，其中一半是正例（label为1），一半是负例（label为0，负例为随机生成）。 ​ 每条实例包括一段上下文信息（context），即Query；和一段可能的回复内容，即Response；Label为1表示该Response确实是Query的回复，Label为0则表示不是。 ​ 数据集的生成使用了NLTK工具，包括分词、stemmed、lemmatized等文本预处理步骤；同时还使用了NER技术，将文本中的实体，如姓名、地点、组织、URL等替换成特殊字符。这些文本预处理并不是必须的，但是能够提升一些模型的性能。 ​ query的平均长度为86个word，而response的平均长度为17个word。 测试集和验证集 ​ 与训练集不同，在测试集和验证集中，对于每一条实例，有一个正例和九个负例数据（也称为干扰数据）。模型的目标在于给正例的得分尽可能的高，而给负例的得分尽可能的低。 ​ 负例生成方法可以参考谷歌的Smart Reply则使用了聚类技术，将每个类的中取一些作为负例，这样生成负例的方式显得更加合理（考虑了负例数据的多样性，同时减少时间开销）。 评测 ​ 模型的评测recall@k，即经模型对候选的response排序后，前k个候选中存在正例数据（正确的那个）的占比；显然k值越大，该指标会越高。 Dual Encoder LSTM Network 大致的流程如下： Query和Response都是经过分词的，分词后每个词embedded为向量形式。初始的词向量使用GloVe vectors，之后词向量随着模型的训练会进行fine-tuned（实验发现，初始的词向量使用GloVe并没有在性能上带来显著的提升）。 Query和Response经过相同的RNN（word by word）。RNN最终生成一个向量表示，捕捉了Query和Response之间的[语义联系]（图中的c和r）；这个向量的维度是可以指定的，这里指定为256维。 将向量c与一个矩阵M相乘，来预测一个可能的回复r’。如果c为一个256维的向量，M维256*256的矩阵，两者相乘的结果为另一个256维的向量，相当于一个生成式的回复向量。矩阵M是需要训练的参数。 通过点乘的方式来预测生成的回复r’和候选的回复r之间的相似程度，点乘结果越大表示候选回复作为回复的可信度越高；之后通过sigmoid函数，转成概率形式。图中把第(3)步和第(4)步结合在一起了。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"retireval","slug":"retireval","permalink":"https://racleray.github.io/tags/retireval/"}]},{"title":"Transformer Model","slug":"Transformer-Model","date":"2020-07-07T09:30:12.000Z","updated":"2023-08-07T11:54:31.035Z","comments":true,"path":"posts/7c7eb002.html","link":"","permalink":"https://racleray.github.io/posts/7c7eb002.html","excerpt":"Transformer 中两个任意输入的信号关联的开销会减少到一个固定的运算量级，使用 Multi-Head Attention 注意力机制可以高效地并行化，并堆叠多层的神经网络。","text":"1.Transformer模型 ​ 序列计算中，传统的RNN在预测下一个符号（token）的时候，会对以往的历史信息有很强的依赖，使得难以充分地并行化，也无法很好地加深网络的层级结构。 ​ 而对于传统的基于CNN的神经机器翻译模型，两个任意输入与输出位置的信号关联所需要的运算数量与它们的位置距离成正比，Facebook提出的CNN NMT 为线性增长。 ​ 这两种常见的结构使得学习较远位置的依赖关系（long-term dependency）非常困难。 ​ 在 Transformer 中，两个任意输入的信号关联的开销会减少到一个固定的运算数量级，使用 Multi-Head Attention 注意力机制可以完全脱离RNN及CNN的结构，使得Transformer可以高效地并行化，并堆叠多层的网络。 ​ 自注意力（Self-attention），是一种涉及单序列不同位置的注意力机制，并能计算序列的表征。自注意力这种在序列内部执行 Attention 的方法可以视为搜索序列内部的隐藏关系，这种内部关系对于翻译以及序列任务的性能非常重要。 1.1 编码器 encoder 编码器encoder由6层结构一样的网络层组成，每一层有2个子层： 第一个子层是multi-head self-attention Layer 第二个子层是一个基于位置编码的全连接网络层（position-wise fully connected feed-forward network） 会使用残差连接的方式，分别对每个子层的输入加到这个子层的输出上，然后再接一个Layer normalization的归一化层。 ​ \\[ \\text{LayerNorm}(x+\\text{Sublayer}(x)) \\] 所有的embedding及hidden state的维度都是512 1.2 解码器 decoder 解码器decoder由6层结构一样的网络层组成，每一层除了跟encode人一样有2个子层以外，还有第3个子层 第一个子层是multi-head self-attention Layer 第二个子层是一个基于位置编码的全连接网络层（position-wise fully connected feed-forward network） 第三个子层用于对encoder的输出向量进行multi-head attention 同样的，会使用残差连接的方式，然后再接一个Layer normalization的归一化层。 \\[ \\text{LayerNorm}(x+\\text{Sublayer}(x))\\\\ \\] decoder还需要将还没有生成的后续序列掩盖（masking），这样做是为了防止decoder在做self-attention的时候关注到后续还未生成的单词上去。 1.3 注意力机制 传统的注意力机制，也称为scaled Dot-Product Attention，可以看成是有一个询问的词（query），去跟一堆哈希表中的键值对（key-value pair）进行匹配，找到最相关的键（key），之后返回该键所对应的值（value）。通常的，如果只返回一个key所对应的value，称之为hard attention。如果对所有的key都计算一个相关系数，（也称之为attention weight），可以将所有key对应的value进行加权求和（weighted sum）这样的操作称之为soft attention。 \\[\\text{Attention}(Q,K,V) = \\text{softmax}\\left({QK^T \\over \\sqrt{d_k}}\\right)V\\] 其中所有的query和key都是维度为\\(d_k\\)的向量，将这些向量分别叠在一起形成 \\(Q\\in\\mathbb{R}^{|Q|\\times d_k}, K\\in\\mathbb{R}^{|K|\\times d_k}\\)的矩阵。 所有的value都是维度为\\(d_v\\)的向量，将这些向量叠在一起形成\\(V\\in\\mathbb{R}^{|V|\\times d_k}\\) 这里如果维度\\(d_k\\)很大的时候，两个向量的乘积会变得很大，使得softmax会得到非常小的数值，所以会在这里除以\\(\\sqrt{d_k}\\)来抵消这个影响。 1.4 Multi-Head Attention 这里假设\\(Q,~K,~V\\in \\mathbb{R}^{d_\\text{model}}\\)都在一个\\({d_\\text{model}}\\)维度的空间中 使用h个不一样权重的线性映射函数\\((QW^Q_i, KW^K_i, VW^V_i)\\)将Q, K, V分别映射到\\(d_k,~d_k,~d_v\\)空间中 对映射之后的Q, K, V 做h次attention，并将h个attention head连接在一起形成一个新的向量 最后再将这个向量映射到\\(d_\\text{model}\\)空间，作为下一层的输入 ​ \\[ \\text{MultiHead}(Q,K,V) = \\text{Concat}(\\text{head}_1,\\cdots, \\text{head}_h) W^O \\\\ \\text{head}_i = \\text{Attention}(QW^Q_i, KW^K_i, VW^V_i) \\] 其中\\(W^Q_i\\in \\mathbb{R}^{d_\\text{model}\\times d_k}, W^K_i\\in \\mathbb{R}^{d_\\text{model}\\times d_k}, W^V_i\\in \\mathbb{R}^{d_\\text{model}\\times d_v}, W^O\\in \\mathbb{R}^{hd_v\\times d_\\text{model}}\\), 常见的设置\\(h=8,d_k=d_v=d_\\text{model}/h=64\\) 模型图 应用 将decoder上一个时刻的hidden state 作为query，将encoder的最顶层的所有输出的hidden state作为key和value，这样可以类似传统的attention机制一样去发现源语言单词与目标语言单词之间的联系 encoder本身会对源语言单词进行multi-head self attention，其中query，key，value都是一样的，都是上一层中输出的单词的hidden state，每一个时刻计算出来的context vector都会作为该层输出的新的单词的hidden state，并作为下一层的输入。 decoder本身也会类似encoder一样去做self attention，不同的是，decoder只对左边已经生成的序列进行attention，对还没有生成的（右边的）序列掩盖（masking） 完整的模型图 1.5 基于位置的前向神经网络（Position-wise Feed-Forward Networks） 对于encoder和decoder的每个attention层之后，还会在连接一个全连接的前向神经网络。这个网络包含了两个线性转换和中间加一个ReLU的激活函数 \\[FFN(x) =\\max(0, xW_1+b_1) W_2+b_2\\] 这里每一层，都用不同的\\(W_1,W_2,b_1,b_2\\)。 1.6 词向量矩阵及Softmax层 这里使用常见的词向量矩阵，并encoder会把词向量映射到\\(d_\\text{model}\\)空间上，作为第一层的输入 在做预测的时候，会将输出向量映射到一个词表大小的概率空间中，并使用softmax来归一化到一个\\([0,1]\\)之间的概率值。 1.7 位置编码（position embeddings） 因为模型没有recurrence及convolution的操作，所以为了让模型能够分辨不同位置的单词，需要对单词的位置进行编码。 ​ \\[PE(pos, 2i)=\\sin(pos/10000^{2i/d_\\text{model}}) \\\\ PE(pos, 2i+1)=\\cos(pos/10000^{2i/d_\\text{model}})\\] pos是这个单词在句子中的位置，i是这个位置向量的第i个维度的编号。这样的波长形成了一个从\\(2\\pi\\)到\\(1000\\cdot 2\\pi\\)的几何级数。这样会使得模型更容易学到相对距离，因为\\(PE_{pos+k}\\)可以表示为\\(PE_{pos}\\)的一个线性变化。 1.8 Transformer 对比RNN及CNN 发现RNN需要进行\\(O(n)\\)个序列操作，而Transformer和CNN只需要\\(O(1)\\)个 CNN会形成一个层级结构，类似树状，所以任意两个单词到达的最大路径长度是\\(O(\\log_k(n))\\) 如果self-attention只对该单词周围r个单词进行attention操作，可以得到restricted版本的self-attention， 这样可以减少每一层的计算复杂度，但未增加两个任意词之间到达的最长路径。 2. Transformer模型的训练细节 优化方法 正则化 （regularization） label smoothing 2.1 优化方法 Adam 优化方法，\\(\\beta_1=0.9, \\beta_2=0.98, \\epsilon=10^{-9}\\) learning rate是随着训练的过程中，通过以下一个函数进行变化。一开始在前 warmup_steps个训练迭代中learning rate是线性增长的，往后随着步长的增加而下降。 一般会设置 warmup_steps = 4000 \\[lr = d_\\text{model}^{-0.5} \\cdot \\min(\\text{step_num}^{-0.5},\\text{step_num}\\cdot \\text{warmup_steps}^{-1.5}) \\] 2.2 正则化 Regularization 对每一个子层的输出，在该子层的输出加上该子层的输入之前进行dropout 对encoder及decoder，词向量和位置向量求和之后都进行dropout 2.3 Label Smoothing 对于正确的标注label，在其one-hot表达上，加上一个均匀分布的向量，这个smoothing的数值是\\(\\epsilon_{ls}=0.1\\) 3 Tranformer Code TensorFlow transformer 官方代码 作者代码 哈佛NLP组pytorch实现","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"transformer","slug":"transformer","permalink":"https://racleray.github.io/tags/transformer/"}]},{"title":"Fairseq Notes","slug":"Fairseq-Notes","date":"2020-07-07T09:01:06.000Z","updated":"2023-08-07T11:54:31.029Z","comments":true,"path":"posts/62a02478.html","link":"","permalink":"https://racleray.github.io/posts/62a02478.html","excerpt":"在自然语言处理中，大部分流行的seq2seq模型都是基于RNN结构去构建encoder和decoder，使得并行化操作难以充分进行，难以发挥完全发挥GPU并行的效率。而Fairseq是一种以CNN为基础的模型。","text":"1.基于CNN的翻译系统模型结构 ​ 在自然语言处理中，大部分流行的seq2seq模型都是基于RNN结构去构建encoder和decoder，但是RNN对于下一个状态的预测需要依赖前面的所有历史状态，使得并行化操作难以充分进行，难以发挥完全发挥GPU并行的效率。相反CNN通过在固定窗口内的计算，使得计算的并行化变得更加简单，而且通过多层CNN网络可以构建层级结构(hierarchical structure)，可以达到利用更短的路径去覆盖更长范围内的信息。 ​ Facebook提出了基于CNN的机器翻译模型，并开源了CNN的机器翻译工具Fairseq 1.1 Pooling Encoder ​ 最简单的non-recurrent encoder就是把k个连续的单词的词向量求平均值，通过在句子左右两边都添加额外的空单词(paddings)，可以使得encoder输出跟原来句子同等长度的hidden embeddings。 假设原来的句子的词向量（word embedding）表示为 \\(w=[w_1,\\cdots,w_m],~\\forall~w_j\\in R^f\\) absolute position embeddings用于编码位置信息 \\(p=[p_1,\\cdots,p_m],~\\forall~p_j\\in R^f\\) \\[e_j = w_j + p_j,~~ z_j = {1\\over k} \\sum_{t=-k/2}^{k/2}e_{j+t} \\] 传统的attention机制 \\[ c_i = \\sum_{j=1}^m a_{ij} e_j\\] 1.2 卷积编码器Convolutional Encoder NMT ​ 卷积编码器在pooling encoder的基础上进行改进，使用一个CNN-a 卷积层来进一步编码源语言句子中的词。输出原句长度的第一层卷积结果Z。 ​ \\[z_j = CNN\\_a(e)_j \\] ​ 注意attention的时候，使用了另一个CNN-c卷积层来编码源语言句子中的单词，还是输出原句长度的第一层卷积结果，作为计算atttention weight的encoder hidden states。然后计算atttention weight，再进行加权求和。 ​ \\[c_i = \\sum_{j=1}^m a_{ij} CNN\\_c(e)_j\\] ​ 该模型的encoder 采用的是CNN，但其decoder还是采用了传统的RNN模型 1.３ 全卷积神经翻译模型 Convolutional NMT 该模型的encoder和decoder都采用的是卷积核CNN，动图演示 卷积核结构 假设有1D的卷积核的窗口大小是k(比如k=5)，每个卷积核都可以用一个权重矩阵\\(W\\in \\mathbb{R}^{2d\\times kd}\\)和 bias \\(b_w\\in \\mathbb{R}^{2d}\\)。对于窗口内的词向量 \\(X\\in \\mathbb{R}^{k\\times d}\\)把所有单词拼接成一个长向量 \\(X&#39;\\in \\mathbb{R}^{kd}\\). \\[Y=WX&#39;+b_w = [A B] \\in \\mathbb{R}^{2d} \\\\ A,B\\in \\mathbb{R}^{d} \\] 接下来采用Gated Linear Unites(GLU)的方式来进行编码, \\(\\sigma()\\)是一个非线性的激活函数， \\(\\otimes\\)是element-wise mulitiplication \\[v([A B] = A \\otimes \\sigma(B) \\in \\mathbb{R}^d\\] 残差连接 Residual Connection: 把这一层的输入也累加到下一层的输出 \\[h_i^l = v(W^l [h_{(i-k)/2}^{l-1},\\cdots,h_{(i+k)/2}^{l-1}]+b_w^l)+h_i^{l-1} \\in \\mathbb{R}^d\\] 编码器 Encoder: 假设原来的句子的词向量（word embedding）表示为 \\(w=[w_1,\\cdots,w_m],~\\forall~w_j\\in \\mathbb{R}^f\\) absolute position embeddings用于编码位置信息 \\(p=[p_1,\\cdots,p_m],~\\forall~p_j\\in \\mathbb{R}^f\\) \\[e_j = w_j + p_j \\\\ \\] encoder 先用一个线性函数\\(f:\\mathbb{R}^f\\rightarrow \\mathbb{R}^d\\)，把词向量映射到d维空间中 接下来encoder会将词向量通过一层层卷积核，得到每一层的单词的隐式表达（hidden state）, 其中 \\(z_j^u\\) 代表的是第u层CNN中第j个单词的表达 Multi-step Attention机制 假设已经翻译的单词的词表达是 \\(g=[g_1,\\cdots, g_n]\\)，跟源语言的词表达一样，这里也是word embeddings加上positional embeddings 假设decoder的卷积核的hidden state \\(h_i^l\\), 可以进一步计算decoder已经生成的单词的每一层的单词表达 \\[d_i^l = W_d^l h_i^l + b_d^l + g_i \\] 假设encoder 最顶层(假设是第u层)中，每个单词的表达是\\(z_j^u\\)。可以计算decoder第l层中第i个已经生成的单词\\(h_i^l\\)与源语言句子中最顶层（也即是第u层）的第j个单词 \\(z_j^u\\)的权重: \\[a_{ij}^l = {\\exp(d_i^l \\cdot z_j^u) \\over \\sum_{t=1}^m \\exp(d_i^l \\cdot z_t^u) } \\] 可以进一步计算在decoder第l层，在第i个时刻的上下文向量（也即是context vector）如以下公式，其中将encoder最顶层(第u层)的词向量\\(z_j^u\\)与最底层的词向量\\(e_j\\)相加。 ​ \\[c_i^l = \\sum_{j=1}^m a_{ij}^l (z_j^u + e_j) \\] 将\\(c_i^l\\)加到\\(h_i^l\\)中，作为decoder 的下一层的输入 解码器 decoder 把decoder最顶层的hidden state \\(h_i^L\\) 通过一个线性的函数映射到词表空间上\\(d\\rightarrow |V|\\)，之后在通过一个softmax函数 归一化成一个条件概率向量： \\[p(y_{i+1}|y_1,\\cdots, y_i, x)= softmax(W_o h_i^L + b_0) \\in \\mathbb{R}^{|V|} \\] 模型的结构图 1.4 全卷积神经翻译模型对比RNN神经翻译模型 全卷积神经网络使用层级结构，可以充分地并行化 对于一个窗口大小为\\(k\\)的CNN，编码一个特征向量可以总结一个窗口为n个单词的信息，只需要做\\(O(n/k)\\)个卷积核操作。对比RNN，RNN编码一个窗口为n个单词的信息，需要做\\(O(n)\\)个操作，跟句子的长度成正比 对于一个CNN的输入，都进行了相同数量的卷积操作及非线性操作。对比RNN，第一个输入的单词进行了n词非线性操作，而最后一个输入的单词只进行了一次非线性操作。对于每个输入都进行相同数量的操作会有利于训练。 训练CNN NMT需要非常小心地设置参数及调整网络中某些层的缩放。 2 使用CNN完成神经机器翻译系统的tricks ​ 训练过程中，需要将网络中某些部分进行缩放(scaling)，需要对权重初始化，需要对超参数进行设置 2.1 缩放操作（scaling） 将残差层的输出乘以 \\(\\sqrt{0.5}\\)， 这样会减小一半的偏差variance 对于attention机制产生的上下文向量 \\(c_{ij}^l\\) 乘以一个系数 \\(m\\sqrt{1/m}\\), 其中m为源语言句子中单词个个数，这样做的好处也是能减小偏差。 对于CNN decoder有multiple atttention的情况，将encoder 每一层的gradient乘以一个系数，该系数是使用的attention的数量。注意，只对encoder中除了源语言单词的词向量矩阵以外的参数，放大gradient，源语言的词向量矩阵的gradient不进行放大。在实验中，这样的操作会使得训练能更加稳定。 2.2 参数初始化 所有的词向量矩阵从一个以０为中心，标准差为0.1的高斯分布中随机初始化 \\(\\mathcal{N}(0, \\sqrt{n_l})\\), 其中\\(n_l\\)为输入到这个神经元的输入个数，一般可以设置为0.1。这样能有助于保持一个正态分布的偏差。 还需要对每一层的激活函数输出进行正规化(normalization)， 比如残差连接中，每一层层的输出向量需要先做正则化，再把这一层的输入加到输出的向量上。 对于GLU，需要对其权重 \\(W\\)从一个正态分布\\(\\mathcal{N}(0, \\sqrt{4p\\over n_l})\\)中随机抽样，而其bias设置成０ 对每一层网络的输入向量都进行dropout处理 2.3 超参数设置 encoder 和decoder都是用512维的hidden units，512维的word embeddings 训练的时候使用Nesterov's accelerated gradient 的方法进行优化模型，momentum 设置成0.99 如果gradient的norm超过0.1就把gradient 重新归一化到0.1以内。 初始的learning rate设置成0.25，如果在每次进行valudation的时候dev数据集中的perplexity没有下降，就将learning rate乘以0.1, 一直持续到learning rate 降到\\(10^{-4}\\)以下停止训练 mini-batch的大小设置成每次处理64句双语句子 3. Facebook CNN 机器翻译系统代码解析 相应的代码可以在github上找到 fairseq 安装 1234git clone https://github.com/pytorch/fairseq.gitcd fairseqpip install -r requirements.txtpython setup.py build develop https://fairseq.readthedocs.io/en/latest/command_line_tools.html 3.1 Code 12345678910111213141516171819202122232425# 预处理数据$ bash prepare-wmt14en2de.sh --icml17$ cd examples/translation/$ bash prepare-wmt14en2de.sh$ cd ../..# 将数据处理成二进制形式，加速读写$ TEXT=examples/translation/wmt14_en_de$ python preprocess.py --source-lang en --target-lang de \\ --trainpref $TEXT/train --validpref $TEXT/valid --testpref $TEXT/test \\ --destdir data-bin/wmt14_en_de --thresholdtgt 0 --thresholdsrc 0# 训练模型# 如果显存不足，可以将--max-tokens设置成1500$ mkdir -p checkpoints/fconv_wmt_en_de$ python train.py data-bin/wmt14_en_de \\ --lr 0.5 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \\ --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\ --lr-scheduler fixed --force-anneal 50 \\ --arch fconv_wmt_en_de --save-dir checkpoints/fconv_wmt_en_de# 测试，生成$ python generate.py data-bin/wmt14_en_de \\ --path checkpoints/fconv_wmt_en_de/checkpoint_best.pt --beam 5 --remove-bpe 3.2 使用预训练好的模型 123456789101112131415# 下载模型及测试数据$ mkdir -p data-bin$ curl https://dl.fbaipublicfiles.com/fairseq/models/wmt14.v2.en-fr.fconv-py.tar.bz2 | tar xvjf - -C data-bin$ curl https://dl.fbaipublicfiles.com/fairseq/data/wmt14.v2.en-fr.newstest2014.tar.bz2 | tar xvjf - -C data-bin# 进行翻译生成$ python generate.py data-bin/wmt14.en-fr.newstest2014 \\ --path data-bin/wmt14.en-fr.fconv-py/model.pt \\ --beam 5 --batch-size 128 --remove-bpe | tee /tmp/gen.out# 对翻译结果打分$ grep ^H /tmp/gen.out | cut -f3- &gt; /tmp/gen.out.sys$ grep ^T /tmp/gen.out | cut -f2- &gt; /tmp/gen.out.ref$ python score.py --sys /tmp/gen.out.sys --ref /tmp/gen.out.ref 3.3 Notes CNN NMT类 FConvModel 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@register_model('fconv')class FConvModel(FairseqModel): \"\"\" Args: encoder (FConvEncoder): the encoder decoder (FConvDecoder): the decoder \"\"\" def __init__(self, encoder, decoder): ... @staticmethod def add_args(parser): parser.add_argument('--dropout', type=float, metavar='D', help='dropout probability') parser.add_argument('--encoder-embed-dim', type=int, metavar='N', help='encoder embedding dimension') parser.add_argument('--encoder-embed-path', type=str, metavar='STR', help='path to pre-trained encoder embedding') parser.add_argument('--encoder-layers', type=str, metavar='EXPR', help='encoder layers [(dim, kernel_size), ...]') parser.add_argument('--decoder-embed-dim', type=int, metavar='N', help='decoder embedding dimension') parser.add_argument('--decoder-embed-path', type=str, metavar='STR', help='path to pre-trained decoder embedding') parser.add_argument('--decoder-layers', type=str, metavar='EXPR', help='decoder layers [(dim, kernel_size), ...]') parser.add_argument('--decoder-out-embed-dim', type=int, metavar='N', help='decoder output embedding dimension') @classmethod def build_model(cls, args, task): base_architecture(args) ... encoder = FConvEncoder( dictionary=task.source_dictionary, embed_dim=args.encoder_embed_dim, embed_dict=encoder_embed_dict, convolutions=eval(args.encoder_layers), dropout=args.dropout, max_positions=args.max_source_positions, ) decoder = FConvDecoder( dictionary=task.target_dictionary, embed_dim=args.decoder_embed_dim, embed_dict=decoder_embed_dict, convolutions=eval(args.decoder_layers), out_embed_dim=args.decoder_out_embed_dim, attention=eval(args.decoder_attention), dropout=args.dropout, max_positions=args.max_target_positions, share_embed=args.share_input_output_embed, ) return FConvModel(encoder, decoder) CNN encoder类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110class FConvEncoder(FairseqEncoder): def __init__( self, dictionary, embed_dim=512, embed_dict=None, max_positions=1024, convolutions=((512, 3),) * 20, dropout=0.1, left_pad=True, ): ... # 定义词向量矩阵及位置矩阵 self.embed_tokens = Embedding(num_embeddings, embed_dim, self.padding_idx) self.embed_positions = PositionalEmbedding( max_positions, embed_dim, self.padding_idx, left_pad=self.left_pad, ) convolutions = extend_conv_spec(convolutions) in_channels = convolutions[0][0] self.fc1 = Linear(embed_dim, in_channels, dropout=dropout) self.projections = nn.ModuleList() self.convolutions = nn.ModuleList() self.residuals = [] # 定义CNN层及残差层 layer_in_channels = [in_channels] for _, (out_channels, kernel_size, residual) in enumerate(convolutions): if residual == 0: residual_dim = out_channels else: residual_dim = layer_in_channels[-residual] self.projections.append(Linear(residual_dim, out_channels) if residual_dim != out_channels else None) if kernel_size % 2 == 1: padding = kernel_size // 2 else: padding = 0 self.convolutions.append( ConvTBC(in_channels, out_channels * 2, kernel_size, dropout=dropout, padding=padding) ) self.residuals.append(residual) in_channels = out_channels layer_in_channels.append(out_channels) self.fc2 = Linear(in_channels, embed_dim) def forward(self, src_tokens, src_lengths): # 查找词向量及位置向量 x = self.embed_tokens(src_tokens) + self.embed_positions(src_tokens) x = F.dropout(x, p=self.dropout, training=self.training) input_embedding = x # 将词的表达映射到CNN的输入空间 fc1: R^f -&gt;R^d x = self.fc1(x) # 在句子左右两边添加padding encoder_padding_mask = src_tokens.eq(self.padding_idx).t() # -&gt; T x B if not encoder_padding_mask.any(): encoder_padding_mask = None # 转置：B x T x C -&gt; T x B x C x = x.transpose(0, 1) residuals = [x] # 多层的CNN 层叠起来 for proj, conv, res_layer in zip(self.projections, self.convolutions, self.residuals): if res_layer &gt; 0: residual = residuals[-res_layer] residual = residual if proj is None else proj(residual) else: residual = None if encoder_padding_mask is not None: x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0) x = F.dropout(x, p=self.dropout, training=self.training) if conv.kernel_size[0] % 2 == 1: # padding is implicit in the conv x = conv(x) else: padding_l = (conv.kernel_size[0] - 1) // 2 padding_r = conv.kernel_size[0] // 2 x = F.pad(x, (0, 0, 0, 0, padding_l, padding_r)) x = conv(x) # GLU 层 x = F.glu(x, dim=2) # 残差层 if residual is not None: x = (x + residual) * math.sqrt(0.5) residuals.append(x) # T x B x C -&gt; B x T x C x = x.transpose(1, 0) # 将x映射回词向量空间 R^d -&gt; R^f x = self.fc2(x) if encoder_padding_mask is not None: encoder_padding_mask = encoder_padding_mask.t() # -&gt; B x T x = x.masked_fill(encoder_padding_mask.unsqueeze(-1), 0) # 将gradient放大 x = GradMultiply.apply(x, 1.0 / (2.0 * self.num_attention_layers)) # 把input embedding加到output中 y = (x + input_embedding) * math.sqrt(0.5) return &#123; 'encoder_out': (x, y), 'encoder_padding_mask': encoder_padding_mask, # B x T &#125; 解码器decoder 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111class FConvDecoder(FairseqIncrementalDecoder): def __init__(self,...): # 定义词向量矩阵及位置向量矩阵 self.embed_tokens = Embedding(num_embeddings, embed_dim, padding_idx) self.embed_positions = PositionalEmbedding( max_positions, embed_dim, padding_idx, left_pad=self.left_pad, ) if positional_embeddings else None convolutions = extend_conv_spec(convolutions) in_channels = convolutions[0][0] self.fc1 = Linear(embed_dim, in_channels, dropout=dropout) self.projections = nn.ModuleList() self.convolutions = nn.ModuleList() self.attention = nn.ModuleList() self.residuals = [] # 定义多层CNN layer_in_channels = [in_channels] for i, (out_channels, kernel_size, residual) in enumerate(convolutions): if residual == 0: residual_dim = out_channels else: residual_dim = layer_in_channels[-residual] self.projections.append(Linear(residual_dim, out_channels) if residual_dim != out_channels else None) self.convolutions.append( LinearizedConv1d(in_channels, out_channels * 2, kernel_size, padding=(kernel_size - 1), dropout=dropout) ) self.attention.append(AttentionLayer(out_channels, embed_dim) if attention[i] else None) self.residuals.append(residual) in_channels = out_channels layer_in_channels.append(out_channels) self.adaptive_softmax = None self.fc2 = self.fc3 = None def forward(self, prev_output_tokens, encoder_out_dict=None, incremental_state=None): ... # 获得位置向量 if self.embed_positions is not None: pos_embed = self.embed_positions(prev_output_tokens, incremental_state) else: pos_embed = 0 # 获得上一个生成的单词的词向量 x = self._embed_tokens(prev_output_tokens, incremental_state) # 将词向量加上位置向量作为当前时刻的输入 x += pos_embed x = F.dropout(x, p=self.dropout, training=self.training) target_embedding = x # 将输入从词向量空间映射到CNN输入空间 x = self.fc1(x) # 转置：B x T x C -&gt; T x B x C x = self._transpose_if_training(x, incremental_state) # 多层的CNN 堆叠 avg_attn_scores = None num_attn_layers = len(self.attention) residuals = [x] for proj, conv, attention, res_layer in zip(self.projections, self.convolutions, self.attention, self.residuals): if res_layer &gt; 0: residual = residuals[-res_layer] residual = residual if proj is None else proj(residual) else: residual = None x = F.dropout(x, p=self.dropout, training=self.training) x = conv(x, incremental_state) x = F.glu(x, dim=2) # 注意力机制 if attention is not None: x = self._transpose_if_training(x, incremental_state) x, attn_scores = attention(x, target_embedding, (encoder_a, encoder_b), encoder_padding_mask) if not self.training and self.need_attn: attn_scores = attn_scores / num_attn_layers if avg_attn_scores is None: avg_attn_scores = attn_scores else: avg_attn_scores.add_(attn_scores) x = self._transpose_if_training(x, incremental_state) # 残差连接 if residual is not None: x = (x + residual) * math.sqrt(0.5) residuals.append(x) # 转置：T x B x C -&gt; B x T x C x = self._transpose_if_training(x, incremental_state) # fc2:将输入映射到词表大小空间，可进行预测 if self.fc2 is not None and self.fc3 is not None: x = self.fc2(x) x = F.dropout(x, p=self.dropout, training=self.training) x = self.fc3(x) return x, avg_attn_scores 123456789101112131415161718192021222324252627282930313233343536def main(args, init_distributed=False): ... # 载入数据 load_dataset_splits(task, ['train', 'valid']) # 构建模型及优化函数 model = task.build_model(args) criterion = task.build_criterion(args) # 构建训练器 trainer trainer = Trainer(args, task, model, criterion, dummy_batch, oom_batch) # 初始化dataloader epoch_itr = task.get_batch_iterator(...) # 训练一直到learning rate太小就停止 max_epoch = args.max_epoch or math.inf max_update = args.max_update or math.inf lr = trainer.get_lr() train_meter = StopwatchMeter() train_meter.start() while lr &gt; args.min_lr and epoch_itr.epoch &lt; max_epoch and trainer.get_num_updates() &lt; max_update: # 训练一个epoch train(args, trainer, task, epoch_itr) if epoch_itr.epoch % args.validate_interval == 0: valid_losses = validate(args, trainer, task, epoch_itr, valid_subsets) # 只用第一个validation loss去更新learning rate lr = trainer.lr_step(epoch_itr.epoch, valid_losses[0]) # 保存模型 if epoch_itr.epoch % args.save_interval == 0: save_checkpoint(args, trainer, epoch_itr, valid_losses[0]) train_meter.stop()","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"cnn","slug":"cnn","permalink":"https://racleray.github.io/tags/cnn/"},{"name":"Fairseq","slug":"Fairseq","permalink":"https://racleray.github.io/tags/Fairseq/"},{"name":"seq2seq","slug":"seq2seq","permalink":"https://racleray.github.io/tags/seq2seq/"}]},{"title":"NLG常用评价指标","slug":"NLG常用评价指标","date":"2020-07-07T06:30:48.000Z","updated":"2023-08-07T11:54:31.032Z","comments":true,"path":"posts/cd85a6be.html","link":"","permalink":"https://racleray.github.io/posts/cd85a6be.html","excerpt":"记录NLG的常用评估指标计算方法，包括BELU、ROUGE、METEOR等。","text":"NLG常用评价指标 客观评价指标 – BLEU – ROUGE – METEOR – CIDEr 主观评价指标 – 流畅度 – 相关性 – 助盲性 这些指标原先都是用来度量机器翻译结果质量的，并且被证明可以很好的反应待评测翻译结果的准确性，并且与人类对待评测翻译结果的评价存在强相关 BLEU 文献 只看中准确率的指标，就是说更加关心候选译文里的多少 n-gram 是对的（即在参考译文里出现了），而不在乎召回率（参考译文里有哪些 n-gram 在候选译文中没出现） 基于准确率，BLEU 得分越高越好 BLEU 是最早提出的机器翻译评价指标，是所有文本评价指标的源头。BLEU的全名为：bilingual evaluation understudy，即：双语互译质量评估辅助工具。 BLEU 的大意是比较候选译文和参考译文里的 n-gram（实践中从 unigram 取到 4-gram） 重合程度，重合程度越高就认为译文质量越高。选不同长度的 n-gram 是因为，unigram 的准确率可以用于衡量单词翻译的准确性，更高阶的 n-gram 的准确率可以用来衡量句子的流畅性。 BLEU 原论文建议大家的测试集里给每个句子配备 4 条参考译文，这样就可以减小语言多样性带来的影响（然而现在很多机器翻译的测试集都是只有 1 条译文，尴尬= =） brevity penalty 来惩罚候选译文过短的情况（候选译文过短在机器翻译中往往意味着漏翻，也就是低召回率） 现在还是普遍认为 BLEU 指标偏向于较短的翻译结果（brevity penalty 没有想象中那么强） 优点很明显：方便、快速、结果有参考价值 缺点也不少，主要有： 不考虑语言表达（语法）上的准确性； 测评精度会受常用词的干扰； 短译句的测评精度有时会较高； 没有考虑同义词或相似表达的情况，可能会导致合理翻译被否定； 1234567from nltk.translate.bleu_score import sentence_bleureference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]candidate = ['this', 'is', 'a', 'test']score = sentence_bleu(reference, candidate)print(score) 只能做到个大概判断，它的目标也只是给出一个快且不差自动评估解决方案 image Hk(Ci) 表示Wk翻译选译文Ci中出现的次数， Hk(Sij) 表示Wk在标准答案Sij中出现的次数， maxi∈mhk(sij)表示某n-gram在多条标准答案中出现最多的次数， ∑i∑kmin(hk(ci),maxj∈mhk(sij))表示取n-gram在翻译译文和标准答案中出现的最小次数。 i为candidate的index；j为reference的index；k为n-gram的index image ROUGE 文献 ROUGE 和 BLEU 几乎一模一样，区别是 BLEU 计算准确率，而 ROUGE 计算召回率。 在 SMT（统计机器翻译）时代，机器翻译效果稀烂，需要同时评价翻译的准确度和流畅度；等到 NMT （神经网络机器翻译）出来以后，神经网络脑补能力极强，翻译出的结果都是通顺的，但是有时候容易瞎翻译，遗漏翻译 不看流畅度只看召回率（参考译文里的 n-gram 有多少出现在了候选译文中）就好了，这样就能知道 NMT 系统到底有没有漏翻（这会导致低召回率）。 所以，ROUGE 适合评价 NMT，而不适用于 SMT，因为它不管候选译文流不流畅。 image 分母是n-gram的个数，分子是参考摘要和自动摘要共有的n-gram的个数。ROUGE 得分越高越好 召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。那也有两种可能，一种是把原来的正类预测成正类(TP)，另一种就是把原来的正类预测为负类(FN) 精确率是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是对的。那么预测为正就有两种可能了，一种就是把正类预测为正类(TP)，另一种就是把负类预测为正类(FP) Rough-L: L即是LCS(longest common subsequence，最长公共子序列)的首字母，因为Rough-L使用了最长公共子序列。 METEOR 文献 METEOR 大意是说有时候翻译模型翻译的结果是对的，只是碰巧跟参考译文没对上（比如用了一个同义词），于是用 WordNet 等知识源扩充了一下同义词集，同时考虑了单词的词形（词干相同的词也认为是部分匹配的，也应该给予一定的奖励，比如说把 likes 翻译成了 like 在评价句子流畅性的时候，用了 chunk 的概念（候选译文和参考译文能够对齐的、空间排列上连续的单词形成一个 chunk，这个对齐算法是一个有点复杂的启发式 beam serach），chunk 的数目越少意味着每个 chunk 的平均长度越长，也就是说候选译文和参考译文的语序越一致 缺点： 有四个超参数 alpha, beta, gamma, delta，这些都是对着某个数据集调出来的（让算法的结果和人的主观评价尽可能一致，方法我记得是 grid search），参数一多听起来就不靠谱. 另外需要有外部知识源（WordNet 等）来进行单词对齐，所以对于 WordNet 中不包含的语言，就没法用 METEOR 来评价了。 使用 WordNet 计算特定的序列匹配，同义词，词根和词缀，释义之间的匹配关系，改善了BLEU的效果，使其跟人工判别共更强的相关性。 同时考虑了准确率和召回率，METEOR 得分越高越好 image CIDEr 文献 Consensus-based image description evaluation，通过度量待评测语句与其他大部分人工描述之间的相似性来评价。 这个指标的motivation之一是刚才提到的BLEU的一个缺点，就是对所有匹配上的词都同等对待，而实际上有些词应该更加重要。 CIDEr-D 是修改版本，为的是让 CIDEr 对于 gaming 问题更加鲁棒。什么是 Gaming 问题？它是一种现象，就是一个句子经过人工判断得分很低，但是在自动计算标准中却得分很高的情况。为了避免这种情况，CIDEr-D 增加了截断（clipping）和基于长度的高斯惩罚 CIDEr 是 BLEU 和向量空间模型的结合。它把每个句子看成文档，然后计算 TF-IDF 向量（只不过 term 是 n-gram 而不是单词）的余弦夹角，据此得到候选句子和参考句子的相似度，同样是不同长度的 n-gram 相似度取平均得到最终结果。优点是不同的 n-gram 随着 TF-IDF 的不同而有不同的权重，因为整个语料里更常见的 n-gram 包含了更小的信息量。 SPICE SPICE SPICE 是专门设计出来用于 image caption 问题的。全称是 Semantic Propositional Image Caption Evaluation。前面四个方法都是基于 n-gram 计算的，SPICE 则不是。 SPICE 使用基于图的语义表示来编码 caption 中的 objects, attributes 和 relationships。它先将待评价 caption 和参考 captions 用 Probabilistic Context-Free Grammar (PCFG) dependency parser parse 成 syntactic dependencies trees，然后用基于规则的方法把 dependency tree 映射成 scene graphs。最后计算待评价的 caption 中 objects, attributes 和 relationships 的 F-score 值。 image 翻译和图像描述的区别 在机器翻译中，译文应该忠实于原文，所以同一句话的多条译文应该互为转述，包含同样的信息；而同一幅图的多条字幕则不一定互为转述，因为不同的字幕可以包含不同数量的图像细节，不管描述得比较详细还是比较粗糙都是正确的字幕。 简单总结 NLG常用metrics： BLEU: ngram precision；长度类似 ROUGE: ngram recall NIST/CIDEr: 降低频繁词的权重 METEOR: 考虑同义词的F score；鼓励连续词匹配 SPICE：匹配语法树与图像特征的F1 其他： STM: 匹配语法树子树 TER: 编辑的距离 TERp: TER+同义替换 常用评测指标的开源实现 MS COCO Caption Evaluation COCO demo","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"bleu","slug":"bleu","permalink":"https://racleray.github.io/tags/bleu/"},{"name":"rouge","slug":"rouge","permalink":"https://racleray.github.io/tags/rouge/"},{"name":"evaluation","slug":"evaluation","permalink":"https://racleray.github.io/tags/evaluation/"}]},{"title":"seq2seq","slug":"seq2seq","date":"2020-07-07T05:34:02.000Z","updated":"2023-08-07T11:54:31.038Z","comments":true,"path":"posts/ef7a7a35.html","link":"","permalink":"https://racleray.github.io/posts/ef7a7a35.html","excerpt":"Sequence-to-sequence (seq2seq) 模型，突破了传统的检索式框架，从一种端到端的角度出发解决问题，将经典深度神经网络模型运用于翻译与职能问答这一类序列型任务。","text":"seq2seq 1.seq2seq（序列到序列模型）简介 ​ 对于很多自然语言处理任务，比如聊天机器人，机器翻译，自动文摘，智能问答等，传统的解决方案都是检索式，这对素材的完善程度要求很高，随着深度学习的发展，研究界将深度学习技术应用与自然语言的生成和自然语言的理解的方面的研究，并取得了一些突破性的成果，比如，Sequence-to-sequence (seq2seq) 模型，该技术突破了传统的固定大小输入问题框架，将经典深度神经网络模型运用于翻译与职能问答这一类序列型任务，并在各主流语言之间的相互翻译以及语音助手中人机短问答的应用。 参考资料:Visualizing A Neural Machine Translation Model 2.编码解码模型 ​ seq2seq模型不仅仅是用在NLP中的模型，它的输入也可以是语音信号或者图像表示。 ​ 在NLP的任务中，其实输入的是文本序列，输出的很多时候也是文本序列。 ​ 这是一个“编码解码器”结构，编码器处理输入序列中的每个元素(在这里可能是1个词)，将捕获的信息编译成向量（称为上下文内容向量）。在处理整个输入序列之后，编码器将上下文发送到解码器，解码器逐项开始产生输出序列。 ​ 在机器翻译的场景下，是下面这样的。 ​ 上下文向量其实就是 ​ 输入的数据(文本序列)中的每个元素(词)被编码成一个稠密的向量word embedding ​ encoder和decoder一般为循环神经网络(RNN)，循环神经网络会接受每个位置(时间点)上的输入，同时经过处理进行信息融合，并可能会在某些位置(时间点)上输出。 ​ 所以动态地展示整个编码器和解码器。 ​ 在更多的时候，为提升效果，会采用一个叫做注意力模型的模型来动态处理和解码 ​ 所谓的注意力机制，可以粗略地理解为是一种对于输入的信息，根据重要程度进行不同权重的加权处理(通常加权的权重来源于softmax后的结果)的机制，如下图所示，是一个在解码阶段，简单地对编码器中的hidden states进行不同权重的加权处理的过程。 更详细一点的注意力解码过程如下图所示。 带注意力的解码器RNN接收的嵌入(embedding)和一个初始的解码器隐藏状态(hidden state)。 RNN处理输入，产生新的隐藏状态向量（h4）。 attention的步骤：使用编码器隐藏状态(hidden state)和h4向量来计算该时间步长的上下文向量（C4）。 把h4和C4拼接成一个向量。 把拼接后的向量连接全连接层和softmax完成解码 每个时间点上重复这个操作 也可以把这个动态解码的过程展示成下述图所示的过程。 注意力机制可以学习源语言和目标语言之间词和词对齐关系的方式。 3.Attention ​ seq2seq 是一个Encoder–Decoder 结构的网络，它的输入是一个序列，输出也是一个序列， Encoder 中将一个可变长度的信号序列变为固定长度的向量表达，Decoder 将这个固定长度的向量变成可变长度的目标的信号序列。 输入： \\(x = (x_1,...,x_{T_x})\\) 输出： \\(y = (y_1,...,y_{T_y})\\) \\(h_t = RNN_{enc}(x_t, h_{t-1})\\) , Encoder方面接受的是每一个单词word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。 \\(s_t = RNN_{dec}(\\hat{y_{t-1}},s_{t-1})\\) ， Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。 \\(c_i = \\sum_{j=1}^{T_x} \\alpha_{ij}h_j\\) , context vector是一个对于encoder输出的hidden states的一个加权平均。 \\(\\alpha_{ij} = \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x}exp(e_{ik})}\\) , 每一个encoder的hidden states对应的权重。 \\(e_{ij} = score(s_i, h_j)\\) , 通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重(4) \\(\\hat{s_t} = tanh(W_c[c_t;s_t])\\), 将context vector 和 decoder的hidden states 串起来。 \\(p(y_t|y_{&lt;t},x) = softmax(W_s\\hat{s_t})\\) ，计算最后的输出概率。 ​ ​ 其中Encoder的hidden state不一定要作为Decoder的hidden state输入，可以将Decoder的hidden state仅仅做常规初始化。 score ​ 一般有三种score的计算方法 第1种 输入是encoder的所有hidden states H: 大小为(hid dim, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid dim, 1）。 第一步：旋转H为（sequence length, hid dim) 与s做点乘得到一个 大小为(sequence length, 1)的分数。 第二步：对分数做softmax得到一个合为1的权重。 第三步：将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。 第2种 输入是encoder的所有hidden states H: 大小为(hid dim1, sequence length)。decoder在一个时间点上的hidden state， s： 大小为（hid dim2, 1）。此处两个hidden state的纬度并不一样。 第一步：旋转H为（sequence length, hid dim1) 与 Wa [大小为 hid dim1, hid dim 2)] 做点乘， 再和s做点乘得到一个 大小为(sequence length, 1)的分数。 第二步：对分数做softmax得到一个合为1的权重。 第三步：将H与第二步得到的权重做点乘得到一个大小为(hid dim, 1)的context vector。 NMT官方Git：https://github.com/tensorflow/nmt NMT官方Git翻译版本：HTML tensorflow attention wrapper实现机制 ref : https://tangshusen.me/2019/03/09/tf-attention/ AttentionWrapper实现相对于理解理论更复杂一些。 ​ 简而言之，增加了attention layer，将attention算法中得到的context vector与decoder当前输出cell_outputs(即hidden state)通过计算得到一个attention向量。当attention layer没有指定时，attention向量直接取context vector(即，算法理论中的计算方式)。 ​ 增加了cell_input_fn，将上一步的attention向量与当前步的inputs，联合成新的cell_inputs。 ​ attention mechanism：输入decoder的cell_outputs(即hidden state)，与memory(encoder的hidden state)计算alignments(权重) Code Demo 对联生成 -- dir 诗歌生成 -- dir","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"seq2seq","slug":"seq2seq","permalink":"https://racleray.github.io/tags/seq2seq/"},{"name":"attention","slug":"attention","permalink":"https://racleray.github.io/tags/attention/"}]},{"title":"主题模型","slug":"主题模型","date":"2020-07-07T05:24:13.000Z","updated":"2023-08-07T11:54:31.039Z","comments":true,"path":"posts/4ede4335.html","link":"","permalink":"https://racleray.github.io/posts/4ede4335.html","excerpt":"此处只记录了问题定义，移步《统计学习方法》第二版15-20章内容进行查阅。","text":"详见《统计学习方法》第二版，15-20章 ​ 主题模型在《统计学习方法》第二版，15-20章有较为详细的介绍。 pLSA ​ 包含“隐含变量”或者“缺失数据”的概率模型参数估计问题可以采用EM算法。 ​ EM算法的步骤本质上是一种交替最优化（二部坐标下降法）： E步骤：求隐含变量Given当前估计的参数条件下的后验概率。 M步骤：最大化Complete data对数似然函数的期望，此时我们使用E步骤里计算的隐含变量的后验概率，得到新的参数值。 EM算法求解PLSA 已知量：w,d 隐变量：z 参数：P(w|z)，P(z|d) E:直接写出 M:拉格朗日乘子法求解 LDA EM算法求解LDA 已知量：w 隐变量：z，θ，φ 参数：a，β E:直接写不出，需要用变分法近似，或者吉布斯采样 M:坐标下降法求解，可以考虑牛顿法 详见李航《统计学习方法》第二版，15-20章","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"lda","slug":"lda","permalink":"https://racleray.github.io/tags/lda/"}]},{"title":"文本分类advanced","slug":"文本分类advanced","date":"2020-07-06T16:48:46.000Z","updated":"2023-08-07T11:54:31.043Z","comments":true,"path":"posts/e6e273ef.html","link":"","permalink":"https://racleray.github.io/posts/e6e273ef.html","excerpt":"做细粒度文本分类任务时的一点记录。","text":"细粒度分类 多标签问题，每一个类别标签训练一个分类器，忽略了不同类别标签间的联系。 多任务学习，特征提取阶段共享参数，最后几层单独输出。优点是考虑了不同任务间的联系，有联系的类别标签可以一块训练，对不均衡的样本数据有增强作用。 Seq2Seq，把多标签的预测问题看成了一个序列到序列的学习，这样既考虑了标签之间的联系，又可以处理大量标签的问题。 Aspect Based，当有每种Aspect（可理解为主题）相关信息，且每个样本属于特定的Aspect。需要根据属于识别Aspect和分类。 数据处理 可选项有 明显噪声处理，例如全篇标点符号，繁体转简体等。 分词器选择与自定义词典扩充。使用word2vec初始化时，训练词向量的词表和模型使用的词表一致（分词器不要混用）。 分词与分字特征可分别利用，训练不同模型集成。 word2vec 与 bert类模型提取的特征，拼接，输入下游任务设计。 EDA：同义词替换，随机删除、交换位置等。翻译效果不稳定。 不平衡数据： 上采样：罕见类数据随机打乱作为扩充。同义词替换，随机删除、交换位置等扩充。使用扩充数据时，不要连续使用增强后的数据，可以相隔一两个epoch使用。 下采样，数据利用率不高。 标签权重加入loss计算。实际效果是，不一定带来提高，尤其是复杂的分类任务，但可选。 Label smoothing。约束神经网络本身对错误标签的极大惩罚（loss在bp时，回传一般是label与predict之差）。提高泛化力。 focal loss。损失计算偏向于没有正确分类的输出修正（理论上）。 使用预训练特征提取器时（EMLo，BERT类）： 使用外部相似预料进行模型pretrain。 长度有限制的模型，可以尝试随机删除句子。（观测数据，如果开头和结尾重要，就删中间部分） 模型 Bi-GRU + Multi Capsule Bi-GRU + Multi ResNet HAN + Attention Transformer Encoder + Convolutional Seq2Seq 解码器三种思路： 使用LSTM（或其他）每一步（#不同种类标签数）的output表示不同种类标签的预测输出 Beam Search尽量好的输出预测序列（只是在inference阶段使用）。 Global Embedding（在训练阶段使用），类似Beam Search： image y为预测标签分布（output softmax后的输出），e为每一步（#每一种标签）的output；g为global embedding的输出，代替LSTM的hidden state，进行序列解码任务。 【SGM: Sequence Generation Model for Multi-Label Classification】 Aspect Based Sentiment Analysis 抽取content特征，Aspect信息，使用各种方法attention到和输出label相关的信息。比如： image 【Capsule Network with Interactive Attention for Aspect-Level Sentiment Classification】 另一种思路，树形（层级搜索）： image 上图中加入aspect信息到输入层 模型训练 Warm up：学习率先增加，然后减小。线性增加即可。也可以使用one cycle fitting，让学习率在一个epoch内，线性增加到一个较大值，然后线性减小为初始的较小学习率。绘制loss--lr曲线图，摘到loss的变化较大处的lr的十分之一。【A DISCIPLINED APPROACH TO NEURAL NETWORK HYPER-PARAMETERS: PART 1 – LEARNING RATE, BATCH SIZE, MOMENTUM, AND WEIGHT DECAY】 找到合适学习率后（实验）， 将模型迭代足够多次（loss可能在一段时间不降之后，突然下降），保留验证正确率最高的模型。加载上一个最优模型，学习率设为当前1/10（实验），继续训练模型，保留验证正确率最高的模型。加载上一个最优模型，去掉正则化策略(dropout 等，如果有)，学习率再降低，训练到收敛。 先调整学习率，再调整其他模型超参数。 sequence模型，序列长度要选取合适。不损失太多信息。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"classification","slug":"classification","permalink":"https://racleray.github.io/tags/classification/"},{"name":"fine-grained","slug":"fine-grained","permalink":"https://racleray.github.io/tags/fine-grained/"}]},{"title":"文本分类深度学习方法","slug":"文本分类深度学习方法","date":"2020-07-06T16:34:54.000Z","updated":"2023-08-07T11:54:31.044Z","comments":true,"path":"posts/bc7ffed8.html","link":"","permalink":"https://racleray.github.io/posts/bc7ffed8.html","excerpt":"整理CNN、RNN、fastText等模型在文本分类中的应用。","text":"fastText ​ 官方Git ​ fastText是Facebook AI Research在16年开源的一个文本分类器。 其特点就是fast。相对于其它文本分类模型，如SVM，Logistic Regression和neural network等模型，fastText在保持分类效果的同时，大大缩短了训练时间。 适合大型数据+高效的训练速度：在使用标准多核CPU的情况下10分钟内处理超过10亿个词 支持多语言表达：利用其语言形态结构，fastText能够被设计用来支持包括英语、德语、西班牙语、法语以及捷克语等多种语言。 fastText专注于文本分类，在许多标准问题上有较好的表现（例如文本倾向性分析或标签预测）。 ​ fastText 模型输入一个词的序列（一段文本或者一句话)，输出这个词序列属于不同类别的概率。 ​ 序列中的词和词组组成特征向量，特征向量通过线性变换映射到中间层，中间层再映射到标签。 fastText 在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。 ​ fastText 模型架构和 Word2Vec 中的 CBOW 模型很类似。不同之处在于，fastText 预测标签，而 CBOW 模型预测中间词。 ​ 基本框架： 字符n-gram ​ 将输入序列（一整句话，而不是CBOW的窗口输入），进行字符n-gram。n-gram一定程度上克服了CBOW这类bag of words模型无视了语序的缺点。 ​ 罕见词仍然可以被分解成字符n-gram。 123apple &gt;&gt;&gt; “&lt;ap”, “app”, “ppl”, “ple”, “le&gt;” &lt;表示前缀，&gt;表示后缀 ​ 隐藏表征在不同类别所有分类器中进行共享，使得文本信息在不同类别中能够共同使用。 层次 Softmax 分类 ​ 基本思想是使用树的层级结构替代扁平化的标准Softmax，使得在计算 P(y=j) 时，只需计算一条路径上的所有节点的概率值，无需在意其它的节点。 ​ 层次 Softmax的原理在文本表示笔记部分，在word2vec的优化部分进行了说明。 ​ 一般情况下，使用fastText进行文本分类的同时也会产生词的embedding，即embedding是fastText分类的产物。 ​ 用单词的embedding叠加获得的文档向量，词向量的重要特点就是向量的距离可以用来衡量单词间的语义相似程度，于是，在fastText模型中，这两段同类文本的向量应该是相似的。 CNN ​ 卷积神经网络经常用来处理具有类似网格拓扑结构（grid-like topology）的数据。 ​ 应用于文本处理很简单。 RNN \\[ h_t=f(x_t,h_{t-1})=\\sigma(W_{xh}x_t+W_{hh}h_{t-1}+b_h) \\] ​ 其中\\(W_{xh}\\)是输入到隐层的矩阵参数，\\(W_{hh}\\)是隐层到隐层的矩阵参数，\\(b_h\\)为隐层的偏置向量（bias）参数，\\(\\sigma\\)为\\(sigmoid\\)函数。 ​ 一般提取最后一个时刻的隐层状态作为句子表示进而使用分类模型。 LSTM ​ LSTM增加了记忆单元𝑐、输入门𝑖、遗忘门𝑓及输出门𝑜。这些门及记忆单元组合起来大大提升了循环神经网络处理长序列数据的能力。 \\(i_t = \\sigma{(W_{xi}x_t+W_{hi}h_{t-1}+b_i)}\\) $ f_t = (W_{xf}x_t+W_{hf}h_{t-1}+b_f) $ $ c_t = f_t c_{t-1}+i_t tanh(W_{xc}x_t+W_{hc}h_{t-1}+b_c) $ $o_t = (W_{xo}x_t+W_{ho}h_{t-1}+b_o) $ $ h_t = o_t tanh(c_t) $ ​ 其中，\\(i_t, f_t, c_t, o_t\\)分别表示输入门，遗忘门，记忆单元及输出门的向量值，带角标的\\(W\\)及\\(b\\)为模型参数，\\(tanh\\)为双曲正切函数，\\(\\odot\\)表示逐元素（elementwise）的乘法操作。 ​ 输入门控制着新输入进入记忆单元\\(c\\)的强度，遗忘门控制着记忆单元维持上一时刻值的强度，输出门控制着输出记忆单元的强度。 ​ 三种门的计算方式类似，但有着完全不同的参数，它们各自以不同的方式控制着记忆单元\\(c\\)： Forget Gate： Input Gate： 更新Cell state： Output Gate： ​ 图中带有方块的线，没有在计算中直接公式求解，s即cell state，通过hidden state的计算，从而进行信息传递。详细内容查找神经网络笔记。 ​ GRU 将忘记门和输入门合并成为一个单一的更新门 同时合并了数据单元状态和隐藏状态 结构比LSTM的结构更加简单 RCNN ​ ​ 卷积层建立在一个BiRNN模型之上，通过正向和反向循环来构造一个单词的下文和上文，然后输入CNN，如下式： ​ 得到单词的上下文表示之后，用拼接的方式来表示这个单词. ​ 然后通过max pooling层和全连接层，得到最后的输出。这一部分相当于文本表示的学习。 ​ 将该词向量放入一个单层神经网络中，得到所谓的潜语义向量（latent semantic vector），这里卷积层的计算结束了，时间复杂度仍是O(n)。接下来进行池化层，这里采用max-pooling可以将向量中最大的特征提取出来，从而获取到整个文本的信息。池化过程时间复杂度也是O(n)，所以整个模型的时间复杂度是O(n)。得到文本特征向量之后，进行分类。 Quasi-RNN ​ 框图显示了QRNN的计算结构与典型值的比较LSTM和CNN架构。 红色表示卷积或矩阵乘法； 连续的块意味着这些计算可以并行进行。 蓝色表示无参数功能沿通道/特征维度并行运行的对象。 LSTM可以分解为（红色）线性块和（蓝色）element-wise部分，但每个时间步的计算仍取决于上一个时间步的结果。 ​ fo-Pool指的是以下公式，包括forget 和 output 的h计算方式 而上图中 ​","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"fastText","slug":"fastText","permalink":"https://racleray.github.io/tags/fastText/"},{"name":"CNN","slug":"CNN","permalink":"https://racleray.github.io/tags/CNN/"},{"name":"RNN","slug":"RNN","permalink":"https://racleray.github.io/tags/RNN/"}]},{"title":"文本分类传统机器学习方法","slug":"文本分类传统机器学习方法","date":"2020-07-06T16:25:53.000Z","updated":"2023-08-07T11:54:31.043Z","comments":true,"path":"posts/f4b85348.html","link":"","permalink":"https://racleray.github.io/posts/f4b85348.html","excerpt":"简要记录了朴素贝叶斯、逻辑回归、SVM等机器学习分类模型。","text":"朴素贝叶斯模型与中文文本分类 \\[ P(Y|X)=\\frac{P(X|Y)P(Y)}{P(X)} \\] \\[ P(Y,X) = P(Y|X)P(X)=P(X|Y)P(Y) \\] ​ 条件独立假设的一个缺陷是，失去了词语之间的顺序信息。这就相当于把所有的词汇扔进到一个袋子里随便搅和，贝叶斯都认为它们一样。因此这种情况也称作词袋模型(bag of words)。 优化 取对数 转换为权重，每个词一个重要度，而不是计数 选取top k关键词 分割样本，根据文本长度选择不同数量的关键词数量，文本长时选取多一些 位置权重：比如在标题中的关键词，权重大一些 Logistic Regression ​ Logistic 回归并非最强大的分类算法，它可以很容易地被更为复杂的算法所超越，另一个缺点是它高度依赖正确的数据表示。但是其计算效率是相对较高的。不能用 logistic 回归来解决非线性分类问题，因为它的决策边界是线性的。 SVM ​ 找到具有最小间隔的样本点，然后拟合出一个到这些样本点距离和最大的线段/平面。 ​ 目标函数： image ​ 优化问题可推导出：【过程见机器学习笔记】 image ​ 得到回归系数： image 实验 notebook","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"https://racleray.github.io/tags/machine-learning/"},{"name":"classification","slug":"classification","permalink":"https://racleray.github.io/tags/classification/"}]},{"title":"文本表示进阶","slug":"文本表示进阶","date":"2020-07-06T15:29:30.000Z","updated":"2023-08-07T11:54:31.046Z","comments":true,"path":"posts/ec2b8b1f.html","link":"","permalink":"https://racleray.github.io/posts/ec2b8b1f.html","excerpt":"记录预训练语言模型的一些总结，包括从ELMo到BERT再到XLNet的小结。没有细化，有点冗长。","text":"文本表示进阶 image ​ 预训练过程是做图像或者视频领域的一种比较常规的做法，这种做法很有效，能明显促进应用的效果。 ​ 两种做法，一种是浅层加载的参数在训练C任务过程中不动，这种方法被称为“Frozen”;另外一种是底层网络参数尽管被初始化了，在C任务训练过程中仍然随着训练的进程不断改变，这种一般叫“Fine-Tuning”。 image ​ 用ImageNet来做网络的预训练，主要有两点，一方面ImageNet是图像领域里有很多事先标注好训练数据的数据集合，是个很大的优势，量越大训练出的参数越靠谱；另外一方面因为ImageNet有1000类，类别多，所以通用性好。 ​ NLP相对图像的特点在于，词作为NLP的基本要素，比像素的抽象程度更高，已经加入了人类数万年进化而来的抽象经验。 ​ 预训练语言模型的优势在于： 近乎无限量的优质数据 无需人工标注 一次学习多次复用 学习到的表征可在多个任务中进行快速迁移 ​ word2vec的问题：Word Embedding本质上是个静态的。不论新句子上下文单词是什么，这个单词的Word Embedding不会跟着上下文场景的变化而改变。这就是问题所在，多义性的消失。 ELMo：Embedding from Language Models ​ ELMo采用了典型的两阶段过程， 第一个阶段是利用语言模型进行预训练； 第二个阶段是在做下游任务时，从预训练网络中提取对应单词的网络各层的Word Embedding作为新特征补充到下游任务中。 image 结构 ​ ELMo 为了利用无标记数据，使用了语言模型： ​ 基本框架是一个双层的 Bi-LSTM，不过在第一层和第二层之间加入了一个残差结构（一般来说，残差结构能让训练过程更稳定）。做预训练的时候，ELMo 的训练目标函数为: \\[ \\sum_{k=1}^{N} \\log p\\left(t_{k} | t_{1}, \\ldots, t_{k-1}\\right)+\\log p\\left(t_{k} | t_{k+1}, \\ldots, t_{N}\\right) \\] ​ Bi-LSTM，一组正向，一组反向。\\(t_k\\)之前的单词序列Context-before称为上文，之后的单词序列Context-after称为下文。 image 输入层和输出层改进 ​ ELMo 借鉴了 2016 年 Google Brain 的 Rafal Jozefowicz 等人发表的 Exploring the Limits of Language Modeling。输入层和输出层不再是 word，而是变为了一个 char-based CNN 结构。 输出层： ​ 将CBOW中的普通softmax: \\[ P\\left(w_{t} | c_{t}\\right)=\\frac{\\exp \\left(e^{\\prime}\\left(w_{t}\\right)^{T} x\\right)}{\\sum_{i=1}^{|V|} \\exp \\left(e^{\\prime}\\left(w_{i}\\right)^{T} x\\right)}, x=\\sum_{i \\in c} e\\left(w_{i}\\right) \\] ​ 转换为： \\[ p\\left(t_{k} | t_{1}, \\ldots, t_{k-1}\\right)=\\frac{\\exp \\left(C N N\\left(t_{k}\\right)^{T} h\\right)}{\\sum_{i=1}^{|V|} \\exp \\left(C N N\\left(t_{i}\\right)^{T} h\\right)}, h=L S T M\\left(t_{k} | t_{1}, \\ldots, t_{k-1}\\right) \\] ​ char-based CNN 模型是现成已有的，对于任意一个目标词都可以得到一个向量表示 CNN(\\(t_k\\)) 。利用 CNN 解决有三点优势: CNN 能减少做 Softmax 时全连接层中的必须要有的 |V|* h 的参数规模，只需保持 CNN 内部的参数大小即可。 (PS: 卷积核参数共享) CNN 可以解决 OOV （Out-of-Vocabulary）问题，这个在翻译问题中尤其头疼 在预测阶段，CNN 对于每一个词向量的计算可以预先做好，更能够减轻 inference 阶段的计算压力。 输入层： ​ 相似结构，不同输出。训练时间会略微增加，因为原来的 look-up 操作可以做到更快一些。 ​ 句子中每个单词都能得到对应的三个Embedding: ​ 最底层是单词的Word Embedding，往上走是第一层双向LSTM中对应单词位置的Embedding，这层编码单词的句法信息更多一些；再往上走是第二层LSTM中对应单词位置的Embedding，这层编码单词的语义信息更多一些。 结果： ​ 有三层的词向量可以利用： 输入层 CNN 的输出，即是 LSTM 的输入向量， 第一层 LSTM 的hidden state：这层编码单词的句法信息更多一些 第二层的hidden state：这层编码单词的语义信息更多一些 LSTM 是双向的，因此对于任意一个词，如果 LSTM 的层数为 L 的话，总共可获得的向量个数为 2L+1。 ​ 对于每一个词，可以根据下面的式子得到它的向量，其中 γ 是一个 scale 因子，加入这个因子主要是想将 ELMo 的向量与具体任务的向量分布拉平到同一个分布水平： \\[ \\mathbf{E} \\mathbf{L} \\mathbf{M} \\mathbf{o} k^{t a s k}=\\gamma^{t a s k} \\sum j= s_{j}^{t a s k} \\mathbf{h}_{k, j} \\] \\(\\mathbf{h}_{k, j}\\)便是针对每一层的输出向量，利用一个 softmax 的参数来学习不同层的权值参数\\(s_{j}^{t a s k}\\)，因为不同任务需要的词语意义粒度也不一致，一般认为浅层的表征比较倾向于句法，而高层输出的向量比较倾向于语义信息。 因此通过一个 softmax 的结构让任务自动去学习各层之间的权重。 计算复杂度 ​ 基于传统统计的 N-gram 还是普通神经网络的 NNLM 结构，都会有一个很严重的问题，那就是计算复杂度随着上下文窗口 N 大小的增大急剧上升。 N-gram 是指数上升；NNLM 是以 |d| × N 的形式增加。 ​ CBOW 和 Skip-gram 以及再后来的 GloVe 终于做到了计算复杂度与所选窗口大小无关，BUT只是预测单个词的计算时间复杂度，如果是求整个输入序列的话，还是避免不了要与序列长度相关。 ​ 这几种方法（N-gram, ..., GloVe），它们都受限于所使用的模型表征能力，某种意义上都只能得到比较偏上下文共现意义上的词向量，并且也很少考虑过词序对于词的意义的影响。 ​ ​ RNN 结构的计算复杂度： 纵向上主要是 RNN 结构本身的时间复杂度 RNN 结构内部的 hidden state 维度 模型结构的复杂度 在 ELMo 中的话还跟词典大小相关（因为最后一层还是一个词典大小上的分类问题，以及输入也需要维护一个词典大小的 loop up 操作） 但是在机器性能提升的情况下，这一部分至少不是阻碍词向量技术发展的最关键的因素了 横向上的计算复杂度，就主要是受制于输入序列的长度 RNN 结构本身因为在时间序列上共享参数，其自身计算复杂度这一部分不变 输入序列长度 从词向量到句子向量 无监督句子表示：将句子表示成定长向量 基线模型：word2vec 模型：AE(Auto Encoder)，LM(language model)，Skip-Thoughts等 本身的信息 上下文的信息 任务的信息 PV-DM 和 PV-DBOW ​ PV-DM 的全称是 Distributed Memory Model of Paragraph Vectors： ​ 类似CBOW，输入=&gt;&gt;文档向量+上下文向量；输出=&gt;&gt;下一个词向量。有新文档需要再走一遍训练流程 ​ PV-DBOW 的全称则是 Distributed Bag of Words version of Paragraph Vector ​ 和 Skip-gram 类似，通过文档来预测文档内的词，训练的时候，随机采样一些文本片段，然后再从这个片段中采样一个词，让 PV-DBOW 模型来预测这个词。 From Mikolov et al. experiment, PV-DM is consistently better than PV-DBOW. Concatenation way is often better than sum/ average. image ​ 问题：很难去表征词语之间的更丰富的语义结构 Skip-thoughts ​ Skip-thoughts 直接在句子间进行预测，也就是将 Skip-gram 中以词为基本单位，替换成了以句子为基本单位 ​ 具体做法就是选定一个窗口，遍历其中的句子，然后分别利用当前句子去预测和输出它的上一句和下一句 对于句子的建模利用的 RNN 的 sequence 结构，预测上一个和下一个句子时候，也是利用的一个 sequence 的 RNN 来生成句子中的每一个词，所以这个结构本质上就是一个 Encoder-Decoder 框架，只不过和普通框架不一样的是，Skip-thoughts 有两个 Decoder。 image Quick-thoughts ​ 解决Skip-thoughts中RNN训练太慢的问题 ​ 把 Skip-thoughts 的生成任务改进成为了一个分类任务，具体说来就是把同一个上下文窗口中的句子对标记为正例，把不是出现在同一个上下文窗口中的句子对标记为负例，并将这些句子对输入模型，让模型判断这些句子对是否是同一个上下文窗口中，很明显，这是一个分类任务。 image InferSent ​ 思想特别简单，先设计一个模型在斯坦福的 SNLI（Stanford Natural Language Inference）数据集上训练，而后将训练好的模型当做特征提取器，以此来获得一个句子的向量表示，再将这个句子的表示应用在新的分类任务上，来评估句子向量的优劣。 ​ 进行多任务学习，不同任务使得模型学习到不同特征的提取能力。 General Purpose Sentence Representation ​ 有很多相似的研究： ​ Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning，就提出了利用四种不同的监督任务来联合学习句子的表征，这四种任务分别是：Natural Language Inference，Skip-thougts，Neural Machine Translation 以及 Constituency Parsing 等 ​ 训练结束后，将模型的输出作为句子的表征（或者把这个联合学习的模型作为特征提取器），然后直接在这个表征上接上非常简单的全连接层做分类器，并且同时保证最底层的特征提取器中参数不动（也就是只把它当做特征提取器），再在新的分类任务上做训练（只训练最后接上的全连接层分类器），最后根据训练出来的简单分类器在各自分类任务的测试集上做评估。 Universal Sentence Encoder ​ 思路类似General Purpose Sentence Representation，只不过作者提出了利用 Transformer 和 DAN（上文提到过的和 CBOW 相似， *Deep Unordered Composition Rivals Syntactic Methods for Text Classification*）两种框架作为句子的 Encoder。 Transformer 结构更为复杂，参数更多，训练也相对比较耗时，但是一般来说效果会更好一些。 DAN 结构简单，只有两个隐藏层（甚至可以减小为只需要一个隐藏层），参数比较少，训练相对比较省时省资源，但是一般来说效果会差一些（并不是绝对，论文中也发现某些场景下 DAN 的效果甚至更好）。 ​ 作者既在无标记数据上训练，也在监督数据上训练，最后在十个分类任务上进行迁移学习的评估。 ​ 作者还放出了他们预训练好的 Encoder，可以供迁移学习的句子特征提取器使用。 预训练 Encoder：https://tfhub.dev/google/universal-sentence-encoder/2 ULMFit ​ Universal Language Model Fine-tuning for Text Classification 中，提出了ULMFit 结构，其实这本质上是他们提出的一个方法，而不是具体的某种结构或模型。主要应用于文本分类问题中。 ​ ULMFiT 最终在分类任务上表现惊艳，尤其是只需要 100 个标记数据，就能够学习到一个表现非常 comparable 的分类器。 ​ 和ELMo基本思路类似，也是预训练完成后在具体任务上进行 finetune，但不同之处也有很多。 ​ 分为三个阶段： 大规模预训练 任务数据预训练 接任务模型部分再次finetune ​ Averaged SGD ​ Averaged SGD 是指先将模型训练到一定 epoch，然后再将其后的每一轮权值进行平均后，得到最终的权值。 \\[ w_{k+1}=w_{k}-\\gamma_{k} \\nabla f\\left(w_{k}\\right) \\] 变成 \\[ w=\\frac{1}{K-T+1} \\sum_{i=T}^{K} w_{i} \\] ​ T 是一个阈值，而 K 是总共的迭代次数，这个式子的意思就是把迭代到第 T 次之后，对该参数在其后的第 T 轮到最后一轮之间的所有值求平均，从而得到最后模型的该参数值。 DropConnect ​ LSTM 上一个时刻和下一个时刻之间的隐藏层之间是有连接的，并且这个连接通过一个全连接的矩阵相连，而这个模型则用了 DropConnect 的方法随机 drop 掉一些连接，从而减少了一些过拟合的风险，当然在输入层到隐藏层之间也有正常的 dropout 操作。 微调的技巧(两次 finetune) 1. discriminative fine-tune ​ 不同层在训练更新参数的时候，赋予不同的学习率。 ​ 不同层的表征有不同的物理含义，比如浅层偏句法信息，高层偏语义信息，因此对于不同层的学习率不同。 \\[ \\theta_{t}^{l}=\\theta_{t-1}^{l}+\\eta^{l} \\nabla_{\\theta^{l}} J(\\theta) \\] \\[ \\eta^{l-1}=\\frac{\\eta^{l}}{2.6} \\] 2. slanted triangular learning rates 在 finetune 的第一阶段，希望能够先稳定住原来已经在大规模语料集上预训练好的参数， 选择比较小的 finetune 学习率 后逐步加大学习率，使得学习过程能够尽量快速。 当训练接近尾声时，逐步减小学习率，这样让模型逐渐平稳收敛。 计算： image T -- the number of training iterations； cut_frac -- the fraction of iterations we increase lr； cut -- the iteration when we switch fromincreasing to decreasing the lr; p -- the fraction ofthe number of iterations we have increased or willdecrease the LR respectively; ratio -- specifies how much smaller the lowest lr is from the maximum lr \\(η_{max}\\) 一般取：cut_frac= 0.1, ratio= 32 and \\(η_{max}\\)= 0.01 image gradual unfreezing ​ 预训练模型在新任务上 finetune 时，逐层解冻模型，先 finetune 最后一层，然后再解冻倒数第二层，把倒数第二层和最后一层一起 finetune，然后再解冻第三层。以此类推，逐层往浅层推进，最终 finetune 整个模型或者终止到某个中间层。这样做的目的也是为了 finetune 过程能够更平稳。 Transformer ​ 因为 Self-attention 的存在，才使得 Transformer 在做类似翻译问题的时候，可以让其 Encoder 不用做序列输入，而是将整个序列一次全输入，并且超长序列的输入也变得可能。而具体到 Self-attention 中，可以用下图表示。优质Blog image ​ Self-attention 中的多头机制便是将这样的操作分别进行多次，让句子的表征充分学习到不同的侧重点，最终将这些多头学习出来的表征 concat 到一起，然后再同一个全连接网络，便可以得到这个句子最终 Self-attention 下新的表示。 ​ 训练时： Decoder 中的输入可以用矩阵形式一次完成当前整个序列的 decode 过程，因为 ground truth 已经提前知道，只需做好每个词的 mask 就好 ​ inference 的时候：Decoder 必须按照序列输入，因为在生成每一个词的时候，必须先生成它的前一个词，无法一次将整个序列全部生成 Decoder 的 attention 实际包含两部分： 第一部分是带有 mask 的 Self-attention，通过 mask 将 decode 阶段的 attention 限定只会 attention 到已经生成过的词上，因此叫做 Mask Self-attention。 第二部分是普通的 Self-attention 操作，不过这时的 K 和 V 矩阵已经替换为 Encoder 的输出结果，所以本质上并非一个 Self-attention。 image ​ 结构展示： ​ Code GPT ​ GPT 使用的 Transformer 是只用了 Decoder，因为对于语言模型来讲，确实不需要 Encoder 的存在。 image ​ 要做超长的序列输入（可以长达 11000 个词），为了能够高效节省时间和内存的处理如此长的序列，做了一些 Memory-Compressed 工作，主要是两方面： 通过 CNN 操作，把 K 和 V 压缩到序列长度更小的一个矩阵，同时保持 Q 不变，这样也能在相当程度上减少计算量 把一个 batch 内部的序列按长度进行分组，然后分别在每个组内部进行 self-attention 操作，避免将一些很短的句子也 padding 到整个语料的最大长度； image ​ 利用语言模型的目标函数预训练完成后，便可以在具体任务上进行 finetune，和 ULMFiT 中的 finetune 分为两个阶段的方法不一样的是，GPT 直接把这两个过程糅合到一个目标函数中： \\[ L_{3}(C)=L_{2}(C)+\\lambda L_{1}(C) \\] ​ 其中 L2 是 task-specific 的目标函数， L1 则是语言模型的目标函数。论文中说这种联合学习方式能够让训练效果更好。 改造任务类型： 分类问题：直接在原序列的开始和末尾添加表示开始和末尾的符号， Text Entailment 问题：将 Premise 和 Hypothesis 通过一个中间分隔符“$”连接起来成为一个序列，然后同样在开头和末尾添加标记符号。 文本相似问题：因为序列 1 和序列 2 没有先后关系，因此将先后关系相反的两个序列作为输入。 Question Aswering ：将 query 和每个候选的 answer 都分别连接成一个序列作为输入，最后按各自的打分进行排序。 image ​ 对输入数据结构进行一定处理。 BERT ​ Bidirectional Encoder Representation from Transformers，进一步完善和扩展了 GPT 中设计的通用任务框架，使得 BERT 能够支持包括：句子对分类任务、单句子分类任务、阅读理解任务和序列标注任务。 模型特点 1. 利用了真双向的 Transformer Encoder 中用了 Self-attention 机制，而这个机制会将每一个词在整个输入序列中进行加权求和得到新的表征 更多的 transformer 的 block（意味着经过更多 Self-attention），那么互相交融的程度将会更高（Base 模型是 12层，Large 模型是 24层） Large 版本 BERT 的多头机制中 Head 个数多达 16 个，多种关系的学习 ELMo 与 GPT 本质上还是一个单向的模型，ELMo 稍微好一点，将两个单向模型的信息 concat起 来。GPT 则只用了单向模型， Decdoer 的天生基因决定的。显然句子中有的单词的语义会同时依赖于它左右两侧的某些词，仅仅从单方向做encoding是不能描述清楚的。 2. Mask-LM (Mask-Language Model) ​ 将单向预测的LM改变为双向的LM，预测目标变为什么？ ​ 为了利用双向信息，改进了普通语言模型成为完形填空式的 Mask-LM (Mask-Language Model)，随机选取15%的词进行Mask，然后预测。 输入序列依然和普通Transformer保持一致，只不过把挖掉的一个词用\"[MASK]\"替换 输出层在被挖掉的词位置，接一个分类层做词典大小上的分类问题，得到被 mask 掉的词概率大小 ​ BERT 针对如何做“[MASK]”，做了一些更深入的研究，它做了如下处理： 选取语料中所有词的 15% 进行随机 mask； 选中的词在 80% 的概率下被真实 mask； 选中的词在 10% 的概率下不做 mask，而被随机替换成其他一个词； 选中的词在 10% 的概率下不做 mask，仍然保留原来真实的词。 3. Next Sentence Prediction 任务学习句子级别信息 ​ 具体做法则是将两个句子组合成一个序列，组合方式会按照下面将要介绍的方式，然后让模型预测这两个句子是否为先后近邻的两个句子，也就是会把\"Next Sentence Prediction\"问题建模成为一个二分类问题。 ​ 句子级负采样： ​ 训练的时候，数据中有 50% 的情况这两个句子是先后关系，而另外 50% 的情况下，这两个句子是随机从语料中凑到一起的，也就是不具备先后关系，以此来构造训练数据。 ​ Multi-task: ​ 在预训练阶段，因为有两个任务需要训练：Mask-LM 和 Next Sentence Prediction 输入表示 ​ 起始标记都用“[CLS]”来表示，结束标记符用\"[SEP]\"表示，对于两个句子的输入情况，除了起始标记和结束标记之外，两个句子间通过\"[SEP]\"来进行区分。 用两个向量表示当前是句子 A 或句子 B 的。引入了“segment embedding”的概念来区分句子。 引入序列中词的位置信息，也用了 position embedding。和Transformer的sin、cos函数编码不同，直接去训练了一个position embedding。给每个位置词一个随机初始化的词向量，再训练。 [CLS]作为句子/句对的表示是直接跟分类器的输出层连接的。 下游任务 ​ NLP的四大任务： 对于单序列文本分类任务和序列对的文本分类任务使用框架基本一致，利用 Encoder 最后一层的第一个时刻“[CLS]”对应的输出作为分类器的输入 对于 SQuAD 1.1 任务来说，需要在给定段落中找到正确答案所在区间，这段区间通过一个起始符与终止符来进行标记 序列标注任务上进行 finetune，对于序列中的每个 token 而言，实际上就是一个分类任务。和前面提到的普通分类任务不一样的是，这里的分类需要针对序列中的每个词做分类，参数增加在 H × K ，这里的 K 是序列标注中标注的种类个数。 对于 SWAG 任务来讲，因为需要在给定一个句子后，从四个候选句子中选择一个最有可能是该句子的下一个句子，这里面通常包含了一些常识推理。将前置句子和四个候选句子分别进行组合成一个句子对, 给每一个候选句子进行打分，从而得到四个候选句子中最有可能是下一个的句子。 image ​ 对比参数及训练 image XLNet ​ 在两阶段新模式（预训练+Finetuning）下，应该会有更多的好工作涌现出来。根本原因在于：这个模式的潜力还没有被充分挖掘，貌似还有很大的提升空间。 ​ XLNet引入了自回归语言模型以及自编码语言模型的方法。 自回归语言模型（Autoregressive LM） ​ 自左向右预测下一个词的语言模型任务，或者反过来自右向左，这种类型的LM被称为自回归语言模型。 ​ GPT 就是典型的自回归语言模型。ELMo是分别有两个方向的自回归LM，然后把LSTM的两个方向的隐节点状态拼接到一起，体现双向语言模型。其实是两个自回归语言模型的拼接，本质上仍然是自回归语言模型。 优点 下游NLP任务有关，比如生成类NLP任务，文本摘要，机器翻译等，在实际生成内容的时候，就是从左向右的，自回归语言模型天然匹配这个过程。 Bert这种DAE模式，在生成类NLP任务中，就面临训练过程和推断过程（没有Mask）不一致的问题，导致生成类的NLP任务到目前为止都做不太好。 PS： DAE（DA Enhanced），Denoising Autoencoder：那些被Mask掉的单词就是在输入侧加入的所谓噪音。类似Bert这种预训练模式，被称为DAE LM AoA, 层叠式注意力机制（Attention-over-Attention） 缺点 ​ 只能利用上文或者下文的信息，不能同时利用上文和下文的信息，但是这在sequence inference这类任务中却更符合实际。ELMo这种双向都做，因为融合模式过于简单，所以效果其实并不是太好。 ​ GPT 2.0的作者却坚持沿用GPT 1.0 单向语言模型的旧瓶，装进去了更高质量更大规模预训练数据的新酒。 image ​ 而它的实验结果也说明了，如果想改善预训练语言模型，走这条扩充预序列模型训练数据的路子，是个多快好但是不省钱的方向。 自编码语言模型（Autoencoder LM） ​ DAE LM的优缺点正好和自回归LM反过来，它能比较自然地融入双向语言模型。 ​ Bert的缺点，主要在输入侧引入[Mask]标记，导致预训练阶段和Fine-tuning阶段不一致的问题，因为Fine-tuning阶段是看不到[Mask]标记的。 ​ XLNet的出发点就是： 能否融合自回归LM和DAE LM两者的优点。 另外一个是，Bert在第一个预训练阶段，假设句子中多个单词被Mask掉，这些被Mask掉的单词之间没有任何关系，是条件独立的，而有时候这些单词之间是有关系的，XLNet则考虑了这种关系 ​ XLNet共有三个因素： Permutation Language Model(简称PLM)：将双向信息学习方式，由预测[Mask]，变成对序列进行全排列，根据每种可能排序由前t - 1个词预测第t个词。来融入双向语言模型。 引入了Transformer-XL：相对位置编码以及分段RNN机制。相对位置编码关注相对位置偏差。分段RNN机制，将长文本划分为较短的文本输入transformer，将transformer单元作为RNN的cell，进行RNN方式的序列运算连接。 增加了预训练阶段使用的数据规模：Bert使用的预训练数据是BooksCorpus和英文Wiki数据，大小13G。XLNet除了使用这些数据外，另外引入了Giga5，ClueWeb以及Common Crawl数据，并排掉了其中的一些低质量数据，大小分别是16G,19G和78G。可以看出，在预训练阶段极大扩充了数据规模，并对质量进行了筛选过滤。 ​ 对于长文档的应用，Bert因为Transformer天然对长文档任务处理有弱点，所以XLNet对于长文档NLP任务相比Bert应该有直接且比较明显的性能提升作用，它在论文中也证明了这点。 总结 ​ 如何使用这些预训练好的模型。一般来说，可以有三种方式来使用： 将预训练模型当做一个特征提取器，直接将预训练模型的输出层去掉，然后使用去掉输出层之后的最后一层输出作为特征，输入到我们自己精心设计好的 Task-specific 模型中去。 在训练过程中，作为特征提取器的部分（比如 BERT Encoder）的参数是不变的。 将预训练模型整体接入 Task-specific 模型，继而重新在新的数据集上整体重新训练。 当然训练技巧可以有很多种，比如 ULMFiT 的三角学习率和逐层解冻或者是 Transformer 的 warmup 策略（上文都有提到），这些训练技巧非常重要，需要好好把控，否则很容易学崩了，甚至让原有预训练语言模型的优势都被新的 finetune 抹去了，因此需要实验设计一个比较好的 finetune 策略。 保留预训练模型的一部分，另外一部分则和 Task-specific 模型一起 finetune。 训练数据不算太多的情况，这个时候一方面要保证预训练模型在大规模语料上曾经学习到的表征，另一方面因为又要做新数据下的迁移，但是数据量比较少，重新 finetune 整个模型可能不太合适，容易导致模型的鲁棒性不高，那么似乎选择最后的一些层进行选择性的 finetune 会是比较好的方案 ​ 以 BERT 为代表的模型，简单粗暴，与人类语言习得过程中的轻量、泛化和低功耗截然相反。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"BERT","slug":"BERT","permalink":"https://racleray.github.io/tags/BERT/"},{"name":"representation","slug":"representation","permalink":"https://racleray.github.io/tags/representation/"},{"name":"pretrained LM","slug":"pretrained-LM","permalink":"https://racleray.github.io/tags/pretrained-LM/"},{"name":"XLNet","slug":"XLNet","permalink":"https://racleray.github.io/tags/XLNet/"}]},{"title":"文本表示","slug":"文本表示","date":"2020-07-06T15:11:07.000Z","updated":"2023-08-07T11:54:31.045Z","comments":true,"path":"posts/2af352b4.html","link":"","permalink":"https://racleray.github.io/posts/2af352b4.html","excerpt":"计算机做不到直接对文本字符串进行语义理解和表示，因此需要进行数值化或者向量化。良好的文本表示形式可以极大的提升机器学习算法效果。记录一下常见的文本表示方法。","text":"文本表示 ​ 计算机做不到直接对文本字符串进行语义理解和表示，因此需要进行数值化或者向量化。良好的文本表示形式可以极大的提升机器学习算法效果。 1.表示方法 离散表示 one-hot表示 multi-hot表示 分布式表示 基于矩阵 基于降维的方法 基于聚类的方法 基于神经网络 CBOW Skip-gram NNLM C&amp;W 2.文本离散表示 bag of words ​ 将字符串视为一个 “装满字符（词）的袋子” ，袋子里的 词语是随便摆放的。 ​ [1,0,1,1,1,0,0,1,…] -- 向量的每个维度唯一对应着词表中的一个词。可见这个向量的大部分位置是0值，这种情况叫作“稀疏”。为了减少存储空间，我们也可以只储存非零值的位置。 优缺点 优点： 简单，方便，快速 在语料充足的前提下，对于简单的自然语言处理任务效果不错。如文本分类。 缺点： 其准确率往往比较低。凡是出现在文本中的词一视同仁，不能体现不同词在一句话中的不同的重要性。 无法关注词语之间的顺序关系，这是词袋子模型最大的缺点。如“武松打老虎”跟“老虎打武松”在词袋子模型中是认为一样的。 TF-IDF ​ 词频——TF（term frequency）：在当前文档中的词频 ​ 统计逆文档频率——IDF：基本假设是如果一个词语在不同的文档中反复出现，那么它对于识别该文本并不重要。 ​ \\[-log({出现该词语的文档占总文档出现的频率})\\] ​ IDF可以进行平滑：假设存在一个文档，包含所有的词。计算IDF时，分子分母都加一。 3.文本分布式表示 ​ one-hot vector：假设我们的词库总共有n个词，那我们开一个1*n的高维向量。 ​ \\[ w^{aardcark}= \\begin{bmatrix} ​ 1 \\\\ ​ 0 \\\\ ​ 0 \\\\ ​ \\vdots \\\\ ​ 0 \\end{bmatrix} , w^{a}= \\begin{bmatrix} ​ 0 \\\\ ​ 1 \\\\ ​ 0 \\\\ ​ \\vdots \\\\ ​ 0 \\end{bmatrix}\\] ​ 向量没办法给我们任何形式的词组相似性权衡。例如: ​ \\[(w^{hotel})^Tw^{motel}=0\\] ​ 一个极高维度的空间，然后每个词语都会占据一个维度，因此没有办法在空间中关联起来。 基于SVD降维的表示方法 ​ 建立一个词组文档矩阵\\(X\\)，具体是这么做的：遍历海量的文件，每次词组i出现在文件j中时，将\\(X_{ij}\\)的值加1。这会是个很大的矩阵\\(R^{|V| ×M}\\)，而且矩阵大小还和文档个数M有关系。 基于窗口的共现矩阵X ​ 规定一个固定大小的滑动窗口，然后统计每个中心词所在窗口中相邻词的词频。 I enjoy flying. I like NLP. I like deep learning. 有： ​ 对X做奇异值分解，只保留前k个维度： ​ 把子矩阵\\(U_{1:|V|,1:k}\\)视作我们的词嵌入矩阵。也就是说，对于词表中的每一个词，我们都用一个k维的向量来表达了。 ​ 问题在于： 矩阵的维度会经常变化（新的词语经常会增加，语料库的大小也会随时变化）。 矩阵是非常稀疏的，因为大多数词并不同时出现。 矩阵的维度通常非常高（\\(≈10^6×10^6\\)） 训练需要\\(O(n^2)\\)的复杂度（比如SVD） 需要专门对矩阵X进行特殊处理，以应对词组频率的极度不平衡的状况 ​ 有一些办法可以缓解一下上述提到的问题： 忽视诸如“he”、“the” 、“has”等功能词。 应用“倾斜窗口”（ramp window），即:根据文件中词组之间的距离给它们的共现次数增加相应的权重。 使用皮尔森的相关性（Pearson correlation），将0记为负数，而不是它原来的数值。 基于神经网络的表示方法 ​ 如果数据量不足，不要从零开始训练自己的词向量 连续词袋模型（CBOW） 以上下文，预测中心词。 \\(w_i\\):单词表V中的第i个单词，i维是1其他维是0的one-hot向量 \\(v\\in R^{n*|V|}\\)：输入词矩阵 \\(v_i\\)：V的第i列，单词\\(w_i\\)的输入向量 \\(u\\in R^{|V|*n}\\)：输出词矩阵 \\(u_i\\)：U的第i行，单词\\(w_i\\)的输出向量 \\(n\\)：“嵌入空间”（embedding space）的维度 整个过程: 对于m个词长度的窗口，one-hot向量（\\(x^{(c-m)},\\cdots,x^{(c-1)},x^{(c+1)},\\cdots,x^{(c+m)}\\)）。 上下文的嵌入词向量（\\(v_{c-m+1}=Vx^{(c-m+1)},\\cdots, v_{c+m}=Vx^{(c+m)}\\) ） 将这些向量取平均\\(\\hat v={v_{c-m}+v_{c-m+1}+\\cdots+v_{c+m}\\over2m}\\) 产生一个logits向量 \\(z=U\\hat v\\) 将得分向量转换成概率分布形式\\(\\hat y=softmax(z)\\) \\(y\\)是 \\(x^{c}\\) 的one-hot向量。计算损失\\[H(\\hat y,y)=-\\sum_{j=1}^{|V|}y_jlog(\\hat y_j)\\] y只是一个one-hot向量，于是上面的损失函数就可以简化为： ​ \\[H(\\hat y,y)=-y_ilog(\\hat y_i)\\] 最终的优化目标为： 用梯度下降法去更新每一个相关的词向量𝑢𝑐和𝑣𝑗 Skip-Gram 以中心词预测上下文。 \\(w_i\\):单词表V中的第i个单词，i维是1其他维是0的one-hot向量 \\(v\\in R^{n*|V|}\\)：输入词矩阵 \\(v_i\\)：V的第i列，单词\\(w_i\\)的输入向量 \\(u\\in R^{|V|*n}\\)：输出词矩阵 \\(u_i\\)：U的第i行，单词\\(w_i\\)的输出向量 \\(n\\)：“嵌入空间”（embedding space）的维度 整个过程: 生成one-hot输入向量x。 得到上下文的嵌入词向量\\(v_c=Vx\\)。 不需要取平均值的操作，所以直接是\\(v_c\\)。 通过\\(u=Uv_c\\)产生2m个logits向量\\(u_{c-m},\\cdots,u_{c-1},u_{c+1},\\cdots,u_{(c+m)}\\)。 将logits向量转换成概率分布形式\\(y=softmax(u)\\)。 产生的概率分布与真实概率分布\\(y^{c-m},\\cdots,y^{c-1},,y^{c+1}\\cdots,y^{c+m}\\)计算交叉熵损失。 最终的优化目标为： 不同的地方是我们这里需要引入朴素贝叶斯假设来将联合概率拆分成独立概率相乘。 负例采样（Negative Sampling） ​ 对整个单词表|V|求和的计算量是非常巨大的，任何一个对目标函数的更新和求值操作都会有O(|V|)的时间复杂度。我们需要一个思路去简化一下，我们想办法去求它的近似。 ​ Mikolov ET AL.在他的《Distributed Representations of Words and Phrases and their Compositionality》中提出了负例采样。 ​ 考虑一个“词-上下文”对（w,c），令P(D = 1|w, c)为(w, c)来自于语料库的概率。相应的，P(D = 0|w, c) 则是不来自于语料库的概率。对P(D = 1|w, c)用sigmoid函数建模： ​ \\[p(D=1|w,c,\\theta)= {1\\over{1+e^{(-v_c^Tv_w)}}}\\] ​ 建立一个新的目标函数。如果(w, c)真是来自于语料库，目标函数能够最大化P(D = 1|w, c)。 ​ \\(\\tilde D\\)表示不来自于语料库的数据。 ​ 在skip gram中，对于\\(c - m + j\\)位置的context 和 center word 的目标函数为：（所有上下文还要求和） ​ K -- 为负例样本的个数。 ​ 这样将 \\(|V|\\) words中的softmax，变成了 K个负例中进行 sigmoid ，减少了计算量。多分类目标变为二分类目标。 ​ 在CBOW中为： ​ 负例采样在word的词频分布的3/4次方上进行采样。使得分布更平滑。 Hierarchical Softmax ​ 对CBOW或者Skip gram的最后一层损失求解方式改进。目标不是目标单词的one hot编码，而是在所有单词预先构建好的Huffam tree中的huffman编码。 ​ 损失函数由Huffam tree中每个node的sigmoid函数预测结果和实际的huffman编码之间计算求得。 ​ Huffman tree保证了任何一个单词的编码不会是另一个单词的前缀。 ​ 构建流程： 根据所有单词的词频构建最小堆。 取出前两个最小单词（left child and right child），将二者词频之和作为新节点插入最小堆。 构建哈夫曼树，建立新树节点作为left child and right child的父节点，将上一步的left child and right child二者词频之和作为的该父节点的值，构建tree。 重复2、3步骤，直到min heap中只有一个node，此时，将这一个node作为tree的root。Huffam tree构建完毕。 ​ huffman编码：从root到leaf node的路径，走left child编码加入0，走right child编码加入1。最终编码为该leaf node对应的编码。 ​ 越常用的词（词频越高的词）拥有更短的编码。 ​ word2vec中正好采用了相反的编码规则，规定沿着左子树走，那么就是负类(哈夫曼树编码1)，沿着右子树走，那么就是正类(哈夫曼树编码0)。 ​ 每个tree node处： ​ \\[p(+)= {1\\over{1+e^{(-v_w^T\\theta)}}}\\] ​ 每个tree node都有参数\\(\\theta\\)，输入是skip gram模型最终输出的向量\\(v_w\\)。 Glove ​ 加入了global statistics，用某个大小window中两个单词的共现次数\\(X_{ij}\\)表示。 目标函数由cross entropy变为least square。 用\\(log(X_{ij})\\)作为“normalization cost”。（\\(X_{ij}\\) : number of times word j occur in the context of word i） \\(v_i\\)和\\(u_j\\)相乘，不需要用指数函数。(推导结果) 加入weighted function \\(f(X_{ij})\\) 损失函数： \\[ J=\\sum_{i=1}^{V} \\sum_{j=1}^{V} f\\left(X_{i j}\\right)\\left(w_{i}^{T} w_{j}+b_{i}+b_{j}-\\log X_{i j}\\right)^{2} \\] \\(X_{ij}\\) --\\(i\\ \\textrm{and}\\ j\\)在某个窗口大小中的共现频率 \\(f(X_{ij})\\)--权重系数，共现越多的 pair 对于目标函数贡献应该越大，但是又不能无限制增大，所以对共现频率过于大的 pair 限定最大值，以防训练的时候被这些频率过大的 pair 主导了整个目标函数。 b --两个偏置项 \\(w_{i}\\) --当前词的向量， \\(w_{j}\\) --对应的是与其在同一个窗口中出现的共现词的词向量，两者的向量点乘要去尽量拟合它们共现频率的对数值 ​ 如果两个词共现频率越高，那么其对数值当然也越高，因而算法要求二者词向量的点乘也越大。 ​ 而两个词向量的点乘越大，其实包含了两层含义： 第一，要求各自词向量的模越大，通常来说，除去频率非常高的词（比如停用词），对于有明确语义的词来说，它们的词向量模长会随着词频增大而增大，因此两个词共现频率越大，要求各自词向量模长越大是有直觉意义的 第二，要求这两个词向量的夹角越小，这也是符合直觉的，因为出现在同一个语境下频率越大，说明这两个词的语义越接近，因而词向量的夹角也偏向于越小。 fastText ​ word2vec 和 GloVe 都不需要人工标记的监督数据，只需要语言内部存在的监督信号即可以完成训练。而与此相对应的，fastText 则是利用带有监督标记的文本分类数据完成训练。 ​ 类似CBOW，但不同点在于： 在输入数据上，CBOW 输入的是一段区间中除去目标词之外的所有其他词的向量加和或平均，而 fastText 为了利用更多的语序信息，将 bag-of-words 变成了 bag-of-features，也就是输入 x 不再仅仅是一个词，还可以加上字符级别的 bigram 或者是 trigram 的信息等等。 第二个不同在于，CBOW 预测目标是语境中的一个词，而 fastText 预测目标是当前这段输入文本的类别，正因为需要这个文本类别，因此才说 fastText 是一个监督模型。 ​ fastText 的网络结构和 CBOW 基本一致，同时在输出层的分类上也使用了 Hierachical Softmax 技巧来加速训练。 ​ 目标函数： \\[ -\\frac{1}{N} \\sum_{n=1}^{N} y_{n} \\log f\\left(B A x_{n}\\right) \\] \\[ x_{n}=\\sum_{i=1}^{l_{n}} x_{n, i} \\] \\(x_{n, i}\\) --语料当中第 n 篇文档的第 i 个（词 或者 char N-gram） A --最终可以获取的词向量信息 词向量的作用与获取 ​ 高阶的深度学习自然语言处理任务，都可以用词向量作为基础。可以从开源链接获取。 词向量的意义 基于词与其他词的某种共现关系 skip-gram with negative-sampling 与 PMI矩阵 的等价性证明《Neural-Word-Embeddings-as-Implicit-Matrix-Factorization》 神经网络与SVD的求解方法只是降维方式的不同 神经网络更像MF，而MF与SVD的降维的约束条件不同，神经网络的目标函数与MF的目标函数也不同 GloVe与MF关系更近 目标函数更像 CBOW没有类似的降维矩阵对应 基于语言模型 词向量与语言模型本来是两个独立的NLP问题领域，因为深度学习联系在了一起。 基于词向量构建成句子向量进而进而完成语言模型的任务 基于其他监督学习任务 词向量并不只是语言模型可以得到，基于有监督学习也可以得到：C&amp;W了解 基于词向量构建成句子向量进而完成文本分类或文本相似度判断的任务 4.关键词提取 TF-IDF文本关键词抽取方法 （1） 对于给定的文本D进行分词、词性标注和去除停用词等数据预处理操作。得到n个候选关键词，即D=[t1,t2,…,tn] ； （2） 计算词语ti 在文本D中的词频； （3） 计算词语ti 在整个语料的IDF=log (Dn /(Dt +1))，Dt 为语料库中词语ti 出现的文档个数； （4） 计算得到词语ti 的TF-IDF=TF*IDF，并重复（2）—（4）得到所有候选关键词的TF-IDF数值； （5） 对候选关键词计算结果进行倒序排列，得到排名前TopN个词汇作为文本关键词。 基于TextRank的文本关键词抽取方法 （1） 对于给定的文本D进行分词、词性标注和去除停用词等数据预处理操作。得到n个候选关键词，即D=[t1,t2,…,tn] ； （2） 构建候选关键词图G=(V,E)，其中V为节点集，由候选关键词组成，并采用共现关系构造任两点之间的边，两个节点之间仅当它们对应的词汇在长度为K的窗口中共现则存在边，K表示窗口大小即最多共现K个词汇； （3） 根据公式迭代计算各节点的权重，直至收敛；(见中文文本处理部分) （4） 对节点权重进行倒序排列，得到排名前TopN个词汇作为文本关键词。 基于Word2Vec词聚类的文本关键词抽取方法 （1） 对Wiki中文语料进行Word2vec模型训练，代码 （2） 对于给定的文本D进行分词、词性标注和去除停用词等数据预处理操作。得到n个候选关键词，即D=[t1,t2,…,tn] ； （3） 遍历候选关键词，从词向量文件中抽取候选关键词的词向量表示，即WV=[v1，v2，…，vm]； （4） 对候选关键词进行K-Means聚类，得到各个类别的聚类中心； （5） 计算各类别下，组内词语与聚类中心的距离（欧几里得距离），按聚类大小进行升序排序； （6） 对候选关键词计算结果得到排名前TopN个词汇作为文本关键词。 步骤（4）中需要人为给定聚类的个数，具体参考文档主题的个数。 123456789101112131415161718192021222324252627282930from sklearn.cluster import KMeansfrom sklearn.decomposition import PCAdef getkeywords_kmeans(data,topK): words = data[\"word\"] # 词汇 vecs = data.ix[:,1:] # 向量表示 kmeans = KMeans(n_clusters=1,random_state=10).fit(vecs) labels = kmeans.labels_ #类别结果标签 labels = pd.DataFrame(labels,columns=['label']) new_df = pd.concat([labels,vecs],axis=1) vec_center = kmeans.cluster_centers_ #聚类中心 # 计算距离（相似性） 采用欧几里得距离（欧式距离） distances = [] vec_words = np.array(vecs) # 候选关键词向量，dataFrame转array vec_center = vec_center[0] # 第一个类别聚类中心,本例只有一个类别 length = len(vec_center) # 向量维度 for index in range(len(vec_words)): # 候选关键词个数 cur_wordvec = vec_words[index] # 当前词语的词向量 dis = 0 # 向量距离 for index2 in range(length): dis += (vec_center[index2]-cur_wordvec[index2])*(vec_center[index2]-cur_wordvec[index2]) dis = math.sqrt(dis) distances.append(dis) distances = pd.DataFrame(distances,columns=['dis']) result = pd.concat([words, labels ,distances], axis=1) # 拼接词语与其对应中心点的距离 result = result.sort_values(by=\"dis\",ascending = True) # 按照距离大小进行升序排序 可使用PCA降维，但具体视效果而定。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"fastText","slug":"fastText","permalink":"https://racleray.github.io/tags/fastText/"},{"name":"word2vec","slug":"word2vec","permalink":"https://racleray.github.io/tags/word2vec/"},{"name":"representation","slug":"representation","permalink":"https://racleray.github.io/tags/representation/"}]},{"title":"语言模型","slug":"语言模型","date":"2020-07-06T14:59:57.000Z","updated":"2023-08-07T11:54:31.049Z","comments":true,"path":"posts/3423f471.html","link":"","permalink":"https://racleray.github.io/posts/3423f471.html","excerpt":"传统语言模型的相关概念记录。重点是几个平滑计数的思想，从拉普拉斯到Modified Kneser-Ney smoothing，设计得越来越复杂。","text":"自然语言处理是关于计算机科学和语言学的交叉学科，常见的研究任务包括： 分词（Word Segmentation或Word Breaker，WB） 信息抽取（Information Extraction，IE）：命名实体识别和关系抽取（Named Entity Recognition &amp; Relation Extraction，NER） 词性标注（Part Of Speech Tagging，POS） 指代消解（Coreference Resolution） 句法分析（Parsing） 词义消歧（Word Sense Disambiguation，WSD） 语音识别（Speech Recognition） 语音合成（Text To Speech，TTS） 机器翻译（Machine Translation，MT） 自动文摘（Automatic Summarization） 问答系统（Question Answering） 自然语言理解（Natural Language Understanding） OCR 信息检索（Information Retrieval，IR） 语言模型与应用 ​ 上个世纪80年代后期，机器学习算法被引入到自然语言处理中，这要归功于不断提高的计算能力。 ​ 研究主要集中在统计模型上，这种方法采用大规模的训练语料（corpus）对模型的参数进行自动的学习，和之前的基于规则的方法相比，这种方法更具鲁棒性。 语言模型 ​ 语言模型简单来讲，就是计算一个句子的概率，更确切的说是计算组成这个句子一系列词语的概率。 对一句话𝑆=𝑥1,𝑥2,𝑥3,𝑥4,𝑥5,…,𝑥𝑛S=x1,x2,x3,x4,x5,…,xn而言，它的概率 𝑃(𝑆)=𝑃(𝑥1,𝑥2,𝑥3,𝑥4,𝑥5,…,𝑥𝑛)=𝑃(𝑥1)𝑃(𝑥2|𝑥1)𝑃(𝑥3|𝑥1,𝑥2)...𝑃(𝑥𝑛|𝑥1,𝑥2,...,𝑥𝑛−1) ​ 联合概率链规则公式考虑到了所有的词和词之间的依赖关系，但是非常复杂。使用马尔科夫假设（Markov Assumption）简化：下一个词的出现仅依赖于它前面的一个或几个词。 ​ 二元语法（bigram，2-gram）: \\(P(x_1,x_2,x_3,x_4,x_5,x_6,x_7,x_8,x_9,x_{10})=P(x_1)P(x_2|x_1)P(x_3|x_2)P(x_4|x_3)..P(x_{10}|x_9)\\) ​ 三元语法（trigram，3-gram）: 𝑃(𝑥1,𝑥2,𝑥3,𝑥4,𝑥5,𝑥6,𝑥7,𝑥8,𝑥9,𝑥10)=𝑃(𝑥1)𝑃(𝑥2|𝑥1)𝑃(𝑥3|𝑥1,𝑥2)𝑃(𝑥4|𝑥2,𝑥3)×...×𝑃(𝑥10|𝑥8,𝑥9) ​ 为什么叫“语言模型”？因为这是统计学意义上的模型，又跟语言相关，所以叫语言模型。统计模型指一系列分布，参数模型指一系列可用有限个参数表示的模型。语言模型就是一种参数模型，它的参数是矩阵的所有cell。 选择N-gram的N ​ 理论上，只要有足够大的语料，n越大越好，毕竟这样考虑的信息更多 条件概率为统计计数： 𝑃(“优惠”|“发票”,“点数”)= (“发票”,“点数”，“优惠”) 出现的次数 / (“发票”,“点数”)出现的次数 实际情况往往是训练语料很有限，很容易产生数据稀疏，不满足大数定律，算出来的概率失真。比如(“发票”,“点数”，“优惠”)在训练集中竟没有出现，就会导致零概率问题。 大数定律：样本数量越多，其算术平均值就越趋近期望值。 另一方面，如果n很大，参数空间过大，产生维数灾难。假设词表的大小为100000，那么n-gram模型的参数数量为\\(100000^n\\)。这么多的参数，估计内存就不够放了。 ​ 如何选择依赖词的个数n呢？ 经验上，trigram用的最多。尽管如此，原则上，能用bigram解决，绝不使用trigram。n取≥4的情况较少。 当ｎ更大时：对下一个词出现的约束信息更多，具有更大的辨别力； 当ｎ更小时：在训练语料库中出现的次数更多，具有更可靠的统计信息，具有更高的可靠性、实用性。 N-gram语言模型应用 词性标注 词性标注是一个典型的多分类问题。 “爱”作为动词还是比较常见的。所以可以统一给“爱”分配为“动词”。这种最简单的思想非常好实现，如果准确率要求不高则也比较常用。它只需要基于词性标注语料库做一个统计就够了，连贝叶斯方法、最大似然法都不要用。 可以引入2-gram模型提升匹配的精确度。 𝑃(词性𝑖|“很”的词性（副词），“爱\") = 前面被“副词”修饰的“爱\"作为“词性𝑖”的次数 / 前面被“副词”修饰的“爱\"出现的次数；𝑖=1,2,3... 垃圾邮件识别 一个可行的思路如下： 先对邮件文本进行断句，以句尾标点符号（“。” “!” “？”等）为分隔符将邮件内容拆分成不同的句子。 用N-gram分类器判断每个句子是否为垃圾邮件中的敏感句子。 当被判断为敏感句子的数量超过一定数量（比如3个）的时候，认为整个邮件就是垃圾邮件。 根据贝叶斯公式有： 𝑃(𝑌𝑖|𝑋)∝𝑃(𝑋|𝑌𝑖)𝑃(𝑌𝑖)；𝑖=1,2 对𝑃(𝑋|𝑌𝑖)P(X|Yi) 套用2-gram模型。 则上式化简为： 𝑃(𝑌𝑖|𝑋)∝𝑃(𝑋|𝑌𝑖)𝑃(𝑌𝑖) ∝𝑃(𝑥1|𝑌𝑖)𝑃(𝑥2|𝑥1，𝑌𝑖)𝑃(𝑥3|𝑥2，𝑌𝑖)...𝑃(𝑥10|𝑥9，𝑌𝑖)𝑃(𝑌𝑖) 因为这种方法考虑到了词语前面的一个词语的信息，同时也考虑到了部分语序信息，因此区分效果会比单纯用朴素贝叶斯方法更好。 N-gram方法在实际应用中有一些tricks 从区分度来看，3-gram方法更好些。 可以考虑在其前面再添加一个句子起始符号如“”，这样我们就不必单独计算𝑃(“我”|𝑌𝑖)，而是替换为计算𝑃(“我”|“”,𝑌𝑖)。形式上与2-gram统一。 这样统计和预测起来都比较方便。(一般地，如果采用N-gram模型，可以在文本开头加入n-1个虚拟的开始符号) N-gram模型也会发生零概率问题，也需要平滑技术 中文分词 中文分词技术是“中文NLP中，最最最重要的技术之一”，重要到某搜索引擎厂有专门的team在集中精力优化这一项工作，重要到能影响双语翻译百分位的准确度，能影响某些query下搜索引擎百分位的广告收入。 中文分词也可以理解成一个多分类的问题。𝑋表示被分词的句子，用𝑌𝑖表示该句子的一个分词方案。 𝑃(𝑌𝑖|𝑋)∝𝑃(𝑋|𝑌𝑖)𝑃(𝑌𝑖)；𝑖=1,2,3... NOTE：任意假想的一种分词方式之下生成的句子总是唯一的（只需把分词之间的分界符号扔掉剩下的内容都一样）。于是可以将 𝑃(𝑋|𝑌𝑖)看作是恒等于1的。这样贝叶斯公式又进一步化简成为： 𝑃(𝑌𝑖|𝑋)∝𝑃(𝑌𝑖)；𝑖=1,2,3... 采用2-gram。于是有： 𝑃(𝑌1)=𝑃(“我司”，“可”，“办理”，“正规发票”) =𝑃(“我司”)𝑃(“可”|“我司”)𝑃(“办理”|“可”)𝑃(“正规发票”|“办理”） 𝑃(𝑌2)=𝑃(“我”，“司可办”，“理正规”，“发票”) =𝑃(“我”)𝑃(“司可办”|“我”)𝑃(“理正规”|“司可办”)𝑃(“发票”|“理正规”） 机器翻译与语音识别 N-gram语言模型在机器翻译和语音识别等顶级NLP应用中也有很大的用途。 用于判断生成的语句的后验概率相对大小。 平滑技术 拉普拉斯平滑 最简单的平滑技术是拉普拉斯平滑， 加一平滑法，其保证每个n-gram在训练语料中至少出现1次（或者加k）。 分母加上\\(k * |V|\\) Back-off方法 考虑k-1元gram 插值法 多种gram加权 一元组 \\((w_{i})\\) 出现的次数平均比二元组 \\((w_{i−1},w_{i})\\) 出现的次数要多很多，根据大数定律，它的相对频度更接近概率分布。类似地，二元组平均出现的次数比三元组要高，二元组的相对频度比三元组更接近概率分布。 同时，低阶模型的零概率问题也比高阶模型轻。 \\[P(w_i\\mid w_{i-2},w_{i-1})=\\lambda(w_{i-2},w_{i-1})\\cdot f(w_i\\mid w_{i-2},w_{i-1}) +\\lambda(w_{i-1})\\cdot f(w_i\\mid w_{i-1})+\\lambda f(w_i)\\] 其中，三个 λ 为插值权重，均为正数且和为 1。 线性插值法的效果比卡茨退避法略差，故现在已经较少使用了。 古德-图灵 在统计中相信可靠的统计数据，而对不可信的统计数据打折扣的一种概率估计方法。 古德-图灵估计的原理是： 对于没看见的事件，我们不能认为它发生的概率就是零，因此我们从概率的总量(Probability Mass)中，分配一个很小的比例给这些没有看见的事件。这样一来，看见了的事件的概率总和就小于 1了。因此，需要将所有看见了的事件概率调小一点，并且按照“越是不可信的统计折扣越多”的方法进行。 假定在语料库中出现 r 次的词有 𝑁𝑟个，特别地，未出现的词数量为 𝑁0。语料库的大小为 N。那么，很显然： \\[N=\\sum_{r=1}^{\\infty}rN_r\\] 出现 r 次的词在整个语料库中的相对频度(Relative Frequency)则是 \\(r/N_r\\)。 当 r 比较小时，它的统计可能不可靠，因此在计算那些出现 r 次的词的概率时，要使用一个更小一点的次数，是 \\(d_r\\)（而不直接使用r）。 \\[d_r = (r+1)\\cdot N_{r+1}/N_r\\] 显然 \\[\\sum_rd_r\\cdot N_r=N\\] 根据 \\(Zipf\\) 定律，一般情况下 \\(N_{r+1}&lt;N_r\\)，因而 \\(d_r&lt;r\\)，从而 \\(d_0&gt;0\\)。 \\(Zipf\\) 定律：一般来说，出现一次的词的数量比出现两次的多，出现两次的比出现三次的多，这种规律称为 Zipf定律(Zipf’s Law)，即 r 越大，词的数量 \\(N_r\\) 越小。 这样就给未出现的词赋予了一个很小的非零值，从而解决了零概率的问题。同时下调了出现频率很低的词的概率。 实际运用中，一般只对出现次数低于某个阈值的词下调频率，然后把下调得到的频率总和给未出现的词。 于是： 对于频率超过一定阈值的词，它们的概率估计就是它们在语料库中的相对频度， 对于频率小于阈值的词，它们的概率估计就小于它们的相对频度，并且出现次数越少，折扣越多 对于未看见的词，也给与了一个比较小的概率 卡茨退避法（折扣） 由前 IBM 科学家卡茨(S.M.Katz)提出。 \\(\\sum_{w_{i-1},w_i\\text{ seen}}P(w_i\\mid w_{i-1})\\lt 1\\)类似古德-图灵估计的方法。这意味着有一部分概率量没有分配出去，留给了没有看到的二元组 \\((w_{i−1},w_{i})\\)： \\[P(w_i\\mid w_{i-1})=\\begin{cases}f(w_i\\mid w_{i-1})\\quad\\text{if }N(w_{i-1},w_i) \\ge T \\\\f_{gt}(w_i\\mid w_{i-1})\\quad\\text{if }0\\lt N(w_{i-1},w_i)\\lt T \\\\ Q(w_{i-1})\\cdot f(w_i)\\quad\\text{otherwise}\\end{cases}\\] 其中 T 是阈值，一般在 8−10 左右，函数 \\(f_{gt}()\\) 表示经过古德-图灵估计后的相对频度。而 \\[Q(w_{i-1})=\\frac{1-\\sum_{w_i \\text{ seen}}P(w_i\\mid w_{i-1})}{\\sum_{w_i\\text{ unseen}}f(w_i)}\\] 类似地，对于三元模型，概率估计的公式如下： \\[P(w_i\\mid w_{i-2},w_{i-1})=\\begin{cases}f(w_i\\mid w_{i-2},w_{i-1})\\quad\\text{if }N(w_{i-2,}w_{i-1},w_i) \\ge T \\\\f_{gt}(w_i\\mid w_{i-2,}w_{i-1})\\quad\\text{if }0\\lt N(w_{i-2},w_{i-1},w_i)\\lt T \\\\ Q(w_{i-2},w_{i-1})\\cdot P(w_i\\mid w_{i-1})\\quad\\text{otherwise}\\end{cases}\\] Absolute discounting Absolute discounting 包括了对高阶和低阶模型的差值，然而它并不是用高阶模型的 P 乘以一个 lambda，而是从每个非零计数里减掉一个固定的 discount δ∈[0,1]，作为test set的计数。 image image Kneser-Ney smoothing Absolute discounting 的一个扩展，对回退部分做了一个修正。 只有在高阶模型的计数很小或者为 0 时，低阶模型才显得重要，(只有在 bigram 没有出现过时，unigram 才有用)。针对该目的进行优化。 目的： capture the diversity of contexts for the word。 image Modified Kneser-Ney smoothing 根据高阶模型（k元）的相对大小，对低阶模型得到的部分进行比例分配。 目前在很多情况下，效果最好的平滑方法。 KenLM工具构建语言模型 https://kheafield.com/code/kenlm/ ​ 使用了Modified Kneser-Ney smoothing的语言模型工具包。安装和使用见notebook。 1pip install https://github.com/kpu/kenlm/archive/master.zip 神经语言预训练语言模型 BERT，GPT等，同样可以应用在以上领域。相关部分在后面笔记中给出。 参考资料: Andrej Karpathy的RNN博客 Language Model: A Survey of the State-of-the-Art Technology 传统n-gram模型简单实用，但是数据的稀疏性和泛化能力有很大的问题。 ​ 当新的文本中出现意义相近但是没有在训练文本中出现的单词或者单词组的时候，传统离散模型无法正确计算这些训练样本中未出现的单词的应有概率，他们都会被赋予0概率预测值。 ​ 除了对未出现的单词本身进行预测非常困难之外，离散模型还依赖于固定单词组合，需要完全的模式匹配，否则也无法正确输出单词组出现的概率。 ​ 离散模型在计算上还存在“维度诅咒”的困难。假设我们的词库有一万个独立单词，对于一个包含4个单词的词组模式，潜在的单词组合多达\\(10000^4\\)。 神经网络模型：前馈神经网络模型（FFLM）和循环神经网络模型（RNNLM）。前者主要设计来解决稀疏性问题，而后者主要设计来解决泛化能力，尤其是对长上下文信息的处理。 前馈神经网络模型（FFLM） Bengio等人提出的第一个前馈神经网络模型利用一个三层，包含一个嵌入层、一个全连接层、一个输出层，的全连接神经网络模型来估计给定n-1个上文的情况下，第n个单词出现的概率。其架构如下图所示： image FFLM假设每个输入都是独立的。而循环神经网络的结构能利用文字的这种上下文序列关系。 循环神经网络模型（RNNLM） 循环神经网络模型不要求固定窗口的数据训练。 image RNN语言模型训练过程： image RNN语言模型反向传播: image 语言模型评估 困惑度（perplexity），其基本思想是给测试集的句子赋予较高概率值的语言模型较好。perplexity越小，句子概率越大，语言模型越好。 image 也就是，\\(2^{-crossentropyloss}\\) image","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"language model","slug":"language-model","permalink":"https://racleray.github.io/tags/language-model/"},{"name":"smoothing","slug":"smoothing","permalink":"https://racleray.github.io/tags/smoothing/"},{"name":"KenLM","slug":"KenLM","permalink":"https://racleray.github.io/tags/KenLM/"}]},{"title":"Linux+Docker深度学习环境","slug":"Linux-Docker深度学习环境","date":"2020-07-06T14:50:10.000Z","updated":"2023-08-07T11:54:31.031Z","comments":true,"path":"posts/accc2125.html","link":"","permalink":"https://racleray.github.io/posts/accc2125.html","excerpt":"记录在Docker中搭建深度学习环境的过程。","text":"Docker中搭建深度学习环境 宿主机部分 apt源修改： 12345sudo mv /etc/apt/sources.list /etc/apt/sources.list.baklsb_release -c # 系统版本sudo vim /etc/apt/sources.list 写入 12345678910deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 12sudo apt-get updatesudo apt-get upgrade 安装python和pip 12apt-get install python3apt-get install python3-pip 配置软连接 12ln -s /usr/local/python3/bin/python3 /usr/bin/python3ln -s /usr/local/python3/bin/pip3 /usr/bin/pip3 123456789mkdir ~/.pipcd ~/.piptouch pip.confcat &gt;&gt; ~/.pip/pip.conf &lt;&lt; EOF[global]timeout = 6000index-url = https://pypi.tuna.tsinghua.edu.cn/simpletrusted-host = pypi.tuna.tsinghua.edu.cnEOF 查询CUDA，按照官网指示安装，可选择不同方法，local或者network等 注意！！ 云主机上安装cuda时，可能需要加上 --no-opengl-libs 查询驱动版本：可手动下载安装包到本地安装 首先卸载掉之前的显卡驱动： 1apt-get remove –purge nvidia* 也可使用命令查看推荐驱动 1ubuntu-drivers devices 1sudo apt install --no-install-recommends &lt;推荐驱动&gt; 下载不了可添加ppa 123sudo add-apt-repository ppa:graphics-drivers/ppasudo apt-get updatesudo apt install &lt;推荐驱动&gt; 之前安装过cuda需要重新安装的话可以选择先卸载，以免和新的 CUDA 版本产生冲突: /usr/local/cuda/bin 目录下有一个 uninstallcuda*.pl 文件 1./uninstall_cuda_10.0.pl 可能会卸载不干净, 可以再执行命令删除cuda目录 12rm -rf /usr/bin/cudarm -rf /usr/bin/cuda-10.0 查询下载cudnn 解压下载的文件，可以看到cuda文件夹，在当前目录打开终端，执行如下命令： 12345cp cuda/include/cudnn.h /usr/local/cuda/include/cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/chmod a+r /usr/local/cuda/include/cudnn.hchmod a+r /usr/local/cuda/lib64/libcudnn* 网络下载速度快也可以参考tensorflow官网的教程： 12345678910111213141516171819202122232425# https://www.tensorflow.org/install/gpu#install_cuda_with_apt# Add NVIDIA package repositorieswget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.debsudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.debsudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pubsudo apt-get updatewget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.debsudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.debsudo apt-get update# Install NVIDIA driversudo apt-get install --no-install-recommends nvidia-driver-418 # 可更改为推荐版本# Reboot. Check that GPUs are visible using the command: nvidia-smi# Install development and runtime libraries (~4GB)sudo apt-get install --no-install-recommends --no-opengl-libs \\ cuda-10-1 \\ libcudnn7=7.6.4.38-1+cuda10.1 \\ libcudnn7-dev=7.6.4.38-1+cuda10.1# 执行inference加速的服务器安装，也可开发调试# Install TensorRT. Requires that libcudnn7 is installed above.sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \\ libnvinfer-dev=6.0.1-1+cuda10.1 \\ libnvinfer-plugin6=6.0.1-1+cuda10.1 云主机挂载新分区 查看是否有多余的盘比如/dev/vdb 1fdisk -l 如果有则进行分区 1fdisk /dev/vdb 进入交互： 1.输入 n 并按回车键：创建一个新分区。 2.输入 p 并按回车键：选择主分区。因为创建的是一个单分区数据盘，所以只需要创建主分区。 3.输入分区编号并按回车键。可以输入 1。 4.输入第一个可用的扇区编号：按回车键采用默认值 1。 5.输入最后一个扇区编号：因为这里仅创建一个分区，所以按回车键采用默认值。 6.输入 wq 并按回车键，开始分区。 12#是否有新分区 /dev/vdb1fdisk -l 挂载文件系统 12#/mnt是宿主机的目录mount /dev/vdb1 /mnt 查看挂载是否成功 1df -h Docker部分 安装docker ce，根据官网教程 目前已经不推荐使用nvidia-docker2了，直接使用 nvidia-container-toolkit 即可。 1234567# Add the package repositoriesdistribution=$(. /etc/os-release;echo $ID$VERSION_ID)curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.listsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-container-toolkitsudo systemctl restart docker Usage 12345678910111213#### Test nvidia-smi with the latest official CUDA imagedocker run --gpus all nvidia/cuda:10.0１base nvidia-smi# Start a GPU enabled container on two GPUsdocker run --gpus 2 nvidia/cuda:10.１-base nvidia-smi# Starting a GPU enabled container on specific GPUsdocker run --gpus '\"device=1,2\"' nvidia/cuda:10.１-base nvidia-smidocker run --gpus '\"device=UUID-ABCDEF,1\"' nvidia/cuda:10.１-base nvidia-smi# Specifying a capability (graphics, compute, ...) for my container# Note this is rarely if ever used this waydocker run --gpus all,capabilities=utility nvidia/cuda:10.１-base nvidia-smi nivdia官方镜像：https://gitlab.com/nvidia/container-images/cuda/blob/master/doc/supported-tags.md pytorch镜像：https://hub.docker.com/r/opencompetition/opencompetition tensorflow镜像：https://www.tensorflow.org/install/docker （Optional）修改Docker本地镜像与容器的存储位置 如果根分区太小的话 首先停掉Docker服务： 1systemctl stop docker 然后移动整个/var/lib/docker目录到目的路径： 12sudo mv /var/lib/docker &lt;目标路径&gt;sudo ln -s &lt;目标路径&gt; /var/lib/docker 然后 1systemctl start docker （Optional）修改用户密码 ubuntu的默认root密码是随机的，每次开机都会有一个新的root密码。 1sudo passwd 然后会提示输入当前用户的密码。 1su root 转为root后，修改用户密码 1sudo passwd user 常用Docker操作 image，镜像，是一个个配置好的环境。 container，容器，是image的具体实例。 docker run [-it] some-image 创建某个镜像的容器 docker ps列出当前运行的容器 docker ps -a列出所有的容器，包括运行的和不运行的 docker rm container-id删除某个容器 docker start [-i] container-id启动某个容器，必须是已经创建的 -i进入交互模式，还有一种方法：docker attach container-id CTRL+D或者输入exit，退出 docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH 容器到主机 docker cp [OPTIONS] SRC_PATH CONTAINER:DEST_PATH 主机到容器 eg: docker cp /home/racle/package_data nlptools:/home/deepspeed/package_data 停止、重启： docker stop container-id docker restart container-id 给镜像打标签 1docker tag &lt;image id&gt; username/imagename:version 映射 12345678sudo docker run -it / --gpus all / -p 8000:22 / -p 9999:8888 / --ipc=host / -v &lt;主机文件路径&gt;:&lt;容器文件路径&gt; / --name &lt;名称&gt; / &lt;image id&gt; -p 9999:8888 把主机的9999端口映射到容器的8888端口 -p 8000:22 留一个端口映射到容器22端口，因为SFTP默认使用22端口。 --ipc=host 让容器与主机共享内存 -v : 文件路径映射 e.g. /home/racle/dockerVol/:/home/ralce/ 或者容器启动的时候挂载目录 1--mount type&#x3D;bind,source&#x3D;paht,target&#x3D;path jupyter notebook 创建了容器之后，启动jupyter notebook： 12345678910jupyter-notebook --generate-configsed -i 's/#c.NotebookApp.allow_password_change/c.NotebookApp.allow_password_change/g' ~/.jupyter/jupyter_notebook_config.pycat &gt;&gt; ~/.jupyter/jupyter_notebook_config.py &lt;&lt; EOFc = get_config()c.NotebookApp.ip = '*'c.NotebookApp.open_browser = Falsec.NotebookApp.port = 8888c.NotebookApp.token='666666'EOF 1jupyter notebook --no-browser --ip=0.0.0.0 --notebook-dir='' --no-browser即不通过浏览器启动 --allow-root允许root模型运行 端口配置 容器里 配置SSH服务，首先安装*openssh-server*: 1apt install -y openssh-server 建立一个配置文件夹并进行必要的配置： 1234567891011$ mkdir /var/run/sshd$ sudo passwd# 这里设置的root密码，一定要记住！# 使用其他用户连接，可用root权限设置密码： passwd username# 将#PermitRootLogin prohibit-password 替换为 PermitRootLogin yes$ sed -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/' /etc/ssh/sshd_config# 将session\\s*required\\s*pam_loginuid.so 替换为 session optional pam_loginuid.so$ sed 's@session\\s*required\\s*pam_loginuid.so@session optional pam_loginuid.so@g' -i /etc/pam.d/sshd$ echo \"export VISIBLE=now\" &gt;&gt; /etc/profile 1$ sudo service ssh restart 在服务器（宿主机）上 测试刚刚新建docker容器中端口转发的22端口： 1$ sudo docker port [your_container_name] 22 NOTE：可能ssh默认开启的端口不是22，需要在/etc/ssh/sshd_config中修改 port 为 22 1netstat -tnlp # 查看当前机器开启的端口 测试能否用SSH连接到远程docker： 1$ ssh root@[your_host_ip] -p 8000 下面就可以通过pychram和jupyter远程连接服务器上的docker了 本地与服务器的端口映射，从而远程登录jupyter：(远程链接到宿主机被映射的端口)： 1ssh username@host-ip -L 8888:127.0.0.1:9999 远程可通过宿主机ip的8888端口访问jupyter docker重启时也需要重启ssh服务： 1$ sudo service ssh restart 备份 通过docker ps或者docker ps -a来查看你想备份的容器的id， 然后通过： 1docker commit -p [your-container-id] [your-backup-name] 来将your-container-id的容器创建成一个镜像快照 通过docker images就可以查看到刚刚创建好的镜像快照了 通过： 1docker save -o [path-you-want-to-save/your-backup-name.tar]] [your-backup-name] 把那个镜像打包成tar文件，保存到服务器上。 恢复： docker load -i your-backup-name.tar 1docker run -d -p 80:80 your-backup-name pycharm连接 PyCharm*Tools &gt; Deployment &gt; Configuration*, 新建一个*SFTP*服务器，名字自己取 image 输入如下图配置 image 在Mappings中配置路径，这里的路径是你本地存放代码的路径，与刚刚配置的Root Path相互映射（同docker中宿主机与容器的关系） image 配置远程解释器 点击PyCharm的File &gt; Setting &gt; Project &gt; Project Interpreter右边的设置按钮新建一个项目的远程解释器： image image 配置完成。配置完成以后需要等本地和远程的环境同步一下。 本地的文件，修改之后可以随时右键deployment-&gt;upload到远程主机，或者直接在本地调试运行。 docker容器停了以后里面的SSH服务也会相应停止， docker重启时也需要重启ssh服务： 1$ service ssh restart 参考： Docker中搭建深度学习环境","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"Docker","slug":"Tools/Docker","permalink":"https://racleray.github.io/categories/Tools/Docker/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://racleray.github.io/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://racleray.github.io/tags/Docker/"},{"name":"NVIDIA Container Toolkit","slug":"NVIDIA-Container-Toolkit","permalink":"https://racleray.github.io/tags/NVIDIA-Container-Toolkit/"}]},{"title":"个人Linux环境配置","slug":"个人Linux环境配置","date":"2020-07-06T14:42:23.000Z","updated":"2023-08-07T11:54:31.038Z","comments":true,"path":"posts/2c27ec14.html","link":"","permalink":"https://racleray.github.io/posts/2c27ec14.html","excerpt":"记录个人桌面版Linux配置过程。但是在本机安装双系统很不稳定，虚拟机在我的小破笔记本上也表现很一般，最后还是转战云主机了。","text":"更新源 找到Software &amp; Updates，将源更新为阿里云的源 在Other Software里将Canonical Partners勾上 然后自己手动更新一下： 123sudo apt updatesudo apt upgrade Sougou Pinyin 123456sudo apt-get install fcitx-binsudo apt-get install fcitx-table # 搜狗输入法Linux官网 https://pinyin.sogou.com/linux/ 下载64bit的程序sudo dpkg -i sogoupinyin*.deb 重启 找到Fcitx Configure，设置输入法 截图软件 Shutter 下载libgoocanvas-common、libgoocanvas3、libgoo-canvas-perl 12345sudo dpkg -i libgoocanvas-common*.debsudo dpkg -i libgoocanvas3*.debsudo dpkg -i libgoo-canvas-perl*deb 将上述三个包给安装上，若安装失败，执行下面代码: 1sudo apt-get install -f 然后再安装这几个包 视频和音频 安装解码器： 1sudo apt-get install ubuntu-restricted-extras 安装VLC视频播放器 1sudo apt-get install vlc browser-plugin-vlc 安装FFmpeg 1sudo apt-get install ffmpeg 网易云音乐 官网 设置: 点击图标最小化 1gsettings set org.gnome.shell.extensions.dash-to-dock click-action 'minimize' 美化 123456789sudo apt-get install gnome-tweak-tool #安装tweaksudo apt-get install gnome-shell-extensions -y #安装shell扩展sudo apt install chrome-gnome-shell #为了能在浏览器内安装gnome插件，火狐和谷歌都能用sudo apt-get install gtk2-engines-pixbuf #防止GTK2错误sudo apt install libxml2-utils 从gnome-look这里下载，或者通过pling和 ocs-url直接安装 或者手动下载下来，接下来解压到指定文件夹，并安装他们。 1234567xz -d Gnome-OSC-HS-light-menu*.tar.xztar -xvf Gnome-OSC-HS-light-menu*.tar -C /usr/share/themes/xz -d Gnome-OSC-HS--2*.tar.xztar -xvf Gnome-OSC-HS--2*.tar -C /usr/share/themes/ 图标：/usr/share/icons/ 其他主题：/usr/share/themes/ 如果Shell显示不可修改， 1sudo apt install gnome-shell-extensions 安装压缩软件 1sudo apt-get install p7zip-full p7zip-rar rar unzip Chrome Brower 123wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.debsudo dpkg -i google-chrome*; sudo apt-get -f install 安装多版本gcc和g++，并共存 1234567891011121314151617181920212223sudo apt-get install gcc-5 gcc-5-multilibsudo apt-get install g++-5 g++-5-multilibsudo apt-get install gcc-6 gcc-6-multilibsudo apt-get install g++-6 g++-6-multilibsudo apt-get install gcc-7 gcc-7-multilibsudo apt-get install g++-7 g++-7-multilibsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-5 50sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-6 60sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 70sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-5 50sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-6 60sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-7 70 然后选择gcc和g++版本 123sudo update-alternatives --config gccsudo update-alternatives --config g++ CMAKE https://cmake.org/download/ 安装在/opt下，软连接到/usr/bin/ 1sudo ln -sf /opt/cmake-3.17.1/bin/* /usr/bin/ complete cmake process with following libs:(boost版本较低，需要手动安装较高版本，见下文Boost部分) 1sudo apt-get install cmake libblkid-dev e2fslibs-dev libboost-all-dev libaudit-dev openSSL 1sudo apt install libssl-dev 编写简单的cmake https://cmake.org/cmake/help/v3.17/guide/tutorial/index.html 使用cmake首先得有个CMakeList.txt文件，你需要把配置信息写在该文件中，然后通过cmake去处理该文件。 将设有下面一个main.cpp文件 1234567//main.cpp文件\\#include&lt;iostream&gt;using namespace std;int main()&#123;cout&lt;&lt;\"hello world!\"&lt;&lt;endl;return 0;&#125; 这时候我们就可以写个如下的CMakeList.txt文件 1234567891011\\#cmake最小需要版本cmake_minimum_required(VERSION 2.8)\\#项目名字project(HELLOWORLD)\\#包含原程序,即把给定目录下的源程序复制给变量DIR_SRCaux_source_directory(DIR_SRC ./)\\#生成程序add_executable(helloworld $&#123;DIR_SRC&#125;) 然后执行如下命令 12345mkdir buildcd buildcmake ..make./helloworld 这样就编译好程序并运行。 添加静态库或者动态库 而假设我们程序用到了在/usr/lib下的一个静态库libmy.a，那就需要添加如下两个命令 12345\\#库所在位置link_directories(/usr/lib)\\#程序编译时候链接库target_link_libraries(helloworld my) Boost 下载https://www.boost.org/users/history/version_1_72_0.html 1sudo tar -zxvf boost_1_72_0.tar.gz -C /usr/local/ Boost库的编译安装还有一些依赖库，需要先安装 1sudo apt-get install mpi-default-dev libicu-dev python-dev libbz2-dev 回到boost库的路径下，运行如下命令 12sudo ./bootstrap.shsudo ./b2 install --prefix=/usr --build-dir=tmp/build-boost --prefix后面跟的是你安装boost库的路径，安装完成后所有的头文件和lib库都会保存在这个路径下 若 ./b2不加 install --prefix=/usr 会编译到当前文件夹下，安装没有链接成功，因此手动链接。 12sudo ln -sf /usr/local/boost_1_72_0/boost/* /usr/include/boost/include/sudo ln -sf /usr/local/boost_1_72_0/stage/lib/* /usr/include/boost/lib/ 然后加入环境变量/etc/profile 1export env=$PATH:/usr/include/boost 只是用头文件示例程序 不需要先编译boost，直接引入头文件就行。 这个程序用到了boost提供的正则匹配库 123456789101112131415161718#include &lt;boost/regex.hpp&gt;#include &lt;iostream&gt;#include &lt;string&gt;int main()&#123; std::string line; boost::regex pat(\"tcp:*\"); while(std::cin) &#123; std::getline(std::cin, line); boost::smatch matches; if(boost::regex_match(line, matches, pat)) std::cout &lt;&lt; matches[0] &lt;&lt; std::endl; &#125;&#125; 依旧通过CMake进行编译 首先设置Boost库的路径，也就是之前的安装路径 然后使用find_package来搜索这个路径下面是否有需要的regex库 最后设置头文件搜索路径以及把找到的库link到应用程序 1234567891011121314project(tutorial-0)cmake_minimum_required(VERSION 3.5)set(CMAKE_CXX_STANDARD 14)set(BOOST_ROOT /usr/local/boost_1_72_0)find_package(Boost COMPONENTS regex REQUIRED)if(Boost_FOUND) include_directories($&#123;Boost_INCLUDE_DIRS&#125;) add_executable(foo foo.cpp) target_link_libraries (foo $&#123;Boost_LIBRARIES&#125;)endif() 编译完成后运行app查看结果 多版本python和pip共存 ubuntu18.04自带python3，但是没有python2，pip2，pip3 12345678910111213141516171819202122232425sudo apt install curlsudo apt-get install python3-distutilssudo apt-get install python3-testresourcessudo apt-get install python3-widgetsnbextensionsudo apt install python-minimalsudo apt install python2.7curl https://bootstrap.pypa.io/get-pip.py -o get-pip.pysudo python3 get-pip.py #安装pip3sudo python2 get-pip.py #安装pip3sudo pip3 install --upgrade pip #升级pip3sudo pip2 install --upgrade pip #升级pip2# 此时pip和python并不知道指向2还是3，需要自己修改。我们使用alias来设置别名vim /etc/profile # 所有用户 。修改~/.bashrc或者.profile只会修改当前用户# 写入 alias pip=/usr/local/bin/pip3.6 alias python=/usr/bin/python3.6source /etc/profile VS Code 微软官网下载deb安装即可 Git 1sudo apt install git 有道词典 官网下载安装即可 typora 官网打包的二进制 Typora 在 Ubuntu (gnome) 上安装为应用程序（可被选择为默认程序） 12345678910111213141516171819202122$ cd Downloads/Software$ wget https://typora.io/linux/Typora-linux-x64.tar.gz$ tar -xzvf Typora-linux-x64.tar.gz$$ sudo mv Typora-linux-x64 /opt/$ $ cd /opt/Typora-linux-x64/$ ./Typora # test could run ?$ cd /usr/share/applications/Typora.desktop$ sudo vim Typora.desktop[Desktop Entry]Name=TyporaGenericName=EditorComment=Typroa - a markdown editorExec=\"/opt/Typora-linux-x64/Typora\" %UIcon=/opt/Typora-linux-x64/resources/app/asserts/icon/icon_256x256.pngTerminal=falseCategories=Markdown;StartupNotify=falseType=Application sublime-text https://www.sublimetext.com/docs/3/linux_repositories.html#apt jetbrains https://www.jetbrains.com/toolbox-app/ Terminator 新的shell界面 常用快捷键 快捷键 作用 Ctrl+Shift+E 垂直分割窗口 Ctrl+Shift+O 水平分割窗口 Ctrl+Shift+W 关闭当前窗格 Ctrl+Shift+C 复制 Ctrl+Shift+V 粘贴 Ctrl+Shift+X 放大或缩小某一窗口 Ctrl+Shift+Z 从放大窗口回到多窗格界面 ALT+↑[↓,←,→] 移动到上[下、左、右]面一个窗口 Ctrl+Tab 切换窗口 Ctrl+- 缩小字体 Ctrl+Shift+=也就是Ctrl++ 放大字体 Zsh fish 使用oh-my-zsh来自动管理配置，可以查看官网：https://ohmyz.sh/ zsh官方的antigen来管理 https://github.com/zsh-users/antigen 1234567sudo apt install zsh# 切换到zshchsh -s /bin/zsh curl -L git.io/antigen &gt; antigen.zsh # 或者直接在git.io/antigen下载文本# or use git.io/antigen-nightly for the latest version# or apt-get install zsh-antigen 在 ~/.zshrc 中添加下面的内容 1234567891011121314151617181920212223242526272829source ~/antigen.zsh# Load the oh-my-zsh's library.antigen use oh-my-zsh# Bundles from the default repo (robbyrussell's oh-my-zsh).antigen bundle brewantigen bundle command-not-foundantigen bundle dockerantigen bundle docker-composeantigen bundle gemantigen bundle gitantigen bundle golangantigen bundle ngantigen bundle osxantigen bundle pip# Syntax highlighting bundle.antigen bundle zsh-users/zsh-syntax-highlightingantigen bundle zsh-users/zsh-completionsantigen bundle zsh-users/zsh-autosuggestionsantigen bundle zsh-users/zsh-apple-touchbar# Load the theme.# antigen theme robbyrussellantigen theme https://github.com/denysdovhan/spaceship-prompt spaceship# Tell Antigen that you're done.antigen apply 进入zsh 乱码问题 1234git clone https://github.com/powerline/fonts.git# installcd fonts./install.sh 在优化中设置等宽字体为powerline类型字体中的一个 其他选择：fish https://link.zhihu.com/?target=https%3A//wiki.archlinux.org/index.php/Fish TLDR https://tldr.sh/：常用命令范式查找工具 文件查找 find locate fd：fd is a simple, fast and user-friendly alternative to find. 代码查找 重要的是：有些问题使用合适的工具就会迎刃而解，而具体选择哪个工具则不是那么重要 grep ack, ag 和 rg 查找Shell历史 Ctrl+R 对命令历史记录进行回溯搜索 文件夹导航 fasd：查找最常用和/或最近使用的文件和目录 概览目录结构，例如 tree, broot 更完整的文件管理器，例如 nnn 或 ranger npm 1234567sudo apt-get install nodejssudo apt-get install npmnpm -vnpm config set registry https://registry.npm.taobao.org 安装nrm工具，用于管理软件源。 123$ sudo npm install -g nrm$ nrm ls 在特定网络环境下需要配置代理的话，可以使用如下命令配置。 123$ npm config set proxy http://127.0.0.1:3128$ npm config set http-proxy http://127.0.0.1:3128$ npm config set https-proxy https://127.0.0.1:3128 vim 安装VIM 1apt-get install vim https://spacevim.org/documentation/：spacevim插件集合 or https://github.com/spf13/spf13-vim：一键配置插件集合 http://vim.spf13.com/#install 1curl https://j.mp/spf13-vim3 -L &gt; spf13-vim.sh &amp;&amp; sh spf13-vim.sh - tmux Ubuntu并不自带tmux，所以我们需要输入以下指令进行安装： 按下 Ctrl + d 或者直接在命令行输入 exit 指令，就可以退出Tmux窗口。 Tmux窗口有很多的快捷键，所有的快捷键都需要前缀键。默认的前缀键是 Ctrl + b，只有先按下 Ctrl + b，快捷键才能生效。举例来说，帮助指令的快捷键是 Ctrl + b ?。那么在Tmux窗口下，先按下 Ctrl + b，再按下 ?，就能显示帮助信息。 tmux重要指令的总结： tmux new -s 新建一个特定名称的会话 tmux detach 将当前会话与窗口分离 tmux ls / tmux list-session 查看当前所有的Tmux会话 tmux attach -t 重新连接某个已存在的会话 tmux kill-session -t 删除某个会话 tmux switch -t 切换会话 tmux rename-session -t 重新命名会话 会话相关的快捷键： Ctrl + b d 分离会话 Ctrl + b s 列出所有会话 Ctrl + b $ 重命名当前会话 Tmux可以在一个窗口中分出多个窗格 (pane)，每个窗格可以运行不同的指令。以下是相关快捷键： Ctrl+b % 划分左右两个窗格 Ctrl+b “ 划分上下两个窗格 Ctrl+b 切换到其他窗格，指向切换的方向 Ctrl+b ; 切换到上一个窗格 Ctrl+b o 切换到下一个窗格 Ctrl+b { 当前窗格左移 Ctrl+b } 当前窗格右移 Ctrl+b x 关闭当前窗格 - 代码折叠 zc 关闭当前打开的折叠 zo 打开当前的折叠 zm 关闭所有折叠 zM 关闭所有折叠及其嵌套的折叠 zr 打开所有折叠 zR 打开所有折叠及其嵌套的折叠 zd 删除当前折叠 zE 删除所有折叠 zj 移动至下一个折叠 zk 移动至上一个折叠 zn 禁用折叠 zN 启用折叠 SimpleScreenRecorder 1sudo apt-get install simplescreenrecorder Dumb downloader that scrapes the web : tget is wget for torrents Unity Tweak 工具 1sudo apt-get install unity-tweak-tool 使用 parcellite剪切板 12$ sudo apt-get install parcellite$ parcellite 设置登录背景 假设我现在用的图片是mypicture.jpg , 将它移动到/usr/share/backgrounds/目录下 1sudo mv currentdir&#x2F;mypicture.jpg &#x2F;usr&#x2F;share&#x2F;backgrounds&#x2F; Ubuntu现在用的Gnome的桌面，和以前Unity时候的配置文件不一样，所以16.04的教程是用不了的，18.04登录背景相关的配置是用css的：/etc/alternatives/gdm3.css 1sudo gedit /etc/alternatives/gdm3.css 123456789101112#找到默认的这个部分#lockDialogGroup &#123; background: #2c001e url(resource:///org/gnome/shell/theme/noise-texture.png); background-repeat: repeat; &#125;#改为#lockDialogGroup &#123; background: #2c001e url(file:///usr/share/backgrounds/mypicture.jpg); background-repeat: no-repeat; background-size: cover; background-position: center; &#125; 保存并重启. apt-get使用 apt-get命令本身并不具有管理软件包功能，只是提供了一个软件包管理的命令行平台。 apt-get命令的一般语法格式为： apt-get subcommands [ -d | -f | -m | -q | --purge | --reinstall | - b | - s | - y | - u | - h | -v ] [pkg] apt-cache提供了搜索功能。 更新或升级操作： apt-get update # 更新源 apt-get upgrade # 更新所有已安装的包 apt-get dist-upgrade # 发行版升级（如，从10.10到11.04） 安装或重装类操作： apt-get install # 安装软件包，多个软件包用空格隔开 apt-get install --reinstall # 重新安装软件包 apt-get install -f # 修复安装（破损的依赖关系）软件包 卸载类操作： apt-get remove # 删除软件包（不包括配置文件） apt-get purge # 删除软件包（包括配置文件） 下载清除类操作： apt-get source # 下载pkg包的源代码到当前目录 apt-get download # 下载pkg包的二进制包到当前目录 apt-get source -d # 下载完源码包后，编译 apt-get build-dep # 构建pkg源码包的依赖环境（编译环境？） apt-get clean # 清除缓存(/var/cache/apt/archives/{,partial}下)中所有已下载的包 apt-get autoclean # 类似于clean，但清除的是缓存中过期的包（即已不能下载或者是无用的包） apt-get autoremove # 删除因安装软件自动安装的依赖，而现在不需要的依赖包 查询类操作： apt-cache stats # 显示系统软件包的统计信息 apt-cache search # 使用关键字pkg搜索软件包 apt-cache show # 显示软件包pkg_name的详细信息 apt-cache depends # 查看pkg所依赖的软件包 apt-cache rdepends # 查看pkg被那些软件包所依赖 关于软件安装目录的说明： 一般的deb包(包括新立得或者apt-get下载的)都在/usr/share。 自己下载的压缩包或者编译的包，有些可以选择安装目录，一般放在/usr/local/，也有在/opt的。 关于apt-get的缓存目录： 默认的缓存目录是/var/cache/apt/archives/ 为日后重装系统后安装软件节省下载时间或者将软件包给别人用 参考链接： Ubuntu18.04安装后应该做的事 Linux玩家必备：Ubuntu完全配置指南 Ubuntu 18.04配置及美化","categories":[{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"Linux","slug":"Tools/Linux","permalink":"https://racleray.github.io/categories/Tools/Linux/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://racleray.github.io/tags/Linux/"}]},{"title":"中文文本处理","slug":"中文文本处理","date":"2020-07-06T13:40:16.000Z","updated":"2023-08-07T11:54:31.038Z","comments":true,"path":"posts/b75580e5.html","link":"","permalink":"https://racleray.github.io/posts/b75580e5.html","excerpt":"简要记录一点中文文本处理的概念和工具使用。","text":"中文文本基本任务与处理 1.分词 中文和日文单纯从文本形态上无法区分具备独立含义的词（拉丁语系纯天然由空格分隔不同的word）。因此在很多中文任务中，我们需要做的第一个处理叫做分词。 ​ 主流的分词方法主要是基于词典匹配的分词方法(正向最大匹配法、逆向最大匹配法和双向匹配分词法等)和基于统计的分词方法(HMM、CRF、和深度学习)；‘ ​ 主流的分词工具库包括 中科院计算所NLPIR、哈工大LTP、清华大学THULAC、Hanlp分词器、Python jieba工具库等。 最大匹配法 ​ 最大匹配是指以词典为依据，取词典中最长单词为第一个次取字数量的扫描串，在词典中进行扫描（为提升扫描效率，还可以跟据字数多少设计多个字典，然后根据字数分别从不同字典中进行扫描）。 ​ 双向最大匹配法：正向最大匹配和逆向最大匹配两种算法都切一遍，然后根据大颗粒度词越多越好，非词典词和单字词越少越好的原则，选取其中一种分词结果输出。 “我们在野生动物园玩” ​ - 正向最大匹配法：“我们/在野/生动/物/园/玩”，其中，单字字典词为2，非词典词为1。 ​ - 逆向最大匹配法：“我们/在/野生动物园/玩”，其中，单字字典词为2，非词典词为0。 ​ 非字典词：正向(1)&gt;逆向(0)（越少越好） ​ 单字字典词：正向(2)=逆向(2)（越少越好） ​ 总词数：正向(6)&gt;逆向(4)（越少越好） 因此最终输出为逆向结果。 2.去停用词与N-gram ​ 同英文 3.词性标注 ​ 词性标注（part-of-speech tagging）,又称为词类标注或者简称标注，是指为分词结果中的每个单词标注一个正确的词性。 ​ 在汉语中，词性标注比较简单，因为汉语词汇词性多变的情况比较少见，大多词语只有一个词性，或者出现频次最高的词性远远高于第二位的词性。 4.句法依存分析 ​ 依存句法分析(Dependency Parsing, DP) 识别句子中的“主谓宾”、“定状补”这些语法成分，并分析各成分之间的关系。 image ​ 依存句法分析标注关系(共14种) 及含义如下(哈工大开源包使用)： 关系类型 Tag Description Example 主谓关系 SBV subject-verb 我送她一束花(我&lt;–送) 动宾关系 VOB 直接宾语，verb-object 我送她一束花(送–&gt; 花) 间宾关系 IOB 间接宾语，indirect-object 我送她一束花(送–&gt; 她) 前置宾语 FOB 前置宾语，fronting-object 他什么书都读(书&lt;–读) 兼语 DBL double 他请我吃饭(请–&gt; 我) 定中关系 ATT attribute 红苹果(红&lt;–苹果) 状中结构 ADV adverbial 非常美丽(非常&lt;–美丽) 动补结构 CMP complement 做完了作业(做–&gt; 完) 并列关系 COO coordinate 大山和大海(大山–&gt; 大海) 介宾关系 POB preposition-object 在贸易区内(在–&gt; 内) 左附加关系 LAD left adjunct 大山和大海(和&lt;–大海) 右附加关系 RAD right adjunct 孩子们(孩子–&gt; 们) 独立结构 IS independent structure 两个单句在结构上彼此独立 核心关系 HED head 指整个句子的核心 5.语义依存分析 ​ Semantic Dependency Parsing, SDP，分析句子各个语言单位间的语义关联。 ​ 使用语义依存刻画句子语义，好处在于不需要去抽象词汇本身，而是通过词汇所承受的语义框架来描述该词汇。 ​ 语义依存分析目标是跨越句子表层句法结构的束缚，直接获取深层的语义信息。 ​ 例如以下三个句子，用不同的表达方式表达了同一个语义信息，即张三实施了一个吃的动作，吃的动作是对苹果实施的。 image ​ 语义依存分析不受句法结构的影响，将具有直接语义关联的语言单元直接连接依存弧并标记上相应的语义关系。 ​ 语义依存关系分为三类，分别是 主要语义角色，每一种语义角色对应存在一个嵌套关系和反关系； 事件关系，描述两个事件间的关系； 语义依附标记，标记说话者语气等依附性信息。 ​ http://ltp.ai/demo.html，http://ltp.ai/docs/index.html：哈工大开源包网站 6.语法解析 ​ 句子可以用主语、谓语、宾语来表示。在自然语言的处理过程中，有许多应用场景都需要考虑句子的语法，因此研究语法解析变得非常重要。 ​ 语法解析有两个主要的问题，其一是句子语法在计算机中的表达与存储方法，以及语料数据集；其二是语法解析的算法。 表达与存储 ​ 用树状结构图来表示 image ​ NP、VP、PP是名词、动词、介词短语（短语级别）；N、V、P分别是名词、动词、介词。 语法解析的算法 上下文无关语法（Context-Free Grammer） ​ 为了生成句子的语法树，我们可以定义如下的一套上下文无关语法。 •1）N表示一组非叶子节点的标注，例如{S、NP、VP、N...} •2）Σ表示一组叶子结点的标注，例如{我们、尊敬...} •3）R表示一组规则，每条规则可以表示为 •4）S表示语法树开始的标注 ​ 当给定一个句子时，我们便可以按照从左到右的顺序来解析语法。 ​ 例如，句子the man sleeps就可以表示为(S (NP (DT the) (NN man)) (VP sleeps))。 ​ 规则R示例如下： S -&gt; NP VP VP -&gt; V NP | V NP PP PP -&gt; P NP V -&gt; \"saw\" | \"ate\" NP -&gt; \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP Det -&gt; \"a\" | \"an\" | \"the\" | \"my\" N -&gt; \"dog\" | \"cat\" | \"cookie\" | \"park\" P -&gt; \"in\" | \"on\" | \"by\" | \"with\" 概率分布的上下文无关语法（Probabilistic Context-Free Grammar） ​ 上下文无关的语法可以很容易的推导出一个句子的语法结构，但是缺点是推导出的结构可能存在二义性。 ​ 由于语法的解析存在二义性，我们就需要找到一种方法从多种可能的语法树种找出最可能的一棵树。 ​ 一种常见的方法既是PCFG （Probabilistic Context-Free Grammar）。除了常规的语法规则以外，我们还对每一条规则赋予了一个概率。对于每一棵生成的语法树，我们将其中所有规则的概率的乘积作为语法树的出现概率。 ​ 分别计算每颗语法树的概率p(t)，出现概率最大的那颗语法树就是我们希望得到的结果，即argmax p(t)。 ​ 训练算法： 算法依赖于CFG中对于N、Σ、R、S的定义以及PCFG中的p(x)。 统计出语料库中所有的N和Σ 利用语料库中的所有规则作为R 针对每个规则A -&gt; B，从语料库中估算p(x) = p(A -&gt; B) / p(A) ​ 在CFG的定义的基础上，重新定义一种叫Chomsky的语法格式。这种格式要求每条规则只能是X -&gt; Y1 Y2或者X -&gt; Y的格式。实际上Chomsky语法格式保证生产的语法树总是二叉树的格式，同时任意一棵语法树总是能够转化成Chomsky语法格式。 ​ 语法树预测算法： 输入一个句子x1, x2, ... , xn时，计算句子对应的语法树有两种方法： 第一种方法是暴力遍历的方法，每个单词x可能有m = len(N)种取值，句子长度是n，每种情况至少存在n个规则，所以在时间复杂度\\(O(mn^2)\\)的情况下，我们可以判断出所有可能的语法树并计算出最佳的那个。 第二种方法当然是动态规划，我们定义w[i, j, X]是第i个单词至第j个单词由标注X来表示的最大概率。w[i, j, PP]代表的是继续往上一层递归时，只选择当前概率最大的组合方式。 ​ 缺点： ​ PCFG也有一些缺点，例如：1）缺乏词法信息；2）连续短语（如名词、介词）的处理等。但总体来讲它给语法解析提供了一种非常有效的实现方法。 7.命名实体识别 ​ 从一段非结构化文本中找出相关实体（triplet中的主词和宾词），并标注出其位置以及类型，它是NLP领域中一些复杂任务（如关系抽取、信息检索、知识问答、知识图谱等）的基础。 关键词抽取 ​ 文本关键词抽取，是对文本信息进行高度凝练的一种有效手段，通过3-5个词语准确概括文本的主题，帮助读者快速理解文本信息。是文本检索、文本摘要等许多下游文本挖掘任务的基础性和必要性的工作。 jieba 基本分词函数与用法 ​ jieba.cut 以及 jieba.cut_for_search 返回的结构都是一个可迭代的 generator。 ​ jieba.lcut以及jieba.lcut_for_search直接返回 list 添加用户自定义字典 1.可以用jieba.load_userdict(file_name)加载用户字典 2.少量的词汇可以自己用下面方法手动添加： 用 add_word(word, freq=None, tag=None) 和 del_word(word) 在程序中动态修改词典用 suggest_freq(segment, tune=True) 可调节单个词语的词频，使其能（或不能）被分出来。 词性标注 123import jieba.posseg as pseg # .posseg模块words = pseg.cut() 关键词抽取 基于 TF-IDF 算法的关键词抽取 import jieba.analyse jieba.analyse.extract_tags(sentence, topK=20, withWeight=False, allowPOS=()) sentence 为待提取的文本 topK 为返回几个 TF/IDF 权重最大的关键词，默认值为 20 withWeight 为是否一并返回关键词权重值，默认值为 False allowPOS 仅包括指定词性的词，默认值为空，即不筛选 自定义逆向文件频率（IDF）文本语料库 用法： jieba.analyse.set_idf_path(file_name) # file_name为自定义语料库的路径 自定义语料库示例见这里 用法示例见这里 自定义停止词（Stop Words）文本语料库 用法： jieba.analyse.set_stop_words(file_name) # file_name为自定义语料库的路径 自定义语料库示例见这里 用法示例见这里 关键词一并返回关键词权重值示例 用法示例见这里 基于 TextRank 算法的关键词抽取 jieba.analyse.textrank(sentence, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')) 直接使用，接口相同，注意默认过滤词性。 jieba.analyse.TextRank() 新建自定义 TextRank 实例 算法论文： TextRank: Bringing Order into Texts 基本思想: 将待抽取关键词的文本进行分词 以固定窗口大小(默认为5，通过span属性调整)，词之间的共现关系，构建图 计算图中节点的PageRank，注意是无向带权图 详解见textrank 123import jieba.analyse as analyseanalyse.textrank(lines, topK=20, withWeight=False, allowPOS=('ns', 'n')) 示例 1234567891011import jieba.analyse as analyseimport pandas as pddf = pd.read_csv(\"./data/file.csv\", encoding='utf-8')df = df.dropna()lines=df.content.values.tolist()content = \"\".join(lines) # 一行输入print(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n', 'vn', 'v')))print(\"-------------------------------------\")print(analyse.textrank(content, topK=20, withWeight=False, allowPOS=('ns', 'n')))","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"chinese process","slug":"chinese-process","permalink":"https://racleray.github.io/tags/chinese-process/"},{"name":"basic","slug":"basic","permalink":"https://racleray.github.io/tags/basic/"}]},{"title":"英文文本处理","slug":"英文文本处理","date":"2020-07-06T13:12:08.000Z","updated":"2023-08-07T11:54:31.048Z","comments":true,"path":"posts/80c1e4ba.html","link":"","permalink":"https://racleray.github.io/posts/80c1e4ba.html","excerpt":"记录英文文本处理的常见概念，以及nltk与SpaCy的简单了解。","text":"英文文本处理 ​ 结合nltk包进行说明。 1 Tokenization(标记化/分词) ​ 文本是不能成段送入模型中进行分析的，我们通常会把文本切成有独立含义的字、词或者短语，这个过程叫做tokenization，这通常是大家解决自然语言处理问题的第一步。 ​ NLTK中提供了2种不同方式的tokenization： sentence tokenization 和 word tokenization，前者把文本进行“断句”，后者对文本进行“分词”。 ​ sentence tokenization相比于split的优势在于，是别不是句号的位置，如：Mr.H 1from nltk import word_tokenize, sent_tokenize 2 去停用词 ​ 在自然语言处理的一些任务中，我们处理的主体“文本”中有一些功能词经常出现，然而对于最后的任务目标并没有帮助，甚至会对结果产生干扰，我们把这类词叫做停用词。 123456nltk.download('stopwords')# 导入内置停用词from nltk.corpus import stopwordsstop_words = stopwords.words('english') 3 词性标注（part-of-speech tagging） ​ 词性（part-of-speech）是词汇基本的语法属性，通常也称为词性。 ​ 词性标注是很多NLP任务的预处理步骤，如句法分析，经过词性标注后的文本会带来很大的便利性，但也不是不可或缺的步骤。 ​ 主流的做法可以分为基于规则和基于统计的方法，包括： 基于最大熵的词性标注 基于统计最大概率输出词性 基于HMM的词性标注 123456nltk.download('averaged_perceptron_tagger')# 词性标注from nltk import pos_tagtags = pos_tag(filtered_corpus)tags[:20] 4 chunking/组块分析 ​ chunking分块是命名实体识别的基础，词性给出来的句子成分的属性，但有时候，更多的信息(比如句子句法结构)可以帮助我们对句子中的模式挖掘更充分。 123456789101112131415161718192021222324from nltk.chunk import RegexpParser # RegexpParserfrom nltk import sent_tokenize,word_tokenize# 写一个匹配名词的模式# JJ adjective; NN noun, singular or mass# CC coordinating conjunctionpattern = \"\"\" NP: &#123;&lt;JJ&gt;*&lt;NN&gt;+&#125; &#123;&lt;JJ&gt;*&lt;NN&gt;&lt;CC&gt;*&lt;NN&gt;+&#125; \"\"\" # 定义组块分析器chunker = RegexpParser(pattern)# 分句tokenized_sentence = nltk.sent_tokenize(text)# 分词tokenized_words = [nltk.word_tokenize(sentence) for sentence in tokenized_sentence]# 词性标注tagged_words = [nltk.pos_tag(word) for word in tokenized_words]# 识别组块word_tree = [chunker.parse(word) for word in tagged_words]word_tree[0].draw() # 绘图 5 命名实体识别（Named Entity Recognition，NER） ​ 命名实体识别包括两部分：1) 实体边界识别；2) 确定实体类别（人名、地名、机构名或其他）。 ​ stanford core nlp modules 速度更快，而且有更高的识别准确度。 1234from nltk import ne_chunk, pos_tag, word_tokenizesentence = \"John studies at Stanford University.\"print(ne_chunk(pos_tag(word_tokenize(sentence)))) 6 Stemming和Lemmatizing 词干算法 和 词型还原 ​ 对英文当中的时态语态等做归一化。 ​ Stemmer 12345678910111213141516171819from nltk.stem import PorterStemmerstemmer = PorterStemmer()stemmer.stem(\"running\")# Out: 'run'from nltk.stem import SnowballStemmerstemmer2 = SnowballStemmer(\"english\")stemmer2.stem(\"growing\")# Out: 'grow'# Create your own stemmer using Regular Expressionfrom nltk.stem import RegexpStemmerrst = RegexpStemmer(r'ing$|s$|e$|able$')rst.stem('controllable')# Out: 'controll' ​ Lemmatization和Stemmer很类似，不同的地方在于Lemmatization会根据语言词根找到原型，而Stemmer只是根据规则截断结尾的 -ing 等后缀。Stemmer的速度更快，但是它通常只是一系列的规则。 12345678910111213from nltk.stem import WordNetLemmatizerlemmatizer = WordNetLemmatizer()lemmatizer.lemmatize(\"makes\")# If you do not provide POS tag of the word, # lemmatizer will consider word as a noun and you may not get the result you expected lemmatizer.lemmatize('spoken')# Out: 'spoken'# 给出词性lemmatizer.lemmatize('spoken','v')# Out: 'speak' NLTK：WordNet ​ NLTK，全称Natural Language Toolkit，自然语言处理工具包，是NLP研究领域常用的一个Python库，由宾夕法尼亚大学的Steven Bird和Edward Loper在Python的基础上开发的一个模块，至今已有超过十万行的代码。这是一个开源项目，包含数据集、Python模块、教程等；NLTK是最常用的英文自然语言处理python基础库之一。 Corpus Module Corpus WordNet与词义解析 1234567891011121314151617181920from nltk.corpus import wordnet as wn# 同义wn.synsets('man')# 第一种词义wn.synsets('man')[0].definition()# 第二种词义wn.synsets('man')[1].definition()# 造句：选择一种词义（dog.n.01）dog = wn.synset('dog.n.01')dog.examples()[0]# 上位词dog.hypernyms()# 查看不同词义下的Lemmatization解析for syn in wn.synsets('spoken'): print(syn,':', syn.lemma_names()) Frequency Distribution 12345678910111213141516171819202122232425262728293031from nltk import ConditionalFreqDist, FreqDist...fd = FreqDist(l)fd.most_common(2)# Return a list of all samples that occur oncefd.hapaxes()# Find the word occuring max number of timesfd_w_humor.max()# Freq = Number of occurences / total number of wordsfd_w_humor.freq('the')# check how many times word 'pen' appearedfd_w_humor.get('pen')...# Conditional Frequency Distribution# Use tabulate mathod to check distribution of modal words in different genre cfd = ConditionalFreqDist( (genre, word) for genre in brown.categories() for word in brown.words(categories=genre))cfd.tabulate(conditions=genres, samples=modals)# 绘制分布图l_names = ([('male',name[-1]) for name in names.words('male.txt')] + [('female',name[-1]) for name in names.words('female.txt')]) cfd_names = ConditionalFreqDist(l_names)cfd_names.plot() SpaCy ​ spaCy是Python和Cython中的高级自然语言处理库，它建立在最新的研究基础之上，从一开始就设计用于实际产品。spaCy 带有预先训练的统计模型和单词向量，目前支持 20 多种语言的标记。它具有快速的句法分析器，用于标签的卷积神经网络模型，解析和命名实体识别以及与深度学习整合。 ​ !pip install -i https://pypi.tuna.tsinghua.edu.cn/simple spaCy ​ !python -m spacy download en ​ 支持的语言 ​ 123456789101112131415161718192021222324252627282930313233# Tokenizationimport spacynlp = spacy.load('en')doc = nlp('Hello World!')for token in doc: print('\"' + token.text + '\"') for token in doc: print(\"&#123;0&#125;\\t&#123;1&#125;\\t&#123;2&#125;\\t&#123;3&#125;\\t&#123;4&#125;\\t&#123;5&#125;\\t&#123;6&#125;\\t&#123;7&#125;\".format( token.text, token.idx, # 开始index token.lemma_, # 原型 token.is_punct,# 判断标点 token.is_space,# 判断空格 token.shape_, # 词格式 token.pos_, # 词性 token.tag_ # 标注 )) # 断句for sent in doc.sents: print(sent) # 词性标注print([(token.text, token.tag_) for token in doc])# NERfor ent in doc.ents: print(ent.text, ent.label_) ​ BIO/IOB tagging 是一种对给定句子中的单元做序列标注的方式，用于从给定句子中抽取连续字/词块构成的有意义短语。 ​ 每个词标注为B（Beginning，指示某短语起始）、I（Inside，指示短语内部）、O（Outside，指示不在短语中）中的一个。如：B-人名 O B-机构名 I-机构名 1234567891011iob_tagged = [ (token.text, token.tag_, \"&#123;0&#125;-&#123;1&#125;\".format(token.ent_iob_, token.ent_type_) if token.ent_iob_ != 'O' else token.ent_iob_) for token in doc]ent_iob = [(token.ent_iob_, token.ent_type_) for token in doc]from nltk.chunk import conlltags2tree# 按照nltk.Tree的格式显示print(conlltags2tree(iob_tagged)) ​ 可视化 12345from spacy import displacydisplacy.render(doc, style='ent', jupyter=True)displacy.render(doc, style='dep', jupyter=True) image chunking/组块分析 12for chunk in doc.noun_chunks: print(chunk.text,'---', chunk.label_,'---', chunk.root.text) 句法依存 12345for token in doc: print(\"&#123;0&#125;/&#123;1&#125; &lt;--&#123;2&#125;-- &#123;3&#125;/&#123;4&#125;\".format( token.text, token.tag_, token.dep_, token.head.text, token.head.tag_)) #head displacy.render(doc, style='dep', jupyter=True, options=&#123;'distance': 90&#125;) 词向量 1234567891011121314151617# 如果要使用英文的词向量，需要先下载预先训练好的结果# !python -m spacy download en_core_web_lgnlp = spacy.load('en_core_web_lg')print(nlp.vocab['banana'].vector)# 在词向量的基础上，spaCy提供了从词到文档的相似度计算的方法target = nlp(\"Cats are beautiful animals.\") doc1 = nlp(\"Dogs are awesome.\")doc2 = nlp(\"Some gorgeous creatures are felines.\")doc3 = nlp(\"Dolphins are swimming mammals.\") print(target.similarity(doc1)) # 0.8901765218466683print(target.similarity(doc2)) # 0.9115828449161616print(target.similarity(doc3)) # 0.7822956752876101","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"english process","slug":"english-process","permalink":"https://racleray.github.io/tags/english-process/"},{"name":"nltk","slug":"nltk","permalink":"https://racleray.github.io/tags/nltk/"}]},{"title":"常用python文本处理函数","slug":"常用python文本处理函数","date":"2020-07-06T12:27:20.000Z","updated":"2023-08-07T11:54:31.041Z","comments":true,"path":"posts/9ad55c64.html","link":"","permalink":"https://racleray.github.io/posts/9ad55c64.html","excerpt":"记录python基本文本处理函数，以及正则表达式模块使用。","text":"python函数 1234567891011121314151617# 去空格及特殊符号 en_str.strip().lstrip().rstrip(',') # 字符串替换en_str.replace('hello', 'hi')# 删除zh_str.strip().replace('大家好，', '')# 通过join的方式连接\"，\".join(strs)或者str1+str2# 通过split的方式切分tmp_str.split(\"；\") 12345678910# 以字母序排列，注意是以返回值形态返回排序结果，不改变原listsorted(en_strs)sorted(en_strs, key=lambda x:x[2].lower())# 查找可以用index和findzh_str.index(\"陆超\") # 第一个indexzh_str.find(\"来了老弟\") # 返回-1，不会报错 12345678# 大小写与其他变化en_str.title()a = en_str.translate(en_str)en_str.lower().upper()en_str.capitalize() # capitalize 函数帮助（help(func)） 正则表达式 正则表达式是处理字符串的强大工具，拥有独特的语法和独立的处理引擎。 http://regexr.com/ https://alf.nu/RegexGolf 练习地址 re模块 12345678910111213# encoding: UTF-8import re # 将正则表达式编译成Pattern对象pattern = re.compile(r'hello.*\\!') # 使用Pattern匹配文本，获得匹配结果，无法匹配时将返回Nonematch = pattern.match('hello, hanxiaoyang! How are you?')# match从头开始匹配 if match: # 使用Match获得分组信息 print match.group() ​ re.compile('pattern', re.I | re.M) ：re.I(re.IGNORECASE): 忽略大小写（括号内是完整写法，下同） re.M(MULTILINE): 多行模式，改变'^'和'$'的行为。 ​ Match对象是一次匹配的结果，包含了很多关于此次匹配的信息，可以使用Match提供的可读属性或方法来获取这些信息 1re.match ​ Pattern对象是一个编译好的正则表达式，通过Pattern提供的一系列方法可以对文本进行匹配查找。 ​ Pattern不能直接实例化，必须使用re.compile()进行构造。 12345678# Pattern提供了几个可读属性用于获取表达式的相关信息：# pattern: 编译时用的表达式字符串。# flags: 编译时用的匹配模式。数字形式。# groups: 表达式中分组的数量。# groupindex: 以表达式中有别名的组的别名为键、以该组对应的编号为值的字典，没有别名的组不包含在内。# re.S(DOTALL): 点任意匹配模式re.compile(r'(\\w+) (\\w+)(?P&lt;sign&gt;.*)', re.DOTALL) 使用pattern match(string[, pos[, endpos]]) | re.match(pattern, string[, flags]): 这个方法将从string的pos下标处起尝试匹配pattern 如果pattern结束时仍可匹配，则返回一个Match对象 如果匹配过程中pattern无法匹配，或者匹配未结束就已到达endpos，则返回None。 pos和endpos的默认值分别为0和len(string)。 注意：这个方法并不是完全匹配。当pattern结束时若string还有剩余字符，仍然视为成功。想要完全匹配，可以在表达式末尾加上边界匹配符'$'。 search(string[, pos[, endpos]]) | re.search(pattern, string[, flags]): 这个方法从string的pos下标处起尝试匹配pattern sub(repl, string[, count]) | re.sub(pattern, repl, string[, count]): 使用repl替换string中每一个匹配的子串后返回替换后的字符串。 当repl是一个字符串时，可以使用、，但不能使用编号0。 当repl是一个方法时，这个方法应当只接受一个参数（Match对象），并返回一个字符串用于替换（返回的字符串中不能再引用分组）。 count用于指定最多替换次数，不指定时全部替换。 split(string[, maxsplit]) | re.split(pattern, string[, maxsplit]): 按照能够匹配的子串将string分割后返回列表。maxsplit 用于指定最大分割次数，不指定将全部分割。 12p = re.compile(r'\\d+')print(p.split('one1two2three3four4')) findall(string[, pos[, endpos]]) | re.findall(pattern, string[, flags]): 搜索string，以列表形式返回全部能匹配的子串。 12p = re.compile(r'\\d+')print(p.findall('one1two2three3four4') finditer(string[, pos[, endpos]]) | re.finditer(pattern, string[, flags]): 搜索string，返回一个顺序访问每一个匹配结果（Match对象）的迭代器。 123p = re.compile(r'\\d+')for m in p.finditer('one1two2three3four4'): print(m.group()) subn(repl, string[, count]) |re.subn(pattern, repl, string[, count]): 返回 (sub(repl, string[, count]), 替换次数)。 123456789p = re.compile(r'(\\w+) (\\w+)')s = 'i say, hello world!' print(p.subn(r'\\2 \\1', s)) def func(m): return m.group(1).title() + ' ' + m.group(2).title() print(p.subn(func, s)) 词条和解释抽取 12345678910111213141516171819202122# 引入爬虫工具库import requests as rqimport re# 发送请求page = rq.get(\"https://baike.sogou.com/v231013.htm\")# 返回状态码正常page.status_code# 词条正则表达式抽取title_pattern = re.compile(r'&lt;h1 id=\"title\".*?&gt;(.*?)&lt;/h1&gt;') title = title_pattern.search(page.text) print(title.group(1))# 词条正则表达式抽取content_pattern = re.compile(r'&lt;p&gt;(.*?)&lt;\\\\/p&gt;') contents = content_pattern.findall(page.text) print(contents)list(map(lambda x:re.sub(\"&lt;a .*?&gt;|&lt;\\\\\\/[ab]&gt;\", \"\",x), contents))","categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"}],"tags":[{"name":"python","slug":"python","permalink":"https://racleray.github.io/tags/python/"},{"name":"regex","slug":"regex","permalink":"https://racleray.github.io/tags/regex/"}]},{"title":"hello","slug":"hello","date":"2020-04-15T16:41:05.000Z","updated":"2023-08-07T11:54:31.036Z","comments":true,"path":"posts/3610a686.html","link":"","permalink":"https://racleray.github.io/posts/3610a686.html","excerpt":"第一篇Blog，测试用。","text":"便签 可选便签： 12345678910111213primarysecondarysuccessdangerwarninginfolight Hello World. Hello World. 行内标签 勾选框 普通示例 默认选中 内联示例 后面文字不换行 也可以只传入一个参数，文字写在后边（这样不支持外联） 按钮 url：跳转链接 text：显示的文字 title：鼠标悬停时显示的文字（可选） text text 组图 total：图片总数量，对应中间包含的图片 url 数量 n1-n2-...：每行的图片数量，可以省略，默认单行最多 3 张图，求和必须相等于 total，否则按默认样式 Mermaid 流程图 mermaid: true 才会在文章页启动流程图渲染","categories":[],"tags":[]}],"categories":[{"name":"Notes","slug":"Notes","permalink":"https://racleray.github.io/categories/Notes/"},{"name":"C++","slug":"Notes/C","permalink":"https://racleray.github.io/categories/Notes/C/"},{"name":"DL","slug":"Notes/DL","permalink":"https://racleray.github.io/categories/Notes/DL/"},{"name":"NLP","slug":"Notes/NLP","permalink":"https://racleray.github.io/categories/Notes/NLP/"},{"name":"Computer Network","slug":"Notes/Computer-Network","permalink":"https://racleray.github.io/categories/Notes/Computer-Network/"},{"name":"Tools","slug":"Tools","permalink":"https://racleray.github.io/categories/Tools/"},{"name":"C","slug":"Tools/C","permalink":"https://racleray.github.io/categories/Tools/C/"},{"name":"Design Patterns","slug":"Notes/Design-Patterns","permalink":"https://racleray.github.io/categories/Notes/Design-Patterns/"},{"name":"Data Structure","slug":"Notes/Data-Structure","permalink":"https://racleray.github.io/categories/Notes/Data-Structure/"},{"name":"Math","slug":"Math","permalink":"https://racleray.github.io/categories/Math/"},{"name":"basic","slug":"Math/basic","permalink":"https://racleray.github.io/categories/Math/basic/"},{"name":"Distillation","slug":"Notes/Distillation","permalink":"https://racleray.github.io/categories/Notes/Distillation/"},{"name":"ML","slug":"Notes/ML","permalink":"https://racleray.github.io/categories/Notes/ML/"},{"name":"Data Augmentation","slug":"Notes/Data-Augmentation","permalink":"https://racleray.github.io/categories/Notes/Data-Augmentation/"},{"name":"Contrastive Learning","slug":"Notes/Contrastive-Learning","permalink":"https://racleray.github.io/categories/Notes/Contrastive-Learning/"},{"name":"python","slug":"Tools/python","permalink":"https://racleray.github.io/categories/Tools/python/"},{"name":"Algorithm","slug":"Notes/Algorithm","permalink":"https://racleray.github.io/categories/Notes/Algorithm/"},{"name":"CS","slug":"Notes/CS","permalink":"https://racleray.github.io/categories/Notes/CS/"},{"name":"spark","slug":"Tools/spark","permalink":"https://racleray.github.io/categories/Tools/spark/"},{"name":"git","slug":"Tools/git","permalink":"https://racleray.github.io/categories/Tools/git/"},{"name":"Docker","slug":"Tools/Docker","permalink":"https://racleray.github.io/categories/Tools/Docker/"},{"name":"Linux","slug":"Tools/Linux","permalink":"https://racleray.github.io/categories/Tools/Linux/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"Book","slug":"Book","permalink":"https://racleray.github.io/tags/Book/"},{"name":"optimizer","slug":"optimizer","permalink":"https://racleray.github.io/tags/optimizer/"},{"name":"lookahead","slug":"lookahead","permalink":"https://racleray.github.io/tags/lookahead/"},{"name":"nlp","slug":"nlp","permalink":"https://racleray.github.io/tags/nlp/"},{"name":"prompt","slug":"prompt","permalink":"https://racleray.github.io/tags/prompt/"},{"name":"adaptor","slug":"adaptor","permalink":"https://racleray.github.io/tags/adaptor/"},{"name":"epoll","slug":"epoll","permalink":"https://racleray.github.io/tags/epoll/"},{"name":"C","slug":"C","permalink":"https://racleray.github.io/tags/C/"},{"name":"测试框架","slug":"测试框架","permalink":"https://racleray.github.io/tags/%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6/"},{"name":"deep learning","slug":"deep-learning","permalink":"https://racleray.github.io/tags/deep-learning/"},{"name":"contrastive learning","slug":"contrastive-learning","permalink":"https://racleray.github.io/tags/contrastive-learning/"},{"name":"cs basic","slug":"cs-basic","permalink":"https://racleray.github.io/tags/cs-basic/"},{"name":"design patterns","slug":"design-patterns","permalink":"https://racleray.github.io/tags/design-patterns/"},{"name":"data structure","slug":"data-structure","permalink":"https://racleray.github.io/tags/data-structure/"},{"name":"math","slug":"math","permalink":"https://racleray.github.io/tags/math/"},{"name":"searching","slug":"searching","permalink":"https://racleray.github.io/tags/searching/"},{"name":"BERT","slug":"BERT","permalink":"https://racleray.github.io/tags/BERT/"},{"name":"topic model","slug":"topic-model","permalink":"https://racleray.github.io/tags/topic-model/"},{"name":"BP","slug":"BP","permalink":"https://racleray.github.io/tags/BP/"},{"name":"sentence embedding","slug":"sentence-embedding","permalink":"https://racleray.github.io/tags/sentence-embedding/"},{"name":"SimCSE","slug":"SimCSE","permalink":"https://racleray.github.io/tags/SimCSE/"},{"name":"initialization","slug":"initialization","permalink":"https://racleray.github.io/tags/initialization/"},{"name":"knowledge distillation","slug":"knowledge-distillation","permalink":"https://racleray.github.io/tags/knowledge-distillation/"},{"name":"CRD","slug":"CRD","permalink":"https://racleray.github.io/tags/CRD/"},{"name":"SRRD","slug":"SRRD","permalink":"https://racleray.github.io/tags/SRRD/"},{"name":"NCE","slug":"NCE","permalink":"https://racleray.github.io/tags/NCE/"},{"name":"language model","slug":"language-model","permalink":"https://racleray.github.io/tags/language-model/"},{"name":"data augmentation","slug":"data-augmentation","permalink":"https://racleray.github.io/tags/data-augmentation/"},{"name":"self-supervise","slug":"self-supervise","permalink":"https://racleray.github.io/tags/self-supervise/"},{"name":"relation extraction","slug":"relation-extraction","permalink":"https://racleray.github.io/tags/relation-extraction/"},{"name":"tools","slug":"tools","permalink":"https://racleray.github.io/tags/tools/"},{"name":"jupyter","slug":"jupyter","permalink":"https://racleray.github.io/tags/jupyter/"},{"name":"pointer net","slug":"pointer-net","permalink":"https://racleray.github.io/tags/pointer-net/"},{"name":"NER","slug":"NER","permalink":"https://racleray.github.io/tags/NER/"},{"name":"FLAT","slug":"FLAT","permalink":"https://racleray.github.io/tags/FLAT/"},{"name":"methodology","slug":"methodology","permalink":"https://racleray.github.io/tags/methodology/"},{"name":"algorithm","slug":"algorithm","permalink":"https://racleray.github.io/tags/algorithm/"},{"name":"kmp","slug":"kmp","permalink":"https://racleray.github.io/tags/kmp/"},{"name":"trie","slug":"trie","permalink":"https://racleray.github.io/tags/trie/"},{"name":"Aho-Corasick","slug":"Aho-Corasick","permalink":"https://racleray.github.io/tags/Aho-Corasick/"},{"name":"ray","slug":"ray","permalink":"https://racleray.github.io/tags/ray/"},{"name":"distributed","slug":"distributed","permalink":"https://racleray.github.io/tags/distributed/"},{"name":"asyc","slug":"asyc","permalink":"https://racleray.github.io/tags/asyc/"},{"name":"python","slug":"python","permalink":"https://racleray.github.io/tags/python/"},{"name":"tool","slug":"tool","permalink":"https://racleray.github.io/tags/tool/"},{"name":"TransE","slug":"TransE","permalink":"https://racleray.github.io/tags/TransE/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","permalink":"https://racleray.github.io/tags/Knowledge-Graph/"},{"name":"sampling","slug":"sampling","permalink":"https://racleray.github.io/tags/sampling/"},{"name":"machine learning","slug":"machine-learning","permalink":"https://racleray.github.io/tags/machine-learning/"},{"name":"CRF","slug":"CRF","permalink":"https://racleray.github.io/tags/CRF/"},{"name":"spark","slug":"spark","permalink":"https://racleray.github.io/tags/spark/"},{"name":"normalization","slug":"normalization","permalink":"https://racleray.github.io/tags/normalization/"},{"name":"gan","slug":"gan","permalink":"https://racleray.github.io/tags/gan/"},{"name":"c++","slug":"c","permalink":"https://racleray.github.io/tags/c/"},{"name":"library","slug":"library","permalink":"https://racleray.github.io/tags/library/"},{"name":"capsule","slug":"capsule","permalink":"https://racleray.github.io/tags/capsule/"},{"name":"git","slug":"git","permalink":"https://racleray.github.io/tags/git/"},{"name":"text similarity","slug":"text-similarity","permalink":"https://racleray.github.io/tags/text-similarity/"},{"name":"DSSM","slug":"DSSM","permalink":"https://racleray.github.io/tags/DSSM/"},{"name":"CDSSM","slug":"CDSSM","permalink":"https://racleray.github.io/tags/CDSSM/"},{"name":"word2vec","slug":"word2vec","permalink":"https://racleray.github.io/tags/word2vec/"},{"name":"doc2vec","slug":"doc2vec","permalink":"https://racleray.github.io/tags/doc2vec/"},{"name":"SimHash","slug":"SimHash","permalink":"https://racleray.github.io/tags/SimHash/"},{"name":"visual text","slug":"visual-text","permalink":"https://racleray.github.io/tags/visual-text/"},{"name":"rasa","slug":"rasa","permalink":"https://racleray.github.io/tags/rasa/"},{"name":"retireval","slug":"retireval","permalink":"https://racleray.github.io/tags/retireval/"},{"name":"transformer","slug":"transformer","permalink":"https://racleray.github.io/tags/transformer/"},{"name":"cnn","slug":"cnn","permalink":"https://racleray.github.io/tags/cnn/"},{"name":"Fairseq","slug":"Fairseq","permalink":"https://racleray.github.io/tags/Fairseq/"},{"name":"seq2seq","slug":"seq2seq","permalink":"https://racleray.github.io/tags/seq2seq/"},{"name":"bleu","slug":"bleu","permalink":"https://racleray.github.io/tags/bleu/"},{"name":"rouge","slug":"rouge","permalink":"https://racleray.github.io/tags/rouge/"},{"name":"evaluation","slug":"evaluation","permalink":"https://racleray.github.io/tags/evaluation/"},{"name":"attention","slug":"attention","permalink":"https://racleray.github.io/tags/attention/"},{"name":"lda","slug":"lda","permalink":"https://racleray.github.io/tags/lda/"},{"name":"classification","slug":"classification","permalink":"https://racleray.github.io/tags/classification/"},{"name":"fine-grained","slug":"fine-grained","permalink":"https://racleray.github.io/tags/fine-grained/"},{"name":"fastText","slug":"fastText","permalink":"https://racleray.github.io/tags/fastText/"},{"name":"CNN","slug":"CNN","permalink":"https://racleray.github.io/tags/CNN/"},{"name":"RNN","slug":"RNN","permalink":"https://racleray.github.io/tags/RNN/"},{"name":"representation","slug":"representation","permalink":"https://racleray.github.io/tags/representation/"},{"name":"pretrained LM","slug":"pretrained-LM","permalink":"https://racleray.github.io/tags/pretrained-LM/"},{"name":"XLNet","slug":"XLNet","permalink":"https://racleray.github.io/tags/XLNet/"},{"name":"smoothing","slug":"smoothing","permalink":"https://racleray.github.io/tags/smoothing/"},{"name":"KenLM","slug":"KenLM","permalink":"https://racleray.github.io/tags/KenLM/"},{"name":"Linux","slug":"Linux","permalink":"https://racleray.github.io/tags/Linux/"},{"name":"Docker","slug":"Docker","permalink":"https://racleray.github.io/tags/Docker/"},{"name":"NVIDIA Container Toolkit","slug":"NVIDIA-Container-Toolkit","permalink":"https://racleray.github.io/tags/NVIDIA-Container-Toolkit/"},{"name":"chinese process","slug":"chinese-process","permalink":"https://racleray.github.io/tags/chinese-process/"},{"name":"basic","slug":"basic","permalink":"https://racleray.github.io/tags/basic/"},{"name":"english process","slug":"english-process","permalink":"https://racleray.github.io/tags/english-process/"},{"name":"nltk","slug":"nltk","permalink":"https://racleray.github.io/tags/nltk/"},{"name":"regex","slug":"regex","permalink":"https://racleray.github.io/tags/regex/"}]}